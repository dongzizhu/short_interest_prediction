ðŸš€ Modular Iterative Agent-Based Feature Selection Examples
======================================================================
âœ… API key loaded successfully

ðŸ” Example: Multi-Ticker Processing with SVM
==================================================
ðŸ¤– Using SVM model with kernel: rbf
   C: 1.0, Gamma: scale
   Epsilon: 0.1, Max iterations: 1000
Using device: cpu
âœ… Claude API client initialized successfully!
ðŸš€ Starting Multi-Ticker Iterative Agent-Based Feature Selection Process
================================================================================
Processing iterative tickers: TRUP, ABCB, EIG, EYE, AAP, FSS, ABM, IART, SRPT, EXTR, SCSC, SLG, HL, ANDE, AROC
Available for validation: AAP, AAT, ABCB, ABG, ABM, ABR, ACAD, ACHC, ACIW, ACLS, ADMA, ADNT, ADUS, AEIS, AEO, AGO, AGYS, AHH, AIN, AIR, AKR, AL, ALEX, ALG, ALGT, ALKS, ALRM, AMN, AMPH, AMSF, AMWD, ANDE, ANGI, ANIP, AOSL, APAM, APLE, APOG, ARCB, ARI, AROC, ARR, ARWR, ASIX, ASTE, ATEN, ATGE, AVA, AWI, AWR, AXL, AZZ, BANC, BANF, BANR, BCC, BCPC, BDN, BFS, BHE, BJRI, BKE, BKU, BL, BLFS, BLMN, BMI, BOH, BOOT, BOX, BRC, BTU, BWA, BXMT, CABO, CAKE, CAL, CALM, CALX, CARG, CARS, CASH, CATY, CBRL, CBU, CC, CCOI, CCS, CE, CENT, CENTA, CENX, CEVA, CFFN, CHCO, CHEF, CLB, CNK, CNMD, CNS, CNXN, COHU, COLL, CORT, CPF, CPK, CPRX, CRI, CRK, CRVL, CSGS, CTRE, CTS, CUBI, CVBF, CVCO, CVI, CWT, CXW, CZR, DAN, DCOM, DEA, DEI, DFIN, DGII, DIOD, DLX, DNOW, DORM, DRH, DVAX, DXC, DXPE, DY, EAT, ECPG, EFC, EGBN, EIG, ENPH, ENR, ENVA, EPC, ESE, ETSY, EVTC, EXPI, EXTR, EYE, EZPW, FBK, FBNC, FBP, FCF, FCPT, FDP, FELE, FFBC, FHB, FIZZ, FMC, FORM, FOXF, FRPT, FSS, FUL, FULT, FUN, FWRD, GBX, GDEN, GEO, GES, GFF, GIII, GKOS, GNL, GNW, GOGO, GOLF, GPI, GRBK, GTY, GVA, HAFC, HASI, HBI, HCC, HCI, HCSG, HELE, HFWA, HI, HIW, HL, HLIT, HLX, HMN, HNI, HOPE, HP, HSII, HSTM, HTH, HTLD, HUBG, HWKN, HZO, IAC, IART, IBP, ICHR, ICUI, IDCC, IIIN, IIPR, INDB, INN, INSW, INVA, IOSP, IPAR, ITGR, ITRI, JBGS, JBLU, JBSS, JJSF, JOE, KAI, KALU, KAR, KFY, KLIC, KMT, KN, KOP, KREF, KRYS, KSS, KW, KWR, LCII, LEG, LGIH, LGND, LKFN, LMAT, LNC, LNN, LPG, LQDT, LRN, LTC, LXP, LZB, MAC, MAN, MARA, MATW, MATX, MC, MCRI, MCY, MD, MDU, MGEE, MGPI, MHO, MKTX, MMI, MMSI, MNRO, MPW, MRCY, MRTN, MSEX, MTH, MTRN, MTX, MWA, MXL, MYGN, MYRG, NAVI, NBHC, NBTB, NEO, NEOG, NGVT, NHC, NMIH, NOG, NPK, NPO, NSIT, NTCT, NWBI, NWL, NWN, NX, NXRT, OFG, OI, OII, OMCL, OSIS, OTTR, OUT, OXM, PAHC, PARR, PATK, PBH, PBI, PCRX, PDFS, PEB, PENN, PFBC, PFS, PI, PINC, PJT, PLAB, PLAY, PLUS, PLXS, PMT, POWL, PRA, PRAA, PRGS, PRK, PRLB, PSMT, PTEN, PTGX, PZZA, QDEL, QNST, QRVO, QTWO, RDN, RDNT, RES, REX, RGR, RHI, RHP, RNST, ROCK, ROG, RUN, RUSHA, RWT, SAFE, SABR, SAFT, SAH, SANM, SBCF, SBH, SBSI, SCHL, SCL, SCSC, SCVL, SEDG, SEE, SEM, SFBS, SFNC, SHAK, SHEN, SHO, SHOO, SIG, SKT, SKY, SKYW, SLG, SM, SMP, SMPL, SMTC, SNDR, SPSC, SPXC, SRPT, SSTK, STAA, STBA, STC, STRA, STRL, SUPN, SXC, SXI, SXT, TBBK, TDC, TDS, TDW, TFX, TGNA, TGTX, THRM, THS, TILE, TMP, TNC, TNDM, TPH, TR, TRIP, TRMK, TRN, TRNO, TRST, TRUP, TTMI, TWI, TWO, UCTT, UE, UFCS, UFPT, UHT, UNF, UNFI, UNIT, URBN, USNA, USPH, UTL, UVV, VBTX, VCEL, VCYT, VECO, VIAV, VICR, VIRT, VRTS, VSAT, VSH, WABC, WAFD, WD, WDFC, WEN, WERN, WGO, WOR, WRLD, WSC, WSFS, WSR, WWW, XHR, XNCR, YELP
================================================================================

================================================================================
PROCESSING TICKER 1/15: TRUP
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for TRUP
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for TRUP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error processing TRUP: 'TRUP'
âš ï¸ Skipping TRUP and continuing with next ticker...

================================================================================
PROCESSING TICKER 2/15: ABCB
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for ABCB
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for ABCB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABCB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 172438.0075
RMSE: 222264.6295
MAPE: 9.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 107
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0005, rank=2
   3. Feature_1_t3: importance=0.0003, rank=3
   4. Feature_65_t2: importance=0.0003, rank=4
   5. Feature_65_t1: importance=0.0003, rank=5

ðŸ“Š Baseline Performance: MAPE = 9.91%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    lookback_window = data.shape[0]
    
    # Initialize the output array
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (critical baseline features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep high-importance features from previous iteration
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Higher values indicate higher short selling pressure relative to available shares
        shares_outstanding = max(abs(data[t, 67]), 1e-8)
        short_interest_ratio = data[t, 0] / shares_outstanding
        eng.append(short_interest_ratio)
        
        # 2. Relative Short Interest Change (if we have previous data)
        # Captures momentum in short interest changes
        if t > 0:
            prev_si = max(abs(data[t-1, 0]), 1e-8)
            si_change = (data[t, 0] - data[t-1, 0]) / prev_si
            eng.append(si_change)
        else:
            eng.append(0.0)
        
        # 3. Short Volume Ratio (Volume / Avg Volume)
        # Indicates if current trading volume is abnormal compared to average
        avg_volume = max(abs(data[t, 1]), 1e-8)
        volume_ratio = data[t, 68] / avg_volume
        eng.append(volume_ratio)
        
        # 4. Price Momentum (5-day)
        # Short-term price trend that may influence short selling decisions
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            eng.append(price_momentum)
        else:
            eng.append(0.0)
        
        # 5. Price Volatility (15-day)
        # Higher volatility often correlates with short selling activity
        if len(close_prices) >= 2:
            returns = np.diff(close_prices) / close_prices[:-1]
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility, often higher during short squeezes
        atr_sum = 0.0
        if len(close_prices) >= 2:
            for i in range(1, min(15, len(close_prices))):
                true_range = max(
                    high_prices[i] - low_prices[i],
                    abs(high_prices[i] - close_prices[i-1]),
                    abs(low_prices[i] - close_prices[i-1])
                )
                atr_sum += true_range
            atr = atr_sum / max(min(14, len(close_prices)-1), 1)
            eng.append(atr)
        else:
            eng.append(0.0)
        
        # 7. Options Put/Call Ratio Trend
        # Trend in options sentiment can predict short interest changes
        if t > 0:
            put_call_change = data[t, 64] - data[t-1, 64]
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # 8. Synthetic Short Cost Trend
        # Rising costs may precede short covering
        if t > 0:
            short_cost_change = data[t, 65] - data[t-1, 65]
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # 9. Implied Volatility vs Realized Volatility Spread
        # Divergence can signal potential short squeeze
        implied_vol = data[t, 66]
        realized_vol = volatility if 'volatility' in locals() else 0.0
        vol_spread = implied_vol - realized_vol
        eng.append(vol_spread)
        
        # 10. RSI (Relative Strength Index)
        # Extreme values can indicate potential reversals relevant to shorts
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain[-14:])
            avg_loss = np.mean(loss[-14:])
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(0.0)
        
        # 11. Short Interest to Volume Ratio
        # Indicates how many days of current volume would be needed to cover shorts
        volume = max(abs(data[t, 68]), 1e-8)
        si_volume_ratio = data[t, 0] / volume
        eng.append(si_volume_ratio)
        
        # 12. Price to Moving Average Ratio (10-day)
        # Indicates potential mean reversion opportunities
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma = close_prices[-1] / max(abs(ma10), 1e-8)
            eng.append(price_to_ma)
        else:
            eng.append(0.0)
        
        # 13. Bollinger Band Position
        # Position within volatility bands can signal potential reversals
        if len(close_prices) >= 20:
            ma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - ma20) / max(abs(std20), 1e-8)
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # 14. Short Interest Acceleration
        # Second derivative of short interest can signal changing trends
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            first_diff1 = si_t - si_t1
            first_diff2 = si_t1 - si_t2
            acceleration = first_diff1 - first_diff2
            eng.append(acceleration)
        else:
            eng.append(0.0)
        
        # 15. Options Implied Volatility Trend
        # Changes in implied volatility can precede short interest changes
        if t > 0:
            iv_change = data[t, 66] - data[t-1, 66]
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # 16. Short Interest to Float Ratio
        # Indicates how much of the tradable float is sold short
        shares_out = max(abs(data[t, 67]), 1e-8)
        si_float_ratio = data[t, 0] / shares_out
        eng.append(si_float_ratio)
        
        # 17. VWAP Deviation
        # Price deviation from volume-weighted average price
        if len(close_prices) >= 5 and 'volume' in locals():
            volumes = np.ones(len(close_prices))  # Assuming equal volume if not available
            vwap = np.sum(close_prices[-5:] * volumes[-5:]) / max(np.sum(volumes[-5:]), 1e-8)
            vwap_dev = (close_prices[-1] / max(abs(vwap), 1e-8)) - 1.0
            eng.append(vwap_dev)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep essential raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_volume)      # Average daily volume
        raw_keep.append(days_to_cover)   # Days to cover
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Keep options data (high importance in baseline)
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0
        
        raw_keep.append(put_call_ratio)
        raw_keep.append(synthetic_short_cost)
        raw_keep.append(implied_volatility)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
            
        # Feature 2: Price momentum (10-day)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
            
        # Feature 3: Volatility (standard deviation of returns)
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-6:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        else:
            eng.append(0.0)
            
        # Feature 4: Average True Range (ATR) - volatility indicator
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(high_low, high_close, low_close))
            atr = np.mean(true_ranges) if true_ranges else 0
            eng.append(atr)
        else:
            eng.append(0.0)
            
        # Feature 5: Short interest to volume ratio
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 6: Short interest to shares outstanding ratio
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 7: RSI (Relative Strength Index)
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 14
            avg_loss = loss / 14
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
            
        # Feature 8: Price to moving average ratio (5-day)
        if len(close_prices) >= 5:
            ma5 = np.mean(close_prices[-5:])
            price_to_ma5 = close_prices[-1] / max(ma5, 1e-8)
            eng.append(price_to_ma5)
        else:
            eng.append(1.0)
            
        # Feature 9: Price to moving average ratio (10-day)
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = close_prices[-1] / max(ma10, 1e-8)
            eng.append(price_to_ma10)
        else:
            eng.append(1.0)
            
        # Feature 10: Bollinger Band Width (volatility measure)
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            bb_width = (2 * std20) / max(ma20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
            
        # Feature 11: Short interest change rate
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            eng.append(si_change)
        else:
            eng.append(0.0)
            
        # Feature 12: Implied volatility to historical volatility ratio
        if len(close_prices) >= 10:
            hist_vol = np.std(np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)) * np.sqrt(252)
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)
            
        # Feature 13: Volume trend
        if volume > 0 and avg_volume > 0:
            volume_trend = volume / max(avg_volume, 1e-8) - 1
            eng.append(volume_trend)
        else:
            eng.append(0.0)
            
        # Feature 14: Price range relative to average
        if len(close_prices) >= 5:
            recent_range = (high_prices[-1] - low_prices[-1])
            avg_range = np.mean(high_prices[-5:] - low_prices[-5:])
            range_ratio = recent_range / max(avg_range, 1e-8)
            eng.append(range_ratio)
        else:
            eng.append(1.0)
            
        # Feature 15: Synthetic short cost to implied volatility ratio
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # Feature 16: Short squeeze potential (days to cover * volatility)
        if len(close_prices) >= 5:
            squeeze_potential = days_to_cover * volatility
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
            
        # Feature 17: Options sentiment indicator
        options_sentiment = np.log(max(put_call_ratio, 1e-8))
        eng.append(options_sentiment)
        
        # Feature 18: Price trend strength
        if len(close_prices) >= 10:
            up_days = np.sum(np.diff(close_prices[-10:]) > 0)
            trend_strength = (up_days / 9) * 2 - 1  # Scale to [-1, 1]
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 142921.0675
RMSE: 201053.1824
MAPE: 8.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 94
   â€¢ Highly important features (top 5%): 89

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0002, rank=1
   2. Feature_0_t3: importance=0.0002, rank=2
   3. Feature_5_t1: importance=0.0002, rank=3
   4. Feature_1_t3: importance=0.0002, rank=4
   5. Feature_8_t1: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 1.58%

ðŸ“ˆ Current best MAPE: 8.33%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0
        
        # Keep essential raw features (based on feature importance from previous iterations)
        raw_keep.append(short_interest)  # Short interest - consistently high importance
        raw_keep.append(avg_volume)      # Average daily volume - high importance
        raw_keep.append(days_to_cover)   # Days to cover - important for short squeeze potential
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-based signals
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to volume ratio (high importance in previous iteration)
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 2: Short interest to shares outstanding ratio (improved measure of short interest intensity)
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 3: RSI (Relative Strength Index) - high importance in previous iteration
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 14
            avg_loss = loss / 14
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 4: Short squeeze potential (days to cover * volatility * si_shares_ratio)
        # Enhanced from previous iteration by incorporating si_shares_ratio
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-6:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            squeeze_potential = days_to_cover * volatility * si_shares_ratio
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 5: Price to moving average ratio (10-day) - high importance in previous iteration
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = close_prices[-1] / max(ma10, 1e-8)
            eng.append(price_to_ma10)
        else:
            eng.append(1.0)
        
        # Feature 6: Short interest change rate - improved with smoothing
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Apply smoothing if we have enough history
            if t > 2:
                prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
                si_change = 0.7 * si_change + 0.3 * prev_si_change  # Smoothed change
            eng.append(si_change)
        else:
            eng.append(0.0)
        
        # Feature 7: Synthetic short cost to implied volatility ratio - important for cost of shorting
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # Feature 8: Options sentiment indicator (log of put/call ratio)
        options_sentiment = np.log(max(put_call_ratio, 1e-8))
        eng.append(options_sentiment)
        
        # Feature 9: Bollinger Band position - where is price relative to volatility bands?
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_width = upper_band - lower_band
            # Normalized position within bands (-1 to 1 scale)
            bb_position = 2 * (close_prices[-1] - lower_band) / max(band_width, 1e-8) - 1
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # Feature 10: MACD Signal - trend strength and direction
        if len(close_prices) >= 12:
            ema12 = np.mean(close_prices[-12:])  # Simple approximation of EMA
            ema26 = np.mean(close_prices[-min(26, len(close_prices)):])
            macd = ema12 - ema26
            # Normalize MACD by price level
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 11: Price momentum with volatility adjustment
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            # Adjust momentum by volatility - higher weight for low-vol momentum
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 12: Short interest acceleration (second derivative)
        if t > 1:
            si_t = short_interest
            si_t_1 = data[t-1, 0]
            si_t_2 = data[t-2, 0]
            
            first_deriv_t = (si_t - si_t_1) / max(si_t_1, 1e-8)
            first_deriv_t_1 = (si_t_1 - si_t_2) / max(si_t_2, 1e-8)
            
            si_acceleration = first_deriv_t - first_deriv_t_1
            eng.append(si_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 13: Implied volatility trend
        if t > 0:
            iv_change = (implied_volatility - data[t-1, 66]) / max(data[t-1, 66], 1e-8)
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 14: Volume pressure (recent volume vs average)
        if len(close_prices) >= 5:
            recent_vol = np.mean(volume) if isinstance(volume, np.ndarray) else volume
            vol_pressure = recent_vol / max(avg_volume, 1e-8) - 1
            eng.append(vol_pressure)
        else:
            eng.append(0.0)
        
        # Feature 15: Price trend consistency
        if len(close_prices) >= 10:
            # Count direction changes
            price_diffs = np.diff(close_prices[-10:])
            direction_changes = np.sum(price_diffs[1:] * price_diffs[:-1] < 0)
            # Normalize to [0,1] where 1 is perfectly consistent trend
            trend_consistency = 1 - (direction_changes / 8)  # Max 8 possible changes in 10 days
            eng.append(trend_consistency)
        else:
            eng.append(0.5)  # Neutral value
        
        # Feature 16: Short interest to days to cover ratio
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 17: Composite short pressure indicator
        # Combines multiple short-related metrics into one signal
        short_pressure = (si_shares_ratio * days_to_cover * synthetic_short_cost) / max(implied_volatility, 1e-8)
        eng.append(short_pressure)
        
        # Feature 18: Price gap significance
        if len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            # Normalize by recent volatility
            if len(close_prices) >= 10:
                recent_volatility = np.std(np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8))
                gap_significance = overnight_gap / max(recent_volatility, 1e-8)
                eng.append(gap_significance)
            else:
                eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 19: Stochastic oscillator (K%)
        if len(close_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            k_percent = 100 * (close_prices[-1] - lowest_low) / max(highest_high - lowest_low, 1e-8)
            eng.append(k_percent / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral value
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 147296.1804
RMSE: 203342.2913
MAPE: 8.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0002, rank=1
   2. Feature_2_t0: importance=0.0002, rank=2
   3. Feature_10_t1: importance=0.0002, rank=3
   4. Feature_1_t3: importance=0.0002, rank=4
   5. Feature_24_t2: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.18%

ðŸ“ˆ Current best MAPE: 8.33%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on feature importance from previous iterations
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Short interest - consistently high importance
        raw_keep.append(avg_volume)      # Average daily volume - high importance
        raw_keep.append(days_to_cover)   # Days to cover - important for short squeeze potential
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-based signals
        
        # Add most recent price data (compact OHLC summary)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to shares outstanding ratio (normalized short interest)
        # This was highly significant in previous iterations
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 2: Short interest to volume ratio (indicates how many days of volume are sold short)
        # High importance in previous iterations
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 3: RSI (Relative Strength Index) - momentum oscillator
        # Consistently important in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 13
            avg_loss = loss / 13
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Feature 4: Short squeeze potential indicator
        # Enhanced by incorporating volatility, days to cover, and short interest ratio
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-5:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            squeeze_potential = days_to_cover * volatility * si_shares_ratio
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 5: Price to moving average ratio (10-day)
        # High importance in previous iterations
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = close_prices[-1] / max(ma10, 1e-8)
            eng.append(price_to_ma10)
        else:
            eng.append(1.0)
        
        # Feature 6: Short interest momentum (rate of change)
        # Improved with exponential weighting for more recent changes
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Apply exponential weighting if we have enough history
            if t > 2:
                prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
                si_change = 0.7 * si_change + 0.3 * prev_si_change
            eng.append(si_change)
        else:
            eng.append(0.0)
        
        # Feature 7: Synthetic short cost to implied volatility ratio
        # Important for cost of shorting relative to expected volatility
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # Feature 8: Options sentiment indicator (normalized put/call ratio)
        # Transformed to better capture extremes in sentiment
        options_sentiment = 2 / (1 + np.exp(-put_call_ratio)) - 1  # Sigmoid transform to [-1,1]
        eng.append(options_sentiment)
        
        # Feature 9: Price volatility (normalized by price level)
        # More stable measure of volatility than raw standard deviation
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            norm_volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(norm_volatility)
        else:
            eng.append(0.0)
        
        # Feature 10: Short interest acceleration (second derivative)
        # Captures acceleration in short interest changes
        if t > 1:
            si_t = short_interest
            si_t_1 = data[t-1, 0]
            si_t_2 = data[t-2, 0]
            
            first_deriv_t = (si_t - si_t_1) / max(si_t_1, 1e-8)
            first_deriv_t_1 = (si_t_1 - si_t_2) / max(si_t_2, 1e-8)
            
            si_acceleration = first_deriv_t - first_deriv_t_1
            eng.append(si_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 11: Implied volatility trend
        # Captures changes in market expectations of future volatility
        if t > 0:
            iv_change = (implied_volatility - data[t-1, 66]) / max(data[t-1, 66], 1e-8)
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 12: Price trend strength (ADX-inspired)
        # Measures the strength of a price trend regardless of direction
        if len(close_prices) >= 14:
            tr_sum = 0
            dx_sum = 0
            
            for i in range(1, min(14, len(close_prices))):
                tr = max(high_prices[-i] - low_prices[-i], 
                         abs(high_prices[-i] - close_prices[-(i+1)]), 
                         abs(low_prices[-i] - close_prices[-(i+1)]))
                tr_sum += tr
                
                dm_plus = max(0, high_prices[-i] - high_prices[-(i+1)])
                dm_minus = max(0, low_prices[-(i+1)] - low_prices[-i])
                
                if dm_plus > dm_minus:
                    dx = dm_plus / max(tr_sum, 1e-8)
                elif dm_minus > dm_plus:
                    dx = dm_minus / max(tr_sum, 1e-8)
                else:
                    dx = 0
                    
                dx_sum += dx
            
            trend_strength = dx_sum / 13  # Average directional movement
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 13: Composite short pressure indicator
        # Combines multiple short-related metrics into one signal
        short_pressure = (si_shares_ratio * days_to_cover * synthetic_short_cost) / max(implied_volatility, 1e-8)
        eng.append(short_pressure)
        
        # Feature 14: Price momentum with volatility adjustment
        # Adjusts momentum by volatility for more stable signal
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 15: Bollinger Band position
        # Where is price relative to volatility bands?
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_width = upper_band - lower_band
            bb_position = (close_prices[-1] - lower_band) / max(band_width, 1e-8)
            eng.append(bb_position)
        else:
            eng.append(0.5)
        
        # Feature 16: Short interest to days to cover ratio
        # Indicates relative difficulty of covering short positions
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 17: Price gap significance
        # Captures overnight price jumps normalized by volatility
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            if len(close_prices) >= 10:
                recent_volatility = np.std(np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8))
                gap_significance = overnight_gap / max(recent_volatility, 1e-8)
                eng.append(gap_significance)
            else:
                eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 18: Relative volume indicator
        # Compares recent volume to longer-term average volume
        if len(close_prices) >= 5:
            recent_vol_avg = np.mean(ohlc[-5:, 3])  # Using close prices as proxy for volume
            vol_ratio = recent_vol_avg / max(avg_volume, 1e-8)
            eng.append(vol_ratio - 1)  # Normalize around 0
        else:
            eng.append(0.0)
        
        # Feature 19: Short interest to synthetic short cost efficiency
        # Measures the efficiency of shorting through options vs direct shorting
        si_synth_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synth_efficiency)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 149231.7884
RMSE: 206826.8285
MAPE: 8.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 87

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0003, rank=1
   2. Feature_24_t3: importance=0.0002, rank=2
   3. Feature_1_t3: importance=0.0002, rank=3
   4. Feature_21_t3: importance=0.0002, rank=4
   5. Feature_0_t3: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.32%

ðŸ“ˆ Current best MAPE: 8.33%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_volume  # Fallback if volume not available
        
        # Keep essential raw features based on feature importance from previous iterations
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Short interest - consistently high importance
        raw_keep.append(avg_volume)      # Average daily volume - high importance
        raw_keep.append(days_to_cover)   # Days to cover - important for short squeeze potential
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-based signals
        
        # Add most recent price data (compact OHLC summary)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(shares_outstanding)  # Important for normalizing short interest
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to shares outstanding ratio (normalized short interest)
        # This was highly significant in previous iterations
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 2: Short interest to volume ratio (indicates how many days of volume are sold short)
        # High importance in previous iterations
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 3: RSI (Relative Strength Index) - momentum oscillator
        # Consistently important in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 13
            avg_loss = loss / 13
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Feature 4: Enhanced short squeeze potential indicator
        # Improved by incorporating volatility, days to cover, and short interest ratio
        # with exponential weighting to emphasize extreme values
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-5:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            # Exponential transformation to emphasize high values
            squeeze_potential = np.exp(days_to_cover * volatility * si_shares_ratio * 10) / 100
            # Cap at a reasonable value to prevent extreme outliers
            squeeze_potential = min(squeeze_potential, 10)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 5: Price to moving average ratio (10-day)
        # High importance in previous iterations
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = close_prices[-1] / max(ma10, 1e-8)
            eng.append(price_to_ma10)
        else:
            eng.append(1.0)
        
        # Feature 6: Short interest momentum with exponential smoothing
        # Improved with exponential weighting for more recent changes
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Apply exponential weighting if we have enough history
            if t > 2:
                prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
                si_change = 0.7 * si_change + 0.3 * prev_si_change
            eng.append(si_change)
        else:
            eng.append(0.0)
        
        # Feature 7: Synthetic short cost to implied volatility ratio
        # Important for cost of shorting relative to expected volatility
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # Feature 8: Options sentiment indicator (normalized put/call ratio)
        # Transformed to better capture extremes in sentiment
        options_sentiment = 2 / (1 + np.exp(-put_call_ratio)) - 1  # Sigmoid transform to [-1,1]
        eng.append(options_sentiment)
        
        # Feature 9: Normalized price volatility with lookback adjustment
        # More stable measure of volatility than raw standard deviation
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            norm_volatility = np.std(returns) if len(returns) > 0 else 0
            # Scale by average price to normalize across different price levels
            avg_price = np.mean(close_prices[-10:])
            scaled_volatility = norm_volatility * (100 / max(avg_price, 1e-8))
            eng.append(scaled_volatility)
        else:
            eng.append(0.0)
        
        # Feature 10: Short interest acceleration (second derivative)
        # Captures acceleration in short interest changes
        if t > 1:
            si_t = short_interest
            si_t_1 = data[t-1, 0]
            si_t_2 = data[t-2, 0]
            
            first_deriv_t = (si_t - si_t_1) / max(si_t_1, 1e-8)
            first_deriv_t_1 = (si_t_1 - si_t_2) / max(si_t_2, 1e-8)
            
            si_acceleration = first_deriv_t - first_deriv_t_1
            eng.append(si_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 11: Implied volatility trend with market context
        # Captures changes in market expectations of future volatility
        if t > 0:
            iv_change = (implied_volatility - data[t-1, 66]) / max(data[t-1, 66], 1e-8)
            # Adjust by recent price movement to provide context
            if len(close_prices) >= 2:
                price_change = (close_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
                # Positive when IV and price move in opposite directions (potential reversal signal)
                iv_context = iv_change * (1 - np.sign(iv_change) * np.sign(price_change))
                eng.append(iv_context)
            else:
                eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 12: Composite short pressure indicator with volatility adjustment
        # Combines multiple short-related metrics into one signal
        vol_factor = max(implied_volatility, 0.01)  # Avoid division by very small numbers
        short_pressure = (si_shares_ratio * days_to_cover * synthetic_short_cost) / vol_factor
        # Apply sigmoid transformation to handle extreme values
        short_pressure_norm = 2 / (1 + np.exp(-short_pressure / 5)) - 1
        eng.append(short_pressure_norm)
        
        # Feature 13: Price momentum with volatility adjustment
        # Adjusts momentum by volatility for more stable signal
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            # Cap extreme values
            vol_adj_momentum = np.clip(vol_adj_momentum, -10, 10)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 14: Bollinger Band position with volume weighting
        # Where is price relative to volatility bands, weighted by volume?
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_width = upper_band - lower_band
            bb_position = (close_prices[-1] - lower_band) / max(band_width, 1e-8)
            
            # Weight by recent volume relative to average
            if len(close_prices) >= 5:
                recent_vol = np.mean(ohlc[-5:, 3])  # Using close as proxy for volume
                vol_weight = recent_vol / max(avg_volume, 1e-8)
                # Emphasize extreme positions (near bands) when volume is high
                vol_weighted_bb = bb_position * vol_weight if bb_position < 0.3 or bb_position > 0.7 else bb_position
                eng.append(vol_weighted_bb)
            else:
                eng.append(bb_position)
        else:
            eng.append(0.5)
        
        # Feature 15: Short interest to days to cover ratio with trend adjustment
        # Indicates relative difficulty of covering short positions
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        # Adjust by recent trend in short interest if available
        if t > 0:
            si_trend = (short_interest - data[t-1, 0]) / max(data[t-1, 0], 1e-8)
            trend_adj_ratio = si_dtc_ratio * (1 + si_trend)
            eng.append(trend_adj_ratio)
        else:
            eng.append(si_dtc_ratio)
        
        # Feature 16: MACD-inspired short interest oscillator
        # Captures momentum and trend changes in short interest
        if t >= 12:
            # Calculate short-term and long-term EMA of short interest
            short_term_ema = 0
            long_term_ema = 0
            alpha_short = 2 / (1 + 6)  # 6-day EMA
            alpha_long = 2 / (1 + 12)  # 12-day EMA
            
            for i in range(12):
                idx = t - i
                if idx >= 0:
                    if i == 0:
                        short_term_ema = data[idx, 0]
                        long_term_ema = data[idx, 0]
                    else:
                        short_term_ema = data[idx, 0] * alpha_short + short_term_ema * (1 - alpha_short)
                        long_term_ema = data[idx, 0] * alpha_long + long_term_ema * (1 - alpha_long)
            
            macd = (short_term_ema - long_term_ema) / max(long_term_ema, 1e-8)
            eng.append(macd)
        else:
            eng.append(0.0)
        
        # Feature 17: Price gap significance with short interest context
        # Captures overnight price jumps normalized by volatility and short interest
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            if len(close_prices) >= 10:
                recent_volatility = np.std(np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8))
                gap_significance = overnight_gap / max(recent_volatility, 1e-8)
                # Adjust by short interest level - gaps are more significant with high short interest
                si_adjusted_gap = gap_significance * (1 + si_shares_ratio)
                eng.append(si_adjusted_gap)
            else:
                eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 18: Composite options market signal
        # Combines put/call ratio, implied volatility, and synthetic short cost
        options_signal = put_call_ratio * implied_volatility / max(synthetic_short_cost, 1e-8)
        # Normalize with sigmoid to handle extreme values
        options_signal_norm = 2 / (1 + np.exp(-options_signal)) - 1
        eng.append(options_signal_norm)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 146056.2757
RMSE: 204144.1112
MAPE: 8.47%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t2: importance=0.0003, rank=1
   2. Feature_12_t3: importance=0.0002, rank=2
   3. Feature_6_t3: importance=0.0002, rank=3
   4. Feature_24_t1: importance=0.0002, rank=4
   5. Feature_13_t2: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.13%

ðŸ“ˆ Current best MAPE: 8.33%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on feature importance from previous iterations
        raw_keep.append(short_interest)  # Always keep short interest - target related
        raw_keep.append(avg_volume)      # Always keep volume - consistently important
        raw_keep.append(days_to_cover)   # Important for short squeeze potential
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-based signals
        
        # Add most recent price data (compact OHLC summary)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to shares outstanding ratio (normalized short interest)
        # This was highly significant in previous iterations
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 2: Short interest to volume ratio
        # High importance in previous iterations
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 3: RSI (Relative Strength Index) - momentum oscillator
        # Consistently important in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 13
            avg_loss = loss / 13
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Feature 4: Enhanced short squeeze potential indicator
        # Improved by incorporating volatility, days to cover, and short interest ratio
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-5:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            # Log transformation to handle extreme values better than exponential
            squeeze_potential = np.log1p(days_to_cover * volatility * si_shares_ratio * 5)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 5: Price to moving average ratio (10-day)
        # High importance in previous iterations
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = close_prices[-1] / max(ma10, 1e-8)
            eng.append(price_to_ma10)
        else:
            eng.append(1.0)
        
        # Feature 6: Short interest momentum (rate of change)
        # Improved with better normalization
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Apply tanh to limit extreme values
            si_change_norm = np.tanh(si_change * 2)
            eng.append(si_change_norm)
        else:
            eng.append(0.0)
        
        # Feature 7: Synthetic short cost to implied volatility ratio
        # Important for cost of shorting relative to expected volatility
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # Feature 8: Options sentiment indicator (normalized put/call ratio)
        # Using arctangent transformation for better handling of extreme values
        options_sentiment = np.arctan(put_call_ratio) * 2 / np.pi  # Maps to [-1,1]
        eng.append(options_sentiment)
        
        # Feature 9: Bollinger Band position (where price is relative to volatility bands)
        # Consistently important in previous iterations
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_width = upper_band - lower_band
            bb_position = (close_prices[-1] - lower_band) / max(band_width, 1e-8)
            # Normalize to [0,1] range
            bb_position = np.clip(bb_position, 0, 1)
            eng.append(bb_position)
        else:
            eng.append(0.5)
        
        # Feature 10: Short interest acceleration (second derivative)
        # Captures acceleration in short interest changes
        if t > 1:
            si_t = short_interest
            si_t_1 = data[t-1, 0]
            si_t_2 = data[t-2, 0]
            
            first_deriv_t = (si_t - si_t_1) / max(si_t_1, 1e-8)
            first_deriv_t_1 = (si_t_1 - si_t_2) / max(si_t_2, 1e-8)
            
            si_acceleration = first_deriv_t - first_deriv_t_1
            # Apply tanh to limit extreme values
            si_accel_norm = np.tanh(si_acceleration * 3)
            eng.append(si_accel_norm)
        else:
            eng.append(0.0)
        
        # Feature 11: Composite short pressure indicator
        # Combines multiple short-related metrics into one signal
        short_pressure = (si_shares_ratio * days_to_cover) / max(implied_volatility, 0.01)
        # Apply log transformation to handle extreme values better
        short_pressure_norm = np.log1p(short_pressure)
        eng.append(short_pressure_norm)
        
        # Feature 12: Price momentum with volatility adjustment
        # Adjusts momentum by volatility for more stable signal
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            # Apply tanh to limit extreme values
            vol_adj_momentum = np.tanh(vol_adj_momentum)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 13: MACD-inspired short interest oscillator
        # Captures momentum and trend changes in short interest
        if t >= 12:
            # Calculate short-term and long-term EMA of short interest
            short_term_ema = 0
            long_term_ema = 0
            alpha_short = 2 / (1 + 6)  # 6-day EMA
            alpha_long = 2 / (1 + 12)  # 12-day EMA
            
            for i in range(12):
                idx = t - i
                if idx >= 0:
                    if i == 0:
                        short_term_ema = data[idx, 0]
                        long_term_ema = data[idx, 0]
                    else:
                        short_term_ema = data[idx, 0] * alpha_short + short_term_ema * (1 - alpha_short)
                        long_term_ema = data[idx, 0] * alpha_long + long_term_ema * (1 - alpha_long)
            
            macd = (short_term_ema - long_term_ema) / max(long_term_ema, 1e-8)
            # Apply tanh to limit extreme values
            macd_norm = np.tanh(macd * 3)
            eng.append(macd_norm)
        else:
            eng.append(0.0)
        
        # Feature 14: Average True Range (ATR) - volatility indicator
        # New feature that captures price volatility
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, 5):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i < len(close_prices)-1 else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i < len(close_prices)-1 else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            
            atr = np.mean(tr_values) if tr_values else 0
            # Normalize by average price
            avg_price = np.mean(close_prices[-5:])
            norm_atr = atr / max(avg_price, 1e-8)
            eng.append(norm_atr)
        else:
            eng.append(0.0)
        
        # Feature 15: Short interest to days to cover ratio
        # Indicates relative difficulty of covering short positions
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        # Apply log transformation to handle extreme values
        si_dtc_norm = np.log1p(si_dtc_ratio)
        eng.append(si_dtc_norm)
        
        # Feature 16: Overnight gap significance
        # Captures overnight price jumps normalized by volatility
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            # Apply tanh to limit extreme values
            gap_norm = np.tanh(overnight_gap * 5)
            eng.append(gap_norm)
        else:
            eng.append(0.0)
        
        # Feature 17: Volume trend indicator
        # Captures recent volume trend relative to average
        if len(close_prices) >= 5:
            recent_vol = np.mean(close_prices[-5:])  # Using close as proxy for volume
            vol_trend = recent_vol / max(avg_volume, 1e-8) - 1
            # Apply tanh to limit extreme values
            vol_trend_norm = np.tanh(vol_trend * 2)
            eng.append(vol_trend_norm)
        else:
            eng.append(0.0)
        
        # Feature 18: Composite options market signal
        # Combines put/call ratio and implied volatility
        options_signal = put_call_ratio * implied_volatility
        # Apply log transformation to handle extreme values
        options_signal_norm = np.log1p(options_signal)
        eng.append(options_signal_norm)
        
        # Feature 19: Stochastic oscillator (K%)
        # New feature that indicates overbought/oversold conditions
        if len(close_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            range_hl = highest_high - lowest_low
            k_percent = (close_prices[-1] - lowest_low) / max(range_hl, 1e-8) * 100
            # Normalize to [0,1]
            k_percent_norm = k_percent / 100
            eng.append(k_percent_norm)
        else:
            eng.append(0.5)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 145933.6491
RMSE: 200929.3396
MAPE: 8.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0003, rank=1
   2. Feature_20_t3: importance=0.0002, rank=2
   3. Feature_3_t1: importance=0.0002, rank=3
   4. Feature_8_t2: importance=0.0002, rank=4
   5. Feature_11_t3: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.11%

ðŸ“ˆ Current best MAPE: 8.33%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_output = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_volume  # Fallback if not available
        
        # Keep essential raw features based on feature importance from previous iterations
        raw_keep.append(short_interest)  # Always keep short interest - target related
        raw_keep.append(avg_volume)      # Always keep volume - consistently important
        raw_keep.append(days_to_cover)   # Important for short squeeze potential
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-based signals
        
        # Add most recent price data (compact OHLC summary)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to shares outstanding ratio (normalized short interest)
        # This was highly significant in previous iterations
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # Feature 2: Short interest to volume ratio
        # High importance in previous iterations
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 3: Improved RSI (Relative Strength Index) with exponential weighting
        # Consistently important in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            # Apply exponential weighting to recent changes (more weight to recent data)
            weights = np.exp(np.linspace(0, 1, len(delta)))
            weights = weights / np.sum(weights)
            
            gain = np.sum(weights * np.where(delta > 0, delta, 0))
            loss = np.sum(weights * np.where(delta < 0, -delta, 0))
            
            rs = gain / max(loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Feature 4: Enhanced short squeeze potential indicator with market context
        # Improved by incorporating volatility, days to cover, and short interest ratio
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-5:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            
            # Add market context: recent price trend direction
            price_trend = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            trend_factor = 1 + np.tanh(price_trend * 3)  # Amplify if uptrend (potential squeeze)
            
            squeeze_potential = np.log1p(days_to_cover * volatility * si_shares_ratio * trend_factor * 5)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 5: Dual moving average crossover signal (5-day vs 10-day)
        # New feature that captures recent trend changes
        if len(close_prices) >= 10:
            ma5 = np.mean(close_prices[-5:])
            ma10 = np.mean(close_prices[-10:])
            # Normalized crossover signal
            ma_crossover = (ma5 / max(ma10, 1e-8)) - 1
            # Apply tanh to limit extreme values
            ma_crossover_norm = np.tanh(ma_crossover * 5)
            eng.append(ma_crossover_norm)
        else:
            eng.append(0.0)
        
        # Feature 6: Short interest momentum with adaptive normalization
        # Improved with better normalization based on historical volatility
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            
            # Calculate historical SI volatility if enough data
            si_vol = 0.1  # Default value
            if t >= 5:
                si_history = [data[t-i, 0] for i in range(5) if t-i >= 0]
                if len(si_history) >= 2:
                    si_returns = np.diff(si_history) / np.maximum(si_history[:-1], 1e-8)
                    si_vol = max(np.std(si_returns), 0.01)
            
            # Normalize by historical volatility
            si_change_norm = np.tanh(si_change / max(si_vol, 0.01) * 2)
            eng.append(si_change_norm)
        else:
            eng.append(0.0)
        
        # Feature 7: Options market pressure indicator
        # Combines put/call ratio with synthetic short cost and implied volatility
        options_pressure = (put_call_ratio * synthetic_short_cost) / max(implied_volatility, 1e-8)
        options_pressure_norm = np.tanh(np.log1p(options_pressure))
        eng.append(options_pressure_norm)
        
        # Feature 8: Bollinger Band squeeze indicator
        # Captures volatility contraction/expansion which often precedes significant moves
        if len(close_prices) >= 10:
            ma20 = np.mean(close_prices[-10:])
            std20 = np.std(close_prices[-10:])
            
            # Calculate bandwidth normalized by price
            band_width = (2 * std20) / max(ma20, 1e-8)
            
            # Invert and scale so that tight bands (potential breakouts) have higher values
            squeeze_indicator = np.exp(-5 * band_width)
            eng.append(squeeze_indicator)
        else:
            eng.append(0.5)
        
        # Feature 9: Short interest acceleration with exponential smoothing
        # Captures acceleration in short interest changes with noise reduction
        if t > 1:
            si_t = short_interest
            si_t_1 = data[t-1, 0]
            si_t_2 = data[t-2, 0]
            
            # Calculate first derivatives
            first_deriv_t = (si_t - si_t_1) / max(si_t_1, 1e-8)
            first_deriv_t_1 = (si_t_1 - si_t_2) / max(si_t_2, 1e-8)
            
            # Apply exponential smoothing if more history is available
            if t > 2:
                alpha = 0.7  # Smoothing factor
                si_t_3 = data[t-3, 0]
                first_deriv_t_2 = (si_t_2 - si_t_3) / max(si_t_3, 1e-8)
                first_deriv_t_1 = alpha * first_deriv_t_1 + (1-alpha) * first_deriv_t_2
            
            si_acceleration = first_deriv_t - first_deriv_t_1
            si_accel_norm = np.tanh(si_acceleration * 3)
            eng.append(si_accel_norm)
        else:
            eng.append(0.0)
        
        # Feature 10: Composite short pressure indicator with volume context
        # Combines multiple short-related metrics with volume consideration
        volume_factor = np.log1p(avg_volume / max(shares_outstanding, 1e-8) * 100)
        short_pressure = (si_shares_ratio * days_to_cover * volume_factor) / max(implied_volatility, 0.01)
        short_pressure_norm = np.tanh(np.log1p(short_pressure))
        eng.append(short_pressure_norm)
        
        # Feature 11: Price momentum with adaptive lookback
        # Adjusts momentum calculation based on volatility regime
        if len(close_prices) >= 10:
            # Calculate recent volatility
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            
            # Adjust lookback period based on volatility (shorter in high vol)
            lookback = max(3, min(10, int(5 / max(vol, 0.01))))
            lookback = min(lookback, len(close_prices)-1)  # Ensure we don't exceed available data
            
            momentum = (close_prices[-1] / max(close_prices[-lookback], 1e-8)) - 1
            # Apply tanh to limit extreme values
            momentum_norm = np.tanh(momentum * 3)
            eng.append(momentum_norm)
        else:
            eng.append(0.0)
        
        # Feature 12: Short interest divergence from price trend
        # Captures when short interest moves contrary to price (potential reversal signal)
        if t > 0 and len(close_prices) >= 5:
            si_change = (short_interest - data[t-1, 0]) / max(data[t-1, 0], 1e-8)
            price_change = (close_prices[-1] - close_prices[-5]) / max(close_prices[-5], 1e-8)
            
            # Divergence occurs when signs differ (one positive, one negative)
            divergence = si_change * price_change
            divergence_indicator = -np.tanh(divergence * 5)  # Negative to highlight divergence
            eng.append(divergence_indicator)
        else:
            eng.append(0.0)
        
        # Feature 13: Volatility-adjusted days to cover
        # Normalizes days to cover by market volatility for better comparison
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            vol_adj_dtc = days_to_cover * vol
            vol_adj_dtc_norm = np.tanh(vol_adj_dtc)
            eng.append(vol_adj_dtc_norm)
        else:
            eng.append(days_to_cover / 10)  # Simple normalization if insufficient data
        
        # Feature 14: Gap analysis with short interest context
        # Captures overnight gaps in context of short interest levels
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            # Amplify gap significance based on short interest level
            si_factor = np.log1p(si_shares_ratio * 10)
            gap_significance = overnight_gap * si_factor
            gap_norm = np.tanh(gap_significance * 3)
            eng.append(gap_norm)
        else:
            eng.append(0.0)
        
        # Feature 15: Short interest to implied volatility ratio
        # Indicates relative short interest compared to market expectations
        si_iv_ratio = short_interest / max(implied_volatility, 1e-8)
        si_iv_norm = np.tanh(np.log1p(si_iv_ratio))
        eng.append(si_iv_norm)
        
        # Feature 16: Price channel breakout indicator
        # Captures when price breaks out of recent trading range
        if len(close_prices) >= 10:
            high_channel = np.max(high_prices[-10:-1])  # Exclude current bar
            low_channel = np.min(low_prices[-10:-1])    # Exclude current bar
            
            # Calculate normalized position within channel
            channel_width = high_channel - low_channel
            if channel_width > 1e-8:
                channel_position = (close_prices[-1] - low_channel) / channel_width
                # Transform to highlight breakouts (>1 is upside breakout, <0 is downside)
                breakout_indicator = (channel_position - 0.5) * 2  # Scale to roughly [-1,1]
            else:
                breakout_indicator = 0
            
            # Apply tanh to limit extreme values while preserving sign
            breakout_norm = np.tanh(breakout_indicator * 2)
            eng.append(breakout_norm)
        else:
            eng.append(0.0)
        
        # Feature 17: Short interest concentration
        # Measures how concentrated short interest is relative to float
        si_concentration = (short_interest / max(shares_outstanding, 1e-8)) * days_to_cover
        si_concentration_norm = np.tanh(np.log1p(si_concentration * 2))
        eng.append(si_concentration_norm)
        
        # Feature 18: Synthetic short cost momentum
        # Captures changes in the cost of shorting
        if t > 0:
            prev_cost = data[t-1, 65]
            cost_change = (synthetic_short_cost - prev_cost) / max(prev_cost, 1e-8)
            cost_change_norm = np.tanh(cost_change * 5)
            eng.append(cost_change_norm)
        else:
            eng.append(0.0)
        
        # Feature 19: Implied volatility skew indicator
        # Proxy for options skew using available data
        iv_skew = implied_volatility * (put_call_ratio - 1)
        iv_skew_norm = np.tanh(iv_skew)
        eng.append(iv_skew_norm)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_output[t] = row
    
    # Handle NaN, inf values
    features_output = np.nan_to_num(features_output, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_output
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 142432.2621
RMSE: 202695.4652
MAPE: 8.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 86

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0003, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_11_t3: importance=0.0002, rank=3
   4. Feature_8_t2: importance=0.0002, rank=4
   5. Feature_3_t0: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.08%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 127805.4723
RMSE: 173206.5231
MAPE: 12.32%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0007, rank=1
   2. Feature_1_t3: importance=0.0006, rank=2
   3. Feature_67_t3: importance=0.0005, rank=3
   4. Feature_0_t1: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0002, rank=5
   Baseline MAPE: 12.32%
   Baseline MAE: 127805.4723
   Baseline RMSE: 173206.5231

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 122840.2030
RMSE: 153518.6241
MAPE: 11.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 88

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0003, rank=1
   2. Feature_0_t3: importance=0.0003, rank=2
   3. Feature_22_t0: importance=0.0002, rank=3
   4. Feature_8_t2: importance=0.0002, rank=4
   5. Feature_1_t3: importance=0.0002, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 11.77%
   MAE: 122840.2030
   RMSE: 153518.6241

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 12.32%
   Best Model MAPE: 11.77%
   Absolute Improvement: 0.55%
   Relative Improvement: 4.5%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  9.91            N/A                 
1          Iteration 1               8.33            +1.58%              
2          Iteration 2               8.51            -0.18%              
3          Iteration 3               8.66            -0.32%              
4          Iteration 4               8.47            -0.13%              
5          Iteration 5               8.45            -0.11%              
6          Iteration 6               8.26            +0.08%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 6 - MAPE: 8.26%
âœ… Saved ABCB results to cache/ABCB_iterative_results_enhanced.pkl
âœ… Summary report saved for ABCB

ðŸŽ‰ Process completed successfully for ABCB!

================================================================================
PROCESSING TICKER 3/15: EIG
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for EIG
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for EIG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EIG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 50051.5515
RMSE: 65143.6023
MAPE: 15.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 115
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0009, rank=2
   3. Feature_67_t2: importance=0.0006, rank=3
   4. Feature_2_t2: importance=0.0005, rank=4
   5. Feature_65_t3: importance=0.0005, rank=5

ðŸ“Š Baseline Performance: MAPE = 15.65%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (highest importance)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        
        # Keep days to cover (high importance from iteration 0)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options_synthetic_short_cost (high importance from iteration 0)
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        
        # Keep shares_outstanding (high importance from iteration 0)
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Keep volume (potentially useful for liquidity analysis)
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw_keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Price momentum and volatility features
        # Recent price change (5-day)
        if len(close_prices) >= 5:
            price_change_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_change_5d)
        else:
            eng.append(0.0)
            
        # Recent price change (10-day)
        if len(close_prices) >= 10:
            price_change_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            eng.append(price_change_10d)
        else:
            eng.append(0.0)
        
        # Volatility (standard deviation of returns)
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        else:
            eng.append(0.0)
            
        # 2. Volume-based features
        # Volume ratio (current volume to average)
        volume = data[t, 68]
        avg_volume = data[t, 1]
        volume_ratio = volume / max(avg_volume, 1e-8)
        eng.append(volume_ratio)
        
        # 3. Short interest relative to shares outstanding
        short_interest = data[t, 0]
        shares_outstanding = data[t, 67]
        short_interest_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_interest_ratio)
        
        # 4. Options-related features
        # Put-call ratio (already in raw data)
        put_call_ratio = data[t, 64]
        eng.append(put_call_ratio)
        
        # Implied volatility (already in raw data)
        implied_vol = data[t, 66]
        eng.append(implied_vol)
        
        # 5. Technical indicators
        # RSI (Relative Strength Index)
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
            
        # MACD (Moving Average Convergence Divergence)
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            eng.append(macd)
        else:
            eng.append(0.0)
            
        # 6. Price range features
        # High-Low range relative to close
        if len(close_prices) >= 5:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            recent_close = close_prices[-1]
            price_range_ratio = (recent_high - recent_low) / max(recent_close, 1e-8)
            eng.append(price_range_ratio)
        else:
            eng.append(0.0)
            
        # 7. Composite features
        # Short interest momentum (change in short interest)
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
            
        # Short interest to volume ratio
        short_to_volume = short_interest / max(volume, 1e-8)
        eng.append(short_to_volume)
        
        # Short cost to volatility ratio
        short_cost = data[t, 65]  # options_synthetic_short_cost
        short_cost_to_vol = short_cost / max(implied_vol, 1e-8)
        eng.append(short_cost_to_vol)
        
        # 8. Trend strength indicators
        # ADX-inspired trend strength (simplified)
        if len(close_prices) >= 14:
            up_moves = np.maximum(0, high_prices[1:] - high_prices[:-1])[-14:]
            down_moves = np.maximum(0, low_prices[:-1] - low_prices[1:])[-14:]
            avg_up = np.mean(up_moves) if len(up_moves) > 0 else 0
            avg_down = np.mean(down_moves) if len(down_moves) > 0 else 0
            trend_strength = (avg_up + avg_down) / max(close_prices[-1], 1e-8)
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (critical baseline features)
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        raw_keep.extend([short_interest, avg_volume, days_to_cover])
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        last_open, last_high, last_low, last_close = open_prices[-1], high_prices[-1], low_prices[-1], close_prices[-1]
        raw_keep.extend([last_open, last_high, last_low, last_close])
        
        # Keep high-importance options and shares data (based on feature importance analysis)
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else 0.0  # Safely handle potential index error
        
        raw_keep.extend([options_synthetic_short_cost, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Relative Volume (Current Volume / Average Volume)
        # Indicates unusual trading activity
        rel_volume = volume / max(avg_volume, 1e-8)
        eng.append(rel_volume)
        
        # 3. Price Volatility (recent high-low range normalized by close)
        # Captures recent price volatility
        price_range = (last_high - last_low) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 4. Short-term price momentum (5-day)
        # Captures recent price direction
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 5. Medium-term price momentum (10-day)
        # Captures medium-term price direction
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
        else:
            momentum_10d = 0.0
        eng.append(momentum_10d)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility
        atr_sum = 0.0
        if len(close_prices) > 1:
            for i in range(1, min(10, len(close_prices))):
                tr1 = high_prices[i] - low_prices[i]
                tr2 = abs(high_prices[i] - close_prices[i-1])
                tr3 = abs(low_prices[i] - close_prices[i-1])
                true_range = max(tr1, tr2, tr3)
                atr_sum += true_range
            atr = atr_sum / max(min(10, len(close_prices)-1), 1)
            atr_relative = atr / max(last_close, 1e-8)  # Normalize by price
        else:
            atr_relative = 0.0
        eng.append(atr_relative)
        
        # 7. RSI (Relative Strength Index) - 14 period if available
        # Momentum oscillator measuring speed and change of price movements
        if len(close_prices) >= 3:
            gains = []
            losses = []
            for i in range(1, len(close_prices)):
                change = close_prices[i] - close_prices[i-1]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = sum(gains) / max(len(gains), 1e-8)
            avg_loss = sum(losses) / max(len(losses), 1e-8)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if avg_gain > 0 else 50
        else:
            rsi = 50.0
        eng.append(rsi / 100.0)  # Normalize to 0-1 range
        
        # 8. Price to Moving Average Ratio (5-day)
        # Shows price relative to recent average
        if len(close_prices) >= 5:
            ma5 = np.mean(close_prices[-5:])
            price_to_ma5 = last_close / max(ma5, 1e-8)
        else:
            price_to_ma5 = 1.0
        eng.append(price_to_ma5)
        
        # 9. Short Interest to Volume Ratio
        # Indicates how many days of current volume would be needed to cover shorts
        si_to_volume = short_interest / max(volume, 1e-8)
        eng.append(si_to_volume)
        
        # 10. Bollinger Band Width (volatility measure)
        # Measures market volatility
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            bb_width = (2 * std20) / max(ma20, 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 11. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 12. Short Interest to Days to Cover Ratio
        # Relationship between short interest and days to cover
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # 13. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 47969.5969
RMSE: 62855.1262
MAPE: 15.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0006, rank=1
   2. Feature_15_t0: importance=0.0006, rank=2
   3. Feature_9_t2: importance=0.0005, rank=3
   4. Feature_14_t3: importance=0.0005, rank=4
   5. Feature_13_t0: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.41%

ðŸ“ˆ Current best MAPE: 15.25%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        last_open, last_high, last_low, last_close = open_prices[-1], high_prices[-1], low_prices[-1], close_prices[-1]
        
        # Extract options and shares data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else avg_volume  # Use avg_volume as fallback
        
        # Keep critical raw features based on previous importance analysis
        raw_keep.extend([
            short_interest,                # Always keep short interest
            avg_volume,                    # Always keep average volume
            days_to_cover,                 # Important for short interest prediction
            last_close,                    # Most recent close price
            options_synthetic_short_cost,  # High importance in baseline
            shares_outstanding,            # High importance in baseline
            options_avg_implied_volatility # Implied volatility is important for short interest
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Relative Volume (Current Volume / Average Volume)
        # Indicates unusual trading activity
        rel_volume = volume / max(avg_volume, 1e-8)
        eng.append(rel_volume)
        
        # 3. Short Interest to Days to Cover Ratio
        # Relationship between short interest and days to cover - high importance in previous iteration
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # 4. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iteration
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 5. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - important trend indicator
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 6. Price Momentum (5-day) - captures recent price direction
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 7. RSI (Relative Strength Index) - momentum oscillator
        # Simplified calculation for stability
        if len(close_prices) >= 5:
            gains = []
            losses = []
            for i in range(1, len(close_prices)):
                change = close_prices[i] - close_prices[i-1]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = sum(gains) / max(len(gains), 1e-8)
            avg_loss = sum(losses) / max(len(losses), 1e-8)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if avg_gain > 0 else 50
            rsi_normalized = rsi / 100.0  # Normalize to 0-1 range
        else:
            rsi_normalized = 0.5
        eng.append(rsi_normalized)
        
        # 8. Bollinger Band Width (volatility measure)
        # Using 10-day window if available, otherwise use what we have
        window = min(10, len(close_prices))
        if window >= 3:
            ma = np.mean(close_prices[-window:])
            std = np.std(close_prices[-window:])
            bb_width = (2 * std) / max(ma, 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 9. Price to Moving Average Ratio (10-day)
        # Shows price relative to recent average
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_to_ma10 = last_close / max(ma10, 1e-8)
        else:
            price_to_ma10 = 1.0
        eng.append(price_to_ma10)
        
        # 10. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations (IV) to realized volatility
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / close_prices[-6:-1]
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = options_avg_implied_volatility / max(hist_vol, 1e-8)
        else:
            iv_hv_ratio = 1.0
        eng.append(iv_hv_ratio)
        
        # 11. Put-Call Ratio to Short Interest Change
        # Relationship between options sentiment and short interest changes
        pc_si_ratio = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            pc_si_ratio = options_put_call_ratio * (1 + si_change)
        else:
            pc_si_ratio = options_put_call_ratio
        eng.append(pc_si_ratio)
        
        # 12. Short Interest Acceleration
        # Second derivative of short interest - captures changing trends
        si_accel = 0.0
        if t >= 2:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            prev_change = (prev_si - prev_prev_si) / max(prev_prev_si, 1e-8)
            current_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            si_accel = current_change - prev_change
        eng.append(si_accel)
        
        # 13. Volatility-Adjusted Short Interest
        # Short interest normalized by price volatility
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / close_prices[-6:-1]
            vol = np.std(returns) * np.sqrt(252)  # Annualized
            vol_adj_si = short_interest * max(vol, 1e-8)
        else:
            vol_adj_si = short_interest
        eng.append(vol_adj_si)
        
        # 14. Short Interest to Implied Volatility Ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # 15. Normalized Price Range (High-Low)
        # Recent price volatility normalized by price level
        price_range = (last_high - last_low) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 16. Average True Range (ATR) - volatility indicator
        # Measures market volatility
        atr_sum = 0.0
        if len(close_prices) > 1:
            for i in range(1, min(10, len(close_prices))):
                tr1 = high_prices[i] - low_prices[i]
                tr2 = abs(high_prices[i] - close_prices[i-1])
                tr3 = abs(low_prices[i] - close_prices[i-1])
                true_range = max(tr1, tr2, tr3)
                atr_sum += true_range
            atr = atr_sum / max(min(10, len(close_prices)-1), 1)
            atr_relative = atr / max(last_close, 1e-8)  # Normalize by price
        else:
            atr_relative = 0.0
        eng.append(atr_relative)
        
        # 17. Short Interest to Volume Trend
        # How short interest is changing relative to volume
        si_vol_trend = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_vol = data[t-1, 68] if data.shape[1] > 68 else data[t-1, 1]
            prev_ratio = prev_si / max(prev_vol, 1e-8)
            curr_ratio = short_interest / max(volume, 1e-8)
            si_vol_trend = (curr_ratio / max(prev_ratio, 1e-8)) - 1.0
        eng.append(si_vol_trend)
        
        # 18. Composite Momentum Indicator
        # Combines price momentum with volume signals
        if len(close_prices) >= 10:
            price_momentum = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            volume_momentum = (volume / max(avg_volume, 1e-8)) - 1.0
            composite_momentum = price_momentum * (1 + volume_momentum)
        else:
            composite_momentum = 0.0
        eng.append(composite_momentum)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (critical baseline features)
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        raw_keep.extend([short_interest, avg_volume, days_to_cover])
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        last_open, last_high, last_low, last_close = open_prices[-1], high_prices[-1], low_prices[-1], close_prices[-1]
        raw_keep.extend([last_close])  # Only keep close price to reduce dimensionality
        
        # Keep high-importance options and shares data (based on feature importance analysis)
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        
        # Based on feature importance, keep only the most important raw features
        raw_keep.extend([options_synthetic_short_cost, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest to Days to Cover Ratio
        # Relationship between short interest and days to cover
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # 3. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iteration
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 4. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 5. Price Volatility (recent high-low range normalized by close)
        # Captures recent price volatility
        price_range = (last_high - last_low) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility - more sophisticated than simple range
        atr_sum = 0.0
        if len(close_prices) > 1:
            for i in range(1, min(10, len(close_prices))):
                tr1 = high_prices[i] - low_prices[i]
                tr2 = abs(high_prices[i] - close_prices[i-1])
                tr3 = abs(low_prices[i] - close_prices[i-1])
                true_range = max(tr1, tr2, tr3)
                atr_sum += true_range
            atr = atr_sum / max(min(10, len(close_prices)-1), 1)
            atr_relative = atr / max(last_close, 1e-8)  # Normalize by price
        else:
            atr_relative = 0.0
        eng.append(atr_relative)
        
        # 7. RSI (Relative Strength Index) - 14 period if available
        # Momentum oscillator measuring speed and change of price movements
        if len(close_prices) >= 3:
            gains = []
            losses = []
            for i in range(1, len(close_prices)):
                change = close_prices[i] - close_prices[i-1]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = sum(gains) / max(len(gains), 1e-8)
            avg_loss = sum(losses) / max(len(losses), 1e-8)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if avg_gain > 0 else 50
        else:
            rsi = 50.0
        eng.append(rsi / 100.0)  # Normalize to 0-1 range
        
        # 8. Short-term price momentum (5-day)
        # Captures recent price direction
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 9. Bollinger Band Width (volatility measure)
        # Measures market volatility
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            bb_width = (2 * std20) / max(ma20, 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 10. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations (IV) to realized volatility
        hist_vol = np.std(close_prices) / max(np.mean(close_prices), 1e-8) if len(close_prices) > 1 else 0.0
        iv_hv_ratio = options_avg_implied_volatility / max(hist_vol, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 11. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 12. Put-Call Ratio to Short Interest Ratio
        # Relationship between options sentiment and short interest
        pc_si_ratio = options_put_call_ratio / max(si_ratio, 1e-8)
        eng.append(pc_si_ratio)
        
        # 13. MACD Signal Line (12-26 EMA difference)
        # Trend-following momentum indicator
        if len(close_prices) >= 26:
            ema12 = close_prices[-12:].mean()  # Simplified EMA calculation
            ema26 = close_prices[-26:].mean()
            macd = (ema12 - ema26) / max(last_close, 1e-8)  # Normalized by price
        else:
            macd = 0.0
        eng.append(macd)
        
        # 14. Price Gap Analysis
        # Captures overnight sentiment changes
        price_gaps = 0.0
        if len(open_prices) > 1 and len(close_prices) > 1:
            gaps = [(open_prices[i] - close_prices[i-1]) / max(close_prices[i-1], 1e-8) 
                   for i in range(1, len(open_prices))]
            price_gaps = sum(gaps) / max(len(gaps), 1e-8)
        eng.append(price_gaps)
        
        # 15. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 16. Normalized Days to Cover
        # Days to cover normalized by its recent history
        if t > 0:
            prev_dtc = data[t-1, 2]
            norm_dtc = days_to_cover / max(prev_dtc, 1e-8) - 1.0
        else:
            norm_dtc = 0.0
        eng.append(norm_dtc)
        
        # 17. Stochastic Oscillator (K%)
        # Momentum indicator comparing close price to price range
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            highest_high = max(high_prices[-5:])
            lowest_low = min(low_prices[-5:])
            range_hl = highest_high - lowest_low
            if range_hl > 1e-8:
                stoch_k = (last_close - lowest_low) / range_hl
            else:
                stoch_k = 0.5
        else:
            stoch_k = 0.5
        eng.append(stoch_k)
        
        # 18. Chaikin Money Flow (CMF)
        # Volume-weighted accumulation/distribution line
        cmf = 0.0
        if len(close_prices) >= 5:
            money_flow_volume = 0.0
            volume_sum = 1e-8
            for i in range(max(0, len(close_prices)-5), len(close_prices)):
                if high_prices[i] - low_prices[i] > 1e-8:
                    money_flow_mult = ((close_prices[i] - low_prices[i]) - 
                                      (high_prices[i] - close_prices[i])) / (high_prices[i] - low_prices[i])
                    money_flow_volume += money_flow_mult * 1.0  # Using 1.0 as proxy for volume
                    volume_sum += 1.0
            cmf = money_flow_volume / volume_sum
        eng.append(cmf)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 48505.2386
RMSE: 62809.9281
MAPE: 15.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_17_t3: importance=0.0007, rank=1
   2. Feature_17_t2: importance=0.0005, rank=2
   3. Feature_12_t0: importance=0.0005, rank=3
   4. Feature_15_t3: importance=0.0004, rank=4
   5. Feature_6_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.16%

ðŸ“ˆ Current best MAPE: 15.25%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Always keep these critical baseline features
        raw_keep.extend([short_interest, avg_volume, days_to_cover])
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent close price (high importance in previous iterations)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Extract other important raw features based on previous iterations
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else 1.0  # Fallback if not available
        
        # Keep only the most important raw features based on previous iterations
        raw_keep.extend([options_synthetic_short_cost, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest to Days to Cover Ratio
        # Relationship between short interest and days to cover - high importance in previous iterations
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # 3. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 4. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 5. Stochastic Oscillator (K%) - high importance in previous iteration
        # Momentum indicator comparing close price to price range
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            highest_high = max(high_prices[-5:])
            lowest_low = min(low_prices[-5:])
            range_hl = highest_high - lowest_low
            if range_hl > 1e-8:
                stoch_k = (last_close - lowest_low) / range_hl
            else:
                stoch_k = 0.5
        else:
            stoch_k = 0.5
        eng.append(stoch_k)
        
        # 6. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - high importance in previous iteration
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 7. Price Volatility (recent high-low range normalized by close)
        # Captures recent price volatility
        price_range = (high_prices[-1] - low_prices[-1]) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 8. Normalized Days to Cover
        # Days to cover normalized by its recent history - high importance in previous iteration
        if t > 0:
            prev_dtc = data[t-1, 2]
            norm_dtc = days_to_cover / max(prev_dtc, 1e-8) - 1.0
        else:
            norm_dtc = 0.0
        eng.append(norm_dtc)
        
        # 9. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations - high importance in previous iteration
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 10. RSI (Relative Strength Index) - 14 period if available
        # Momentum oscillator measuring speed and change of price movements
        if len(close_prices) >= 3:
            gains = []
            losses = []
            for i in range(1, len(close_prices)):
                change = close_prices[i] - close_prices[i-1]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = sum(gains) / max(len(gains), 1e-8)
            avg_loss = sum(losses) / max(len(losses), 1e-8)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if avg_gain > 0 else 50
        else:
            rsi = 50.0
        eng.append(rsi / 100.0)  # Normalize to 0-1 range
        
        # 11. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - new feature
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 12. Exponential Moving Average (EMA) Crossover Signal
        # Trend indicator based on fast vs slow EMA crossover
        if len(close_prices) >= 10:
            # Simple approximation of EMA
            ema_fast = np.mean(close_prices[-5:])
            ema_slow = np.mean(close_prices[-10:])
            ema_signal = (ema_fast / max(ema_slow, 1e-8)) - 1.0
        else:
            ema_signal = 0.0
        eng.append(ema_signal)
        
        # 13. Bollinger Band Position
        # Where current price is within the Bollinger Bands
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_range = upper_band - lower_band
            if band_range > 1e-8:
                bb_position = (last_close - lower_band) / band_range
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
        eng.append(bb_position)
        
        # 14. Short Interest Momentum (rate of change of SI change)
        # Captures momentum in short interest changes - new feature
        si_momentum = 0.0
        if t >= 3:
            si_change_prev = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_change_prev2 = (data[t-2, 0] - data[t-3, 0]) / max(data[t-3, 0], 1e-8)
            si_momentum = si_change - 2*si_change_prev + si_change_prev2
        eng.append(si_momentum)
        
        # 15. Price to Synthetic Short Cost Ratio
        # Inverse of feature 3, captures different relationship aspect - new feature
        price_to_synth_cost = last_close / max(options_synthetic_short_cost, 1e-8)
        eng.append(price_to_synth_cost)
        
        # 16. Implied Volatility to Days to Cover Ratio
        # Relationship between market expectations and short covering time - new feature
        iv_dtc_ratio = options_avg_implied_volatility / max(days_to_cover, 1e-8)
        eng.append(iv_dtc_ratio)
        
        # 17. Short Interest Change Volatility
        # Volatility in short interest changes - new feature
        si_change_vol = 0.0
        if t >= 3:
            changes = []
            for i in range(1, min(4, t+1)):
                prev_change = (data[t-i+1, 0] - data[t-i, 0]) / max(data[t-i, 0], 1e-8)
                changes.append(prev_change)
            si_change_vol = np.std(changes) if len(changes) > 1 else 0.0
        eng.append(si_change_vol)
        
        # 18. Price Momentum (5-day)
        # Captures recent price direction - high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 19. Short Interest to Price Ratio
        # Relationship between short interest and stock price - new feature
        si_price_ratio = short_interest / max(last_close * shares_outstanding, 1e-8)
        eng.append(si_price_ratio)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 46690.1861
RMSE: 60913.2083
MAPE: 14.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 85

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t0: importance=0.0004, rank=1
   2. Feature_23_t2: importance=0.0004, rank=2
   3. Feature_12_t0: importance=0.0004, rank=3
   4. Feature_21_t1: importance=0.0004, rank=4
   5. Feature_18_t2: importance=0.0003, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.49%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important raw features based on previous iterations
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (required)
        raw_keep.extend([short_interest, avg_volume])
        
        # Keep days to cover (consistently high importance)
        raw_keep.append(days_to_cover)
        
        # Keep last close price (most recent is most relevant)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Keep synthetic short cost and shares outstanding (high importance in baseline)
        raw_keep.append(options_synthetic_short_cost)
        raw_keep.append(shares_outstanding)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - high importance in previous iteration
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 4. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - new feature
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 5. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 6. Normalized Days to Cover
        # Days to cover normalized by its recent history - high importance in previous iteration
        if t > 0:
            prev_dtc = data[t-1, 2]
            norm_dtc = days_to_cover / max(prev_dtc, 1e-8) - 1.0
        else:
            norm_dtc = 0.0
        eng.append(norm_dtc)
        
        # 7. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 8. Price Momentum (5-day)
        # Captures recent price direction - high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 9. Short Interest Change Volatility
        # Volatility in short interest changes - high importance in previous iteration
        si_change_vol = 0.0
        if t >= 3:
            changes = []
            for i in range(1, min(4, t+1)):
                prev_change = (data[t-i+1, 0] - data[t-i, 0]) / max(data[t-i, 0], 1e-8)
                changes.append(prev_change)
            si_change_vol = np.std(changes) if len(changes) > 1 else 0.0
        eng.append(si_change_vol)
        
        # 10. Bollinger Band Position
        # Where current price is within the Bollinger Bands - high importance in previous iteration
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_range = upper_band - lower_band
            if band_range > 1e-8:
                bb_position = (last_close - lower_band) / band_range
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
        eng.append(bb_position)
        
        # 11. Relative Volume Indicator
        # Compares current volume to historical average - new feature
        if t > 0:
            prev_avg_volume = data[t-1, 1]
            rel_volume = avg_volume / max(prev_avg_volume, 1e-8) - 1.0
        else:
            rel_volume = 0.0
        eng.append(rel_volume)
        
        # 12. Short Interest to Days to Cover Ratio
        # Relationship between short interest and days to cover
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # 13. Price Volatility (recent high-low range normalized by close)
        # Captures recent price volatility
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = (max(recent_highs) - min(recent_lows)) / max(last_close, 1e-8)
        else:
            price_range = (high_prices[-1] - low_prices[-1]) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 14. Short Interest Trend Strength
        # Measures consistency of short interest direction - new feature
        si_trend_strength = 0.0
        if t >= 3:
            changes = []
            for i in range(1, min(4, t+1)):
                prev_change = (data[t-i+1, 0] - data[t-i, 0])
                changes.append(prev_change)
            
            # Count how many changes are in the same direction as the most recent change
            recent_dir = 1 if changes[0] > 0 else (-1 if changes[0] < 0 else 0)
            same_dir_count = sum(1 for c in changes if (c > 0 and recent_dir > 0) or (c < 0 and recent_dir < 0))
            si_trend_strength = same_dir_count / len(changes)
        eng.append(si_trend_strength)
        
        # 15. Implied Volatility to Price Volatility Ratio
        # Compares market expectations to actual price behavior - new feature
        if price_range > 1e-8:
            iv_to_price_vol = options_avg_implied_volatility / price_range
        else:
            iv_to_price_vol = 1.0
        eng.append(iv_to_price_vol)
        
        # 16. Short Interest to Put/Call Ratio
        # Relationship between short interest and options sentiment - new feature
        if options_put_call_ratio > 1e-8:
            si_to_pc_ratio = short_interest / (options_put_call_ratio * shares_outstanding)
        else:
            si_to_pc_ratio = 0.0
        eng.append(si_to_pc_ratio)
        
        # 17. Price Gap Analysis
        # Identifies significant overnight price gaps - new feature
        price_gaps = 0.0
        if len(close_prices) >= 5 and len(open_prices) >= 5:
            gaps = []
            for i in range(1, 5):
                gap = (open_prices[-i] - close_prices[-(i+1)]) / max(close_prices[-(i+1)], 1e-8)
                gaps.append(abs(gap))
            price_gaps = np.mean(gaps)
        eng.append(price_gaps)
        
        # 18. Short Interest Momentum (weighted)
        # Weighted version of short interest momentum - new feature
        si_momentum_weighted = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate weighted changes
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            change_3 = (si_2 - si_3) / max(si_3, 1e-8)
            
            # Apply exponential weighting
            si_momentum_weighted = 0.6 * change_1 + 0.3 * change_2 + 0.1 * change_3
        eng.append(si_momentum_weighted)
        
        # 19. Composite Short Pressure Indicator
        # Combines multiple short-related metrics - new feature
        short_pressure = (si_ratio + si_volume_ratio + synth_cost_to_price) / 3.0
        eng.append(short_pressure)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 46560.6000
RMSE: 60428.4741
MAPE: 14.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0005, rank=1
   2. Feature_15_t0: importance=0.0005, rank=2
   3. Feature_15_t2: importance=0.0005, rank=3
   4. Feature_2_t1: importance=0.0004, rank=4
   5. Feature_20_t0: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.01%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important raw features
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else np.mean(avg_volume)
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (required)
        raw_keep.extend([short_interest, avg_volume])
        
        # Keep days to cover (consistently high importance)
        raw_keep.append(days_to_cover)
        
        # Keep last close price (most recent is most relevant)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Keep synthetic short cost and shares outstanding (high importance in baseline)
        raw_keep.append(options_synthetic_short_cost)
        raw_keep.append(shares_outstanding)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - high importance in previous iteration
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 4. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - new feature
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 5. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 6. Normalized Days to Cover
        # Days to cover normalized by its recent history - high importance in previous iteration
        if t > 0:
            prev_dtc = data[t-1, 2]
            norm_dtc = days_to_cover / max(prev_dtc, 1e-8) - 1.0
        else:
            norm_dtc = 0.0
        eng.append(norm_dtc)
        
        # 7. Price Momentum (5-day)
        # Captures recent price direction - high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 8. Bollinger Band Position
        # Where current price is within the Bollinger Bands - high importance in previous iteration
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_range = upper_band - lower_band
            if band_range > 1e-8:
                bb_position = (last_close - lower_band) / band_range
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
        eng.append(bb_position)
        
        # 9. Relative Volume Indicator
        # Compares current volume to historical average - new feature
        if t > 0:
            prev_avg_volume = data[t-1, 1]
            rel_volume = avg_volume / max(prev_avg_volume, 1e-8) - 1.0
        else:
            rel_volume = 0.0
        eng.append(rel_volume)
        
        # 10. Price Volatility (recent high-low range normalized by close)
        # Captures recent price volatility
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = (max(recent_highs) - min(recent_lows)) / max(last_close, 1e-8)
        else:
            price_range = (high_prices[-1] - low_prices[-1]) / max(last_close, 1e-8)
        eng.append(price_range)
        
        # 11. Implied Volatility to Price Volatility Ratio
        # Compares market expectations to actual price behavior - high importance in previous iteration
        if price_range > 1e-8:
            iv_to_price_vol = options_avg_implied_volatility / price_range
        else:
            iv_to_price_vol = 1.0
        eng.append(iv_to_price_vol)
        
        # 12. Short Interest Momentum (weighted)
        # Weighted version of short interest momentum - high importance in previous iteration
        si_momentum_weighted = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate weighted changes
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            change_3 = (si_2 - si_3) / max(si_3, 1e-8)
            
            # Apply exponential weighting
            si_momentum_weighted = 0.6 * change_1 + 0.3 * change_2 + 0.1 * change_3
        eng.append(si_momentum_weighted)
        
        # 13. RSI (Relative Strength Index) - 5 day
        # Technical indicator showing overbought/oversold conditions
        rsi = 0.5  # Default to neutral
        if len(close_prices) >= 6:
            gains = []
            losses = []
            for i in range(1, 6):
                change = close_prices[-i] - close_prices[-(i+1)]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = np.mean(gains) if gains else 0
            avg_loss = np.mean(losses) if losses else 0
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            elif avg_gain > 0:
                rsi = 100
            else:
                rsi = 50
            
            # Normalize to 0-1
            rsi = rsi / 100
        eng.append(rsi)
        
        # 14. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations - high importance in previous iteration
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 15. MACD Signal Line Crossover
        # Momentum indicator showing trend direction and strength
        macd_signal = 0.0
        if len(close_prices) >= 12:
            # Simple implementation of MACD
            ema12 = np.mean(close_prices[-12:])
            ema26 = np.mean(close_prices[-min(len(close_prices), 26):])
            macd = ema12 - ema26
            
            # Signal line (9-day EMA of MACD)
            if t > 0 and len(close_prices) >= 9:
                prev_macd = 0
                if t >= 9:
                    prev_prices = [data[t-i, 3:63].reshape(15, 4)[:, 3] for i in range(1, 10)]
                    prev_ema12s = [np.mean(p[-12:]) if len(p) >= 12 else np.mean(p) for p in prev_prices]
                    prev_ema26s = [np.mean(p[-min(len(p), 26):]) for p in prev_prices]
                    prev_macds = [e12 - e26 for e12, e26 in zip(prev_ema12s, prev_ema26s)]
                    signal_line = np.mean(prev_macds)
                    macd_signal = macd - signal_line
                else:
                    macd_signal = 0
            
            # Normalize
            macd_signal = np.tanh(macd_signal / max(last_close * 0.01, 1e-8))
        eng.append(macd_signal)
        
        # 16. Price Gap Significance
        # Measures the significance of overnight price gaps relative to volatility
        gap_significance = 0.0
        if len(close_prices) >= 5 and len(open_prices) >= 5:
            # Calculate average volatility
            daily_returns = [(close_prices[-i] / close_prices[-(i+1)] - 1) for i in range(1, 5)]
            avg_volatility = np.std(daily_returns) if len(daily_returns) > 1 else 0.01
            
            # Calculate most recent gap
            recent_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            
            # Normalize gap by volatility
            gap_significance = recent_gap / max(avg_volatility, 1e-8)
            
            # Cap extreme values
            gap_significance = np.tanh(gap_significance)
        eng.append(gap_significance)
        
        # 17. Short Squeeze Potential Indicator
        # Combines multiple factors that could indicate a short squeeze
        short_squeeze_potential = 0.0
        if si_ratio > 0.1:  # Only meaningful if short interest is significant
            # Factors that contribute to short squeeze potential:
            # 1. High short interest ratio
            # 2. Recent price increase (momentum)
            # 3. Increasing volume
            # 4. High implied volatility
            
            squeeze_factors = [
                min(si_ratio * 5, 1.0),  # Normalize SI ratio (cap at 20%)
                max(min(momentum_5d * 5, 1.0), 0) if momentum_5d > 0 else 0,  # Recent price increase
                max(min(rel_volume * 2, 1.0), 0) if rel_volume > 0 else 0,  # Increasing volume
                min(options_avg_implied_volatility / 0.5, 1.0)  # High IV (normalized to 50%)
            ]
            
            # Weight the factors
            weights = [0.4, 0.3, 0.15, 0.15]
            short_squeeze_potential = sum(f * w for f, w in zip(squeeze_factors, weights))
        eng.append(short_squeeze_potential)
        
        # 18. Composite Short Pressure Indicator
        # Combines multiple short-related metrics - high importance in previous iteration
        short_pressure = (si_ratio + si_volume_ratio + synth_cost_to_price) / 3.0
        eng.append(short_pressure)
        
        # 19. Directional Movement Index (DMI)
        # Technical indicator showing trend strength and direction
        dmi = 0.0
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            up_moves = []
            down_moves = []
            
            for i in range(1, 5):
                up_move = high_prices[-i] - high_prices[-(i+1)]
                down_move = low_prices[-(i+1)] - low_prices[-i]
                
                if up_move > down_move and up_move > 0:
                    up_moves.append(up_move)
                    down_moves.append(0)
                elif down_move > up_move and down_move > 0:
                    down_moves.append(down_move)
                    up_moves.append(0)
                else:
                    up_moves.append(0)
                    down_moves.append(0)
            
            avg_up = np.mean(up_moves) if up_moves else 0
            avg_down = np.mean(down_moves) if down_moves else 0
            
            if avg_up + avg_down > 1e-8:
                dmi = (avg_up - avg_down) / (avg_up + avg_down)
            else:
                dmi = 0
        eng.append(dmi)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 46909.3878
RMSE: 61436.8217
MAPE: 14.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0005, rank=1
   2. Feature_7_t1: importance=0.0004, rank=2
   3. Feature_12_t2: importance=0.0004, rank=3
   4. Feature_5_t2: importance=0.0004, rank=4
   5. Feature_17_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.12%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important raw features
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else np.mean(avg_volume)
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (required)
        raw_keep.extend([short_interest, avg_volume])
        
        # Keep days to cover (consistently high importance)
        raw_keep.append(days_to_cover)
        
        # Keep last close price (most recent is most relevant)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Keep synthetic short cost (high importance in baseline)
        raw_keep.append(options_synthetic_short_cost)
        
        # Keep shares outstanding (high importance in baseline)
        raw_keep.append(shares_outstanding)
        
        # Keep options implied volatility (important for short interest dynamics)
        raw_keep.append(options_avg_implied_volatility)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - high importance in previous iteration
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 4. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - consistently important
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 5. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 6. Price Momentum (5-day)
        # Captures recent price direction - high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 7. Bollinger Band Position
        # Where current price is within the Bollinger Bands - high importance in previous iteration
        if len(close_prices) >= 5:
            ma20 = np.mean(close_prices[-5:])
            std20 = np.std(close_prices[-5:])
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            band_range = upper_band - lower_band
            if band_range > 1e-8:
                bb_position = (last_close - lower_band) / band_range
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
        eng.append(bb_position)
        
        # 8. Short Interest Momentum (weighted)
        # Weighted version of short interest momentum - high importance in previous iteration
        si_momentum_weighted = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate weighted changes
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            change_3 = (si_2 - si_3) / max(si_3, 1e-8)
            
            # Apply exponential weighting
            si_momentum_weighted = 0.6 * change_1 + 0.3 * change_2 + 0.1 * change_3
        eng.append(si_momentum_weighted)
        
        # 9. RSI (Relative Strength Index) - 5 day
        # Technical indicator showing overbought/oversold conditions
        rsi = 0.5  # Default to neutral
        if len(close_prices) >= 6:
            gains = []
            losses = []
            for i in range(1, 6):
                change = close_prices[-i] - close_prices[-(i+1)]
                if change > 0:
                    gains.append(change)
                    losses.append(0)
                else:
                    gains.append(0)
                    losses.append(abs(change))
            
            avg_gain = np.mean(gains) if gains else 0
            avg_loss = np.mean(losses) if losses else 0
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            elif avg_gain > 0:
                rsi = 100
            else:
                rsi = 50
            
            # Normalize to 0-1
            rsi = rsi / 100
        eng.append(rsi)
        
        # 10. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations - high importance in previous iteration
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 11. Short Squeeze Potential Indicator
        # Combines multiple factors that could indicate a short squeeze - high importance in previous iterations
        short_squeeze_potential = 0.0
        if si_ratio > 0.1:  # Only meaningful if short interest is significant
            # Factors that contribute to short squeeze potential:
            # 1. High short interest ratio
            # 2. Recent price increase (momentum)
            # 3. High implied volatility
            # 4. High days to cover
            
            squeeze_factors = [
                min(si_ratio * 5, 1.0),  # Normalize SI ratio (cap at 20%)
                max(min(momentum_5d * 5, 1.0), 0) if momentum_5d > 0 else 0,  # Recent price increase
                min(options_avg_implied_volatility / 0.5, 1.0),  # High IV (normalized to 50%)
                min(days_to_cover / 10, 1.0)  # High days to cover (normalized to 10 days)
            ]
            
            # Weight the factors
            weights = [0.4, 0.3, 0.15, 0.15]
            short_squeeze_potential = sum(f * w for f, w in zip(squeeze_factors, weights))
        eng.append(short_squeeze_potential)
        
        # 12. Composite Short Pressure Indicator
        # Combines multiple short-related metrics - high importance in previous iteration
        short_pressure = (si_ratio + si_volume_ratio + synth_cost_to_price) / 3.0
        eng.append(short_pressure)
        
        # 13. Price Volatility (normalized)
        # Captures recent price volatility relative to price level
        if len(close_prices) >= 5:
            returns = [(close_prices[i] / close_prices[i-1] - 1) for i in range(1, len(close_prices))]
            volatility = np.std(returns) if len(returns) > 1 else 0
        else:
            volatility = 0
        eng.append(volatility)
        
        # 14. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations to actual price behavior
        iv_hv_ratio = options_avg_implied_volatility / max(volatility, 1e-8)
        iv_hv_ratio = min(iv_hv_ratio, 10.0)  # Cap extreme values
        eng.append(iv_hv_ratio)
        
        # 15. Short Interest to Days to Cover Ratio Change
        # Captures changes in the relationship between SI and DTC
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        si_dtc_ratio_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_dtc = data[t-1, 2]
            prev_ratio = prev_si / max(prev_dtc, 1e-8)
            si_dtc_ratio_change = (si_dtc_ratio / max(prev_ratio, 1e-8)) - 1.0
        eng.append(si_dtc_ratio_change)
        
        # 16. Put-Call Ratio Normalized by Short Interest
        # Relates options market sentiment to short interest
        pc_si_ratio = options_put_call_ratio / max(si_ratio, 1e-8)
        pc_si_ratio = min(pc_si_ratio, 10.0)  # Cap extreme values
        eng.append(pc_si_ratio)
        
        # 17. Exponential Moving Average Crossover Signal
        # Technical indicator for trend direction
        ema_signal = 0.0
        if len(close_prices) >= 10:
            # Simple EMA calculation
            ema_fast = np.mean(close_prices[-5:])
            ema_slow = np.mean(close_prices[-10:])
            ema_signal = (ema_fast / max(ema_slow, 1e-8)) - 1.0
        eng.append(ema_signal)
        
        # 18. Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 46823.7122
RMSE: 62678.5189
MAPE: 14.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0006, rank=1
   2. Feature_12_t2: importance=0.0004, rank=2
   3. Feature_11_t3: importance=0.0004, rank=3
   4. Feature_6_t0: importance=0.0004, rank=4
   5. Feature_10_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.09%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important raw features
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (required)
        raw_keep.extend([short_interest, avg_volume])
        
        # Keep days to cover (consistently high importance)
        raw_keep.append(days_to_cover)
        
        # Keep last close price (most recent is most relevant)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Keep synthetic short cost (high importance in baseline)
        raw_keep.append(options_synthetic_short_cost)
        
        # Keep shares outstanding (high importance in baseline)
        raw_keep.append(shares_outstanding)
        
        # Keep options implied volatility (important for short interest dynamics)
        raw_keep.append(options_avg_implied_volatility)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - high importance in previous iteration
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 4. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - consistently important
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 5. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 6. Price Momentum (5-day)
        # Captures recent price direction - high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 7. Short Interest Momentum (weighted)
        # Weighted version of short interest momentum - high importance in previous iteration
        si_momentum_weighted = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate weighted changes
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            change_3 = (si_2 - si_3) / max(si_3, 1e-8)
            
            # Apply exponential weighting
            si_momentum_weighted = 0.6 * change_1 + 0.3 * change_2 + 0.1 * change_3
        eng.append(si_momentum_weighted)
        
        # 8. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations - high importance in previous iteration
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 9. Short Squeeze Potential Indicator
        # Combines multiple factors that could indicate a short squeeze - high importance in previous iterations
        short_squeeze_potential = 0.0
        if si_ratio > 0.1:  # Only meaningful if short interest is significant
            # Factors that contribute to short squeeze potential:
            # 1. High short interest ratio
            # 2. Recent price increase (momentum)
            # 3. High implied volatility
            # 4. High days to cover
            
            squeeze_factors = [
                min(si_ratio * 5, 1.0),  # Normalize SI ratio (cap at 20%)
                max(min(momentum_5d * 5, 1.0), 0) if momentum_5d > 0 else 0,  # Recent price increase
                min(options_avg_implied_volatility / 0.5, 1.0),  # High IV (normalized to 50%)
                min(days_to_cover / 10, 1.0)  # High days to cover (normalized to 10 days)
            ]
            
            # Weight the factors
            weights = [0.4, 0.3, 0.15, 0.15]
            short_squeeze_potential = sum(f * w for f, w in zip(squeeze_factors, weights))
        eng.append(short_squeeze_potential)
        
        # 10. Composite Short Pressure Indicator
        # Combines multiple short-related metrics - high importance in previous iteration
        short_pressure = (si_ratio + si_volume_ratio + synth_cost_to_price) / 3.0
        eng.append(short_pressure)
        
        # 11. Short Interest to Days to Cover Ratio Change
        # Captures changes in the relationship between SI and DTC
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        si_dtc_ratio_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_dtc = data[t-1, 2]
            prev_ratio = prev_si / max(prev_dtc, 1e-8)
            si_dtc_ratio_change = (si_dtc_ratio / max(prev_ratio, 1e-8)) - 1.0
        eng.append(si_dtc_ratio_change)
        
        # 12. Put-Call Ratio Normalized by Short Interest
        # Relates options market sentiment to short interest
        pc_si_ratio = options_put_call_ratio / max(si_ratio, 1e-8)
        pc_si_ratio = min(pc_si_ratio, 10.0)  # Cap extreme values
        eng.append(pc_si_ratio)
        
        # 13. NEW: Short Interest Trend Strength
        # Measures consistency of short interest direction over time
        si_trend_strength = 0.0
        if t >= 3:
            # Get last 4 short interest values
            si_values = [data[t-i, 0] for i in range(4)]
            # Calculate consecutive differences
            diffs = [si_values[i] - si_values[i+1] for i in range(3)]
            # Check if all differences have the same sign (consistent trend)
            if all(d > 0 for d in diffs):
                # Upward trend strength
                si_trend_strength = sum(diffs) / max(si_values[0], 1e-8)
            elif all(d < 0 for d in diffs):
                # Downward trend strength
                si_trend_strength = sum(diffs) / max(si_values[0], 1e-8)
        eng.append(si_trend_strength)
        
        # 14. NEW: Price Volatility Adjusted Short Interest
        # Adjusts short interest by recent price volatility to capture risk-adjusted short exposure
        price_volatility = 0.0
        if len(close_prices) >= 5:
            returns = [close_prices[i]/max(close_prices[i-1], 1e-8) - 1 for i in range(1, 5)]
            price_volatility = np.std(returns) if returns else 0.0
        
        vol_adjusted_si = short_interest * (1 + price_volatility)
        eng.append(vol_adjusted_si)
        
        # 15. NEW: Short Interest Concentration
        # Measures how concentrated short interest is relative to trading volume and float
        # Higher values indicate more concentrated short positions
        si_concentration = (si_ratio * days_to_cover) / max(options_avg_implied_volatility, 1e-8)
        si_concentration = min(si_concentration, 10.0)  # Cap extreme values
        eng.append(si_concentration)
        
        # 16. NEW: Short Interest Divergence from Price
        # Measures divergence between short interest and price movements
        # Positive values: short interest increasing while price increases (potential short squeeze)
        # Negative values: short interest decreasing while price decreases (shorts covering)
        si_price_divergence = 0.0
        if t > 0 and len(close_prices) >= 2:
            prev_si = data[t-1, 0]
            prev_close = close_prices[-2]
            
            si_change_norm = (short_interest - prev_si) / max(prev_si, 1e-8)
            price_change_norm = (last_close - prev_close) / max(prev_close, 1e-8)
            
            # Positive when SI and price move in same direction, negative when opposite
            si_price_divergence = si_change_norm * price_change_norm
        eng.append(si_price_divergence)
        
        # 17. NEW: Normalized Days to Cover Change
        # Captures changes in days to cover normalized by its previous value
        dtc_change = 0.0
        if t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (days_to_cover - prev_dtc) / max(prev_dtc, 1e-8)
        eng.append(dtc_change)
        
        # 18. NEW: Synthetic Short Cost Momentum
        # Captures changes in the cost of shorting
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            synth_cost_momentum = (options_synthetic_short_cost - prev_cost) / max(prev_cost, 1e-8)
        eng.append(synth_cost_momentum)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 7 (claude) (SVM)
==================================================
Training SVM model...

Iteration 7 (claude) Performance:
MAE: 47082.2272
RMSE: 62171.1351
MAPE: 14.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0007, rank=1
   2. Feature_18_t2: importance=0.0006, rank=2
   3. Feature_12_t2: importance=0.0005, rank=3
   4. Feature_8_t2: importance=0.0004, rank=4
   5. Feature_19_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 7 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.07%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 8
======================================================================
ðŸ¤– Calling Claude for iteration 8...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important raw features
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance in baseline
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance in baseline
        volume = data[t, 68] if data.shape[1] > 68 else avg_volume  # Fallback if not available
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (required)
        raw_keep.extend([short_interest, avg_volume])
        
        # Keep days to cover (consistently high importance)
        raw_keep.append(days_to_cover)
        
        # Keep last close price (most recent is most relevant)
        last_close = close_prices[-1]
        raw_keep.append(last_close)
        
        # Keep synthetic short cost (high importance in baseline)
        raw_keep.append(options_synthetic_short_cost)
        
        # Keep shares outstanding (high importance in baseline)
        raw_keep.append(shares_outstanding)
        
        # Keep options implied volatility (important for short interest dynamics)
        raw_keep.append(options_avg_implied_volatility)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Important relationship showing what percentage of float is shorted
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - critical for prediction
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest to Volume Ratio
        # Relationship between short interest and trading volume - consistently important
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 4. Synthetic Short Cost to Price Ratio
        # Cost of shorting relative to price - high importance in previous iterations
        synth_cost_to_price = options_synthetic_short_cost / max(last_close, 1e-8)
        eng.append(synth_cost_to_price)
        
        # 5. Short Interest Momentum (weighted)
        # Weighted version of short interest momentum - high importance in previous iteration
        si_momentum_weighted = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate weighted changes
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            change_3 = (si_2 - si_3) / max(si_3, 1e-8)
            
            # Apply exponential weighting
            si_momentum_weighted = 0.6 * change_1 + 0.3 * change_2 + 0.1 * change_3
        eng.append(si_momentum_weighted)
        
        # 6. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and market expectations - high importance in previous iteration
        si_iv_ratio = short_interest / max(options_avg_implied_volatility * shares_outstanding, 1e-8)
        eng.append(si_iv_ratio)
        
        # 7. Short Squeeze Potential Indicator
        # Combines multiple factors that could indicate a short squeeze - high importance in previous iterations
        short_squeeze_potential = 0.0
        if si_ratio > 0.1:  # Only meaningful if short interest is significant
            # Calculate recent price momentum
            momentum_5d = 0.0
            if len(close_prices) >= 5:
                momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Factors that contribute to short squeeze potential
            squeeze_factors = [
                min(si_ratio * 5, 1.0),  # Normalize SI ratio (cap at 20%)
                max(min(momentum_5d * 5, 1.0), 0) if momentum_5d > 0 else 0,  # Recent price increase
                min(options_avg_implied_volatility / 0.5, 1.0),  # High IV (normalized to 50%)
                min(days_to_cover / 10, 1.0)  # High days to cover (normalized to 10 days)
            ]
            
            # Weight the factors
            weights = [0.4, 0.3, 0.15, 0.15]
            short_squeeze_potential = sum(f * w for f, w in zip(squeeze_factors, weights))
        eng.append(short_squeeze_potential)
        
        # 8. Short Interest Trend Strength
        # Measures consistency of short interest direction over time
        si_trend_strength = 0.0
        if t >= 3:
            # Get last 4 short interest values
            si_values = [data[t-i, 0] for i in range(4)]
            # Calculate consecutive differences
            diffs = [si_values[i] - si_values[i+1] for i in range(3)]
            # Check if all differences have the same sign (consistent trend)
            if all(d > 0 for d in diffs):
                # Upward trend strength
                si_trend_strength = sum(diffs) / max(si_values[0], 1e-8)
            elif all(d < 0 for d in diffs):
                # Downward trend strength
                si_trend_strength = sum(diffs) / max(si_values[0], 1e-8)
        eng.append(si_trend_strength)
        
        # 9. Price Volatility Adjusted Short Interest
        # Adjusts short interest by recent price volatility to capture risk-adjusted short exposure
        price_volatility = 0.0
        if len(close_prices) >= 5:
            returns = [close_prices[i]/max(close_prices[i-1], 1e-8) - 1 for i in range(1, 5)]
            price_volatility = np.std(returns) if returns else 0.0
        
        vol_adjusted_si = short_interest * (1 + price_volatility)
        eng.append(vol_adjusted_si)
        
        # 10. Short Interest Concentration
        # Measures how concentrated short interest is relative to trading volume and float
        si_concentration = (si_ratio * days_to_cover) / max(options_avg_implied_volatility, 1e-8)
        si_concentration = min(si_concentration, 10.0)  # Cap extreme values
        eng.append(si_concentration)
        
        # 11. Synthetic Short Cost Momentum
        # Captures changes in the cost of shorting - high importance in previous iteration
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            synth_cost_momentum = (options_synthetic_short_cost - prev_cost) / max(prev_cost, 1e-8)
        eng.append(synth_cost_momentum)
        
        # 12. NEW: Short Interest Relative to Historical Range
        # Measures current short interest relative to its historical range
        si_relative_to_range = 0.0
        if t >= 5:
            historical_si = [data[t-i, 0] for i in range(6)]
            si_min = min(historical_si)
            si_max = max(historical_si)
            si_range = si_max - si_min
            if si_range > 1e-8:
                si_relative_to_range = (short_interest - si_min) / si_range
            else:
                si_relative_to_range = 0.5  # Default to middle if range is too small
        eng.append(si_relative_to_range)
        
        # 13. NEW: Short Interest to Put-Call Ratio Correlation
        # Measures alignment between short interest and options market sentiment
        si_pc_correlation = 0.0
        if t >= 4:
            si_changes = []
            pc_changes = []
            for i in range(1, 5):
                if t >= i:
                    prev_si = data[t-i, 0]
                    curr_si = data[t-i+1, 0]
                    si_change = (curr_si - prev_si) / max(prev_si, 1e-8)
                    
                    prev_pc = data[t-i, 64]
                    curr_pc = data[t-i+1, 64]
                    pc_change = (curr_pc - prev_pc) / max(prev_pc, 1e-8)
                    
                    si_changes.append(si_change)
                    pc_changes.append(pc_change)
            
            # Calculate a simple correlation-like measure
            if len(si_changes) >= 3:
                # Normalize the changes
                si_mean = sum(si_changes) / len(si_changes)
                pc_mean = sum(pc_changes) / len(pc_changes)
                
                si_norm = [s - si_mean for s in si_changes]
                pc_norm = [p - pc_mean for p in pc_changes]
                
                # Calculate dot product
                dot_product = sum(s * p for s, p in zip(si_norm, pc_norm))
                
                # Calculate magnitudes
                si_mag = max(sum(s * s for s in si_norm) ** 0.5, 1e-8)
                pc_mag = max(sum(p * p for p in pc_norm) ** 0.5, 1e-8)
                
                # Calculate correlation-like measure
                si_pc_correlation = dot_product / (si_mag * pc_mag)
        eng.append(si_pc_correlation)
        
        # 14. NEW: Short Interest Efficiency Ratio
        # Measures how efficiently short sellers are timing their positions
        # Higher values indicate shorts are more efficiently timing market tops
        si_efficiency = 0.0
        if t >= 3:
            # Get recent short interest and price changes
            si_changes = []
            price_changes = []
            for i in range(1, 4):
                if t >= i and i < len(close_prices):
                    prev_si = data[t-i, 0]
                    curr_si = data[t-i+1, 0]
                    si_change = (curr_si - prev_si) / max(prev_si, 1e-8)
                    
                    prev_close = close_prices[-i-1] if i < len(close_prices)-1 else close_prices[0]
                    curr_close = close_prices[-i]
                    price_change = (curr_close - prev_close) / max(prev_close, 1e-8)
                    
                    si_changes.append(si_change)
                    price_changes.append(price_change)
            
            # Calculate efficiency: positive when SI increases before price drops
            # and when SI decreases before price rises
            if len(si_changes) >= 2 and len(price_changes) >= 2:
                efficiency_terms = []
                for i in range(len(si_changes)-1):
                    # SI change followed by opposite price change is efficient
                    efficiency_terms.append(-1 * si_changes[i] * price_changes[i+1])
                
                si_efficiency = sum(efficiency_terms) / len(efficiency_terms)
        eng.append(si_efficiency)
        
        # 15. NEW: Short Interest Volatility
        # Measures the volatility of short interest changes
        si_volatility = 0.0
        if t >= 4:
            si_changes = []
            for i in range(1, 5):
                if t >= i:
                    prev_si = data[t-i, 0]
                    curr_si = data[t-i+1, 0]
                    si_change = (curr_si - prev_si) / max(prev_si, 1e-8)
                    si_changes.append(si_change)
            
            if si_changes:
                si_volatility = np.std(si_changes)
        eng.append(si_volatility)
        
        # 16. NEW: Short Interest Divergence from Implied Volatility
        # Measures divergence between short interest and implied volatility
        si_iv_divergence = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_iv = data[t-1, 66]
            
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            iv_change = (options_avg_implied_volatility - prev_iv) / max(prev_iv, 1e-8)
            
            # Positive when they move in opposite directions
            si_iv_divergence = si_change * (-1 * iv_change)
        eng.append(si_iv_divergence)
        
        # 17. NEW: Normalized Short Interest Acceleration
        # Improved version of SI acceleration normalized by SI level
        si_accel_norm = 0.0
        if t >= 2:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            
            change_1 = (si_0 - si_1) / max(si_1, 1e-8)
            change_2 = (si_1 - si_2) / max(si_2, 1e-8)
            
            si_accel = change_1 - change_2
            si_accel_norm = si_accel / max(abs(si_0), 1e-8)
        eng.append(si_accel_norm)
        
        # 18. NEW: Short Interest Reversal Signal
        # Detects potential reversals in short interest trends
        si_reversal = 0.0
        if t >= 3:
            si_0 = data[t, 0]
            si_1 = data[t-1, 0]
            si_2 = data[t-2, 0]
            si_3 = data[t-3, 0]
            
            # Calculate consecutive changes
            change_1 = si_0 - si_1
            change_2 = si_1 - si_2
            change_3 = si_2 - si_3
            
            # Reversal occurs when the sign of changes flips
            if (change_1 * change_2 < 0) and (abs(change_1) > abs(change_2) * 0.5):
                # Strong reversal
                si_reversal = change_1 / max(abs(si_1), 1e-8)
            elif (change_2 * change_3 < 0) and (abs(change_2) > abs(change_3) * 0.5):
                # Recent reversal
                si_reversal = change_2 / max(abs(si_2), 1e-8) * 0.5
        eng.append(si_reversal)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 8 (claude) (SVM)
==================================================
Training SVM model...

Iteration 8 (claude) Performance:
MAE: 47444.8333
RMSE: 61218.6828
MAPE: 15.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0006, rank=1
   2. Feature_2_t3: importance=0.0006, rank=2
   3. Feature_20_t3: importance=0.0006, rank=3
   4. Feature_6_t3: importance=0.0006, rank=4
   5. Feature_6_t2: importance=0.0006, rank=5
ðŸ’¾ Saved code for iteration 8 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.36%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 56904.6234
RMSE: 72986.9420
MAPE: 15.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 196
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0007, rank=1
   2. Feature_1_t2: importance=0.0007, rank=2
   3. Feature_65_t3: importance=0.0007, rank=3
   4. Feature_65_t0: importance=0.0006, rank=4
   5. Feature_64_t2: importance=0.0005, rank=5
   Baseline MAPE: 15.11%
   Baseline MAE: 56904.6234
   Baseline RMSE: 72986.9420

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 57065.8847
RMSE: 73079.5753
MAPE: 15.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_8_t1: importance=0.0009, rank=1
   2. Feature_5_t3: importance=0.0008, rank=2
   3. Feature_22_t3: importance=0.0007, rank=3
   4. Feature_12_t2: importance=0.0006, rank=4
   5. Feature_15_t2: importance=0.0006, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 15.36%
   MAE: 57065.8847
   RMSE: 73079.5753

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 15.11%
   Best Model MAPE: 15.36%
   Absolute Improvement: -0.26%
   Relative Improvement: -1.7%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  15.65           N/A                 
1          Iteration 1               15.25           +0.41%              
2          Iteration 2               15.41           -0.16%              
3          Iteration 3               14.76           +0.49%              
4          Iteration 4               14.77           -0.01%              
5          Iteration 5               14.87           -0.12%              
6          Iteration 6               14.67           +0.09%              
7          Iteration 7               14.83           -0.07%              
8          Iteration 8               15.12           -0.36%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 6 - MAPE: 14.67%
âœ… Saved EIG results to cache/EIG_iterative_results_enhanced.pkl
âœ… Summary report saved for EIG

ðŸŽ‰ Process completed successfully for EIG!

================================================================================
PROCESSING TICKER 4/15: EYE
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for EYE
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for EYE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error processing EYE: 'EYE'
âš ï¸ Skipping EYE and continuing with next ticker...

================================================================================
PROCESSING TICKER 5/15: AAP
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for AAP
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for AAP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error processing AAP: 'AAP'
âš ï¸ Skipping AAP and continuing with next ticker...

================================================================================
PROCESSING TICKER 6/15: FSS
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for FSS
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for FSS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FSS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 84720.1036
RMSE: 112209.8663
MAPE: 11.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 106
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0021, rank=1
   2. Feature_0_t3: importance=0.0019, rank=2
   3. Feature_63_t1: importance=0.0019, rank=3
   4. Feature_67_t0: importance=0.0018, rank=4
   5. Feature_2_t2: importance=0.0016, rank=5

ðŸ“Š Baseline Performance: MAPE = 11.77%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (critical baseline features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep high importance features from previous iteration
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost (Feature_65)
        raw_keep.append(data[t, 67])  # shares_outstanding (Feature_67)
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Add options data which showed importance
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        raw_keep.append(data[t, 68])  # volume
        
        # Calculate MAX_NEW based on how many raw features we're keeping
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio to Outstanding Shares
        # This normalizes short interest by the total shares available
        si_to_outstanding = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_outstanding)
        
        # 2. Short Interest to Volume Ratio
        # Measures how many days of current volume would be needed to cover all short positions
        si_to_volume = data[t, 0] / max(data[t, 68], 1e-8)
        eng.append(si_to_volume)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend direction and strength
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 4. Price Volatility (standard deviation of returns)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
        else:
            volatility = 0.0
        eng.append(volatility)
        
        # 5. RSI (Relative Strength Index)
        # Indicates overbought/oversold conditions which may attract short sellers
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 14
            avg_loss = loss / 14
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50.0  # Neutral RSI when not enough data
        eng.append(rsi)
        
        # 6. Put-Call Ratio Change
        # Rate of change in options sentiment
        if t > 0:
            pc_ratio_change = data[t, 64] / max(data[t-1, 64], 1e-8) - 1.0
        else:
            pc_ratio_change = 0.0
        eng.append(pc_ratio_change)
        
        # 7. Implied Volatility to Historical Volatility Ratio
        # Measures market's expectation vs realized volatility
        iv_hv_ratio = data[t, 66] / max(volatility, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 8. Short Cost Momentum
        # Tracks changes in cost to short, which can predict short interest changes
        if t > 0:
            short_cost_momentum = data[t, 65] / max(data[t-1, 65], 1e-8) - 1.0
        else:
            short_cost_momentum = 0.0
        eng.append(short_cost_momentum)
        
        # 9. Volume Spike Indicator
        # Unusual volume often precedes short interest changes
        if len(close_prices) >= 5:
            avg_vol_5d = np.mean(data[max(0, t-4):t+1, 68])
            vol_spike = data[t, 68] / max(avg_vol_5d, 1e-8) - 1.0
        else:
            vol_spike = 0.0
        eng.append(vol_spike)
        
        # 10. Price Range Ratio
        # Wider price ranges may indicate increased volatility and short interest
        if len(high_prices) > 0 and len(low_prices) > 0:
            price_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
        else:
            price_range = 0.0
        eng.append(price_range)
        
        # 11. Short Interest Momentum
        # Rate of change in short interest
        if t > 0:
            si_momentum = data[t, 0] / max(data[t-1, 0], 1e-8) - 1.0
        else:
            si_momentum = 0.0
        eng.append(si_momentum)
        
        # 12. Days to Cover Change
        # Tracks changes in short covering difficulty
        if t > 0:
            dtc_change = data[t, 2] / max(data[t-1, 2], 1e-8) - 1.0
        else:
            dtc_change = 0.0
        eng.append(dtc_change)
        
        # 13. Short Interest to Float Ratio
        # Measures short interest relative to tradable shares
        si_to_float = data[t, 0] / max(data[t, 67] * 0.8, 1e-8)  # Assuming ~80% of shares are in float
        eng.append(si_to_float)
        
        # 14. MACD Signal
        # Momentum indicator that can signal potential reversals
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
        else:
            macd = 0.0
        eng.append(macd)
        
        # 15. Bollinger Band Width
        # Measures volatility which can attract short sellers
        if len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_width = (2 * std20) / max(sma20, 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 16. Options Leverage Ratio
        # Measures potential leverage effect from options on stock price
        options_leverage = data[t, 64] * data[t, 66] / max(data[t, 1], 1e-8)
        eng.append(options_leverage)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Clean up any NaN or infinite values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_volume)      # Average daily volume
        raw_keep.append(days_to_cover)   # Days to cover
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep important options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        raw_keep.append(put_call_ratio)       # Options put/call volume ratio
        raw_keep.append(synthetic_short_cost)  # Options synthetic short cost
        raw_keep.append(implied_volatility)    # Options avg implied volatility
        raw_keep.append(shares_outstanding)    # Shares outstanding
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price volatility (normalized range)
        # Higher volatility often correlates with short interest changes
        recent_highs = high_prices[-5:]
        recent_lows = low_prices[-5:]
        if len(recent_highs) > 0 and len(recent_lows) > 0:
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
            
        # Feature 2: Short interest to float ratio
        # Higher ratio indicates more shares are sold short relative to available float
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 3: Relative volume
        # Unusual volume can signal short covering or increased short positions
        if len(close_prices) >= 10:
            recent_volume = volume
            denom = max(abs(avg_volume), 1e-8)
            relative_volume = recent_volume / denom
            eng.append(relative_volume)
        else:
            eng.append(0.0)
            
        # Feature 4: RSI (Relative Strength Index)
        # Extreme RSI values often precede short interest changes
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            denom = max(abs(loss), 1e-8)
            rs = gain / denom
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(0.0)
            
        # Feature 5: Price momentum (5-day)
        # Short sellers often target stocks with negative momentum
        if len(close_prices) >= 5:
            momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            eng.append(momentum)
        else:
            eng.append(0.0)
            
        # Feature 6: Short interest momentum
        # Rate of change in short interest can signal trend continuation/reversal
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
            
        # Feature 7: Bollinger Band width
        # Volatility measure that can indicate potential short squeezes
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(sma), 1e-8)
            bb_width = (2 * std) / denom
            eng.append(bb_width)
        else:
            eng.append(0.0)
            
        # Feature 8: Short cost to volatility ratio
        # Expensive shorts with low volatility may indicate crowded shorts
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 9: Days to cover momentum
        # Increasing days to cover can signal potential short squeeze
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_momentum = (days_to_cover / denom) - 1
            eng.append(dtc_momentum)
        else:
            eng.append(0.0)
            
        # Feature 10: Price to moving average ratio
        # Deviation from moving average can signal potential reversals
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            denom = max(abs(sma10), 1e-8)
            price_to_ma = close_prices[-1] / denom
            eng.append(price_to_ma)
        else:
            eng.append(0.0)
            
        # Feature 11: OHLC spread
        # Wide spreads can indicate volatility attractive to short sellers
        if len(open_prices) > 0 and len(close_prices) > 0:
            recent_open = open_prices[-1]
            recent_close = close_prices[-1]
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            denom = max(abs(recent_open), 1e-8)
            ohlc_spread = (recent_high - recent_low) / denom
            eng.append(ohlc_spread)
        else:
            eng.append(0.0)
            
        # Feature 12: Short interest to volume ratio
        # Indicates how many days of volume the short interest represents
        denom = max(abs(volume), 1e-8)
        si_volume_ratio = short_interest / denom
        eng.append(si_volume_ratio)
        
        # Feature 13: Implied volatility change
        # Changes in implied volatility can signal changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            denom = max(abs(prev_iv), 1e-8)
            iv_change = (implied_volatility / denom) - 1
            eng.append(iv_change)
        else:
            eng.append(0.0)
            
        # Feature 14: Put-call ratio momentum
        # Changing options sentiment can precede short interest changes
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_momentum = (put_call_ratio / denom) - 1
            eng.append(pc_momentum)
        else:
            eng.append(0.0)
            
        # Feature 15: Price trend direction
        # Trend direction can indicate potential for short positions
        if len(close_prices) >= 5:
            up_days = np.sum(np.diff(close_prices[-5:]) > 0)
            trend_direction = (up_days / 4.0) * 2 - 1  # Scale to [-1, 1]
            eng.append(trend_direction)
        else:
            eng.append(0.0)
            
        # Feature 16: Synthetic short cost momentum
        # Changes in borrowing costs can signal changing short demand
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_momentum = (synthetic_short_cost / denom) - 1
            eng.append(cost_momentum)
        else:
            eng.append(0.0)
            
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
            
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 97073.8805
RMSE: 129049.9249
MAPE: 12.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0037, rank=1
   2. Feature_11_t0: importance=0.0026, rank=2
   3. Feature_11_t2: importance=0.0019, rank=3
   4. Feature_22_t3: importance=0.0017, rank=4
   5. Feature_13_t3: importance=0.0017, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.83%

ðŸ“ˆ Current best MAPE: 11.77%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and average volume (critical for prediction)
        raw_keep.append(short_interest)
        raw_keep.append(avg_volume)
        raw_keep.append(days_to_cover)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # Feature_65 had high importance
        raw_keep.append(shares_outstanding)  # Feature_67 had high importance
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: OHLC spread (Feature_11 had high importance in previous iteration)
        # Wide spreads indicate volatility that may attract short sellers
        if len(open_prices) > 0:
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            recent_close = close_prices[-1]
            denom = max(abs(recent_close), 1e-8)
            ohlc_spread = (recent_high - recent_low) / denom
            eng.append(ohlc_spread)
        else:
            eng.append(0.0)
        
        # Feature 2: Implied volatility change (Feature_13 had high importance)
        # Changes in implied volatility can signal changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            denom = max(abs(prev_iv), 1e-8)
            iv_change = (implied_volatility / denom) - 1
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Short interest to float ratio
        # Key metric for short squeeze potential
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 4: Short interest momentum (rate of change)
        # Acceleration in short interest can signal trend changes
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
        
        # Feature 5: Price volatility (normalized range over 5 days)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
        
        # Feature 6: Short cost to volatility ratio
        # Expensive shorts with low volatility may indicate crowded shorts
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 7: Days to cover momentum
        # Increasing days to cover can signal potential short squeeze
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_momentum = (days_to_cover / denom) - 1
            eng.append(dtc_momentum)
        else:
            eng.append(0.0)
        
        # Feature 8: Price momentum (5-day)
        # Short sellers often target stocks with negative momentum
        if len(close_prices) >= 5:
            momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            eng.append(momentum)
        else:
            eng.append(0.0)
        
        # Feature 9: Bollinger Band squeeze indicator
        # Tight BBs followed by expansion often precede significant moves
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(sma), 1e-8)
            bb_width = (2 * std) / denom
            
            # Compare current BB width to previous period
            if t > 0 and len(data[t-1, 3:63].reshape(15, 4)[:, 3]) >= 10:
                prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3]
                prev_sma = np.mean(prev_close[-10:])
                prev_std = np.std(prev_close[-10:])
                prev_denom = max(abs(prev_sma), 1e-8)
                prev_bb_width = (2 * prev_std) / prev_denom
                
                # BB squeeze indicator: positive when bands are expanding
                denom = max(abs(prev_bb_width), 1e-8)
                bb_squeeze = (bb_width / denom) - 1
                eng.append(bb_squeeze)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # Feature 10: Volume spike indicator
        # Unusual volume can signal short covering or increased short positions
        if len(close_prices) >= 5 and avg_volume > 0:
            recent_volume = volume
            avg_5d_volume = np.mean([data[max(0, t-i), 68] if data.shape[1] > 68 else 0.0 for i in range(5)])
            denom = max(abs(avg_5d_volume), 1e-8)
            volume_spike = recent_volume / denom - 1
            eng.append(volume_spike)
        else:
            eng.append(0.0)
        
        # Feature 11: RSI (Relative Strength Index)
        # Extreme RSI values often precede short interest changes
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss == 0:
                rsi = 100.0
            else:
                rs = gain / max(loss, 1e-8)
                rsi = 100 - (100 / (1 + rs))
            
            # RSI extremes are more important than middle values
            # Transform to emphasize extremes: 0-30 â†’ 0-1, 70-100 â†’ 1-2, 30-70 â†’ 0.5
            if rsi < 30:
                rsi_transformed = rsi / 30
            elif rsi > 70:
                rsi_transformed = 1 + (rsi - 70) / 30
            else:
                rsi_transformed = 0.5
                
            eng.append(rsi_transformed)
        else:
            eng.append(0.0)
        
        # Feature 12: Put-call ratio momentum
        # Changing options sentiment can precede short interest changes
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_momentum = (put_call_ratio / denom) - 1
            eng.append(pc_momentum)
        else:
            eng.append(0.0)
        
        # Feature 13: Short interest to volume ratio
        # Indicates how many days of volume the short interest represents
        if volume > 0:
            denom = max(abs(volume), 1e-8)
            si_volume_ratio = short_interest / denom
            eng.append(si_volume_ratio)
        else:
            eng.append(0.0)
        
        # Feature 14: Price trend strength
        # Combines direction and magnitude of recent price moves
        if len(close_prices) >= 5:
            price_changes = np.diff(close_prices[-5:])
            up_moves = np.sum(np.where(price_changes > 0, price_changes, 0))
            down_moves = np.sum(np.where(price_changes < 0, -price_changes, 0))
            
            if up_moves + down_moves > 0:
                trend_strength = (up_moves - down_moves) / (up_moves + down_moves)
            else:
                trend_strength = 0.0
                
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 15: Synthetic short cost momentum
        # Changes in borrowing costs can signal changing short demand
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_momentum = (synthetic_short_cost / denom) - 1
            eng.append(cost_momentum)
        else:
            eng.append(0.0)
        
        # Feature 16: Price gap indicator
        # Overnight gaps often indicate significant sentiment shifts
        if t > 0 and len(open_prices) > 0 and len(data[t-1, 3:63].reshape(15, 4)[:, 3]) > 0:
            prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3][-1]
            current_open = open_prices[-1]
            denom = max(abs(prev_close), 1e-8)
            gap = (current_open - prev_close) / denom
            eng.append(gap)
        else:
            eng.append(0.0)
        
        # Feature 17: Short interest acceleration
        # Second derivative of short interest can signal trend changes
        if t > 1:
            si_t = short_interest
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            mom_t = (si_t / max(abs(si_t1), 1e-8)) - 1
            mom_t1 = (si_t1 / max(abs(si_t2), 1e-8)) - 1
            
            acceleration = mom_t - mom_t1
            eng.append(acceleration)
        else:
            eng.append(0.0)
        
        # Feature 18: Volatility ratio (implied vs. realized)
        # Discrepancy between implied and realized volatility can signal market mispricing
        if len(close_prices) >= 10:
            # Calculate realized volatility (annualized)
            returns = np.diff(close_prices[-10:]) / np.maximum(np.abs(close_prices[-11:-1]), 1e-8)
            realized_vol = np.std(returns) * np.sqrt(252)
            
            denom = max(abs(realized_vol), 1e-8)
            vol_ratio = implied_volatility / denom
            eng.append(vol_ratio)
        else:
            eng.append(0.0)
        
        # Feature 19: Combined short pressure indicator
        # Combines multiple short-related metrics into a single indicator
        short_to_float_norm = min(max(short_to_float * 5, 0), 1)  # Normalize to 0-1 range
        days_to_cover_norm = min(max(days_to_cover / 10, 0), 1)   # Normalize to 0-1 range
        cost_norm = min(max(synthetic_short_cost / 0.05, 0), 1)   # Normalize to 0-1 range
        
        short_pressure = (short_to_float_norm + days_to_cover_norm + cost_norm) / 3
        eng.append(short_pressure)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
            
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (9,) (10,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep volume if available
        volume = 0.0
        if data.shape[1] > 68:
            volume = data[t, 68]
        
        # Keep essential raw features based on importance analysis
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_volume)      # Average daily volume - always keep
        raw_keep.append(days_to_cover)   # Days to cover - high importance
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost - high importance
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(shares_outstanding)  # Shares outstanding - high importance
        raw_keep.append(implied_volatility)  # Implied volatility - high importance
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio
        # Higher ratio indicates more shares are sold short relative to available float
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 2: Short cost to volatility ratio (high importance in previous iteration)
        # Expensive shorts with low volatility may indicate crowded shorts
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 3: OHLC spread (high importance in previous iteration)
        # Wide spreads can indicate volatility attractive to short sellers
        if len(open_prices) > 0 and len(close_prices) > 0:
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            denom = max(abs(close_prices[-1]), 1e-8)
            ohlc_spread = (recent_high - recent_low) / denom
            eng.append(ohlc_spread)
        else:
            eng.append(0.0)
        
        # Feature 4: Implied volatility change (high importance in previous iteration)
        # Changes in implied volatility can signal changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            denom = max(abs(prev_iv), 1e-8)
            iv_change = (implied_volatility / denom) - 1
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 5: Short interest momentum
        # Rate of change in short interest can signal trend continuation/reversal
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
        
        # Feature 6: Days to cover momentum
        # Increasing days to cover can signal potential short squeeze
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_momentum = (days_to_cover / denom) - 1
            eng.append(dtc_momentum)
        else:
            eng.append(0.0)
        
        # Feature 7: Price volatility (normalized range)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
        
        # Feature 8: Bollinger Band width
        # Volatility measure that can indicate potential short squeezes
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(sma), 1e-8)
            bb_width = (2 * std) / denom
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 9: Price momentum (5-day)
        # Short sellers often target stocks with negative momentum
        if len(close_prices) >= 5:
            momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            eng.append(momentum)
        else:
            eng.append(0.0)
        
        # Feature 10: RSI (Relative Strength Index)
        # Extreme RSI values often precede short interest changes
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            denom = max(abs(avg_loss), 1e-8)
            rs = avg_gain / denom
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(0.0)
        
        # Feature 11: Synthetic short cost change
        # Changes in borrowing costs can signal changing short demand
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_change = (synthetic_short_cost / denom) - 1
            eng.append(cost_change)
        else:
            eng.append(0.0)
        
        # Feature 12: Put-call ratio change
        # Changing options sentiment can precede short interest changes
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_change = (put_call_ratio / denom) - 1
            eng.append(pc_change)
        else:
            eng.append(0.0)
        
        # Feature 13: Short interest to volume ratio
        # Indicates how many days of volume the short interest represents
        if volume > 0:
            denom = max(abs(volume), 1e-8)
            si_volume_ratio = short_interest / denom
            eng.append(si_volume_ratio)
        else:
            denom = max(abs(avg_volume), 1e-8)
            si_volume_ratio = short_interest / denom
            eng.append(si_volume_ratio)
        
        # Feature 14: Price trend strength
        # Trend strength can indicate potential for short positions
        if len(close_prices) >= 10:
            # Calculate linear regression slope
            x = np.arange(10)
            y = close_prices[-10:]
            mean_x = np.mean(x)
            mean_y = np.mean(y)
            numerator = np.sum((x - mean_x) * (y - mean_y))
            denominator = max(np.sum((x - mean_x) ** 2), 1e-8)
            slope = numerator / denominator
            # Normalize slope by average price
            denom = max(abs(mean_y), 1e-8)
            norm_slope = slope / denom
            eng.append(norm_slope)
        else:
            eng.append(0.0)
        
        # Feature 15: Volatility ratio (short-term vs long-term)
        # Changing volatility regimes can signal potential short interest changes
        if len(close_prices) >= 10:
            short_vol = np.std(close_prices[-5:])
            long_vol = np.std(close_prices[-10:])
            denom = max(abs(long_vol), 1e-8)
            vol_ratio = short_vol / denom
            eng.append(vol_ratio)
        else:
            eng.append(0.0)
        
        # Feature 16: Gap analysis
        # Large overnight gaps can trigger short covering or new short positions
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 17: Price to moving average ratio
        # Deviation from moving average can signal potential reversals
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            denom = max(abs(sma10), 1e-8)
            price_to_ma = close_prices[-1] / denom
            eng.append(price_to_ma)
        else:
            eng.append(0.0)
        
        # Feature 18: Short interest acceleration
        # Second derivative of short interest can indicate changing trends
        if t >= 2:
            prev_short = data[t-1, 0]
            prev_prev_short = data[t-2, 0]
            prev_momentum = (prev_short / max(abs(prev_prev_short), 1e-8)) - 1
            current_momentum = (short_interest / max(abs(prev_short), 1e-8)) - 1
            short_acceleration = current_momentum - prev_momentum
            eng.append(short_acceleration)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 97168.6826
RMSE: 126734.8787
MAPE: 12.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0019, rank=1
   2. Feature_16_t0: importance=0.0017, rank=2
   3. Feature_11_t2: importance=0.0015, rank=3
   4. Feature_0_t3: importance=0.0013, rank=4
   5. Feature_20_t0: importance=0.0013, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.99%

ðŸ“ˆ Current best MAPE: 11.77%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep volume if available
        volume = 0.0
        if data.shape[1] > 68:
            volume = data[t, 68]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume
        raw_keep.append(short_interest)
        raw_keep.append(avg_volume)
        
        # Keep high-importance features from previous iterations
        raw_keep.append(days_to_cover)
        raw_keep.append(synthetic_short_cost)
        raw_keep.append(shares_outstanding)
        
        # Keep only the most recent close price instead of all OHLC data
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Gap analysis - consistently high importance in previous iterations
        # Large overnight gaps can trigger short covering or new short positions
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 2: Short interest to float ratio
        # Higher ratio indicates more shares are sold short relative to available float
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 3: Short cost to volatility ratio - high importance in previous iterations
        # Expensive shorts with low volatility may indicate crowded shorts
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 4: Short interest momentum - rate of change
        # Rate of change in short interest can signal trend continuation/reversal
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
        
        # Feature 5: Synthetic short cost change - high importance in previous iterations
        # Changes in borrowing costs can signal changing short demand
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_change = (synthetic_short_cost / denom) - 1
            eng.append(cost_change)
        else:
            eng.append(0.0)
        
        # Feature 6: Price volatility (normalized range)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
        
        # Feature 7: Short interest acceleration - second derivative
        # Second derivative of short interest can indicate changing trends
        if t >= 2:
            prev_short = data[t-1, 0]
            prev_prev_short = data[t-2, 0]
            prev_momentum = (prev_short / max(abs(prev_prev_short), 1e-8)) - 1
            current_momentum = (short_interest / max(abs(prev_short), 1e-8)) - 1
            short_acceleration = current_momentum - prev_momentum
            eng.append(short_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 8: Days to cover change
        # Increasing days to cover can signal potential short squeeze
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_change = (days_to_cover / denom) - 1
            eng.append(dtc_change)
        else:
            eng.append(0.0)
        
        # Feature 9: Short interest to volume ratio
        # Indicates how many days of volume the short interest represents
        denom = max(abs(avg_volume), 1e-8)
        si_volume_ratio = short_interest / denom
        eng.append(si_volume_ratio)
        
        # Feature 10: Price trend - 5-day momentum
        # Short sellers often target stocks with negative momentum
        if len(close_prices) >= 5:
            price_5d_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            eng.append(price_5d_momentum)
        else:
            eng.append(0.0)
        
        # Feature 11: OHLC spread of most recent day
        # Wide spreads can indicate volatility attractive to short sellers
        if len(open_prices) > 0 and len(close_prices) > 0:
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            denom = max(abs(close_prices[-1]), 1e-8)
            recent_spread = (recent_high - recent_low) / denom
            eng.append(recent_spread)
        else:
            eng.append(0.0)
        
        # Feature 12: Put-call ratio change
        # Changing options sentiment can precede short interest changes
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_change = (put_call_ratio / denom) - 1
            eng.append(pc_change)
        else:
            eng.append(0.0)
        
        # Feature 13: Bollinger Band width - volatility measure
        # Volatility measure that can indicate potential short squeezes
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(sma), 1e-8)
            bb_width = (2 * std) / denom
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 14: Short interest to days to cover ratio
        # Relationship between short interest and days to cover
        denom = max(abs(days_to_cover), 1e-8)
        si_dtc_ratio = short_interest / denom
        eng.append(si_dtc_ratio)
        
        # Feature 15: Implied volatility change
        # Changes in implied volatility can signal changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            denom = max(abs(prev_iv), 1e-8)
            iv_change = (implied_volatility / denom) - 1
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 16: Price to 10-day moving average ratio
        # Deviation from moving average can signal potential reversals
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            denom = max(abs(sma10), 1e-8)
            price_to_ma = close_prices[-1] / denom
            eng.append(price_to_ma)
        else:
            eng.append(0.0)
        
        # Feature 17: Short interest to put-call ratio
        # Relationship between short interest and options sentiment
        denom = max(abs(put_call_ratio), 1e-8)
        si_pc_ratio = short_interest / denom
        eng.append(si_pc_ratio)
        
        # Feature 18: Average true range (ATR) - volatility indicator
        # Volatility measure that accounts for gaps
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, 5):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values)
            denom = max(abs(close_prices[-1]), 1e-8)
            normalized_atr = atr / denom
            eng.append(normalized_atr)
        else:
            eng.append(0.0)
        
        # Feature 19: RSI (Relative Strength Index)
        # Extreme RSI values often precede short interest changes
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            denom = max(abs(avg_loss), 1e-8)
            rs = avg_gain / denom
            rsi = 100 - (100 / (1 + rs))
            # Normalize RSI to [0,1] range
            eng.append(rsi / 100.0)
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 98105.0692
RMSE: 123402.6620
MAPE: 12.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t0: importance=0.0018, rank=1
   2. Feature_13_t2: importance=0.0016, rank=2
   3. Feature_21_t2: importance=0.0015, rank=3
   4. Feature_3_t1: importance=0.0014, rank=4
   5. Feature_12_t2: importance=0.0014, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.18%

ðŸ“ˆ Current best MAPE: 11.77%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep volume if available
        volume = 0.0
        if data.shape[1] > 68:
            volume = data[t, 68]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume (required)
        raw_keep.append(short_interest)
        raw_keep.append(avg_volume)
        
        # Keep high-importance features from previous iterations
        raw_keep.append(days_to_cover)
        raw_keep.append(synthetic_short_cost)  # Feature_65 was top important in baseline
        raw_keep.append(shares_outstanding)    # Feature_67 was high importance in baseline
        
        # Keep only the most recent close price instead of all OHLC data
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized)
        # Higher ratio indicates more shares are sold short relative to available float
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 2: Short interest momentum - rate of change
        # Rate of change in short interest can signal trend continuation/reversal
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
        
        # Feature 3: Short interest acceleration - second derivative
        # Second derivative of short interest can indicate changing trends
        if t >= 2:
            prev_short = data[t-1, 0]
            prev_prev_short = data[t-2, 0]
            prev_momentum = (prev_short / max(abs(prev_prev_short), 1e-8)) - 1
            current_momentum = (short_interest / max(abs(prev_short), 1e-8)) - 1
            short_acceleration = current_momentum - prev_momentum
            eng.append(short_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 4: Days to cover change - high importance in previous iterations
        # Increasing days to cover can signal potential short squeeze
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_change = (days_to_cover / denom) - 1
            eng.append(dtc_change)
        else:
            eng.append(0.0)
        
        # Feature 5: Short cost to volatility ratio - high importance in previous iterations
        # Expensive shorts with low volatility may indicate crowded shorts
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 6: Synthetic short cost change - high importance in previous iterations
        # Changes in borrowing costs can signal changing short demand
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_change = (synthetic_short_cost / denom) - 1
            eng.append(cost_change)
        else:
            eng.append(0.0)
        
        # Feature 7: Recent price volatility (5-day)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
        
        # Feature 8: Overnight gap - consistently high importance in previous iterations
        # Large overnight gaps can trigger short covering or new short positions
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 9: Short interest to volume ratio
        # Indicates how many days of volume the short interest represents
        denom = max(abs(avg_volume), 1e-8)
        si_volume_ratio = short_interest / denom
        eng.append(si_volume_ratio)
        
        # Feature 10: OHLC spread of most recent day - high importance in previous iterations
        # Wide spreads can indicate volatility attractive to short sellers
        if len(open_prices) > 0 and len(close_prices) > 0:
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            denom = max(abs(close_prices[-1]), 1e-8)
            recent_spread = (recent_high - recent_low) / denom
            eng.append(recent_spread)
        else:
            eng.append(0.0)
        
        # Feature 11: Price to 10-day moving average ratio - high importance in previous iterations
        # Deviation from moving average can signal potential reversals
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            denom = max(abs(sma10), 1e-8)
            price_to_ma = close_prices[-1] / denom
            eng.append(price_to_ma)
        else:
            eng.append(1.0)  # Neutral value
        
        # Feature 12: Bollinger Band position - volatility-adjusted price position
        # Position within Bollinger Bands can indicate potential mean reversion or breakout
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(std), 1e-8)
            bb_position = (close_prices[-1] - sma) / (2 * denom)  # Normalized to typically fall in [-1, 1]
            # Clip to prevent extreme values
            bb_position = max(min(bb_position, 3.0), -3.0)
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # Feature 13: Put-call ratio change - options sentiment shift
        # Changing options sentiment can precede short interest changes
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_change = (put_call_ratio / denom) - 1
            eng.append(pc_change)
        else:
            eng.append(0.0)
        
        # Feature 14: Implied volatility change
        # Changes in implied volatility can signal changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            denom = max(abs(prev_iv), 1e-8)
            iv_change = (implied_volatility / denom) - 1
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 15: Short squeeze potential indicator
        # Combines high short interest, high days to cover, and recent price increase
        # This composite feature captures potential for a short squeeze
        if len(close_prices) >= 5:
            price_change_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            short_squeeze_potential = short_to_float * days_to_cover * (1 + max(0, price_change_5d))
            # Normalize to prevent extreme values
            short_squeeze_potential = min(short_squeeze_potential, 10.0)
            eng.append(short_squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 16: Price momentum with volume confirmation
        # Price momentum weighted by volume can indicate stronger trends
        if len(close_prices) >= 5 and volume > 0:
            price_5d_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            vol_ratio = volume / max(abs(avg_volume), 1e-8)
            momentum_vol_confirmed = price_5d_momentum * min(vol_ratio, 3.0)  # Cap volume impact
            eng.append(momentum_vol_confirmed)
        else:
            eng.append(0.0)
        
        # Feature 17: RSI (Relative Strength Index) - improved calculation
        # Extreme RSI values often precede short interest changes
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            denom = max(abs(avg_loss), 1e-8)
            rs = avg_gain / denom
            rsi = 100 - (100 / (1 + rs))
            # Transform RSI to emphasize extremes (overbrought/oversold)
            # Center at 0, range typically [-0.5, 0.5]
            transformed_rsi = (rsi - 50) / 100
            eng.append(transformed_rsi)
        else:
            eng.append(0.0)  # Neutral RSI value
        
        # Feature 18: Short interest to put-call ratio
        # Relationship between short interest and options sentiment
        denom = max(abs(put_call_ratio), 1e-8)
        si_pc_ratio = short_interest / denom
        # Normalize to prevent extreme values
        si_pc_ratio = min(si_pc_ratio, 1000.0)
        eng.append(si_pc_ratio)
        
        # Feature 19: Volatility-adjusted price change
        # Price changes normalized by volatility can indicate abnormal moves
        if len(close_prices) >= 10:
            price_change = (close_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            rolling_std = np.std(close_prices[-10:])
            denom = max(abs(rolling_std), 1e-8)
            vol_adj_change = price_change / denom
            # Clip to prevent extreme values
            vol_adj_change = max(min(vol_adj_change, 5.0), -5.0)
            eng.append(vol_adj_change)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 96051.7956
RMSE: 121110.4214
MAPE: 12.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0026, rank=1
   2. Feature_22_t0: importance=0.0019, rank=2
   3. Feature_19_t3: importance=0.0015, rank=3
   4. Feature_7_t2: importance=0.0015, rank=4
   5. Feature_8_t2: importance=0.0013, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.04%

ðŸ“ˆ Current best MAPE: 11.77%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]  # Consistently high importance
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep volume if available
        volume = 0.0
        if data.shape[1] > 68:
            volume = data[t, 68]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume (required)
        raw_keep.append(short_interest)
        raw_keep.append(avg_volume)
        
        # Keep high-importance features from previous iterations
        raw_keep.append(days_to_cover)  # Consistently important
        raw_keep.append(synthetic_short_cost)  # Top important in baseline
        raw_keep.append(shares_outstanding)  # High importance in baseline
        
        # Keep only the most recent close price instead of all OHLC data
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized)
        # Higher ratio indicates more shares are sold short relative to available float
        denom = max(abs(shares_outstanding), 1e-8)
        short_to_float = short_interest / denom
        eng.append(short_to_float)
        
        # Feature 2: Short interest momentum - rate of change over 1 period
        # Consistently high importance in previous iterations
        if t > 0:
            prev_short = data[t-1, 0]
            denom = max(abs(prev_short), 1e-8)
            short_momentum = (short_interest / denom) - 1
            eng.append(short_momentum)
        else:
            eng.append(0.0)
        
        # Feature 3: Days to cover change - high importance in previous iterations
        if t > 0:
            prev_dtc = data[t-1, 2]
            denom = max(abs(prev_dtc), 1e-8)
            dtc_change = (days_to_cover / denom) - 1
            eng.append(dtc_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Short cost to volatility ratio - high importance in previous iterations
        denom = max(abs(implied_volatility), 1e-8)
        short_cost_vol_ratio = synthetic_short_cost / denom
        eng.append(short_cost_vol_ratio)
        
        # Feature 5: Synthetic short cost change - high importance in previous iterations
        if t > 0:
            prev_cost = data[t-1, 65]
            denom = max(abs(prev_cost), 1e-8)
            cost_change = (synthetic_short_cost / denom) - 1
            eng.append(cost_change)
        else:
            eng.append(0.0)
        
        # Feature 6: Recent price volatility (5-day)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            price_range = np.max(recent_highs) - np.min(recent_lows)
            avg_price = np.mean(close_prices[-5:])
            denom = max(abs(avg_price), 1e-8)
            normalized_range = price_range / denom
            eng.append(normalized_range)
        else:
            eng.append(0.0)
        
        # Feature 7: OHLC spread of most recent day - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            recent_high = high_prices[-1]
            recent_low = low_prices[-1]
            denom = max(abs(close_prices[-1]), 1e-8)
            recent_spread = (recent_high - recent_low) / denom
            eng.append(recent_spread)
        else:
            eng.append(0.0)
        
        # Feature 8: Price to 10-day moving average ratio - high importance in previous iterations
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            denom = max(abs(sma10), 1e-8)
            price_to_ma = close_prices[-1] / denom
            eng.append(price_to_ma)
        else:
            eng.append(1.0)  # Neutral value
        
        # Feature 9: Bollinger Band position - volatility-adjusted price position
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            denom = max(abs(std), 1e-8)
            bb_position = (close_prices[-1] - sma) / (2 * denom)
            bb_position = max(min(bb_position, 3.0), -3.0)  # Clip to prevent extreme values
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # Feature 10: Put-call ratio change - options sentiment shift
        if t > 0:
            prev_pc_ratio = data[t-1, 64]
            denom = max(abs(prev_pc_ratio), 1e-8)
            pc_change = (put_call_ratio / denom) - 1
            eng.append(pc_change)
        else:
            eng.append(0.0)
        
        # Feature 11: Short squeeze potential indicator - composite feature
        # Combines high short interest, high days to cover, and recent price increase
        if len(close_prices) >= 5:
            price_change_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
            short_squeeze_potential = short_to_float * days_to_cover * (1 + max(0, price_change_5d))
            short_squeeze_potential = min(short_squeeze_potential, 10.0)  # Normalize
            eng.append(short_squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 12: RSI (Relative Strength Index) - improved calculation
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            denom = max(abs(avg_loss), 1e-8)
            rs = avg_gain / denom
            rsi = 100 - (100 / (1 + rs))
            # Transform RSI to emphasize extremes (overbrought/oversold)
            transformed_rsi = (rsi - 50) / 100
            eng.append(transformed_rsi)
        else:
            eng.append(0.0)
        
        # Feature 13: Short interest to volume ratio - days to cover alternative
        # Consistently important in previous iterations
        denom = max(abs(avg_volume), 1e-8)
        si_volume_ratio = short_interest / denom
        eng.append(si_volume_ratio)
        
        # Feature 14: Volatility-adjusted price change - high importance in previous iterations
        if len(close_prices) >= 10:
            price_change = (close_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            rolling_std = np.std(close_prices[-10:])
            denom = max(abs(rolling_std), 1e-8)
            vol_adj_change = price_change / denom
            vol_adj_change = max(min(vol_adj_change, 5.0), -5.0)  # Clip extreme values
            eng.append(vol_adj_change)
        else:
            eng.append(0.0)
        
        # Feature 15: Short interest acceleration - second derivative
        # New feature to capture changing trends in short interest
        if t >= 2:
            prev_short = data[t-1, 0]
            prev_prev_short = data[t-2, 0]
            prev_momentum = (prev_short / max(abs(prev_prev_short), 1e-8)) - 1
            current_momentum = (short_interest / max(abs(prev_short), 1e-8)) - 1
            short_acceleration = current_momentum - prev_momentum
            eng.append(short_acceleration)
        else:
            eng.append(0.0)
        
        # Feature 16: Overnight gap - consistently high importance in previous iterations
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 17: MACD Signal - trend strength and direction indicator
        # New feature to capture momentum and trend changes
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            # Normalize MACD by price level
            avg_price = np.mean(close_prices[-26:])
            denom = max(abs(avg_price), 1e-8)
            norm_macd = macd / denom
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 18: Implied volatility to historical volatility ratio
        # New feature to capture market expectations vs. realized volatility
        if len(close_prices) >= 10:
            hist_vol = np.std(np.diff(np.log(close_prices[-10:])) * 100)
            denom = max(abs(hist_vol), 1e-8)
            iv_hv_ratio = implied_volatility / denom
            iv_hv_ratio = min(iv_hv_ratio, 5.0)  # Cap extreme values
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)  # Neutral value
        
        # Feature 19: Short interest to put-call ratio - relationship between short interest and options
        denom = max(abs(put_call_ratio), 1e-8)
        si_pc_ratio = short_interest / denom
        si_pc_ratio = min(si_pc_ratio, 1000.0)  # Normalize extreme values
        eng.append(si_pc_ratio)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 96789.2378
RMSE: 121022.9441
MAPE: 12.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t2: importance=0.0046, rank=1
   2. Feature_17_t2: importance=0.0024, rank=2
   3. Feature_17_t0: importance=0.0021, rank=3
   4. Feature_7_t3: importance=0.0021, rank=4
   5. Feature_20_t2: importance=0.0013, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.09%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 241702.7153
RMSE: 335490.0291
MAPE: 13.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 108
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0043, rank=1
   2. Feature_64_t3: importance=0.0021, rank=2
   3. Feature_65_t1: importance=0.0019, rank=3
   4. Feature_1_t2: importance=0.0019, rank=4
   5. Feature_67_t3: importance=0.0010, rank=5
   Baseline MAPE: 13.59%
   Baseline MAE: 241702.7153
   Baseline RMSE: 335490.0291

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 243663.7875
RMSE: 334926.7037
MAPE: 13.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t2: importance=0.0020, rank=1
   2. Feature_22_t3: importance=0.0014, rank=2
   3. Feature_13_t3: importance=0.0014, rank=3
   4. Feature_13_t2: importance=0.0014, rank=4
   5. Feature_5_t3: importance=0.0013, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 13.92%
   MAE: 243663.7875
   RMSE: 334926.7037

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 13.59%
   Best Model MAPE: 13.92%
   Absolute Improvement: -0.34%
   Relative Improvement: -2.5%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  11.77           N/A                 
1          Iteration 1               12.60           -0.83%              
2          Iteration 2               12.76           -0.99%              
3          Iteration 3               12.95           -1.18%              
4          Iteration 4               12.80           -1.04%              
5          Iteration 5               12.86           -1.09%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 1 - MAPE: 12.60%
âœ… Saved FSS results to cache/FSS_iterative_results_enhanced.pkl
âœ… Summary report saved for FSS

ðŸŽ‰ Process completed successfully for FSS!

================================================================================
PROCESSING TICKER 7/15: ABM
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for ABM
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for ABM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 192656.8650
RMSE: 243353.3685
MAPE: 14.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0009, rank=1
   2. Feature_65_t2: importance=0.0007, rank=2
   3. Feature_63_t3: importance=0.0006, rank=3
   4. Feature_63_t1: importance=0.0006, rank=4
   5. Feature_67_t0: importance=0.0004, rank=5

ðŸ“Š Baseline Performance: MAPE = 14.65%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep high importance features identified in previous iteration
        raw_keep.append(data[t, 63])  # Feature_63 (options_put_call_volume_ratio)
        raw_keep.append(data[t, 65])  # Feature_65 (options_synthetic_short_cost)
        raw_keep.append(data[t, 67])  # Feature_67 (shares_outstanding)
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # This shows what percentage of total shares are sold short
        shares_outstanding = data[t, 67]
        si_ratio = 0
        if abs(shares_outstanding) > 1e-8:
            si_ratio = data[t, 0] / shares_outstanding
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        # Measures short interest relative to trading volume
        avg_volume = data[t, 1]
        si_volume_ratio = 0
        if abs(avg_volume) > 1e-8:
            si_volume_ratio = data[t, 0] / avg_volume
        eng.append(si_volume_ratio)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
        else:
            momentum_5d = 0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Captures medium-term price trend
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
        else:
            momentum_10d = 0
        eng.append(momentum_10d)
        
        # 5. Volatility (5-day)
        # Measures recent price volatility
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility_5d = np.std(returns) if len(returns) > 0 else 0
        else:
            volatility_5d = 0
        eng.append(volatility_5d)
        
        # 6. Average True Range (ATR) - Volatility measure
        # Measures market volatility
        atr = 0
        if len(high_prices) >= 2 and len(low_prices) >= 2 and len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(high_prices))):
                tr1 = high_prices[-i] - low_prices[-i]
                tr2 = abs(high_prices[-i] - close_prices[-(i+1)])
                tr3 = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(tr1, tr2, tr3))
            atr = np.mean(true_ranges) if true_ranges else 0
        eng.append(atr)
        
        # 7. RSI (Relative Strength Index)
        # Momentum oscillator that measures speed and change of price movements
        rsi = 0
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            if abs(loss) > 1e-8:
                rs = gain / loss
                rsi = 100 - (100 / (1 + rs))
            elif gain > 0:
                rsi = 100
        eng.append(rsi)
        
        # 8. Put-Call Ratio Change
        # Measures change in sentiment from options market
        put_call_ratio = data[t, 64]
        put_call_ratio_change = 0
        if t > 0:
            prev_put_call_ratio = data[t-1, 64]
            if abs(prev_put_call_ratio) > 1e-8:
                put_call_ratio_change = (put_call_ratio / prev_put_call_ratio) - 1
        eng.append(put_call_ratio_change)
        
        # 9. Synthetic Short Cost Change
        # Tracks changes in cost to short
        synthetic_short_cost = data[t, 65]
        synthetic_short_cost_change = 0
        if t > 0:
            prev_synthetic_short_cost = data[t-1, 65]
            if abs(prev_synthetic_short_cost) > 1e-8:
                synthetic_short_cost_change = (synthetic_short_cost / prev_synthetic_short_cost) - 1
        eng.append(synthetic_short_cost_change)
        
        # 10. Implied Volatility vs Historical Volatility Ratio
        # Compares market expectations with realized volatility
        implied_vol = data[t, 66]
        iv_hv_ratio = 0
        if volatility_5d > 1e-8 and implied_vol > 0:
            iv_hv_ratio = implied_vol / volatility_5d
        eng.append(iv_hv_ratio)
        
        # 11. Volume Trend (5-day)
        # Measures recent volume trend
        volume = data[t, 68]
        volume_trend = 0
        if t >= 4:
            past_volumes = [data[t-i, 68] for i in range(5)]
            avg_past_volume = np.mean(past_volumes)
            if abs(avg_past_volume) > 1e-8:
                volume_trend = (volume / avg_past_volume) - 1
        eng.append(volume_trend)
        
        # 12. Short Interest Change
        # Tracks changes in short interest
        si_change = 0
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] / prev_si) - 1
        eng.append(si_change)
        
        # 13. Days to Cover Change
        # Tracks changes in days to cover
        dtc_change = 0
        if t > 0:
            prev_dtc = data[t-1, 2]
            if abs(prev_dtc) > 1e-8:
                dtc_change = (data[t, 2] / prev_dtc) - 1
        eng.append(dtc_change)
        
        # 14. Price to Volume Ratio
        # Measures price relative to trading activity
        price_volume_ratio = 0
        if abs(volume) > 1e-8:
            price_volume_ratio = close_prices[-1] / volume
        eng.append(price_volume_ratio)
        
        # 15. Bollinger Band Width
        # Measures volatility based on standard deviation
        bb_width = 0
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            if abs(sma_20) > 1e-8:
                bb_width = (2 * std_20) / sma_20
        eng.append(bb_width)
        
        # 16. MACD Line
        # Trend-following momentum indicator
        macd = 0
        if len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema_26 = np.mean(close_prices[-26:])
            macd = ema_12 - ema_26
        eng.append(macd)
        
        # 17. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations
        si_iv_ratio = 0
        if abs(implied_vol) > 1e-8:
            si_iv_ratio = data[t, 0] / implied_vol
        eng.append(si_iv_ratio)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        raw_keep.append(short_interest)
        raw_keep.append(avg_daily_volume)
        raw_keep.append(days_to_cover)
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        last_open = open_prices[-1]
        last_high = high_prices[-1]
        last_low = low_prices[-1]
        last_close = close_prices[-1]
        
        raw_keep.append(last_close)  # Keep only the last close price
        
        # Keep options data which showed high importance
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        
        raw_keep.append(options_put_call_ratio)
        raw_keep.append(options_synthetic_short_cost)
        raw_keep.append(options_avg_implied_volatility)
        
        # Keep shares outstanding and volume
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0  # Handle potential index error
        
        raw_keep.append(shares_outstanding)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Price momentum and volatility features
        if len(close_prices) >= 5:
            # 5-day price momentum
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
            
            # Volatility (standard deviation of returns)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.extend([0.0, 0.0])  # Placeholders if not enough data
        
        # 2. Short interest relative to shares outstanding
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price range features
        if len(high_prices) > 0 and len(low_prices) > 0:
            # High-Low range relative to close
            hl_range = (last_high - last_low) / max(last_close, 1e-8)
            eng.append(hl_range)
            
            # Average true range (ATR)
            true_ranges = []
            for i in range(1, len(close_prices)):
                tr1 = high_prices[i] - low_prices[i]
                tr2 = abs(high_prices[i] - close_prices[i-1])
                tr3 = abs(low_prices[i] - close_prices[i-1])
                true_ranges.append(max(tr1, tr2, tr3))
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.extend([0.0, 0.0])  # Placeholders
        
        # 5. Volume-based features
        if volume > 0:
            # Volume relative to average
            vol_to_avg = volume / max(avg_daily_volume, 1e-8)
            eng.append(vol_to_avg)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. Options-based features
        # Implied volatility to historical volatility ratio
        if len(returns) > 1:
            iv_to_hv_ratio = options_avg_implied_volatility / max(volatility, 1e-8)
            eng.append(iv_to_hv_ratio)
        else:
            eng.append(0.0)  # Placeholder
        
        # 7. RSI (Relative Strength Index)
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(0.0)  # Placeholder
        
        # 8. MACD (Moving Average Convergence Divergence)
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            eng.append(macd)
        else:
            eng.append(0.0)  # Placeholder
        
        # 9. Bollinger Band Width
        if len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_width = (2 * std20) / max(sma20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Short interest to volume ratio
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 11. Options put/call ratio change
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (options_put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 12. Price to volatility ratio
        price_to_vol = last_close / max(options_avg_implied_volatility, 1e-8)
        eng.append(price_to_vol)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 192493.3576
RMSE: 236629.8346
MAPE: 14.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0011, rank=1
   2. Feature_5_t2: importance=0.0009, rank=2
   3. Feature_11_t1: importance=0.0007, rank=3
   4. Feature_12_t2: importance=0.0005, rank=4
   5. Feature_6_t0: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.01%

ðŸ“ˆ Current best MAPE: 14.65%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Last close price
        raw_keep.append(options_put_call_ratio)  # Important from previous analysis
        raw_keep.append(options_synthetic_short_cost)  # Important from previous analysis
        raw_keep.append(options_avg_implied_volatility)  # Important from previous analysis
        raw_keep.append(shares_outstanding)  # Important from previous analysis
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (key ratio for short interest analysis)
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio (highly significant in previous iteration)
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price momentum features (multiple timeframes for better signal)
        if len(close_prices) >= 5:
            # 5-day price momentum (showed high importance)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
            
            # 3-day price momentum (shorter timeframe)
            momentum_3d = (close_prices[-1] / max(close_prices[-3], 1e-8)) - 1.0
            eng.append(momentum_3d)
        else:
            eng.extend([0.0, 0.0])  # Placeholders
        
        # 5. Volatility (standard deviation of returns)
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(0.0)  # Placeholder
        
        # 7. Price to volatility ratio (showed high importance)
        price_to_vol = close_prices[-1] / max(options_avg_implied_volatility, 1e-8)
        eng.append(price_to_vol)
        
        # 8. Options put/call ratio change (options data showed high importance)
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (options_put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 9. Implied volatility to historical volatility ratio
        if len(returns) > 1 and volatility > 0:
            iv_to_hv_ratio = options_avg_implied_volatility / max(volatility, 1e-8)
            eng.append(iv_to_hv_ratio)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. High-Low range relative to close (price range feature)
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. Bollinger Band Width (volatility measure)
        if len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_width = (2 * std20) / max(sma20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)  # Placeholder
        
        # 12. Short interest acceleration (second derivative)
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 13. Short cost to implied volatility ratio
        short_cost_to_iv = options_synthetic_short_cost / max(options_avg_implied_volatility, 1e-8)
        eng.append(short_cost_to_iv)
        
        # 14. Price trend strength (directional movement)
        if len(close_prices) >= 10:
            up_days = sum(1 for i in range(1, 10) if close_prices[-i] > close_prices[-(i+1)])
            trend_strength = (up_days / 9.0) * 2 - 1  # Scale to [-1, 1]
            eng.append(trend_strength)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Normalized short interest (z-score)
        if t >= 5:
            si_history = [data[t-i, 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 16. Gap analysis (overnight price jumps)
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            eng.append(overnight_gap)
        else:
            eng.append(0.0)  # Placeholder
        
        # 17. Short interest to days to cover ratio
        si_to_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 194256.2157
RMSE: 242326.1801
MAPE: 15.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t0: importance=0.0009, rank=1
   2. Feature_10_t3: importance=0.0007, rank=2
   3. Feature_12_t1: importance=0.0006, rank=3
   4. Feature_5_t2: importance=0.0005, rank=4
   5. Feature_20_t3: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.35%

ðŸ“ˆ Current best MAPE: 14.65%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep essential raw features based on importance analysis from previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Last close price (most recent)
        raw_keep.append(options_put_call_ratio)  # High importance in previous iterations
        raw_keep.append(options_synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(options_avg_implied_volatility)  # High importance in previous iterations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio (consistently high importance)
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # This captures momentum in short interest which is a key predictor
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price momentum (5-day) - showed high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Volatility (standard deviation of returns)
        # Important for capturing market uncertainty which affects short interest
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. RSI (Relative Strength Index) - momentum oscillator
        # High importance in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as placeholder
        
        # 7. Price to volatility ratio
        # Captures how expensive the stock is relative to its risk
        price_to_vol = close_prices[-1] / max(options_avg_implied_volatility, 1e-8)
        eng.append(price_to_vol)
        
        # 8. Short cost efficiency - how expensive is shorting relative to stock price
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 9. High-Low range relative to close (price range feature)
        # Captures intraday volatility
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. MACD Signal - Moving Average Convergence Divergence
        # Trend-following momentum indicator
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            eng.append(macd / max(close_prices[-1], 1e-8))  # Normalized MACD
        else:
            eng.append(0.0)  # Placeholder
        
        # 12. Price trend strength (directional movement)
        # Captures the consistency of price movement direction
        if len(close_prices) >= 10:
            up_days = sum(1 for i in range(1, min(10, len(close_prices))) if close_prices[-i] > close_prices[-(i+1)])
            trend_strength = (up_days / min(9.0, len(close_prices)-1)) * 2 - 1  # Scale to [-1, 1]
            eng.append(trend_strength)
        else:
            eng.append(0.0)  # Placeholder
        
        # 13. Short interest to options put/call ratio
        # Relates short interest to options market sentiment
        si_to_put_call = short_interest / max(options_put_call_ratio, 1e-8)
        eng.append(si_to_put_call)
        
        # 14. Normalized short interest (z-score)
        # Shows how extreme current short interest is relative to recent history
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Gap analysis (overnight price jumps)
        # Captures significant overnight sentiment changes
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            eng.append(overnight_gap)
        else:
            eng.append(0.0)  # Placeholder
        
        # 16. Volume pressure - ratio of volume to average volume
        # Captures unusual trading activity
        if volume > 0:
            volume_pressure = volume / max(avg_daily_volume, 1e-8)
            eng.append(volume_pressure)
        else:
            eng.append(1.0)  # Neutral placeholder
        
        # 17. Composite momentum indicator
        # Combines price momentum with volume for stronger signal
        if len(close_prices) >= 5 and avg_daily_volume > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            composite_momentum = price_change * (volume / max(avg_daily_volume, 1e-8))
            eng.append(composite_momentum)
        else:
            eng.append(0.0)  # Placeholder
            
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 193778.8071
RMSE: 240983.2383
MAPE: 14.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0009, rank=1
   2. Feature_13_t0: importance=0.0008, rank=2
   3. Feature_17_t3: importance=0.0006, rank=3
   4. Feature_12_t0: importance=0.0005, rank=4
   5. Feature_5_t2: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.33%

ðŸ“ˆ Current best MAPE: 14.65%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(options_put_call_ratio)  # High importance in previous iterations
        raw_keep.append(options_synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(options_avg_implied_volatility)  # High importance in previous iterations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        # This normalizes short interest by company size
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio
        # Consistently high importance in previous iterations
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # Captures momentum in short interest which is a key predictor
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Volatility (standard deviation of returns)
        # Important for capturing market uncertainty which affects short interest
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Short cost efficiency - how expensive is shorting relative to stock price
        # This was highly important in previous iterations
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 6. High-Low range relative to close (price range feature)
        # Captures intraday volatility which can signal short squeeze potential
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 7. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 8. Short interest to options put/call ratio
        # Relates short interest to options market sentiment
        # This was highly important in feature importance analysis
        si_to_put_call = short_interest / max(options_put_call_ratio, 1e-8)
        eng.append(si_to_put_call)
        
        # 9. Normalized short interest (z-score)
        # Shows how extreme current short interest is relative to recent history
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Price momentum (5-day)
        # Showed high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. Bollinger Band position
        # Indicates if price is overbought/oversold relative to volatility
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_position = (close_prices[-1] - sma) / max(2 * std, 1e-8)  # Normalized to typical 2-sigma bands
            eng.append(bb_position)
        else:
            eng.append(0.0)  # Placeholder
        
        # 12. Options implied volatility to historical volatility ratio
        # Measures market expectation vs. realized volatility
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = options_avg_implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)  # Neutral placeholder
        
        # 13. Short interest to days to cover ratio
        # Combines two important metrics for short squeeze potential
        si_to_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc_ratio)
        
        # 14. Average True Range (ATR) - volatility indicator
        # Captures average price range including gaps
        if len(high_prices) >= 5 and len(low_prices) >= 5 and len(close_prices) >= 5:
            tr_values = []
            for i in range(1, 5):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values)
            normalized_atr = atr / max(close_prices[-1], 1e-8)  # Normalize by price
            eng.append(normalized_atr)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Price to synthetic short cost ratio
        # Measures relative cost of shorting compared to stock price
        price_to_short_cost = close_prices[-1] / max(options_synthetic_short_cost, 1e-8)
        eng.append(price_to_short_cost)
        
        # 16. Exponential moving average crossover signal
        # Trend indicator that was significant in previous iterations
        if len(close_prices) >= 10:
            ema5 = np.mean(close_prices[-5:])  # Simplified EMA calculation
            ema10 = np.mean(close_prices[-10:])
            ema_crossover = (ema5 / max(ema10, 1e-8)) - 1.0
            eng.append(ema_crossover)
        else:
            eng.append(0.0)  # Placeholder
        
        # 17. Short interest to implied volatility ratio
        # Relates short interest to expected future volatility
        si_to_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_to_iv_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (9,) (10,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data - reshape to get 15 days of OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis from previous iterations
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(options_put_call_ratio)  # High importance in previous iterations
        raw_keep.append(options_synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(options_avg_implied_volatility)  # High importance in previous iterations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        # This normalizes short interest by company size
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio
        # Consistently high importance in previous iterations
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # This captures momentum in short interest which is a key predictor
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price momentum (5-day)
        # Showed high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Volatility (standard deviation of returns)
        # Important for capturing market uncertainty which affects short interest
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. RSI (Relative Strength Index) - momentum oscillator
        # High importance in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as placeholder
        
        # 7. Short cost efficiency - how expensive is shorting relative to stock price
        # This was highly significant in previous iterations
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 8. High-Low range relative to close (price range feature)
        # Captures intraday volatility
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 9. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Bollinger Band Width - measure of volatility
        # New feature that captures volatility in a different way than standard deviation
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. Short interest to implied volatility ratio
        # Relates short interest to market's expectation of future volatility
        si_to_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_to_iv_ratio)
        
        # 12. Price to days to cover ratio
        # Relates stock price to short interest coverage time
        price_to_dtc = close_prices[-1] / max(days_to_cover, 1e-8)
        eng.append(price_to_dtc)
        
        # 13. Average True Range (ATR) - volatility indicator
        # New feature that captures volatility in a different way
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, 5):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values)
            normalized_atr = atr / max(close_prices[-1], 1e-8)
            eng.append(normalized_atr)
        else:
            eng.append(0.0)  # Placeholder
        
        # 14. Volume-weighted price momentum
        # Combines price changes with volume for a stronger signal
        if len(close_prices) >= 5:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_weight = avg_daily_volume / max(np.mean(avg_daily_volume), 1e-8)
            vw_momentum = price_change * vol_weight
            eng.append(vw_momentum)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Short interest to put/call ratio
        # Relates short interest to options market sentiment
        si_to_options = short_interest / max(options_put_call_ratio, 1e-8)
        eng.append(si_to_options)
        
        # 16. Normalized short interest (z-score)
        # Shows how extreme current short interest is relative to recent history
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 17. Price trend strength (directional movement)
        # Captures the consistency of price movement direction
        if len(close_prices) >= 10:
            up_days = sum(1 for i in range(1, 10) if close_prices[-i] > close_prices[-(i+1)])
            trend_strength = (up_days / 9.0) * 2 - 1  # Scale to [-1, 1]
            eng.append(trend_strength)
        else:
            eng.append(0.0)  # Placeholder
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 188269.5785
RMSE: 238047.1360
MAPE: 14.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t1: importance=0.0006, rank=1
   2. Feature_19_t3: importance=0.0006, rank=2
   3. Feature_17_t1: importance=0.0005, rank=3
   4. Feature_14_t2: importance=0.0005, rank=4
   5. Feature_13_t0: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.04%

ðŸ“ˆ Current best MAPE: 14.65%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data - reshape to get 15 days of OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep essential raw features based on importance analysis from previous iterations
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(options_put_call_ratio)  # High importance in previous iterations
        raw_keep.append(options_synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(options_avg_implied_volatility)  # High importance in previous iterations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        # This normalizes short interest by company size - consistently important
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio
        # Consistently high importance in previous iterations
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # This captures momentum in short interest which is a key predictor
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Exponential price momentum (weighted recent price changes)
        # Improved version of price momentum with exponential weighting for recency
        if len(close_prices) >= 5:
            weights = np.array([0.4, 0.3, 0.15, 0.1, 0.05])  # Exponential weights
            returns = np.zeros(5)
            for i in range(5):
                if i < len(close_prices) - 1:
                    returns[i] = (close_prices[-(i+1)] / max(close_prices[-(i+2)], 1e-8)) - 1.0
            exp_momentum = np.sum(returns * weights)
            eng.append(exp_momentum)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Volatility (Parkinson's measure - uses high-low range)
        # More robust volatility measure than standard deviation
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            ln_hl = np.log(high_prices[-5:] / np.maximum(low_prices[-5:], 1e-8))
            parkinson_vol = np.sqrt(np.sum(ln_hl**2) / (4 * np.log(2) * 5))
            eng.append(parkinson_vol)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. Improved RSI with volume weighting
        # Enhances RSI by incorporating volume information
        if len(close_prices) >= 14 and len(close_prices) == len(ohlc):
            delta = np.diff(close_prices)
            volume_last_14 = np.ones(len(delta))  # Default weight if volume not available
            
            # Use actual volume if available, otherwise use avg_daily_volume
            if data.shape[1] > 68:
                volume_last_14 = data[t, 68] * np.ones(len(delta))
            
            # Normalize volume
            volume_last_14 = volume_last_14 / max(np.mean(volume_last_14), 1e-8)
            
            # Volume-weighted gains and losses
            gain = np.where(delta > 0, delta * volume_last_14[:-1], 0)
            loss = np.where(delta < 0, -delta * volume_last_14[:-1], 0)
            
            avg_gain = np.sum(gain) / max(np.sum(volume_last_14[:-1]), 1e-8)
            avg_loss = np.sum(loss) / max(np.sum(volume_last_14[:-1]), 1e-8)
            
            rs = avg_gain / max(avg_loss, 1e-8)
            vol_rsi = 100 - (100 / (1 + rs))
            eng.append(vol_rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as placeholder
        
        # 7. Short cost efficiency - how expensive is shorting relative to stock price
        # This was highly significant in previous iterations
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 8. Implied volatility to realized volatility ratio
        # Captures market expectations vs actual volatility - new feature
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            realized_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_rv_ratio = options_avg_implied_volatility / max(realized_vol, 1e-8)
            eng.append(iv_rv_ratio)
        else:
            eng.append(1.0)  # Placeholder
        
        # 9. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Bollinger Band %B - position within volatility bands
        # More informative than just band width - shows where price is relative to bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            upper_band = sma + 2 * std
            lower_band = sma - 2 * std
            bb_b = (close_prices[-1] - lower_band) / max((upper_band - lower_band), 1e-8)
            eng.append(bb_b)
        else:
            eng.append(0.5)  # Middle of the band as placeholder
        
        # 11. Short interest to implied volatility ratio with normalization
        # Improved version that normalizes by historical relationship
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            iv_history = np.array([data[max(0, t-i), 66] for i in range(5)])
            
            # Calculate historical average ratio
            historical_ratios = si_history / np.maximum(iv_history, 1e-8)
            avg_historical_ratio = np.mean(historical_ratios)
            
            # Current ratio normalized by historical average
            current_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
            normalized_ratio = current_ratio / max(avg_historical_ratio, 1e-8)
            eng.append(normalized_ratio)
        else:
            # Simple ratio if not enough history
            si_to_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
            eng.append(si_to_iv_ratio)
        
        # 12. Composite momentum indicator (price + volume + volatility)
        # Combines multiple factors for a more robust momentum signal
        if len(close_prices) >= 10:
            # Price momentum component
            price_mom = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            
            # Volume trend component
            vol_ratio = avg_daily_volume / max(np.mean([data[max(0, t-i), 1] for i in range(1, 6)]), 1e-8)
            
            # Volatility component (inverse)
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_component = 1.0 / max(np.std(returns), 1e-8)
            
            # Normalize components
            price_mom_norm = np.tanh(price_mom * 5)  # Squash extreme values
            vol_ratio_norm = np.tanh((vol_ratio - 1.0) * 2)
            vol_component_norm = np.tanh((vol_component / 100) - 1.0)
            
            # Combine with weights
            composite_mom = 0.5 * price_mom_norm + 0.3 * vol_ratio_norm + 0.2 * vol_component_norm
            eng.append(composite_mom)
        else:
            eng.append(0.0)  # Placeholder
        
        # 13. Options market sentiment indicator
        # Combines put/call ratio with implied volatility for market sentiment
        options_sentiment = options_put_call_ratio * options_avg_implied_volatility / 100.0
        eng.append(options_sentiment)
        
        # 14. Short squeeze potential indicator
        # Combines days to cover with short ratio and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            short_ratio_val = short_interest / max(shares_outstanding, 1e-8)
            
            # Higher values indicate higher squeeze potential
            squeeze_potential = days_to_cover * short_ratio_val * (1 + max(0, price_momentum))
            # Normalize to prevent extreme values
            squeeze_potential = np.tanh(squeeze_potential / 5.0)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Normalized short interest trend
        # Shows how current short interest compares to its recent trend
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_trend = np.polyfit(np.arange(len(si_history)), si_history, 1)[0]  # Slope of trend
            si_trend_normalized = si_trend / max(np.mean(si_history), 1e-8)
            eng.append(si_trend_normalized)
        else:
            eng.append(0.0)  # Placeholder
        
        # 16. Price gap analysis
        # Identifies significant overnight gaps which can affect short interest
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            overnight_gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1.0
                overnight_gaps.append(gap)
            
            # Use the maximum absolute gap as the feature
            max_gap = max(overnight_gaps, key=abs) if overnight_gaps else 0.0
            eng.append(max_gap)
        else:
            eng.append(0.0)  # Placeholder
        
        # 17. Short interest to price ratio
        # Relates short interest to stock price level
        si_price_ratio = short_interest / max(close_prices[-1], 1e-8)
        eng.append(si_price_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (14,) (13,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data - reshape to get 15 days of OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep essential raw features based on importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(options_put_call_ratio)  # Important for sentiment
        raw_keep.append(options_synthetic_short_cost)  # Cost of shorting
        raw_keep.append(options_avg_implied_volatility)  # Market expectations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        # Normalizes short interest by company size - key metric for short squeeze potential
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio
        # Consistently high importance in previous iterations - shows how many days of volume are sold short
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # Captures momentum in short interest - strong predictor of future changes
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price momentum (5-day)
        # Showed high importance in previous iterations - captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Volatility (standard deviation of returns)
        # Important for capturing market uncertainty which affects short interest
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. RSI (Relative Strength Index) - momentum oscillator
        # High importance in previous iterations - indicates overbought/oversold conditions
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as placeholder
        
        # 7. Short cost efficiency - how expensive is shorting relative to stock price
        # This was highly significant in previous iterations - indicates shorting pressure
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 8. High-Low range relative to close (price range feature)
        # Captures intraday volatility - important for short-term trading decisions
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 9. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum - leading indicator
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Bollinger Band Width - measure of volatility
        # Captures volatility in a different way than standard deviation - indicates potential breakouts
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. Short interest to implied volatility ratio
        # Relates short interest to market's expectation of future volatility - sentiment indicator
        si_to_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_to_iv_ratio)
        
        # 12. Average True Range (ATR) - volatility indicator
        # Captures volatility in a different way - important for risk assessment
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            normalized_atr = atr / max(close_prices[-1], 1e-8)
            eng.append(normalized_atr)
        else:
            eng.append(0.0)  # Placeholder
        
        # 13. Normalized short interest (z-score)
        # Shows how extreme current short interest is relative to recent history - statistical outlier detection
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 14. MACD Signal Line Crossover
        # Momentum indicator that shows relationship between two moving averages
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            signal = np.mean(close_prices[-9:])  # Simplified signal line
            macd_crossover = macd - signal
            eng.append(macd_crossover)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Short Interest to Price Ratio
        # Relates short interest to current price - indicates potential downward pressure
        si_price_ratio = short_interest / max(close_prices[-1], 1e-8)
        eng.append(si_price_ratio)
        
        # 16. Options Implied Volatility to Historical Volatility Ratio
        # Compares market expectations to realized volatility - sentiment indicator
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = options_avg_implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)  # Neutral placeholder
        
        # 17. Short Interest Concentration
        # Measures how concentrated short interest is relative to market cap
        market_cap = shares_outstanding * close_prices[-1]
        si_concentration = (short_interest * close_prices[-1]) / max(market_cap, 1e-8)
        eng.append(si_concentration)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 2/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data - reshape to get 15 days of OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and other data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]
        options_avg_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep essential raw features based on importance analysis from previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(options_put_call_ratio)  # High importance in previous iterations
        raw_keep.append(options_synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(options_avg_implied_volatility)  # High importance in previous iterations
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative to shares outstanding (fundamental ratio)
        # This normalizes short interest by company size
        short_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_ratio)
        
        # 2. Short interest to volume ratio
        # Consistently high importance in previous iterations
        si_to_vol_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_vol_ratio)
        
        # 3. Short interest change rate (if we have previous data)
        # This captures momentum in short interest which is a key predictor
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)  # Placeholder
        
        # 4. Price momentum (5-day)
        # Showed high importance in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)  # Placeholder
        
        # 5. Volatility (standard deviation of returns)
        # Important for capturing market uncertainty which affects short interest
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 1 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)  # Placeholder
        
        # 6. RSI (Relative Strength Index) - momentum oscillator
        # High importance in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as placeholder
        
        # 7. Short cost efficiency - how expensive is shorting relative to stock price
        # This was highly significant in previous iterations
        short_cost_efficiency = options_synthetic_short_cost / max(close_prices[-1], 1e-8)
        eng.append(short_cost_efficiency)
        
        # 8. High-Low range relative to close (price range feature)
        # Captures intraday volatility
        if len(high_prices) > 0 and len(low_prices) > 0:
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0.0)  # Placeholder
        
        # 9. Short interest acceleration (second derivative)
        # Captures the rate of change in short interest momentum
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
            eng.append(si_acceleration)
        else:
            eng.append(0.0)  # Placeholder
        
        # 10. Bollinger Band Width - measure of volatility
        # New feature that captures volatility in a different way than standard deviation
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)  # Placeholder
        
        # 11. Short interest to implied volatility ratio
        # Relates short interest to market's expectation of future volatility
        si_to_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_to_iv_ratio)
        
        # 12. Price to days to cover ratio
        # Relates stock price to short interest coverage time
        price_to_dtc = close_prices[-1] / max(days_to_cover, 1e-8)
        eng.append(price_to_dtc)
        
        # 13. Average True Range (ATR) - volatility indicator
        # New feature that captures volatility in a different way
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            normalized_atr = atr / max(close_prices[-1], 1e-8)
            eng.append(normalized_atr)
        else:
            eng.append(0.0)  # Placeholder
        
        # 14. MACD Signal - Moving Average Convergence Divergence
        # Trend-following momentum indicator showing relationship between two moving averages
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            normalized_macd = macd / max(close_prices[-1], 1e-8)  # Normalize by price
            eng.append(normalized_macd)
        else:
            eng.append(0.0)  # Placeholder
        
        # 15. Short interest to put/call ratio
        # Relates short interest to options market sentiment
        si_to_options = short_interest / max(options_put_call_ratio, 1e-8)
        eng.append(si_to_options)
        
        # 16. Normalized short interest (z-score)
        # Shows how extreme current short interest is relative to recent history
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            norm_si = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(norm_si)
        else:
            eng.append(0.0)  # Placeholder
        
        # 17. Stochastic Oscillator - momentum indicator comparing close price to price range
        # New feature that provides a different perspective on price momentum
        if len(close_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            k_percent = 100 * (close_prices[-1] - lowest_low) / max(highest_high - lowest_low, 1e-8)
            eng.append(k_percent / 100.0)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Placeholder
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 189105.8004
RMSE: 235779.7552
MAPE: 14.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0007, rank=1
   2. Feature_10_t3: importance=0.0007, rank=2
   3. Feature_15_t2: importance=0.0006, rank=3
   4. Feature_19_t2: importance=0.0006, rank=4
   5. Feature_10_t1: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.02%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 208682.8330
RMSE: 270310.4956
MAPE: 12.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 179
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0006, rank=1
   2. Feature_67_t1: importance=0.0006, rank=2
   3. Feature_65_t0: importance=0.0005, rank=3
   4. Feature_63_t1: importance=0.0005, rank=4
   5. Feature_63_t3: importance=0.0005, rank=5
   Baseline MAPE: 12.59%
   Baseline MAE: 208682.8330
   Baseline RMSE: 270310.4956

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 246523.9023
RMSE: 298818.0111
MAPE: 14.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t1: importance=0.0010, rank=1
   2. Feature_10_t3: importance=0.0009, rank=2
   3. Feature_12_t0: importance=0.0007, rank=3
   4. Feature_15_t1: importance=0.0007, rank=4
   5. Feature_19_t2: importance=0.0007, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 14.86%
   MAE: 246523.9023
   RMSE: 298818.0111

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 12.59%
   Best Model MAPE: 14.86%
   Absolute Improvement: -2.26%
   Relative Improvement: -18.0%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  14.65           N/A                 
1          Iteration 1               14.66           -0.01%              
2          Iteration 2               15.00           -0.35%              
3          Iteration 3               14.98           -0.33%              
4          Iteration 4               14.69           -0.04%              
5          Iteration 5               14.63           +0.02%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 5 - MAPE: 14.63%
âœ… Saved ABM results to cache/ABM_iterative_results_enhanced.pkl
âœ… Summary report saved for ABM

ðŸŽ‰ Process completed successfully for ABM!

================================================================================
PROCESSING TICKER 8/15: IART
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for IART
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for IART from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IART...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 319476.6476
RMSE: 481313.3114
MAPE: 9.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0007, rank=2
   3. Feature_65_t3: importance=0.0007, rank=3
   4. Feature_63_t0: importance=0.0005, rank=4
   5. Feature_1_t3: importance=0.0004, rank=5

ðŸ“Š Baseline Performance: MAPE = 9.36%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Select raw features to keep
        raw_keep = []
        
        # Always keep short interest and average daily volume as they are critical
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options data which showed high importance
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume which were important
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the last 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Price momentum features
        if len(close_prices) >= 5:
            # 5-day price momentum
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
            
        if len(close_prices) >= 10:
            # 10-day price momentum
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
        
        # 2. Volatility measures
        if len(close_prices) >= 5:
            # 5-day price volatility
            returns = np.diff(close_prices[-6:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility_5d = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility_5d)
        else:
            eng.append(0.0)
        
        # 3. Short interest relative to volume
        short_interest = data[t, 0]
        avg_volume = max(data[t, 1], 1e-8)
        short_to_volume = short_interest / avg_volume
        eng.append(short_to_volume)
        
        # 4. Short interest relative to shares outstanding
        shares_outstanding = max(data[t, 67], 1e-8)
        short_to_shares = short_interest / shares_outstanding
        eng.append(short_to_shares)
        
        # 5. Price range features
        if len(high_prices) > 0 and len(low_prices) > 0:
            # Normalized price range over last 5 days
            high_5d = np.max(high_prices[-5:]) if len(high_prices) >= 5 else high_prices[-1]
            low_5d = np.min(low_prices[-5:]) if len(low_prices) >= 5 else low_prices[-1]
            avg_price_5d = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else close_prices[-1]
            price_range_norm = (high_5d - low_5d) / max(avg_price_5d, 1e-8)
            eng.append(price_range_norm)
        else:
            eng.append(0.0)
        
        # 6. Options-based features
        put_call_ratio = data[t, 64]
        implied_vol = data[t, 66]
        synthetic_short_cost = data[t, 65]
        
        # Options sentiment indicator
        options_sentiment = put_call_ratio * implied_vol
        eng.append(options_sentiment)
        
        # Short cost to volatility ratio
        short_cost_vol_ratio = synthetic_short_cost / max(implied_vol, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # 7. Volume-based features
        volume = data[t, 68]
        volume_to_avg = volume / max(avg_volume, 1e-8)
        eng.append(volume_to_avg)
        
        # 8. Days to cover relative to implied volatility
        days_to_cover = data[t, 2]
        dtc_to_vol = days_to_cover * implied_vol
        eng.append(dtc_to_vol)
        
        # 9. RSI (Relative Strength Index) if we have enough data
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # 10. OHLC-based features
        if len(close_prices) >= 5 and len(open_prices) >= 5:
            # Gap analysis
            overnight_gaps = open_prices[1:] - close_prices[:-1]
            avg_gap = np.mean(overnight_gaps[-4:]) if len(overnight_gaps) >= 4 else 0
            normalized_gap = avg_gap / max(np.mean(close_prices[-5:]), 1e-8)
            eng.append(normalized_gap)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
        
        features[t, :] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (most important features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        
        # Keep days to cover (important for short interest prediction)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options-related features (high importance in baseline)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume (high importance in baseline)
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68]) if t < data.shape[0] and data.shape[1] > 68 else raw_keep.append(0.0)  # volume
        
        # Extract OHLC data for the past 15 days
        try:
            ohlc = data[t, 3:63].reshape(15, 4)
            open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
            
            # Keep only the most recent OHLC values
            raw_keep.append(open_prices[-1])   # Latest open
            raw_keep.append(high_prices[-1])   # Latest high
            raw_keep.append(low_prices[-1])    # Latest low
            raw_keep.append(close_prices[-1])  # Latest close
        except:
            # Handle edge cases
            raw_keep.extend([0.0, 0.0, 0.0, 0.0])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price volatility (standard deviation of close prices)
        try:
            price_volatility = np.std(close_prices)
            eng.append(price_volatility)
        except:
            eng.append(0.0)
            
        # Feature 2: Short interest to volume ratio
        try:
            si_volume_ratio = data[t, 0] / (data[t, 1] + 1e-8)
            eng.append(si_volume_ratio)
        except:
            eng.append(0.0)
            
        # Feature 3: Short interest to shares outstanding ratio
        try:
            si_shares_ratio = data[t, 0] / (data[t, 67] + 1e-8)
            eng.append(si_shares_ratio)
        except:
            eng.append(0.0)
            
        # Feature 4: RSI (Relative Strength Index)
        try:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / (avg_loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        except:
            eng.append(0.0)
            
        # Feature 5: Average True Range (ATR)
        try:
            tr_values = []
            for i in range(1, len(close_prices)):
                high_low = high_prices[i] - low_prices[i]
                high_close_prev = abs(high_prices[i] - close_prices[i-1])
                low_close_prev = abs(low_prices[i] - close_prices[i-1])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            eng.append(atr)
        except:
            eng.append(0.0)
            
        # Feature 6: Bollinger Band Width
        try:
            sma = np.mean(close_prices)
            std = np.std(close_prices)
            bb_width = (2 * std) / (sma + 1e-8)
            eng.append(bb_width)
        except:
            eng.append(0.0)
            
        # Feature 7: Price momentum (5-day)
        try:
            momentum = close_prices[-1] / (close_prices[-6] + 1e-8) - 1 if len(close_prices) >= 6 else 0.0
            eng.append(momentum)
        except:
            eng.append(0.0)
            
        # Feature 8: Options implied volatility to historical volatility ratio
        try:
            hist_vol = np.std(close_prices) / (np.mean(close_prices) + 1e-8)
            iv_hv_ratio = data[t, 66] / (hist_vol + 1e-8)
            eng.append(iv_hv_ratio)
        except:
            eng.append(0.0)
            
        # Feature 9: Short cost to volatility ratio
        try:
            short_cost_vol_ratio = data[t, 65] / (data[t, 66] + 1e-8)
            eng.append(short_cost_vol_ratio)
        except:
            eng.append(0.0)
            
        # Feature 10: MACD (Moving Average Convergence Divergence)
        try:
            if len(close_prices) >= 12:
                ema12 = np.mean(close_prices[-12:])
                ema26 = np.mean(close_prices)
                macd = ema12 - ema26
                eng.append(macd)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
            
        # Feature 11: Price Range Ratio
        try:
            price_range = (np.max(high_prices) - np.min(low_prices)) / (np.mean(close_prices) + 1e-8)
            eng.append(price_range)
        except:
            eng.append(0.0)
            
        # Feature 12: Volume Trend
        try:
            if data.shape[1] > 68:
                volume_trend = data[t, 68] / (data[t, 1] + 1e-8)
                eng.append(volume_trend)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
            
        # Feature 13: Options Put/Call Ratio Change
        try:
            if t > 0:
                pc_ratio_change = data[t, 64] / (data[t-1, 64] + 1e-8) - 1
                eng.append(pc_ratio_change)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features = np.stack(features_list, axis=0)
    
    # Handle NaN and Inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 328794.7867
RMSE: 488014.2480
MAPE: 9.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t1: importance=0.0010, rank=1
   2. Feature_4_t3: importance=0.0007, rank=2
   3. Feature_15_t2: importance=0.0006, rank=3
   4. Feature_12_t1: importance=0.0005, rank=4
   5. Feature_17_t1: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.21%

ðŸ“ˆ Current best MAPE: 9.36%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Keep the most important raw features based on previous iterations
        raw_keep.append(data[t, 0])  # short interest (always keep)
        raw_keep.append(data[t, 1])  # average daily volume (always keep)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options-related features (high importance in previous iterations)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding (important for normalization)
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Extract OHLC data for the past 15 days
        try:
            ohlc = data[t, 3:63].reshape(15, 4)
            open_prices = ohlc[:, 0]
            high_prices = ohlc[:, 1]
            low_prices = ohlc[:, 2]
            close_prices = ohlc[:, 3]
            
            # Keep only the most recent close price (most informative)
            raw_keep.append(close_prices[-1])  # Latest close
        except:
            raw_keep.append(0.0)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # This is a key metric for short squeeze potential
        try:
            shares_out = max(data[t, 67], 1e-8)
            si_float_ratio = data[t, 0] / shares_out
            eng.append(si_float_ratio)
        except:
            eng.append(0.0)
        
        # Feature 2: Short interest momentum (rate of change)
        # Captures acceleration in short interest which can signal changing sentiment
        try:
            if t > 0:
                prev_si = max(abs(data[t-1, 0]), 1e-8)
                si_momentum = (data[t, 0] / prev_si) - 1
                eng.append(si_momentum)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 3: Bollinger Band Width (normalized by price)
        # Measures volatility and potential for mean reversion
        try:
            sma = np.mean(close_prices)
            std = np.std(close_prices)
            sma = max(abs(sma), 1e-8)
            bb_width = (2 * std) / sma
            eng.append(bb_width)
        except:
            eng.append(0.0)
        
        # Feature 4: RSI (Relative Strength Index)
        # Momentum oscillator that measures speed and change of price movements
        try:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0
            avg_loss = max(avg_loss, 1e-8)
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        except:
            eng.append(0.0)
        
        # Feature 5: Short interest to days to cover ratio
        # Combines two important metrics to create a more informative feature
        try:
            days_to_cover = max(data[t, 2], 1e-8)
            si_dtc_ratio = data[t, 0] / days_to_cover
            eng.append(si_dtc_ratio)
        except:
            eng.append(0.0)
        
        # Feature 6: Options put/call ratio to implied volatility ratio
        # Relates market sentiment (put/call) to expected volatility
        try:
            iv = max(data[t, 66], 1e-8)
            pc_iv_ratio = data[t, 64] / iv
            eng.append(pc_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 7: Price trend (normalized 5-day return)
        # Captures recent price momentum
        try:
            if len(close_prices) >= 5:
                price_5d_ago = max(abs(close_prices[-5]), 1e-8)
                price_trend = (close_prices[-1] / price_5d_ago) - 1
                eng.append(price_trend)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 8: Synthetic short cost to short interest ratio
        # Relates cost of shorting to actual short interest
        try:
            si = max(abs(data[t, 0]), 1e-8)
            cost_si_ratio = data[t, 65] / si
            eng.append(cost_si_ratio)
        except:
            eng.append(0.0)
        
        # Feature 9: VWAP (Volume Weighted Average Price) deviation
        # Measures price deviation from VWAP, indicating potential over/undervaluation
        try:
            # Calculate a simple approximation of VWAP using available data
            typical_prices = (high_prices + low_prices + close_prices) / 3
            vwap = np.mean(typical_prices)
            vwap = max(abs(vwap), 1e-8)
            vwap_deviation = (close_prices[-1] / vwap) - 1
            eng.append(vwap_deviation)
        except:
            eng.append(0.0)
        
        # Feature 10: Price volatility (normalized)
        # Measures price dispersion relative to mean price
        try:
            mean_price = np.mean(close_prices)
            mean_price = max(abs(mean_price), 1e-8)
            norm_volatility = np.std(close_prices) / mean_price
            eng.append(norm_volatility)
        except:
            eng.append(0.0)
        
        # Feature 11: Short squeeze potential indicator
        # Combines short interest, days to cover, and recent price momentum
        try:
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
                squeeze_potential = data[t, 0] * data[t, 2] * (1 + max(0, price_change))
                eng.append(squeeze_potential)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 12: Options implied volatility to price ratio
        # Normalizes implied volatility by price level
        try:
            price = max(abs(close_prices[-1]), 1e-8)
            iv_price_ratio = data[t, 66] / price
            eng.append(iv_price_ratio)
        except:
            eng.append(0.0)
        
        # Feature 13: Average True Range (ATR) - volatility measure
        # Measures average price range, accounting for gaps
        try:
            tr_values = []
            for i in range(1, len(close_prices)):
                high_low = high_prices[i] - low_prices[i]
                high_close_prev = abs(high_prices[i] - close_prices[i-1])
                low_close_prev = abs(low_prices[i] - close_prices[i-1])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            # Normalize ATR by price
            price = max(abs(close_prices[-1]), 1e-8)
            norm_atr = atr / price
            eng.append(norm_atr)
        except:
            eng.append(0.0)
        
        # Feature 14: Short interest to volume ratio
        # Relates short interest to trading volume
        try:
            vol = max(data[t, 1], 1e-8)  # Use average daily volume
            si_vol_ratio = data[t, 0] / vol
            eng.append(si_vol_ratio)
        except:
            eng.append(0.0)
        
        # Feature 15: Price to 15-day moving average ratio
        # Measures current price relative to recent average
        try:
            ma15 = np.mean(close_prices)
            ma15 = max(abs(ma15), 1e-8)
            price_ma_ratio = close_prices[-1] / ma15
            eng.append(price_ma_ratio)
        except:
            eng.append(0.0)
        
        # Feature 16: Chaikin Money Flow (CMF) - volume-based indicator
        # Measures buying and selling pressure
        try:
            mfv_sum = 0
            vol_sum = 0
            for i in range(min(10, len(close_prices))):
                if high_prices[i] == low_prices[i]:
                    mf_multiplier = 0
                else:
                    mf_multiplier = ((close_prices[i] - low_prices[i]) - (high_prices[i] - close_prices[i])) / (high_prices[i] - low_prices[i])
                mfv = mf_multiplier * 1  # Using 1 as a proxy for volume
                mfv_sum += mfv
                vol_sum += 1
            cmf = mfv_sum / max(vol_sum, 1e-8) if vol_sum > 0 else 0
            eng.append(cmf)
        except:
            eng.append(0.0)
        
        # Feature 17: Rate of change of options put/call ratio
        # Captures changing market sentiment
        try:
            if t > 0:
                prev_pc = max(abs(data[t-1, 64]), 1e-8)
                pc_roc = (data[t, 64] / prev_pc) - 1
                eng.append(pc_roc)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features = np.stack(features_list, axis=0)
    
    # Handle NaN and Inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 328950.5347
RMSE: 507338.3124
MAPE: 9.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0005, rank=1
   2. Feature_10_t1: importance=0.0005, rank=2
   3. Feature_14_t3: importance=0.0005, rank=3
   4. Feature_16_t3: importance=0.0005, rank=4
   5. Feature_6_t1: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.29%

ðŸ“ˆ Current best MAPE: 9.36%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume (critical base features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        
        # Keep days to cover (consistently high importance in previous iterations)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options-related features (high importance in previous iterations)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding (important for normalization)
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Keep volume (important for liquidity assessment)
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        try:
            ohlc = data[t, 3:63].reshape(15, 4)
            open_prices = ohlc[:, 0]
            high_prices = ohlc[:, 1]
            low_prices = ohlc[:, 2]
            close_prices = ohlc[:, 3]
            
            # Keep only the most recent close price
            raw_keep.append(close_prices[-1])  # Latest close
        except:
            raw_keep.append(0.0)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # Key metric for short squeeze potential - consistently high importance
        try:
            shares_out = max(data[t, 67], 1e-8)
            si_float_ratio = data[t, 0] / shares_out
            eng.append(si_float_ratio)
        except:
            eng.append(0.0)
        
        # Feature 2: Short interest momentum (rate of change)
        # Captures acceleration in short interest which can signal changing sentiment
        try:
            if t > 0:
                prev_si = max(abs(data[t-1, 0]), 1e-8)
                si_momentum = (data[t, 0] / prev_si) - 1
                eng.append(si_momentum)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 3: Options put/call ratio to implied volatility ratio
        # Relates market sentiment (put/call) to expected volatility
        # High importance in previous iterations (Feature_6_t1)
        try:
            iv = max(data[t, 66], 1e-8)
            pc_iv_ratio = data[t, 64] / iv
            eng.append(pc_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 4: RSI (Relative Strength Index) - 14-day
        # Momentum oscillator that measures speed and change of price movements
        # High importance in previous iterations (Feature_4_t3)
        try:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0
            avg_loss = max(avg_loss, 1e-8)
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        except:
            eng.append(0.0)
        
        # Feature 5: Short interest to volume ratio
        # Relates short interest to trading volume - high importance in previous iterations
        try:
            vol = max(data[t, 1], 1e-8)  # Use average daily volume
            si_vol_ratio = data[t, 0] / vol
            eng.append(si_vol_ratio)
        except:
            eng.append(0.0)
        
        # Feature 6: Price trend (normalized 5-day return)
        # Captures recent price momentum
        try:
            if len(close_prices) >= 5:
                price_5d_ago = max(abs(close_prices[-5]), 1e-8)
                price_trend = (close_prices[-1] / price_5d_ago) - 1
                eng.append(price_trend)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 7: Short squeeze potential indicator
        # Combines short interest, days to cover, and recent price momentum
        # More sophisticated version focusing on key factors
        try:
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
                vol_ratio = data[t, 68] / max(data[t, 1], 1e-8)  # Current volume to avg volume
                squeeze_potential = data[t, 0] * data[t, 2] * (1 + max(0, price_change)) * vol_ratio
                eng.append(squeeze_potential)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 8: Normalized Average True Range (ATR)
        # Measures volatility accounting for gaps - important for risk assessment
        try:
            tr_values = []
            for i in range(1, len(close_prices)):
                high_low = high_prices[i] - low_prices[i]
                high_close_prev = abs(high_prices[i] - close_prices[i-1])
                low_close_prev = abs(low_prices[i] - close_prices[i-1])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            # Normalize ATR by price
            price = max(abs(close_prices[-1]), 1e-8)
            norm_atr = atr / price
            eng.append(norm_atr)
        except:
            eng.append(0.0)
        
        # Feature 9: Price to 15-day moving average ratio
        # Measures current price relative to recent average - high importance in previous iterations
        try:
            ma15 = np.mean(close_prices)
            ma15 = max(abs(ma15), 1e-8)
            price_ma_ratio = close_prices[-1] / ma15
            eng.append(price_ma_ratio)
        except:
            eng.append(0.0)
        
        # Feature 10: Short interest change to price change ratio
        # New feature: Relates changes in short interest to price movements
        try:
            if t > 0 and len(close_prices) > 1:
                si_change = (data[t, 0] / max(abs(data[t-1, 0]), 1e-8)) - 1
                price_change = (close_prices[-1] / max(abs(close_prices[-2]), 1e-8)) - 1
                price_change = max(abs(price_change), 1e-8)
                si_price_change_ratio = si_change / price_change
                eng.append(si_price_change_ratio)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 11: Synthetic short cost to implied volatility ratio
        # New feature: Relates cost of shorting to market's expectation of volatility
        try:
            iv = max(data[t, 66], 1e-8)
            synthetic_cost_iv_ratio = data[t, 65] / iv
            eng.append(synthetic_cost_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 12: Bollinger Band Squeeze Indicator
        # Identifies potential breakout situations when volatility is low
        try:
            sma = np.mean(close_prices)
            std = np.std(close_prices)
            sma = max(abs(sma), 1e-8)
            bb_width = (2 * std) / sma
            
            # Calculate Keltner Channel width (simplified)
            atr = np.mean([high_prices[i] - low_prices[i] for i in range(len(high_prices))])
            kc_width = (2 * atr) / sma
            
            # BB squeeze occurs when BB width is less than KC width
            kc_width = max(kc_width, 1e-8)
            squeeze_ratio = bb_width / kc_width
            eng.append(squeeze_ratio)
        except:
            eng.append(0.0)
        
        # Feature 13: Volume Oscillator (short-term vs long-term volume)
        # Identifies volume trends that may precede price movements
        try:
            if t > 0:
                curr_vol = data[t, 68]
                prev_vol = data[t-1, 68]
                avg_vol = data[t, 1]
                avg_vol = max(avg_vol, 1e-8)
                vol_oscillator = ((curr_vol - prev_vol) / avg_vol)
                eng.append(vol_oscillator)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 14: Short Interest to Days to Cover Ratio Change
        # Tracks changes in the relationship between SI and DTC
        try:
            if t > 0:
                curr_si_dtc = data[t, 0] / max(data[t, 2], 1e-8)
                prev_si_dtc = data[t-1, 0] / max(data[t-1, 2], 1e-8)
                prev_si_dtc = max(abs(prev_si_dtc), 1e-8)
                si_dtc_change = (curr_si_dtc / prev_si_dtc) - 1
                eng.append(si_dtc_change)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 15: Price Momentum with Volume Confirmation
        # Combines price momentum with volume support
        try:
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
                avg_vol = max(data[t, 1], 1e-8)
                vol_ratio = data[t, 68] / avg_vol
                momentum_with_volume = price_change * vol_ratio
                eng.append(momentum_with_volume)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 16: Implied Volatility to Historical Volatility Ratio
        # Compares market expectations (IV) to realized volatility
        try:
            hist_vol = np.std(close_prices) / max(abs(np.mean(close_prices)), 1e-8)
            hist_vol = max(hist_vol, 1e-8)
            iv_hv_ratio = data[t, 66] / hist_vol
            eng.append(iv_hv_ratio)
        except:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features = np.stack(features_list, axis=0)
    
    # Handle NaN and Inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep these critical raw features
        raw_keep.append(data[t, 0])  # short interest (always keep)
        raw_keep.append(data[t, 1])  # average daily volume (always keep)
        raw_keep.append(data[t, 2])  # days to cover (important for short squeeze potential)
        
        # Keep options-related features (high importance in previous iterations)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume (important for normalization)
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        try:
            ohlc = data[t, 3:63].reshape(15, 4)
            open_prices = ohlc[:, 0]
            high_prices = ohlc[:, 1]
            low_prices = ohlc[:, 2]
            close_prices = ohlc[:, 3]
            
            # Keep only the most recent close price
            raw_keep.append(close_prices[-1])  # Latest close
        except:
            # Handle case where OHLC data might be malformed
            raw_keep.append(0.0)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # Key metric for short squeeze potential
        try:
            shares_out = max(data[t, 67], 1e-8)
            si_float_ratio = data[t, 0] / shares_out
            eng.append(si_float_ratio)
        except:
            eng.append(0.0)
        
        # Feature 2: Short interest momentum (rate of change)
        # Captures acceleration in short interest which can signal changing sentiment
        try:
            if t > 0:
                prev_si = max(abs(data[t-1, 0]), 1e-8)
                si_momentum = (data[t, 0] / prev_si) - 1
                eng.append(si_momentum)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 3: Short interest to days to cover ratio
        # Combines two important metrics to create a more informative feature
        try:
            days_to_cover = max(data[t, 2], 1e-8)
            si_dtc_ratio = data[t, 0] / days_to_cover
            eng.append(si_dtc_ratio)
        except:
            eng.append(0.0)
        
        # Feature 4: Options put/call ratio to implied volatility ratio
        # Relates market sentiment (put/call) to expected volatility
        try:
            iv = max(data[t, 66], 1e-8)
            pc_iv_ratio = data[t, 64] / iv
            eng.append(pc_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 5: Short interest to volume ratio
        # Relates short interest to trading volume - high importance in previous iterations
        try:
            vol = max(data[t, 1], 1e-8)  # Use average daily volume
            si_vol_ratio = data[t, 0] / vol
            eng.append(si_vol_ratio)
        except:
            eng.append(0.0)
        
        # Feature 6: Synthetic short cost to short interest ratio
        # Relates cost of shorting to actual short interest
        try:
            si = max(abs(data[t, 0]), 1e-8)
            cost_si_ratio = data[t, 65] / si
            eng.append(cost_si_ratio)
        except:
            eng.append(0.0)
        
        # Feature 7: Price volatility (normalized)
        # Measures price dispersion relative to mean price
        try:
            mean_price = np.mean(close_prices)
            mean_price = max(abs(mean_price), 1e-8)
            norm_volatility = np.std(close_prices) / mean_price
            eng.append(norm_volatility)
        except:
            eng.append(0.0)
        
        # Feature 8: Short squeeze potential indicator
        # Combines short interest, days to cover, and recent price momentum
        try:
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
                squeeze_potential = data[t, 0] * data[t, 2] * (1 + max(0, price_change))
                eng.append(squeeze_potential)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 9: Options implied volatility to price ratio
        # Normalizes implied volatility by price level
        try:
            price = max(abs(close_prices[-1]), 1e-8)
            iv_price_ratio = data[t, 66] / price
            eng.append(iv_price_ratio)
        except:
            eng.append(0.0)
        
        # Feature 10: Price to 15-day moving average ratio
        # Measures current price relative to recent average
        try:
            ma15 = np.mean(close_prices)
            ma15 = max(abs(ma15), 1e-8)
            price_ma_ratio = close_prices[-1] / ma15
            eng.append(price_ma_ratio)
        except:
            eng.append(0.0)
        
        # Feature 11: Rate of change of options put/call ratio
        # Captures changing market sentiment
        try:
            if t > 0:
                prev_pc = max(abs(data[t-1, 64]), 1e-8)
                pc_roc = (data[t, 64] / prev_pc) - 1
                eng.append(pc_roc)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 12: Short interest acceleration (second derivative)
        # Captures rapid changes in short interest trends
        try:
            if t >= 2:
                si_t = data[t, 0]
                si_t1 = data[t-1, 0]
                si_t2 = data[t-2, 0]
                
                # First differences
                d1 = si_t - si_t1
                d2 = si_t1 - si_t2
                
                # Second difference (acceleration)
                si_accel = d1 - d2
                
                # Normalize by current short interest
                si_norm = max(abs(si_t), 1e-8)
                norm_si_accel = si_accel / si_norm
                eng.append(norm_si_accel)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 13: Relative volume (current volume to average volume ratio)
        # Indicates unusual trading activity
        try:
            avg_vol = max(data[t, 1], 1e-8)
            curr_vol = max(data[t, 68], 1e-8)
            rel_volume = curr_vol / avg_vol
            eng.append(rel_volume)
        except:
            eng.append(0.0)
        
        # Feature 14: Short interest to implied volatility ratio
        # Relates short interest to market expectations of volatility
        try:
            iv = max(data[t, 66], 1e-8)
            si_iv_ratio = data[t, 0] / iv
            eng.append(si_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 15: Price momentum (5-day rate of change)
        # Captures recent price momentum
        try:
            if len(close_prices) >= 5:
                price_5d_ago = max(abs(close_prices[-5]), 1e-8)
                price_momentum = (close_prices[-1] / price_5d_ago) - 1
                eng.append(price_momentum)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 16: Volatility-adjusted short interest
        # Scales short interest by market volatility expectations
        try:
            iv = max(data[t, 66], 1e-8)
            vol_adj_si = data[t, 0] * iv
            eng.append(vol_adj_si)
        except:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features = np.stack(features_list, axis=0)
    
    # Handle NaN and Inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 2/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Keep the most important raw features based on previous iterations
        raw_keep.append(data[t, 0])  # short interest (always keep)
        raw_keep.append(data[t, 1])  # average daily volume (always keep)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options-related features (high importance in previous iterations)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume (important for normalization)
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        try:
            ohlc = data[t, 3:63].reshape(15, 4)
            open_prices = ohlc[:, 0]
            high_prices = ohlc[:, 1]
            low_prices = ohlc[:, 2]
            close_prices = ohlc[:, 3]
            
            # Keep only the most recent close price
            raw_keep.append(close_prices[-1])  # Latest close
        except:
            # Handle potential reshape errors
            raw_keep.append(0.0)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # Key metric for short squeeze potential
        try:
            shares_out = max(data[t, 67], 1e-8)
            si_float_ratio = data[t, 0] / shares_out
            eng.append(si_float_ratio)
        except:
            eng.append(0.0)
        
        # Feature 2: Short interest momentum (rate of change)
        # Captures acceleration in short interest which can signal changing sentiment
        try:
            if t > 0:
                prev_si = max(abs(data[t-1, 0]), 1e-8)
                si_momentum = (data[t, 0] / prev_si) - 1
                eng.append(si_momentum)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 3: Short interest to volume ratio
        # Relates short interest to trading volume - high importance in previous iterations
        try:
            vol = max(data[t, 1], 1e-8)  # Use average daily volume
            si_vol_ratio = data[t, 0] / vol
            eng.append(si_vol_ratio)
        except:
            eng.append(0.0)
        
        # Feature 4: Options put/call ratio to implied volatility ratio
        # Relates market sentiment (put/call) to expected volatility - high importance in previous iterations
        try:
            iv = max(data[t, 66], 1e-8)
            pc_iv_ratio = data[t, 64] / iv
            eng.append(pc_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 5: Short interest to days to cover ratio
        # Combines two important metrics to create a more informative feature
        try:
            days_to_cover = max(data[t, 2], 1e-8)
            si_dtc_ratio = data[t, 0] / days_to_cover
            eng.append(si_dtc_ratio)
        except:
            eng.append(0.0)
        
        # Feature 6: RSI (Relative Strength Index) - high importance in previous iterations
        # Momentum oscillator that measures speed and change of price movements
        try:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0
            avg_loss = max(avg_loss, 1e-8)
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        except:
            eng.append(0.0)
        
        # Feature 7: Price to 15-day moving average ratio - high importance in previous iterations
        # Measures current price relative to recent average
        try:
            ma15 = np.mean(close_prices)
            ma15 = max(abs(ma15), 1e-8)
            price_ma_ratio = close_prices[-1] / ma15
            eng.append(price_ma_ratio)
        except:
            eng.append(0.0)
        
        # Feature 8: Price volatility (normalized)
        # Measures price dispersion relative to mean price - high importance in previous iterations
        try:
            mean_price = np.mean(close_prices)
            mean_price = max(abs(mean_price), 1e-8)
            norm_volatility = np.std(close_prices) / mean_price
            eng.append(norm_volatility)
        except:
            eng.append(0.0)
        
        # Feature 9: Short squeeze potential indicator - combining multiple factors
        # Combines short interest, days to cover, and recent price momentum
        try:
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
                squeeze_potential = data[t, 0] * data[t, 2] * (1 + max(0, price_change))
                eng.append(squeeze_potential)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 10: Synthetic short cost to short interest ratio
        # Relates cost of shorting to actual short interest
        try:
            si = max(abs(data[t, 0]), 1e-8)
            cost_si_ratio = data[t, 65] / si
            eng.append(cost_si_ratio)
        except:
            eng.append(0.0)
        
        # Feature 11: Rate of change of options put/call ratio
        # Captures changing market sentiment
        try:
            if t > 0:
                prev_pc = max(abs(data[t-1, 64]), 1e-8)
                pc_roc = (data[t, 64] / prev_pc) - 1
                eng.append(pc_roc)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 12: Bollinger Band Width (normalized by price)
        # Measures volatility and potential for mean reversion
        try:
            sma = np.mean(close_prices)
            std = np.std(close_prices)
            sma = max(abs(sma), 1e-8)
            bb_width = (2 * std) / sma
            eng.append(bb_width)
        except:
            eng.append(0.0)
        
        # Feature 13: MACD Line (Moving Average Convergence Divergence)
        # Trend-following momentum indicator showing relationship between two moving averages
        try:
            if len(close_prices) >= 12:
                ema12 = np.mean(close_prices[-12:])  # Simple approximation of EMA
                ema26 = np.mean(close_prices)  # Using all available data as approximation
                macd_line = ema12 - ema26
                # Normalize by price
                price = max(abs(close_prices[-1]), 1e-8)
                norm_macd = macd_line / price
                eng.append(norm_macd)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Feature 14: Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations of volatility
        try:
            iv = max(data[t, 66], 1e-8)
            si_iv_ratio = data[t, 0] / iv
            eng.append(si_iv_ratio)
        except:
            eng.append(0.0)
        
        # Feature 15: Normalized Price Range
        # Measures recent price range relative to price level
        try:
            price_range = np.max(high_prices) - np.min(low_prices)
            avg_price = np.mean(close_prices)
            avg_price = max(abs(avg_price), 1e-8)
            norm_range = price_range / avg_price
            eng.append(norm_range)
        except:
            eng.append(0.0)
        
        # Feature 16: Volume Oscillator
        # Measures volume momentum
        try:
            if t > 0:
                curr_vol = data[t, 68]
                prev_vol = data[t-1, 68]
                prev_vol = max(abs(prev_vol), 1e-8)
                vol_osc = (curr_vol / prev_vol) - 1
                eng.append(vol_osc)
            else:
                eng.append(0.0)
        except:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features = np.stack(features_list, axis=0)
    
    # Handle NaN and Inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 3/3)
âš ï¸ All function execution attempts failed, using fallback

ðŸ”§ Applying feature selection using fallback function...
Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 1: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 1 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 1}), (1, 'Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 1}), (2, 'Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 1})]
ðŸ”„ Retrying... (1/5)
Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 2: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 2 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 2}), (1, 'Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 2}), (2, 'Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 2})]
ðŸ”„ Retrying... (2/5)
Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 3: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 3 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 3}), (1, 'Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 3}), (2, 'Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 3})]
ðŸ”„ Retrying... (3/5)
Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 4: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 4 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 4}), (1, 'Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 4}), (2, 'Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 4})]
ðŸ”„ Retrying... (4/5)
Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 5: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 5 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 5}), (1, 'Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 5}), (2, 'Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 5})]
âš ï¸ All 5 attempts failed, using fallback function
ðŸ†˜ Using fallback feature selection...
Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 1: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 1 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 1}), (1, 'Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 1}), (2, 'Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 1})]
ðŸ”„ Retrying... (1/5)
Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 2: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 2 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 2}), (1, 'Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 2}), (2, 'Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 2})]
ðŸ”„ Retrying... (2/5)
Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 3: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 3 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 3}), (1, 'Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 3}), (2, 'Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 3})]
ðŸ”„ Retrying... (3/5)
Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 4: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 4 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 4}), (1, 'Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 4}), (2, 'Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 4})]
ðŸ”„ Retrying... (4/5)
Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 5: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 5 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 5}), (1, 'Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 5}), (2, 'Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 5})]
âš ï¸ All 5 attempts failed, using fallback function
ðŸ†˜ Using fallback feature selection...
Training data shape: (106, 4, 68) -> (106, 4, 15)
Validation data shape: (36, 4, 68) -> (36, 4, 15)
âš ï¸ Total errors encountered: 720
  Error 1: ValueError - cannot reshape array of size 66 into shape (15,4)
  Error 2: ValueError - cannot reshape array of size 66 into shape (15,4)
  Error 3: ValueError - cannot reshape array of size 66 into shape (15,4)

==================================================
Training Iteration 3 (fallback) (SVM)
==================================================
Training SVM model...

Iteration 3 (fallback) Performance:
MAE: 392038.5028
RMSE: 562162.7434
MAPE: 11.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 60
   â€¢ Important features (top 10%): 36
   â€¢ Highly important features (top 5%): 19

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t0: importance=0.0014, rank=1
   2. Feature_1_t2: importance=0.0012, rank=2
   3. Feature_1_t3: importance=0.0010, rank=3
   4. Feature_2_t3: importance=0.0007, rank=4
   5. Feature_0_t3: importance=0.0006, rank=5
ðŸ“Š No significant improvement. Change: -2.12%

ðŸ“ˆ Current best MAPE: 9.36%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Construct features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep key options data (high importance in previous iterations)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume (high importance)
        raw_keep.append(data[t, 67])  # shares outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio to Outstanding Shares
        # This normalizes short interest by shares outstanding to get relative short exposure
        si_to_outstanding = data[t, 0] / max(abs(data[t, 67]), 1e-8)
        eng.append(si_to_outstanding)
        
        # 2. Short Interest to Volume Ratio
        # Measures how many days of current volume would be needed to cover short positions
        si_to_volume = data[t, 0] / max(abs(data[t, 68]), 1e-8)
        eng.append(si_to_volume)
        
        # 3. Price Momentum (5-day)
        # Strong price momentum may indicate short squeeze potential
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Longer-term momentum to capture broader trends
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
        else:
            momentum_10d = 0.0
        eng.append(momentum_10d)
        
        # 5. Volatility (5-day)
        # Higher volatility often correlates with short interest changes
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(abs(close_prices[-6:-1]), 1e-8)
            volatility_5d = np.std(returns) if len(returns) > 0 else 0.0
        else:
            volatility_5d = 0.0
        eng.append(volatility_5d)
        
        # 6. Average True Range (ATR) - Volatility measure
        # ATR is a volatility indicator that can signal potential short covering
        atr_sum = 0.0
        atr_count = 0
        for i in range(1, min(10, len(high_prices))):
            true_range = max(
                high_prices[i] - low_prices[i],
                abs(high_prices[i] - close_prices[i-1]),
                abs(low_prices[i] - close_prices[i-1])
            )
            atr_sum += true_range
            atr_count += 1
        atr = atr_sum / max(atr_count, 1)
        eng.append(atr)
        
        # 7. Options Implied Volatility to Historical Volatility Ratio
        # Compares market expectations (implied vol) to realized volatility
        if volatility_5d > 0:
            iv_to_hv_ratio = data[t, 66] / max(volatility_5d, 1e-8)
        else:
            iv_to_hv_ratio = 0.0
        eng.append(iv_to_hv_ratio)
        
        # 8. Put-Call Ratio Change
        # Significant changes in put-call ratio can signal changing sentiment
        if t > 0:
            pc_ratio_change = data[t, 64] / max(abs(data[t-1, 64]), 1e-8) - 1.0
        else:
            pc_ratio_change = 0.0
        eng.append(pc_ratio_change)
        
        # 9. Volume Surge Indicator
        # Unusual volume can precede short covering
        avg_vol = data[t, 1]
        current_vol = data[t, 68]
        vol_surge = current_vol / max(abs(avg_vol), 1e-8) - 1.0
        eng.append(vol_surge)
        
        # 10. RSI (Relative Strength Index)
        # Extreme RSI values can indicate potential reversals relevant to shorts
        if len(close_prices) >= 14:
            diff = np.diff(close_prices[-14:])
            gains = np.sum(np.where(diff > 0, diff, 0))
            losses = np.sum(np.where(diff < 0, -diff, 0))
            
            if losses > 0:
                rs = gains / max(losses, 1e-8)
                rsi = 100 - (100 / (1 + rs))
            elif gains > 0:
                rsi = 100.0
            else:
                rsi = 50.0
        else:
            rsi = 50.0
        eng.append(rsi)
        
        # 11. Bollinger Band Width
        # Measures volatility and potential for mean reversion
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            bb_width = (2 * std_20) / max(abs(sma_20), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 12. Short Cost Pressure
        # Combines synthetic short cost with days to cover
        short_cost_pressure = data[t, 65] * data[t, 2]
        eng.append(short_cost_pressure)
        
        # 13. Price to Moving Average Ratio
        # Measures current price relative to trend
        if len(close_prices) >= 10:
            sma_10 = np.mean(close_prices[-10:])
            price_to_ma = close_prices[-1] / max(abs(sma_10), 1e-8)
        else:
            price_to_ma = 1.0
        eng.append(price_to_ma)
        
        # 14. Short Interest Momentum
        # Rate of change in short interest
        if t > 0:
            si_momentum = data[t, 0] / max(abs(data[t-1, 0]), 1e-8) - 1.0
        else:
            si_momentum = 0.0
        eng.append(si_momentum)
        
        # 15. Options Synthetic Short Cost Change
        # Rate of change in cost to short
        if t > 0:
            short_cost_change = data[t, 65] / max(abs(data[t-1, 65]), 1e-8) - 1.0
        else:
            short_cost_change = 0.0
        eng.append(short_cost_change)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Construct features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    # Constants
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (most important from previous iterations)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        raw_keep.append(open_prices[-1])
        raw_keep.append(high_prices[-1])
        raw_keep.append(low_prices[-1])
        raw_keep.append(close_prices[-1])
        
        # Keep options data which showed importance in previous iterations
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68] if data.shape[1] > 68 else 0)  # volume (with safety check)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio to Outstanding Shares
        # This normalizes short interest by company size
        si_to_outstanding = 0.0
        if data[t, 67] > 1e-8:  # Avoid division by zero
            si_to_outstanding = data[t, 0] / data[t, 67]
        eng.append(si_to_outstanding)
        
        # 2. Short Interest to Volume Ratio
        # Measures how many days of current volume would be needed to cover shorts
        si_to_volume = 0.0
        if data[t, 1] > 1e-8:  # Avoid division by zero
            si_to_volume = data[t, 0] / data[t, 1]
        eng.append(si_to_volume)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Captures medium-term price trend
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
        else:
            momentum_10d = 0.0
        eng.append(momentum_10d)
        
        # 5. Volatility (standard deviation of returns over 10 days)
        # High volatility often correlates with short interest changes
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
        else:
            volatility = 0.0
        eng.append(volatility)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility
        atr = 0.0
        if len(high_prices) >= 2 and len(low_prices) >= 2 and len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(high_prices))):
                tr1 = high_prices[-i] - low_prices[-i]
                tr2 = abs(high_prices[-i] - close_prices[-(i+1)])
                tr3 = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(tr1, tr2, tr3))
            atr = np.mean(true_ranges) if true_ranges else 0.0
        eng.append(atr)
        
        # 7. Options Put/Call Ratio Change
        # Rate of change in sentiment indicator
        put_call_change = 0.0
        if t > 0 and abs(data[t-1, 64]) > 1e-8:
            put_call_change = (data[t, 64] / data[t-1, 64]) - 1.0
        eng.append(put_call_change)
        
        # 8. Synthetic Short Cost Change
        # Rate of change in cost to short
        synth_cost_change = 0.0
        if t > 0 and abs(data[t-1, 65]) > 1e-8:
            synth_cost_change = (data[t, 65] / data[t-1, 65]) - 1.0
        eng.append(synth_cost_change)
        
        # 9. Implied Volatility Change
        # Rate of change in market's expectation of future volatility
        iv_change = 0.0
        if t > 0 and abs(data[t-1, 66]) > 1e-8:
            iv_change = (data[t, 66] / data[t-1, 66]) - 1.0
        eng.append(iv_change)
        
        # 10. Volume Trend (5-day)
        # Indicates unusual trading activity
        if len(close_prices) >= 5 and data.shape[1] > 68:
            recent_volumes = [data[max(0, t-i), 68] for i in range(5)]
            avg_volume = np.mean(recent_volumes) if recent_volumes else 0.0
            volume_trend = (data[t, 68] / max(avg_volume, 1e-8)) - 1.0 if avg_volume > 0 else 0.0
        else:
            volume_trend = 0.0
        eng.append(volume_trend)
        
        # 11. RSI (Relative Strength Index)
        # Momentum oscillator measuring speed and change of price movements
        rsi = 0.0
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss > 1e-8:
                rs = gain / loss
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gain > 0:
                rsi = 100.0
            else:
                rsi = 50.0
        eng.append(rsi)
        
        # 12. Price to Volume Ratio
        # Indicates how much price movement is generated by a unit of volume
        price_to_volume = 0.0
        if data.shape[1] > 68 and data[t, 68] > 1e-8:
            price_to_volume = close_prices[-1] / data[t, 68]
        eng.append(price_to_volume)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 332554.2511
RMSE: 497213.3054
MAPE: 9.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t1: importance=0.0010, rank=1
   2. Feature_8_t3: importance=0.0007, rank=2
   3. Feature_20_t1: importance=0.0006, rank=3
   4. Feature_14_t3: importance=0.0006, rank=4
   5. Feature_15_t2: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.29%

ðŸ“ˆ Current best MAPE: 9.36%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Construct features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    # Constants
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Keep the most important raw features based on previous iterations
        raw_keep.append(data[t, 0])  # short interest - consistently important
        raw_keep.append(data[t, 1])  # average daily volume - consistently important
        raw_keep.append(data[t, 2])  # days to cover - important for short interest dynamics
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last day)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep options data which showed importance in previous iterations
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Outstanding Shares Ratio
        # Normalizes short interest by company size - key metric for short squeeze potential
        si_to_outstanding = 0.0
        if data[t, 67] > 1e-8:
            si_to_outstanding = data[t, 0] / data[t, 67]
        eng.append(si_to_outstanding)
        
        # 2. Short Interest Change Rate (if we have previous data)
        # Rate of change in short interest - strong predictor of future movements
        si_change = 0.0
        if t > 0 and data[t-1, 0] > 1e-8:
            si_change = (data[t, 0] / data[t-1, 0]) - 1.0
        eng.append(si_change)
        
        # 3. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - indicates momentum shifts
        si_acceleration = 0.0
        if t > 1 and data[t-2, 0] > 1e-8 and data[t-1, 0] > 1e-8:
            prev_change = (data[t-1, 0] / data[t-2, 0]) - 1.0
            current_change = (data[t, 0] / data[t-1, 0]) - 1.0
            si_acceleration = current_change - prev_change
        eng.append(si_acceleration)
        
        # 4. Price Trend (5-day)
        # Short-term price trend - shorts often target downtrending stocks
        price_trend_5d = 0.0
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_trend_5d)
        
        # 5. Price Volatility (10-day)
        # Volatility often correlates with short interest changes
        price_volatility = 0.0
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-10:-1], 1e-8)
            price_volatility = np.std(returns) if len(returns) > 0 else 0.0
        eng.append(price_volatility)
        
        # 6. Options Put/Call Ratio Change
        # Rate of change in sentiment indicator - shows shifting market sentiment
        put_call_change = 0.0
        if t > 0 and abs(data[t-1, 64]) > 1e-8:
            put_call_change = (data[t, 64] / data[t-1, 64]) - 1.0
        eng.append(put_call_change)
        
        # 7. Synthetic Short Cost Change
        # Rate of change in cost to short - indicates changing difficulty to short
        synth_cost_change = 0.0
        if t > 0 and abs(data[t-1, 65]) > 1e-8:
            synth_cost_change = (data[t, 65] / data[t-1, 65]) - 1.0
        eng.append(synth_cost_change)
        
        # 8. Implied Volatility Change
        # Rate of change in market's expectation of future volatility
        iv_change = 0.0
        if t > 0 and abs(data[t-1, 66]) > 1e-8:
            iv_change = (data[t, 66] / data[t-1, 66]) - 1.0
        eng.append(iv_change)
        
        # 9. RSI (Relative Strength Index)
        # Momentum oscillator - extreme values often precede short interest changes
        rsi = 50.0  # Default to neutral
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss > 1e-8:
                rs = gain / loss
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gain > 0:
                rsi = 100.0
        eng.append(rsi)
        
        # 10. Short Interest to Volume Ratio
        # Measures how many days of current volume would be needed to cover shorts
        si_to_volume = 0.0
        if data[t, 1] > 1e-8:
            si_to_volume = data[t, 0] / data[t, 1]
        eng.append(si_to_volume)
        
        # 11. Bollinger Band Position
        # Position of price within volatility bands - indicates potential mean reversion
        bb_position = 0.5  # Default to middle
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            if std_20 > 1e-8:
                upper_band = sma_20 + (2 * std_20)
                lower_band = sma_20 - (2 * std_20)
                band_width = upper_band - lower_band
                if band_width > 1e-8:
                    bb_position = (close_prices[-1] - lower_band) / band_width
        eng.append(bb_position)
        
        # 12. MACD Signal
        # Momentum and trend indicator - captures price momentum shifts
        macd_signal = 0.0
        if len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema_26 = np.mean(close_prices[-26:])
            macd_signal = ema_12 - ema_26
        eng.append(macd_signal)
        
        # 13. Price to Implied Volatility Ratio
        # Normalized price by expected volatility - indicates relative valuation
        price_to_iv = 0.0
        if data[t, 66] > 1e-8:
            price_to_iv = close_prices[-1] / data[t, 66]
        eng.append(price_to_iv)
        
        # 14. Short Interest to Implied Volatility Ratio
        # Relationship between short interest and expected volatility
        si_to_iv = 0.0
        if data[t, 66] > 1e-8:
            si_to_iv = data[t, 0] / data[t, 66]
        eng.append(si_to_iv)
        
        # 15. Composite Momentum Indicator
        # Combines price and options sentiment - multi-factor momentum signal
        composite_momentum = 0.0
        if len(close_prices) >= 10 and data[t, 64] > 1e-8:
            price_momentum = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            inverse_put_call = 1.0 / data[t, 64]
            composite_momentum = price_momentum * inverse_put_call
        eng.append(composite_momentum)
        
        # 16. Short Interest Trend vs Price Trend
        # Divergence between short interest and price - indicates potential reversals
        si_price_divergence = 0.0
        if t > 0 and data[t-1, 0] > 1e-8 and len(close_prices) >= 2 and close_prices[-2] > 1e-8:
            si_trend = (data[t, 0] / data[t-1, 0]) - 1.0
            price_trend = (close_prices[-1] / close_prices[-2]) - 1.0
            si_price_divergence = si_trend - price_trend
        eng.append(si_price_divergence)
        
        # 17. Average True Range (ATR) - volatility indicator
        # Measures market volatility - high ATR often precedes short interest changes
        atr = 0.0
        if len(high_prices) >= 2 and len(low_prices) >= 2 and len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(high_prices))):
                tr1 = high_prices[-i] - low_prices[-i]
                tr2 = abs(high_prices[-i] - close_prices[-(i+1)])
                tr3 = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(tr1, tr2, tr3))
            atr = np.mean(true_ranges) if true_ranges else 0.0
        eng.append(atr)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 322311.2678
RMSE: 487785.7291
MAPE: 9.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t2: importance=0.0016, rank=1
   2. Feature_12_t1: importance=0.0009, rank=2
   3. Feature_7_t1: importance=0.0008, rank=3
   4. Feature_11_t3: importance=0.0007, rank=4
   5. Feature_24_t1: importance=0.0007, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.10%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 431841.2019
RMSE: 572325.1398
MAPE: 10.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0007, rank=1
   2. Feature_67_t0: importance=0.0006, rank=2
   3. Feature_65_t1: importance=0.0005, rank=3
   4. Feature_1_t3: importance=0.0004, rank=4
   5. Feature_67_t1: importance=0.0003, rank=5
   Baseline MAPE: 10.73%
   Baseline MAE: 431841.2019
   Baseline RMSE: 572325.1398

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 390371.3609
RMSE: 529987.6960
MAPE: 9.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0008, rank=1
   2. Feature_11_t3: importance=0.0005, rank=2
   3. Feature_7_t0: importance=0.0004, rank=3
   4. Feature_14_t1: importance=0.0004, rank=4
   5. Feature_5_t2: importance=0.0004, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 9.95%
   MAE: 390371.3609
   RMSE: 529987.6960

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 10.73%
   Best Model MAPE: 9.95%
   Absolute Improvement: 0.78%
   Relative Improvement: 7.3%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  9.36            N/A                 
1          Iteration 1               9.57            -0.21%              
2          Iteration 2               9.65            -0.29%              
3          Iteration 3               11.48           -2.12%              
4          Iteration 4               9.66            -0.29%              
5          Iteration 5               9.27            +0.10%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 5 - MAPE: 9.27%
âœ… Saved IART results to cache/IART_iterative_results_enhanced.pkl
âœ… Summary report saved for IART

ðŸŽ‰ Process completed successfully for IART!

================================================================================
PROCESSING TICKER 9/15: SRPT
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for SRPT
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for SRPT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SRPT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 512372.3945
RMSE: 767191.3221
MAPE: 9.20%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_66_t1: importance=0.0003, rank=1
   2. Feature_2_t0: importance=0.0002, rank=2
   3. Feature_64_t3: importance=0.0001, rank=3
   4. Feature_66_t2: importance=0.0001, rank=4
   5. Feature_65_t3: importance=0.0001, rank=5

ðŸ“Š Baseline Performance: MAPE = 9.20%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these critical raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract options data (high importance in baseline)
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep high-importance options data
        raw_keep.append(implied_volatility)  # High importance in baseline
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        si_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            momentum_5d = 0.0
        eng.append(momentum_5d)
        
        # 4. Price momentum (10-day)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
        else:
            momentum_10d = 0.0
        eng.append(momentum_10d)
        
        # 5. Volatility (standard deviation of returns over 10 days)
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(np.abs(close_prices[-11:-1]), 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
        else:
            volatility = 0.0
        eng.append(volatility)
        
        # 6. Average True Range (ATR) - volatility indicator
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
        else:
            atr = 0.0
        eng.append(atr)
        
        # 7. Relative Strength Index (RSI)
        if len(close_prices) >= 15:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0.0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0.0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50.0  # Neutral RSI when not enough data
        eng.append(rsi)
        
        # 8. Bollinger Band Width (volatility measure)
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 9. Price to Volume Ratio
        price_volume_ratio = close_prices[-1] / max(abs(avg_daily_volume), 1e-8)
        eng.append(price_volume_ratio)
        
        # 10. Short Interest Growth Rate (if we have previous data)
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 11. Implied Volatility to Historical Volatility Ratio
        if volatility > 0:
            iv_hv_ratio = implied_volatility / max(abs(volatility), 1e-8)
        else:
            iv_hv_ratio = 0.0
        eng.append(iv_hv_ratio)
        
        # 12. Short Interest to Float Ratio
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 13. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 14. Price Range Ratio (High-Low)/Close
        if len(close_prices) > 0:
            price_range_ratio = (high_prices[-1] - low_prices[-1]) / max(abs(close_prices[-1]), 1e-8)
        else:
            price_range_ratio = 0.0
        eng.append(price_range_ratio)
        
        # 15. Short Cost Pressure (synthetic_short_cost * days_to_cover)
        short_cost_pressure = synthetic_short_cost * days_to_cover
        eng.append(short_cost_pressure)
        
        # 16. OHLC Volatility (High-Low)/(Open+Close)
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 17. Short Interest Acceleration (if we have enough history)
        si_acceleration = 0.0
        if t >= 2:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            prev_growth = (prev_si / max(abs(prev_prev_si), 1e-8)) - 1.0
            current_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            si_acceleration = current_growth - prev_growth
        eng.append(si_acceleration)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 480650.7386
RMSE: 724016.0696
MAPE: 8.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_17_t2: importance=0.0002, rank=1
   2. Feature_11_t3: importance=0.0001, rank=2
   3. Feature_21_t0: importance=0.0001, rank=3
   4. Feature_23_t3: importance=0.0001, rank=4
   5. Feature_20_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.64%

ðŸ“ˆ Current best MAPE: 8.56%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data (high importance in baseline)
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        raw_keep.append(implied_volatility)  # High importance in baseline
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (improved version)
        # This was highly significant in previous iteration
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio (improved version)
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Cost Pressure (synthetic_short_cost * days_to_cover)
        # This combines two important metrics to measure the pressure on short sellers
        short_cost_pressure = synthetic_short_cost * days_to_cover
        eng.append(short_cost_pressure)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Short Interest Acceleration (if we have enough history)
        # Second derivative of short interest - captures acceleration in shorting activity
        si_acceleration = 0.0
        if t >= 2:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            prev_growth = (prev_si / max(abs(prev_prev_si), 1e-8)) - 1.0
            current_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            si_acceleration = current_growth - prev_growth
        eng.append(si_acceleration)
        
        # 6. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 7. Implied Volatility to Price Ratio
        # Normalizes implied volatility by price level
        iv_price_ratio = implied_volatility / max(abs(close_prices[-1]), 1e-8)
        eng.append(iv_price_ratio)
        
        # 8. Recent Price Trend (5-day)
        # Captures recent price momentum
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 9. Price Volatility (standard deviation of returns over 10 days)
        # Measures recent price volatility
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(np.abs(close_prices[-11:-1]), 1e-8)
            price_volatility = np.std(returns) if len(returns) > 0 else 0.0
        else:
            price_volatility = 0.0
        eng.append(price_volatility)
        
        # 10. Average True Range (ATR) - volatility indicator
        # More robust measure of volatility that accounts for gaps
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
        else:
            atr = 0.0
        eng.append(atr)
        
        # 11. Normalized ATR (ATR/Close)
        # ATR normalized by price level for better comparison across time
        norm_atr = atr / max(abs(close_prices[-1]), 1e-8)
        eng.append(norm_atr)
        
        # 12. Volume Trend (ratio of recent volume to longer-term average)
        # Captures unusual volume activity
        if len(close_prices) >= 10:
            recent_vol = avg_daily_volume
            if t >= 2:
                prev_vol = data[t-2, 1]
                volume_trend = recent_vol / max(abs(prev_vol), 1e-8)
            else:
                volume_trend = 1.0
        else:
            volume_trend = 1.0
        eng.append(volume_trend)
        
        # 13. Short Interest to Days to Cover Ratio
        # Normalizes short interest by days to cover
        si_dtc_ratio = short_interest / max(abs(days_to_cover), 1e-8)
        eng.append(si_dtc_ratio)
        
        # 14. Price to Synthetic Short Cost Ratio
        # Measures the cost of shorting relative to price
        price_short_cost_ratio = close_prices[-1] / max(abs(synthetic_short_cost), 1e-8)
        eng.append(price_short_cost_ratio)
        
        # 15. Relative Strength Index (RSI)
        # Popular momentum oscillator
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain) if len(gain) > 0 else 0.0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0.0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50.0  # Neutral RSI when not enough data
        eng.append(rsi)
        
        # 16. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 17. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations of volatility
        si_iv_ratio = short_interest / max(abs(implied_volatility), 1e-8)
        eng.append(si_iv_ratio)
        
        # 18. Bollinger Band Width (volatility measure)
        # Measures price volatility relative to moving average
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 481008.7579
RMSE: 722444.0221
MAPE: 8.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t3: importance=0.0001, rank=1
   2. Feature_21_t0: importance=0.0001, rank=2
   3. Feature_13_t2: importance=0.0001, rank=3
   4. Feature_5_t3: importance=0.0001, rank=4
   5. Feature_15_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.01%

ðŸ“ˆ Current best MAPE: 8.56%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data (high importance in baseline)
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        raw_keep.append(implied_volatility)  # High importance in baseline
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio (improved version)
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Cost Pressure (synthetic_short_cost * days_to_cover)
        # This combines two important metrics to measure the pressure on short sellers
        short_cost_pressure = synthetic_short_cost * days_to_cover
        eng.append(short_cost_pressure)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Short Interest Acceleration (if we have enough history)
        # Second derivative of short interest - captures acceleration in shorting activity
        si_acceleration = 0.0
        if t >= 2:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            prev_growth = (prev_si / max(abs(prev_prev_si), 1e-8)) - 1.0
            current_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            si_acceleration = current_growth - prev_growth
        eng.append(si_acceleration)
        
        # 6. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 7. Implied Volatility to Price Ratio
        # Normalizes implied volatility by price level
        iv_price_ratio = implied_volatility / max(abs(close_prices[-1]), 1e-8)
        eng.append(iv_price_ratio)
        
        # 8. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 9. Price Volatility (standard deviation of returns over 10 days)
        # Measures recent price volatility
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(np.abs(close_prices[-11:-1]), 1e-8)
            price_volatility = np.std(returns) if len(returns) > 0 else 0.0
        else:
            price_volatility = 0.0
        eng.append(price_volatility)
        
        # 10. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 11. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations of volatility - high importance in previous iterations
        si_iv_ratio = short_interest / max(abs(implied_volatility), 1e-8)
        eng.append(si_iv_ratio)
        
        # 12. Bollinger Band Width (volatility measure)
        # Measures price volatility relative to moving average
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 13. Short Interest Momentum Oscillator (new)
        # Measures the rate of change in short interest relative to its recent range
        si_oscillator = 0.0
        if t >= 5:
            si_history = [data[t-i, 0] for i in range(5)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(abs(si_max - si_min), 1e-8)
            si_oscillator = (short_interest - si_min) / si_range
        eng.append(si_oscillator)
        
        # 14. Price to Short Interest Ratio (new)
        # Measures the relationship between price and short interest
        price_si_ratio = close_prices[-1] / max(abs(short_interest), 1e-8)
        eng.append(price_si_ratio)
        
        # 15. Short Interest Concentration (new)
        # Measures how concentrated short interest is relative to trading volume
        si_concentration = (short_interest / max(abs(avg_daily_volume * 15), 1e-8))  # 15 days of volume
        eng.append(si_concentration)
        
        # 16. Synthetic Short Cost to Price Ratio (new)
        # Measures the cost of shorting relative to price level
        short_cost_price_ratio = synthetic_short_cost / max(abs(close_prices[-1]), 1e-8)
        eng.append(short_cost_price_ratio)
        
        # 17. Short Interest to Put/Call Ratio (new)
        # Relates short interest to options market sentiment
        si_pc_ratio = short_interest / max(abs(put_call_ratio), 1e-8)
        eng.append(si_pc_ratio)
        
        # 18. Price Momentum Indicator (new)
        # Measures recent price momentum using a more sophisticated approach
        price_momentum = 0.0
        if len(close_prices) >= 10:
            # Weighted sum of recent returns (more weight to recent days)
            weights = np.array([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])
            returns = np.zeros(9)
            for i in range(9):
                returns[i] = (close_prices[-i-1] / max(abs(close_prices[-i-2]), 1e-8)) - 1.0
            price_momentum = np.sum(returns * weights[:9]) / np.sum(weights[:9])
        eng.append(price_momentum)
        
        # 19. Volatility-Adjusted Short Interest (new)
        # Adjusts short interest by implied volatility to account for risk
        vol_adj_si = short_interest * implied_volatility
        eng.append(vol_adj_si)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 482116.8972
RMSE: 722301.2827
MAPE: 8.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0002, rank=1
   2. Feature_10_t3: importance=0.0001, rank=2
   3. Feature_20_t0: importance=0.0001, rank=3
   4. Feature_14_t2: importance=0.0001, rank=4
   5. Feature_3_t1: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.04%

ðŸ“ˆ Current best MAPE: 8.56%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Cost Pressure (synthetic_short_cost * days_to_cover)
        # This combines two important metrics to measure the pressure on short sellers
        short_cost_pressure = synthetic_short_cost * days_to_cover
        eng.append(short_cost_pressure)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations of volatility - high importance in previous iterations
        si_iv_ratio = short_interest / max(abs(implied_volatility), 1e-8)
        eng.append(si_iv_ratio)
        
        # 9. Bollinger Band Width (volatility measure)
        # Measures price volatility relative to moving average
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 10. Price to Short Interest Ratio
        # Measures the relationship between price and short interest
        price_si_ratio = close_prices[-1] / max(abs(short_interest), 1e-8)
        eng.append(price_si_ratio)
        
        # 11. Short Interest Concentration
        # Measures how concentrated short interest is relative to trading volume
        si_concentration = (short_interest / max(abs(avg_daily_volume * 15), 1e-8))  # 15 days of volume
        eng.append(si_concentration)
        
        # 12. RSI (Relative Strength Index) - New
        # Technical indicator that measures the magnitude of recent price changes
        rsi = 0.0
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            rs = gain / max(abs(loss), 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
        eng.append(rsi)
        
        # 13. MACD Signal - New
        # Moving Average Convergence Divergence - technical indicator showing momentum
        macd = 0.0
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
        eng.append(macd)
        
        # 14. Average True Range (ATR) - New
        # Volatility indicator that shows how much an asset moves on average
        atr = 0.0
        if len(close_prices) >= 14:
            tr_values = []
            for i in range(1, 14):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-i-1])
                low_close = abs(low_prices[-i] - close_prices[-i-1])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
        eng.append(atr)
        
        # 15. Short Interest to ATR Ratio - New
        # Relates short interest to price volatility
        si_atr_ratio = short_interest / max(abs(atr), 1e-8)
        eng.append(si_atr_ratio)
        
        # 16. Price Gap Analysis - New
        # Measures significant overnight price gaps which can indicate sentiment shifts
        price_gap = 0.0
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            price_gap = (open_prices[-1] - close_prices[-2]) / max(abs(close_prices[-2]), 1e-8)
        eng.append(price_gap)
        
        # 17. Volume Weighted Average Price (VWAP) - New
        # Price that considers both price and volume - important for institutional traders
        vwap = 0.0
        if len(close_prices) >= 5:
            # Using a simplified VWAP calculation with equal volume weights
            vwap = np.mean(close_prices[-5:])
            # Price relative to VWAP
            price_to_vwap = close_prices[-1] / max(abs(vwap), 1e-8)
            eng.append(price_to_vwap)
        else:
            eng.append(0.0)
        
        # 18. Chaikin Money Flow (CMF) - New
        # Volume-based indicator to measure buying/selling pressure
        cmf = 0.0
        if len(close_prices) >= 10:
            money_flow_volume = []
            for i in range(10):
                if high_prices[-i-1] - low_prices[-i-1] > 1e-8:
                    mf_multiplier = ((close_prices[-i-1] - low_prices[-i-1]) - 
                                    (high_prices[-i-1] - close_prices[-i-1])) / (high_prices[-i-1] - low_prices[-i-1])
                    money_flow_volume.append(mf_multiplier)
            cmf = np.mean(money_flow_volume) if money_flow_volume else 0.0
        eng.append(cmf)
        
        # 19. Short Interest Rate of Change - New
        # Measures the acceleration of short interest changes
        si_roc = 0.0
        if t >= 2:
            si_roc = (short_interest - data[t-2, 0]) / max(abs(data[t-2, 0]), 1e-8)
        eng.append(si_roc)
        
        # 20. Synthetic Short Cost Momentum - New
        # Measures changes in the cost of shorting
        short_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            short_cost_momentum = (synthetic_short_cost / max(abs(prev_cost), 1e-8)) - 1.0
        eng.append(short_cost_momentum)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 464813.6468
RMSE: 699653.2697
MAPE: 8.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0001, rank=1
   2. Feature_22_t0: importance=0.0001, rank=2
   3. Feature_3_t2: importance=0.0001, rank=3
   4. Feature_1_t2: importance=0.0001, rank=4
   5. Feature_24_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.29%

ðŸ“ˆ Current best MAPE: 8.27%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Put/call ratio - important for sentiment
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Cost Pressure (synthetic_short_cost * days_to_cover)
        # This combines two important metrics to measure the pressure on short sellers
        short_cost_pressure = synthetic_short_cost * days_to_cover
        eng.append(short_cost_pressure)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations of volatility - high importance in previous iterations
        si_iv_ratio = short_interest / max(abs(implied_volatility), 1e-8)
        eng.append(si_iv_ratio)
        
        # 9. Bollinger Band Width (volatility measure)
        # Measures price volatility relative to moving average - high importance in previous iterations
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 10. Short Interest Concentration
        # Measures how concentrated short interest is relative to trading volume
        si_concentration = (short_interest / max(abs(avg_daily_volume * 15), 1e-8))  # 15 days of volume
        eng.append(si_concentration)
        
        # 11. RSI (Relative Strength Index) - Improved calculation
        # Technical indicator that measures the magnitude of recent price changes
        rsi = 50.0  # Default to neutral
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
        eng.append(rsi)
        
        # 12. Average True Range (ATR) - Improved calculation
        # Volatility indicator that shows how much an asset moves on average
        atr = 0.0
        if len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-i-1])
                low_close = abs(low_prices[-i] - close_prices[-i-1])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
        eng.append(atr)
        
        # 13. Short Interest to ATR Ratio - High importance in previous iterations
        # Relates short interest to price volatility
        si_atr_ratio = short_interest / max(abs(atr), 1e-8)
        eng.append(si_atr_ratio)
        
        # 14. Short Interest Rate of Change - High importance in previous iterations
        # Measures the acceleration of short interest changes
        si_roc = 0.0
        if t >= 2:
            si_roc = (short_interest - data[t-2, 0]) / max(abs(data[t-2, 0]), 1e-8)
        eng.append(si_roc)
        
        # 15. Synthetic Short Cost Momentum - High importance in previous iterations
        # Measures changes in the cost of shorting
        short_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            short_cost_momentum = (synthetic_short_cost / max(abs(prev_cost), 1e-8)) - 1.0
        eng.append(short_cost_momentum)
        
        # 16. NEW: Short Squeeze Potential Index
        # Combines days to cover, short interest ratio, and price momentum
        # Higher values indicate higher potential for a short squeeze
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            short_squeeze_potential = days_to_cover * si_float_ratio * (1 + max(0, price_momentum))
        else:
            short_squeeze_potential = days_to_cover * si_float_ratio
        eng.append(short_squeeze_potential)
        
        # 17. NEW: Short Interest Efficiency Ratio
        # Measures how efficiently short sellers are timing their positions
        # Lower values suggest short sellers are more efficient
        if len(close_prices) >= 10:
            price_change_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
            si_efficiency = short_interest * (1 + price_change_10d)
            si_efficiency = si_efficiency / max(abs(short_interest), 1e-8)
        else:
            si_efficiency = 1.0
        eng.append(si_efficiency)
        
        # 18. NEW: Normalized Short Interest Trend
        # Measures the trend of short interest relative to its recent range
        si_norm = 0.0
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = si_max - si_min
            if si_range > 1e-8:
                si_norm = (short_interest - si_min) / si_range
            else:
                si_norm = 0.5
        eng.append(si_norm)
        
        # 19. NEW: Implied Volatility Change Rate
        # Measures the rate of change in implied volatility
        iv_change = 0.0
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(abs(prev_iv), 1e-8)) - 1.0
        eng.append(iv_change)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 466888.0496
RMSE: 701004.4520
MAPE: 8.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 61
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t2: importance=0.0003, rank=1
   2. Feature_10_t3: importance=0.0001, rank=2
   3. Feature_20_t2: importance=0.0001, rank=3
   4. Feature_8_t0: importance=0.0001, rank=4
   5. Feature_22_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.03%

ðŸ“ˆ Current best MAPE: 8.27%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Put/call ratio - important for sentiment
        raw_keep.append(synthetic_short_cost)  # Consistently important in previous iterations
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Squeeze Potential Index (high importance in previous iteration)
        # Combines days to cover, short interest ratio, and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            short_squeeze_potential = days_to_cover * si_float_ratio * (1 + max(0, price_momentum))
        else:
            short_squeeze_potential = days_to_cover * si_float_ratio
        eng.append(short_squeeze_potential)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. Bollinger Band Width (volatility measure)
        # Measures price volatility relative to moving average - high importance in previous iterations
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(abs(sma), 1e-8)
        else:
            bb_width = 0.0
        eng.append(bb_width)
        
        # 9. RSI (Relative Strength Index) - Improved calculation
        # Technical indicator that measures the magnitude of recent price changes
        rsi = 50.0  # Default to neutral
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
        eng.append(rsi)
        
        # 10. Average True Range (ATR) - Improved calculation
        # Volatility indicator that shows how much an asset moves on average
        atr = 0.0
        if len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-i-1])
                low_close = abs(low_prices[-i] - close_prices[-i-1])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
        eng.append(atr)
        
        # 11. Short Interest Rate of Change - High importance in previous iterations
        # Measures the acceleration of short interest changes
        si_roc = 0.0
        if t >= 2:
            si_roc = (short_interest - data[t-2, 0]) / max(abs(data[t-2, 0]), 1e-8)
        eng.append(si_roc)
        
        # 12. Synthetic Short Cost Momentum - High importance in previous iterations
        # Measures changes in the cost of shorting
        short_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            short_cost_momentum = (synthetic_short_cost / max(abs(prev_cost), 1e-8)) - 1.0
        eng.append(short_cost_momentum)
        
        # 13. NEW: Short Interest Efficiency Ratio with Improved Calculation
        # Measures how efficiently short sellers are timing their positions
        # Lower values suggest short sellers are more efficient
        if len(close_prices) >= 10:
            price_change_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
            # Improved calculation that better captures the relationship
            si_efficiency = (short_interest * (1 + price_change_10d)) / max(abs(short_interest), 1e-8)
            # Normalize to a more interpretable range
            si_efficiency = np.tanh(si_efficiency)  # Bound between -1 and 1
        else:
            si_efficiency = 0.0
        eng.append(si_efficiency)
        
        # 14. NEW: Implied Volatility to Price Ratio
        # Relates market expectations of volatility to current price level
        # Higher values indicate higher expected volatility relative to price
        iv_price_ratio = implied_volatility / max(abs(close_prices[-1]), 1e-8)
        eng.append(iv_price_ratio)
        
        # 15. NEW: Short Interest Momentum Oscillator
        # Combines short interest changes with price momentum to create an oscillator
        si_momentum_osc = 0.0
        if t > 0 and len(close_prices) >= 5:
            si_change = (short_interest / max(abs(data[t-1, 0]), 1e-8)) - 1.0
            price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            # Oscillator that measures divergence between short interest and price
            si_momentum_osc = si_change - price_change
        eng.append(si_momentum_osc)
        
        # 16. NEW: Short Interest Concentration Relative to Volatility
        # Measures how concentrated short interest is relative to price volatility
        # Higher values indicate higher short concentration in volatile stocks
        if atr > 0:
            si_vol_concentration = (short_interest / max(abs(avg_daily_volume), 1e-8)) / max(atr, 1e-8)
        else:
            si_vol_concentration = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_vol_concentration)
        
        # 17. NEW: Options-Adjusted Short Interest
        # Adjusts short interest based on options market sentiment
        # Provides a more comprehensive view of bearish positioning
        options_adjusted_si = short_interest * (1 + 0.5 * (put_call_ratio - 1))
        eng.append(options_adjusted_si)
        
        # 18. NEW: Short Interest Divergence from Moving Average
        # Measures how current short interest deviates from its recent trend
        si_ma_divergence = 0.0
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_ma = np.mean(si_history)
            si_ma_divergence = (short_interest / max(abs(si_ma), 1e-8)) - 1.0
        eng.append(si_ma_divergence)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 473027.9769
RMSE: 707457.5001
MAPE: 8.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t2: importance=0.0002, rank=1
   2. Feature_16_t0: importance=0.0001, rank=2
   3. Feature_1_t2: importance=0.0001, rank=3
   4. Feature_22_t0: importance=0.0001, rank=4
   5. Feature_14_t3: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.12%

ðŸ“ˆ Current best MAPE: 8.27%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Put/call ratio - important for sentiment
        raw_keep.append(synthetic_short_cost)  # Consistently important in previous iterations
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Squeeze Potential Index (high importance in previous iteration)
        # Combines days to cover, short interest ratio, and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            short_squeeze_potential = days_to_cover * si_float_ratio * (1 + max(0, price_momentum))
        else:
            short_squeeze_potential = days_to_cover * si_float_ratio
        eng.append(short_squeeze_potential)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. NEW: Improved Short Interest Efficiency Ratio
        # Measures how efficiently short sellers are timing their positions relative to price movements
        if len(close_prices) >= 10:
            # Calculate cumulative return over last 10 days
            cum_return = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
            # Negative returns with high short interest indicate efficient short positioning
            si_efficiency = short_interest * (1 - cum_return)  # Higher when shorts are right
            # Normalize using sigmoid-like function
            si_efficiency = 2 / (1 + np.exp(-si_efficiency)) - 1  # Bound between -1 and 1
        else:
            si_efficiency = 0.0
        eng.append(si_efficiency)
        
        # 9. NEW: Short Interest Concentration Index
        # Measures how concentrated short interest is relative to market metrics
        # Higher values indicate potentially higher short squeeze risk
        si_concentration = (short_interest / max(abs(shares_outstanding), 1e-8)) * days_to_cover
        eng.append(si_concentration)
        
        # 10. NEW: Short Interest Momentum Signal
        # Combines short interest changes with volume changes to create a momentum signal
        si_momentum_signal = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_vol = data[t-1, 1]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            vol_change = (avg_daily_volume / max(abs(prev_vol), 1e-8)) - 1.0
            # Positive when short interest increases more than volume (bearish)
            # Negative when short interest decreases more than volume (bullish)
            si_momentum_signal = si_change - vol_change
        eng.append(si_momentum_signal)
        
        # 11. NEW: Options-Adjusted Short Interest Ratio
        # Adjusts short interest based on options market sentiment and volatility
        # Provides a more comprehensive view of bearish positioning
        options_adjusted_si_ratio = (short_interest / max(abs(avg_daily_volume), 1e-8)) * (1 + 0.5 * (put_call_ratio - 1)) * (1 + 0.2 * implied_volatility)
        eng.append(options_adjusted_si_ratio)
        
        # 12. NEW: Short Interest Divergence Signal
        # Measures divergence between short interest and price trends
        # Positive values indicate potential reversal points (shorts increasing while price rises)
        si_divergence = 0.0
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            # Positive when shorts increase while price rises (bearish divergence)
            # Negative when shorts decrease while price falls (bullish divergence)
            si_divergence = si_change * np.sign(price_change)
        eng.append(si_divergence)
        
        # 13. NEW: Short Interest Volatility Ratio
        # Relates short interest to price volatility
        # Higher values indicate higher short interest relative to volatility
        if len(close_prices) >= 10:
            price_std = np.std(close_prices[-10:])
            price_mean = np.mean(close_prices[-10:])
            price_cv = price_std / max(abs(price_mean), 1e-8)  # Coefficient of variation
            si_vol_ratio = (short_interest / max(abs(avg_daily_volume), 1e-8)) / max(price_cv, 1e-8)
        else:
            si_vol_ratio = 0.0
        eng.append(si_vol_ratio)
        
        # 14. NEW: Short Interest Rate of Change Acceleration
        # Measures the second derivative of short interest changes
        # Captures acceleration in short interest movements
        si_roc_accel = 0.0
        if t >= 2:
            current_si = short_interest
            prev_si = data[t-1, 0]
            prev2_si = data[t-2, 0]
            
            current_roc = (current_si / max(abs(prev_si), 1e-8)) - 1.0
            prev_roc = (prev_si / max(abs(prev2_si), 1e-8)) - 1.0
            
            si_roc_accel = current_roc - prev_roc
        eng.append(si_roc_accel)
        
        # 15. NEW: Synthetic Short Cost to Implied Volatility Ratio
        # Relates the cost of shorting to market expectations of volatility
        # Higher values indicate expensive shorting relative to expected volatility
        short_cost_iv_ratio = synthetic_short_cost / max(abs(implied_volatility), 1e-8)
        eng.append(short_cost_iv_ratio)
        
        # 16. NEW: Short Interest Mean Reversion Signal
        # Measures how far short interest is from its recent average
        # Useful for identifying potential mean reversion opportunities
        si_mean_reversion = 0.0
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_mean = np.mean(si_history)
            si_std = np.std(si_history) if len(si_history) > 1 else 1.0
            # Z-score of current short interest relative to recent history
            si_mean_reversion = (short_interest - si_mean) / max(abs(si_std), 1e-8)
        eng.append(si_mean_reversion)
        
        # 17. NEW: Price-Volume-Short Interest Correlation
        # Measures the relationship between price, volume, and short interest
        # Useful for identifying potential market inefficiencies
        pvsi_correlation = 0.0
        if len(close_prices) >= 5 and t >= 2:
            price_change = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            volume_change = (avg_daily_volume / max(abs(data[t-1, 1]), 1e-8)) - 1.0
            si_change = (short_interest / max(abs(data[t-1, 0]), 1e-8)) - 1.0
            
            # Simplified correlation measure that doesn't use np.corrcoef
            # Positive when all three move in same direction, negative when divergent
            pvsi_correlation = np.sign(price_change) * np.sign(volume_change) * np.sign(si_change)
        eng.append(pvsi_correlation)
        
        # 18. NEW: Short Interest to Options Volume Ratio
        # Relates short interest to options activity
        # Higher values indicate higher short interest relative to options activity
        si_options_ratio = short_interest / max(abs(put_call_ratio * avg_daily_volume), 1e-8)
        eng.append(si_options_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 7 (claude) (SVM)
==================================================
Training SVM model...

Iteration 7 (claude) Performance:
MAE: 474008.0330
RMSE: 721728.2025
MAPE: 8.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0002, rank=1
   2. Feature_11_t3: importance=0.0001, rank=2
   3. Feature_6_t2: importance=0.0001, rank=3
   4. Feature_10_t2: importance=0.0001, rank=4
   5. Feature_24_t3: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 7 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.22%

ðŸ“ˆ Current best MAPE: 8.27%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 8
======================================================================
ðŸ¤– Calling Claude for iteration 8...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Put/call ratio - important for sentiment
        raw_keep.append(synthetic_short_cost)  # Consistently important in previous iterations
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Squeeze Potential Index (high importance in previous iteration)
        # Combines days to cover, short interest ratio, and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            short_squeeze_potential = days_to_cover * si_float_ratio * (1 + max(0, price_momentum))
        else:
            short_squeeze_potential = days_to_cover * si_float_ratio
        eng.append(short_squeeze_potential)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. Short Interest Concentration Index
        # Measures how concentrated short interest is relative to market metrics
        # Higher values indicate potentially higher short squeeze risk
        si_concentration = (short_interest / max(abs(shares_outstanding), 1e-8)) * days_to_cover
        eng.append(si_concentration)
        
        # 9. NEW: Improved Short Interest Momentum with Volume Weighting
        # Combines short interest changes with volume changes, weighted by relative volume
        si_vol_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_vol = data[t-1, 1]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            vol_change = (avg_daily_volume / max(abs(prev_vol), 1e-8)) - 1.0
            vol_weight = avg_daily_volume / max(abs(np.mean([avg_daily_volume, prev_vol])), 1e-8)
            # Higher weight when volume is increasing (more significant signal)
            si_vol_momentum = si_change * vol_weight * (1 + 0.5 * vol_change)
        eng.append(si_vol_momentum)
        
        # 10. NEW: Short Interest Volatility-Adjusted Ratio
        # Adjusts short interest by recent price volatility to identify abnormal patterns
        si_vol_adjusted = 0.0
        if len(close_prices) >= 5:
            # Calculate recent price volatility (standard deviation / mean)
            recent_prices = close_prices[-5:]
            price_std = np.std(recent_prices)
            price_mean = np.mean(recent_prices)
            price_volatility = price_std / max(abs(price_mean), 1e-8)
            
            # Adjust short interest by volatility - higher when shorts increase during low volatility
            si_vol_adjusted = si_float_ratio / max(abs(price_volatility), 1e-8)
        eng.append(si_vol_adjusted)
        
        # 11. NEW: Short Interest Divergence from Price Trend
        # Measures divergence between short interest and price trends with exponential weighting
        si_price_divergence = 0.0
        if len(close_prices) >= 10 and t > 0:
            # Calculate price trend over different timeframes
            price_trend_short = (close_prices[-1] / max(abs(close_prices[-3]), 1e-8)) - 1.0
            price_trend_medium = (close_prices[-1] / max(abs(close_prices[-7]), 1e-8)) - 1.0
            price_trend_long = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1.0
            
            # Weight recent trends more heavily
            weighted_price_trend = 0.5 * price_trend_short + 0.3 * price_trend_medium + 0.2 * price_trend_long
            
            # Calculate short interest change
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            
            # Divergence is positive when shorts increase while price rises (bearish divergence)
            # Exponential weighting to amplify strong signals
            si_price_divergence = si_change * np.sign(weighted_price_trend) * np.exp(abs(weighted_price_trend))
        eng.append(si_price_divergence)
        
        # 12. NEW: Options-Adjusted Short Interest Pressure
        # Combines short interest with options market signals to measure overall shorting pressure
        options_adjusted_pressure = (si_float_ratio * (1 + 0.5 * (put_call_ratio - 1))) * (1 + 0.2 * implied_volatility)
        eng.append(options_adjusted_pressure)
        
        # 13. NEW: Short Interest Mean Reversion Potential
        # Measures how far short interest is from its recent average with volatility adjustment
        si_mean_reversion = 0.0
        if t >= 3:
            # Get recent short interest history
            si_history = np.array([data[max(0, t-i), 0] for i in range(3)])
            si_mean = np.mean(si_history)
            si_std = np.std(si_history) if len(si_history) > 1 else 1.0
            
            # Calculate z-score with sign preservation
            z_score = (short_interest - si_mean) / max(abs(si_std), 1e-8)
            
            # Apply sigmoid-like transformation to bound extreme values
            si_mean_reversion = 2 / (1 + np.exp(-0.5 * z_score)) - 1
        eng.append(si_mean_reversion)
        
        # 14. NEW: Short Interest Efficiency Ratio
        # Measures how efficiently short sellers are timing their positions
        si_efficiency = 0.0
        if len(close_prices) >= 7 and t > 0:
            # Calculate returns over different timeframes
            ret_3d = (close_prices[-1] / max(abs(close_prices[-3]), 1e-8)) - 1.0
            ret_7d = (close_prices[-1] / max(abs(close_prices[-7]), 1e-8)) - 1.0
            
            # Calculate short interest change
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            
            # Efficiency is high when shorts increase before price drops
            # and when shorts decrease before price rises
            si_efficiency = -1 * si_change * (0.7 * ret_3d + 0.3 * ret_7d)
        eng.append(si_efficiency)
        
        # 15. NEW: Short Interest to Synthetic Short Cost Ratio
        # Relates actual short interest to the cost of synthetic shorting
        # Higher values indicate higher short interest relative to the cost of shorting
        si_cost_ratio = short_interest / max(abs(synthetic_short_cost), 1e-8)
        eng.append(si_cost_ratio)
        
        # 16. NEW: Relative Short Interest Strength Index (RSI-like)
        # Measures the strength of short interest changes using RSI-like calculation
        si_rsi = 50.0  # Neutral value
        if t >= 14:
            # Get recent short interest history
            si_history = np.array([data[max(0, t-i), 0] for i in range(14)])
            si_changes = np.diff(si_history)
            
            # Calculate gains and losses
            gains = np.sum(np.maximum(si_changes, 0))
            losses = np.sum(np.maximum(-si_changes, 0))
            
            # Calculate RSI
            if losses > 0:
                rs = gains / max(abs(losses), 1e-8)
                si_rsi = 100 - (100 / (1 + rs))
            elif gains > 0:
                si_rsi = 100
        eng.append(si_rsi / 100.0)  # Normalize to 0-1 range
        
        # 17. NEW: Short Interest Acceleration
        # Measures the second derivative of short interest (acceleration)
        si_acceleration = 0.0
        if t >= 2:
            current_si = short_interest
            prev_si = data[t-1, 0]
            prev2_si = data[t-2, 0]
            
            # First derivatives (velocity)
            current_velocity = (current_si - prev_si) / max(abs(prev_si), 1e-8)
            prev_velocity = (prev_si - prev2_si) / max(abs(prev2_si), 1e-8)
            
            # Second derivative (acceleration)
            si_acceleration = current_velocity - prev_velocity
        eng.append(si_acceleration)
        
        # 18. NEW: Composite Short Squeeze Risk Score
        # Combines multiple factors to create a comprehensive short squeeze risk score
        if len(close_prices) >= 5:
            # Price momentum component
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            
            # Volume trend component
            volume_trend = 1.0
            if t > 0:
                prev_vol = data[t-1, 1]
                volume_trend = avg_daily_volume / max(abs(prev_vol), 1e-8)
            
            # Combine factors with appropriate weights
            squeeze_risk = (
                0.4 * days_to_cover +
                0.3 * si_float_ratio +
                0.2 * max(0, price_momentum) +  # Only consider positive momentum
                0.1 * max(1.0, volume_trend)    # Only consider increasing volume
            )
            
            # Apply non-linear transformation to emphasize extreme values
            squeeze_risk = np.tanh(squeeze_risk * 2)  # Bound between -1 and 1
        else:
            squeeze_risk = 0.0
        eng.append(squeeze_risk)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 8 (claude) (SVM)
==================================================
Training SVM model...

Iteration 8 (claude) Performance:
MAE: 463408.8198
RMSE: 711173.5102
MAPE: 8.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t1: importance=0.0002, rank=1
   2. Feature_5_t3: importance=0.0002, rank=2
   3. Feature_6_t3: importance=0.0001, rank=3
   4. Feature_20_t3: importance=0.0001, rank=4
   5. Feature_17_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 8 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.06%

ðŸ“ˆ Current best MAPE: 8.27%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 9
======================================================================
ðŸ¤– Calling Claude for iteration 9...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep these critical raw features (based on importance analysis)
        raw_keep.append(short_interest)  # Short interest - always keep
        raw_keep.append(avg_daily_volume)  # Average daily volume - always keep
        raw_keep.append(days_to_cover)  # Days to cover - consistently important
        
        # Reshape OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Put/call ratio - important for sentiment
        raw_keep.append(synthetic_short_cost)  # Consistently important in previous iterations
        raw_keep.append(implied_volatility)  # High importance in baseline
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (consistently high importance)
        # This captures what percentage of available shares are being shorted
        si_float_ratio = short_interest / max(abs(shares_outstanding), 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio
        # This ratio captures how many days of trading volume the short interest represents
        si_volume_ratio = short_interest / max(abs(avg_daily_volume), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Squeeze Potential Index (high importance in previous iteration)
        # Combines days to cover, short interest ratio, and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
            short_squeeze_potential = days_to_cover * si_float_ratio * (1 + max(0, price_momentum))
        else:
            short_squeeze_potential = days_to_cover * si_float_ratio
        eng.append(short_squeeze_potential)
        
        # 4. Short Interest Growth Rate (if we have previous data)
        # Measures momentum in short interest changes - key predictor
        si_growth = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
        eng.append(si_growth)
        
        # 5. Options Sentiment Indicator (combining put/call ratio and implied volatility)
        # This combines two important options metrics to create a sentiment indicator
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # 6. Recent Price Trend (5-day)
        # Captures recent price momentum - showed high importance in previous iterations
        if len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1.0
        else:
            price_trend_5d = 0.0
        eng.append(price_trend_5d)
        
        # 7. OHLC Volatility (High-Low)/(Open+Close)
        # Alternative measure of intraday volatility - high importance in previous iterations
        if len(open_prices) > 0 and len(close_prices) > 0:
            ohlc_volatility = (high_prices[-1] - low_prices[-1]) / max(abs(open_prices[-1] + close_prices[-1]), 1e-8)
        else:
            ohlc_volatility = 0.0
        eng.append(ohlc_volatility)
        
        # 8. Short Interest Concentration Index
        # Measures how concentrated short interest is relative to market metrics
        si_concentration = (short_interest / max(abs(shares_outstanding), 1e-8)) * days_to_cover
        eng.append(si_concentration)
        
        # 9. Improved Short Interest Momentum with Volume Weighting
        # Combines short interest changes with volume changes, weighted by relative volume
        si_vol_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            prev_vol = data[t-1, 1]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            vol_change = (avg_daily_volume / max(abs(prev_vol), 1e-8)) - 1.0
            vol_weight = avg_daily_volume / max(abs(np.mean([avg_daily_volume, prev_vol])), 1e-8)
            # Higher weight when volume is increasing (more significant signal)
            si_vol_momentum = si_change * vol_weight * (1 + 0.5 * vol_change)
        eng.append(si_vol_momentum)
        
        # 10. NEW: Enhanced Short Interest Volatility-Adjusted Ratio
        # Improved version with exponential weighting to emphasize recent volatility
        si_vol_adjusted = 0.0
        if len(close_prices) >= 5:
            # Calculate recent price volatility with exponential weighting
            recent_prices = close_prices[-5:]
            weights = np.exp(np.linspace(0, 1, len(recent_prices)))
            weights = weights / np.sum(weights)  # Normalize weights
            
            # Calculate weighted mean and variance
            price_mean = np.sum(weights * recent_prices)
            price_var = np.sum(weights * (recent_prices - price_mean)**2)
            price_volatility = np.sqrt(price_var) / max(abs(price_mean), 1e-8)
            
            # Adjust short interest by volatility with non-linear transformation
            si_vol_adjusted = si_float_ratio * np.tanh(1.0 / max(abs(price_volatility), 1e-8))
        eng.append(si_vol_adjusted)
        
        # 11. NEW: Composite Short Interest Trend Indicator
        # Combines multiple timeframes of short interest changes with adaptive weighting
        si_trend = 0.0
        if t >= 3:
            # Get recent short interest history
            si_history = np.array([data[max(0, t-i), 0] for i in range(4)])
            
            # Calculate changes at different timeframes
            si_change_1d = (si_history[0] / max(abs(si_history[1]), 1e-8)) - 1.0
            si_change_2d = (si_history[0] / max(abs(si_history[2]), 1e-8)) - 1.0
            si_change_3d = (si_history[0] / max(abs(si_history[3]), 1e-8)) - 1.0
            
            # Calculate volatility of changes to determine weights
            changes = np.array([si_change_1d, si_change_2d, si_change_3d])
            change_vol = np.std(changes) if len(changes) > 1 else 1.0
            
            # Higher weights for more stable trends (lower volatility)
            weight_factor = np.exp(-2 * change_vol)
            
            # Combine with adaptive weighting
            si_trend = (0.5 * si_change_1d + 
                       0.3 * si_change_2d + 
                       0.2 * si_change_3d) * (1 + weight_factor)
        eng.append(si_trend)
        
        # 12. NEW: Short Interest Divergence from Price Trend with Volatility Adjustment
        # Enhanced version that accounts for price volatility in the divergence calculation
        si_price_divergence = 0.0
        if len(close_prices) >= 10 and t > 0:
            # Calculate price trends with volatility adjustment
            price_changes = []
            for i in range(1, min(10, len(close_prices))):
                price_changes.append((close_prices[-1] / max(abs(close_prices[-i-1]), 1e-8)) - 1.0)
            
            # Calculate price trend volatility
            price_vol = np.std(price_changes) if len(price_changes) > 1 else 0.01
            
            # Calculate weighted price trend
            weights = np.exp(-np.arange(len(price_changes)) / 3)  # Exponential decay weights
            weights = weights / np.sum(weights)  # Normalize weights
            weighted_price_trend = np.sum(weights * np.array(price_changes))
            
            # Calculate short interest change
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            
            # Divergence with volatility adjustment
            # Higher divergence when price trend is strong and consistent (low volatility)
            volatility_factor = 1.0 / max(abs(price_vol), 0.01)
            si_price_divergence = si_change * np.sign(weighted_price_trend) * abs(weighted_price_trend) * min(volatility_factor, 5.0)
        eng.append(si_price_divergence)
        
        # 13. NEW: Options-Adjusted Short Interest Pressure with Synthetic Cost Weighting
        # Enhanced version that incorporates synthetic short cost as a weighting factor
        synthetic_cost_factor = np.tanh(synthetic_short_cost)  # Bound between -1 and 1
        options_adjusted_pressure = (si_float_ratio * (1 + 0.5 * (put_call_ratio - 1))) * (1 + 0.2 * implied_volatility)
        options_adjusted_pressure *= (1 + 0.3 * synthetic_cost_factor)  # Higher pressure when synthetic shorting is expensive
        eng.append(options_adjusted_pressure)
        
        # 14. NEW: Short Interest Mean Reversion Potential with Momentum
        # Enhanced version that incorporates momentum in the mean reversion calculation
        si_mean_reversion = 0.0
        if t >= 3:
            # Get recent short interest history
            si_history = np.array([data[max(0, t-i), 0] for i in range(4)])
            si_mean = np.mean(si_history)
            si_std = np.std(si_history) if len(si_history) > 1 else 1.0
            
            # Calculate z-score with sign preservation
            z_score = (short_interest - si_mean) / max(abs(si_std), 1e-8)
            
            # Calculate momentum (direction of recent changes)
            si_momentum = (si_history[0] - si_history[-1]) / max(abs(si_history[-1]), 1e-8)
            
            # Adjust mean reversion by momentum
            # Stronger signal when z-score and momentum are in opposite directions
            # (indicating potential reversal)
            momentum_factor = 1.0 - 0.5 * np.sign(z_score) * np.sign(si_momentum)
            
            # Apply sigmoid-like transformation with momentum adjustment
            si_mean_reversion = (2 / (1 + np.exp(-0.5 * z_score)) - 1) * momentum_factor
        eng.append(si_mean_reversion)
        
        # 15. NEW: Short Interest Efficiency Ratio with Price Volatility Adjustment
        # Enhanced version that accounts for price volatility in efficiency calculation
        si_efficiency = 0.0
        if len(close_prices) >= 7 and t > 0:
            # Calculate returns over different timeframes
            ret_3d = (close_prices[-1] / max(abs(close_prices[-3]), 1e-8)) - 1.0
            ret_7d = (close_prices[-1] / max(abs(close_prices[-7]), 1e-8)) - 1.0
            
            # Calculate price volatility
            recent_returns = []
            for i in range(1, min(7, len(close_prices))):
                recent_returns.append((close_prices[-i] / max(abs(close_prices[-i-1]), 1e-8)) - 1.0)
            price_vol = np.std(recent_returns) if len(recent_returns) > 1 else 0.01
            
            # Calculate short interest change
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(abs(prev_si), 1e-8)) - 1.0
            
            # Efficiency is high when shorts increase before price drops
            # and when shorts decrease before price rises
            # Adjust by volatility - more significant in low volatility environments
            volatility_factor = 1.0 / max(abs(price_vol), 0.01)
            si_efficiency = -1 * si_change * (0.7 * ret_3d + 0.3 * ret_7d) * min(volatility_factor, 5.0)
        eng.append(si_efficiency)
        
        # 16. NEW: Adaptive Short Interest to Synthetic Short Cost Ratio
        # Enhanced version with adaptive weighting based on market conditions
        si_cost_ratio = short_interest / max(abs(synthetic_short_cost), 1e-8)
        
        # Adjust by implied volatility - more significant in high volatility environments
        volatility_factor = np.tanh(implied_volatility)  # Bound between -1 and 1
        adaptive_si_cost_ratio = si_cost_ratio * (1 + 0.3 * volatility_factor)
        eng.append(adaptive_si_cost_ratio)
        
        # 17. NEW: Short Interest Momentum Oscillator
        # RSI-like oscillator for short interest with adaptive lookback
        si_oscillator = 50.0  # Neutral value
        if t >= 5:
            # Determine adaptive lookback based on volatility
            si_history = np.array([data[max(0, t-i), 0] for i in range(14)])
            si_changes = np.diff(si_history)
            si_vol = np.std(si_changes) if len(si_changes) > 1 else 0.01
            
            # Shorter lookback in high volatility environments
            lookback = max(5, min(14, int(10 / max(abs(si_vol), 0.01))))
            
            # Get recent short interest history with adaptive lookback
            si_recent = np.array([data[max(0, t-i), 0] for i in range(lookback)])
            si_recent_changes = np.diff(si_recent)
            
            # Calculate gains and losses
            gains = np.sum(np.maximum(si_recent_changes, 0))
            losses = np.sum(np.maximum(-si_recent_changes, 0))
            
            # Calculate oscillator
            if losses > 0:
                rs = gains / max(abs(losses), 1e-8)
                si_oscillator = 100 - (100 / (1 + rs))
            elif gains > 0:
                si_oscillator = 100
        eng.append(si_oscillator / 100.0)  # Normalize to 0-1 range
        
        # 18. NEW: Enhanced Short Interest Acceleration with Smoothing
        # Improved version with exponential smoothing to reduce noise
        si_acceleration = 0.0
        if t >= 3:
            # Get recent short interest history
            si_history = np.array([data[max(0, t-i), 0] for i in range(4)])
            
            # Calculate smoothed changes
            alpha = 0.7  # Smoothing factor
            smoothed_si = np.zeros_like(si_history)
            smoothed_si[0] = si_history[0]
            for i in range(1, len(si_history)):
                smoothed_si[i] = alpha * si_history[i] + (1 - alpha) * smoothed_si[i-1]
            
            # Calculate first derivatives (velocity)
            velocities = np.diff(smoothed_si) / np.maximum(abs(smoothed_si[1:]), 1e-8)
            
            # Calculate second derivative (acceleration) with sign preservation
            if len(velocities) >= 2:
                si_acceleration = (velocities[0] - velocities[1]) * np.sign(velocities[0])
        eng.append(si_acceleration)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 9 (claude) (SVM)
==================================================
Training SVM model...

Iteration 9 (claude) Performance:
MAE: 465457.8973
RMSE: 702005.1342
MAPE: 8.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0001, rank=1
   2. Feature_10_t2: importance=0.0001, rank=2
   3. Feature_6_t2: importance=0.0001, rank=3
   4. Feature_11_t3: importance=0.0001, rank=4
   5. Feature_3_t2: importance=0.0001, rank=5
ðŸ’¾ Saved code for iteration 9 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.00%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 663654.1963
RMSE: 1132887.0055
MAPE: 8.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0006, rank=1
   2. Feature_66_t1: importance=0.0005, rank=2
   3. Feature_63_t3: importance=0.0004, rank=3
   4. Feature_2_t1: importance=0.0002, rank=4
   5. Feature_65_t2: importance=0.0001, rank=5
   Baseline MAPE: 8.84%
   Baseline MAE: 663654.1963
   Baseline RMSE: 1132887.0055

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 678790.1083
RMSE: 1163328.7661
MAPE: 8.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t1: importance=0.0004, rank=1
   2. Feature_16_t1: importance=0.0003, rank=2
   3. Feature_5_t3: importance=0.0003, rank=3
   4. Feature_19_t3: importance=0.0003, rank=4
   5. Feature_4_t3: importance=0.0002, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 8.92%
   MAE: 678790.1083
   RMSE: 1163328.7661

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 8.84%
   Best Model MAPE: 8.92%
   Absolute Improvement: -0.08%
   Relative Improvement: -0.9%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  9.20            N/A                 
1          Iteration 1               8.56            +0.64%              
2          Iteration 2               8.57            -0.01%              
3          Iteration 3               8.60            -0.04%              
4          Iteration 4               8.27            +0.29%              
5          Iteration 5               8.30            -0.03%              
6          Iteration 6               8.39            -0.12%              
7          Iteration 7               8.49            -0.22%              
8          Iteration 8               8.21            +0.06%              
9          Iteration 9               8.28            -0.00%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 8 - MAPE: 8.21%
âœ… Saved SRPT results to cache/SRPT_iterative_results_enhanced.pkl
âœ… Summary report saved for SRPT

ðŸŽ‰ Process completed successfully for SRPT!

================================================================================
PROCESSING TICKER 10/15: EXTR
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for EXTR
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for EXTR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EXTR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 870930.1552
RMSE: 1256685.6772
MAPE: 7.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0035, rank=1
   2. Feature_67_t3: importance=0.0013, rank=2
   3. Feature_0_t2: importance=0.0012, rank=3
   4. Feature_65_t2: importance=0.0012, rank=4
   5. Feature_65_t1: importance=0.0011, rank=5

ðŸ“Š Baseline Performance: MAPE = 7.61%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (highest importance features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        
        # Keep days to cover (important for short interest prediction)
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep options-related features (high importance in previous iteration)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        
        # Keep shares outstanding and volume (high importance)
        raw_keep.append(data[t, 67])  # shares_outstanding
        raw_keep.append(data[t, 68])  # volume
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # This shows what percentage of total shares are sold short
        shares_out = max(abs(data[t, 67]), 1e-8)
        si_ratio = data[t, 0] / shares_out
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        # Higher values may indicate increasing difficulty to cover short positions
        avg_vol = max(abs(data[t, 1]), 1e-8)
        si_vol_ratio = data[t, 0] / avg_vol
        eng.append(si_vol_ratio)
        
        # 3. Price Momentum (5-day)
        # Short sellers often target stocks with negative momentum
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
        else:
            momentum_5d = 0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Longer-term momentum to capture different trend horizons
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1
        else:
            momentum_10d = 0
        eng.append(momentum_10d)
        
        # 5. Volatility (standard deviation of returns over 10 days)
        # High volatility stocks often attract short sellers
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / np.maximum(abs(close_prices[-11:-1]), 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
        else:
            volatility = 0
        eng.append(volatility)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility, important for short sellers
        atr_sum = 0
        atr_count = 0
        for i in range(1, min(10, len(high_prices))):
            true_range = max(
                high_prices[i] - low_prices[i],
                abs(high_prices[i] - close_prices[i-1]),
                abs(low_prices[i] - close_prices[i-1])
            )
            atr_sum += true_range
            atr_count += 1
        atr = atr_sum / max(atr_count, 1)
        eng.append(atr)
        
        # 7. Relative Strength Index (RSI)
        # Overbought conditions (high RSI) might attract short sellers
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 14
            avg_loss = loss / 14
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral value if not enough data
        eng.append(rsi)
        
        # 8. Options Put/Call Ratio Change
        # Increasing put/call ratio may indicate bearish sentiment
        if t > 0:
            pc_ratio_change = data[t, 64] / max(abs(data[t-1, 64]), 1e-8) - 1
        else:
            pc_ratio_change = 0
        eng.append(pc_ratio_change)
        
        # 9. Short Cost Change
        # Increasing short cost may indicate crowded short positions
        if t > 0:
            short_cost_change = data[t, 65] / max(abs(data[t-1, 65]), 1e-8) - 1
        else:
            short_cost_change = 0
        eng.append(short_cost_change)
        
        # 10. Implied Volatility vs Historical Volatility Ratio
        # Discrepancy may indicate market expectations diverging from reality
        if volatility > 0:
            iv_hv_ratio = data[t, 66] / max(volatility, 1e-8)
        else:
            iv_hv_ratio = 1
        eng.append(iv_hv_ratio)
        
        # 11. Volume Trend (5-day)
        # Increasing volume may indicate changing sentiment
        if t >= 4:
            vol_5d_avg = np.mean([data[t-i, 68] for i in range(5)])
            vol_prev_5d_avg = np.mean([data[t-i-5, 68] for i in range(5) if t-i-5 >= 0])
            vol_trend = vol_5d_avg / max(abs(vol_prev_5d_avg), 1e-8) - 1
        else:
            vol_trend = 0
        eng.append(vol_trend)
        
        # 12. Price Range Ratio (High-Low)/Close
        # Wide ranges may indicate volatility attractive to shorts
        if len(close_prices) > 0:
            price_range = (high_prices[-1] - low_prices[-1]) / max(abs(close_prices[-1]), 1e-8)
        else:
            price_range = 0
        eng.append(price_range)
        
        # 13. Short Interest Growth Rate
        # Acceleration in short interest is a key predictor
        if t > 0:
            si_growth = data[t, 0] / max(abs(data[t-1, 0]), 1e-8) - 1
        else:
            si_growth = 0
        eng.append(si_growth)
        
        # 14. Days to Cover Change
        # Increasing DTC may indicate harder-to-cover short positions
        if t > 0:
            dtc_change = data[t, 2] / max(abs(data[t-1, 2]), 1e-8) - 1
        else:
            dtc_change = 0
        eng.append(dtc_change)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep important options data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0  # Safely handle potential index error
        
        raw_keep.append(put_call_ratio)
        raw_keep.append(synthetic_short_cost)
        raw_keep.append(implied_volatility)
        raw_keep.append(shares_outstanding)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
            
        # Feature 2: Price momentum (10-day)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
            
        # Feature 3: Volatility (standard deviation of returns)
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-6:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)
            
        # Feature 4: Short interest to float ratio
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 5: Short interest to volume ratio
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 6: Relative strength index (RSI)
        if len(close_prices) >= 14:
            diff = np.diff(close_prices[-15:])
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100.0 - (100.0 / (1.0 + rs))
            else:
                rsi = 100.0
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value as default
            
        # Feature 7: Price range relative to average
        if len(close_prices) >= 5:
            price_range = (high_prices[-5:].max() - low_prices[-5:].min()) / max(np.mean(close_prices[-5:]), 1e-8)
            eng.append(price_range)
        else:
            eng.append(0.0)
            
        # Feature 8: Volume trend (ratio of recent volume to average)
        if len(close_prices) >= 5 and avg_daily_volume > 1e-8:
            volume_trend = volume / avg_daily_volume
            eng.append(volume_trend)
        else:
            eng.append(1.0)
            
        # Feature 9: Short interest momentum (change)
        si_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            if prev_si > 1e-8:
                si_momentum = (short_interest / prev_si) - 1.0
        eng.append(si_momentum)
        
        # Feature 10: Implied volatility to historical volatility ratio
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)
            
        # Feature 11: Short squeeze potential (days to cover * volatility)
        squeeze_potential = days_to_cover * implied_volatility
        eng.append(squeeze_potential)
        
        # Feature 12: MACD-like indicator (difference between short and long moving averages)
        if len(close_prices) >= 12:
            short_ma = np.mean(close_prices[-5:])
            long_ma = np.mean(close_prices[-12:])
            macd = (short_ma / max(long_ma, 1e-8)) - 1.0
            eng.append(macd)
        else:
            eng.append(0.0)
            
        # Feature 13: Bollinger Band width
        if len(close_prices) >= 10:
            ma_20 = np.mean(close_prices[-10:])
            std_20 = np.std(close_prices[-10:])
            bb_width = (2 * std_20) / max(ma_20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
            
        # Feature 14: Put-call ratio momentum
        put_call_momentum = 0.0
        if t > 0:
            prev_pc = data[t-1, 64]
            if prev_pc > 1e-8:
                put_call_momentum = (put_call_ratio / prev_pc) - 1.0
        eng.append(put_call_momentum)
        
        # Feature 15: Synthetic short cost momentum
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            if prev_cost > 1e-8:
                synth_cost_momentum = (synthetic_short_cost / prev_cost) - 1.0
        eng.append(synth_cost_momentum)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 903040.7667
RMSE: 1279887.7970
MAPE: 8.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0022, rank=1
   2. Feature_14_t3: importance=0.0019, rank=2
   3. Feature_0_t3: importance=0.0015, rank=3
   4. Feature_10_t3: importance=0.0012, rank=4
   5. Feature_22_t2: importance=0.0012, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.44%

ðŸ“ˆ Current best MAPE: 7.61%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Keep critical raw features based on importance analysis
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Safely handle potential index error
        
        # Keep only the most important raw features based on previous iterations
        raw_keep.append(short_interest)  # Always keep short interest (top importance)
        raw_keep.append(avg_daily_volume)
        raw_keep.append(days_to_cover)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)
        raw_keep.append(put_call_ratio)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (key metric for short squeeze potential)
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest momentum (change) - improved calculation
        si_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            if prev_si > 1e-8:
                si_momentum = (short_interest / prev_si) - 1.0
        eng.append(si_momentum)
        
        # Feature 3: Short squeeze potential (days to cover * volatility * put_call_ratio)
        # Enhanced with put-call ratio to better capture market sentiment
        squeeze_potential = days_to_cover * implied_volatility * max(put_call_ratio, 0.1)
        eng.append(squeeze_potential)
        
        # Feature 4: Bollinger Band width (high importance in previous iteration)
        if len(close_prices) >= 10:
            ma_10 = np.mean(close_prices[-10:])
            std_10 = np.std(close_prices[-10:])
            bb_width = (2 * std_10) / max(ma_10, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 5: Put-call ratio momentum (high importance in previous iteration)
        put_call_momentum = 0.0
        if t > 0:
            prev_pc = data[t-1, 64]
            if prev_pc > 1e-8:
                put_call_momentum = (put_call_ratio / prev_pc) - 1.0
        eng.append(put_call_momentum)
        
        # Feature 6: Synthetic short cost momentum (high importance in previous iteration)
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            if prev_cost > 1e-8:
                synth_cost_momentum = (synthetic_short_cost / prev_cost) - 1.0
        eng.append(synth_cost_momentum)
        
        # Feature 7: Price volatility - improved calculation using true range
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                tr = max(
                    high_prices[-i] - low_prices[-i],
                    abs(high_prices[-i] - close_prices[-(i+1)]),
                    abs(low_prices[-i] - close_prices[-(i+1)])
                )
                true_ranges.append(tr)
            avg_tr = np.mean(true_ranges) if true_ranges else 0.0
            norm_atr = avg_tr / max(close_prices[-1], 1e-8)  # Normalized ATR
            eng.append(norm_atr)
        else:
            eng.append(0.0)
        
        # Feature 8: Short interest to volume ratio (normalized)
        si_to_volume_norm = (short_interest / max(avg_daily_volume * 5, 1e-8))  # 5-day equivalent
        eng.append(si_to_volume_norm)
        
        # Feature 9: Relative strength index (RSI) - high importance in previous iteration
        if len(close_prices) >= 14:
            diff = np.diff(close_prices[-15:])
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100.0 - (100.0 / (1.0 + rs))
            else:
                rsi = 100.0
            # Normalize RSI to [-1, 1] range to better capture extremes
            norm_rsi = (rsi - 50) / 50
            eng.append(norm_rsi)
        else:
            eng.append(0.0)
        
        # Feature 10: Implied volatility to historical volatility ratio (enhanced)
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            # Apply log transformation to handle extreme values better
            log_iv_hv = np.log1p(iv_hv_ratio) if iv_hv_ratio > 0 else 0.0
            eng.append(log_iv_hv)
        else:
            eng.append(0.0)
        
        # Feature 11: Price momentum with volatility adjustment (new composite feature)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            # Momentum scaled by inverse volatility (higher weight to stable trends)
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 12: MACD signal line crossover indicator
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            if t > 0 and len(data[t-1, 3:63].reshape(15, 4)[:, 3]) >= 26:
                prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3]
                prev_ema12 = np.mean(prev_close[-12:])
                prev_ema26 = np.mean(prev_close[-26:])
                prev_macd = prev_ema12 - prev_ema26
                
                # MACD momentum (change in MACD)
                macd_momentum = macd - prev_macd
                eng.append(macd_momentum)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # Feature 13: Short interest acceleration (second derivative)
        si_accel = 0.0
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            mom_t = (si_t / max(si_t1, 1e-8)) - 1.0
            mom_t1 = (si_t1 / max(si_t2, 1e-8)) - 1.0
            
            si_accel = mom_t - mom_t1
        eng.append(si_accel)
        
        # Feature 14: Volume pressure (ratio of volume to average volume, weighted by price direction)
        if volume > 0 and avg_daily_volume > 1e-8:
            price_direction = 1.0 if len(close_prices) >= 2 and close_prices[-1] > close_prices[-2] else -1.0
            vol_pressure = (volume / avg_daily_volume) * price_direction
            eng.append(vol_pressure)
        else:
            eng.append(0.0)
        
        # Feature 15: Composite short squeeze indicator
        # Combines multiple factors that contribute to short squeeze potential
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_ratio = volume / max(avg_daily_volume, 1e-8)
            squeeze_indicator = days_to_cover * si_to_float * (1 + max(price_momentum, 0)) * vol_ratio
            eng.append(squeeze_indicator)
        else:
            eng.append(0.0)
        
        # Feature 16: Options market sentiment indicator
        options_sentiment = put_call_ratio * synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(options_sentiment)
        
        # Feature 17: Normalized short interest change rate
        if t > 0:
            prev_si = data[t-1, 0]
            si_change_rate = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Normalize to typical range
            norm_si_change = np.tanh(si_change_rate * 5)  # Squash extreme values
            eng.append(norm_si_change)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 876600.4231
RMSE: 1272906.3001
MAPE: 7.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0022, rank=1
   2. Feature_0_t3: importance=0.0012, rank=2
   3. Feature_10_t3: importance=0.0010, rank=3
   4. Feature_15_t1: importance=0.0009, rank=4
   5. Feature_3_t2: importance=0.0008, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.33%

ðŸ“ˆ Current best MAPE: 7.61%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract critical raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Always keep the most important raw features based on previous iterations
        raw_keep.append(short_interest)  # Always keep short interest (consistently high importance)
        raw_keep.append(avg_daily_volume)
        raw_keep.append(days_to_cover)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)
        raw_keep.append(put_call_ratio)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (key metric for short squeeze potential)
        # Consistently important in previous iterations
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest acceleration (second derivative)
        # Top important feature in previous iteration (Feature_13_t3)
        si_accel = 0.0
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # Improved calculation using log returns for better numerical stability
            mom_t = np.log(max(si_t, 1e-8) / max(si_t1, 1e-8))
            mom_t1 = np.log(max(si_t1, 1e-8) / max(si_t2, 1e-8))
            
            si_accel = mom_t - mom_t1
        eng.append(si_accel)
        
        # Feature 3: Composite short squeeze indicator (Feature_10_t3 was important)
        # Enhanced version combining multiple factors that contribute to short squeeze potential
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_ratio = avg_daily_volume / max(np.mean(close_prices[-5:]), 1e-8)
            squeeze_indicator = days_to_cover * si_to_float * (1 + max(price_momentum, 0)) * vol_ratio
            # Apply tanh to normalize and handle extreme values better
            squeeze_indicator = np.tanh(squeeze_indicator)
            eng.append(squeeze_indicator)
        else:
            eng.append(0.0)
        
        # Feature 4: Short interest momentum with volatility adjustment
        # Enhanced version of previous high-importance feature
        si_vol_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            if prev_si > 1e-8:
                si_change = (short_interest / prev_si) - 1.0
                # Scale by implied volatility for better signal during volatile periods
                si_vol_momentum = si_change / max(implied_volatility, 0.01)
                # Apply sigmoid-like transformation to handle outliers
                si_vol_momentum = np.tanh(si_vol_momentum * 3)
        eng.append(si_vol_momentum)
        
        # Feature 5: Bollinger Band width (normalized)
        # Improved calculation with better normalization
        if len(close_prices) >= 10:
            ma_10 = np.mean(close_prices[-10:])
            std_10 = np.std(close_prices[-10:])
            bb_width = (2 * std_10) / max(ma_10, 1e-8)
            # Log transform to better handle the distribution of width values
            bb_width = np.log1p(bb_width)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 6: Relative strength index (RSI) - with improved calculation
        # Important in previous iterations
        if len(close_prices) >= 14:
            diff = np.diff(close_prices[-15:])
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100.0 - (100.0 / (1.0 + rs))
            else:
                rsi = 100.0
            # Transform to better capture extremes (overbrought/oversold)
            norm_rsi = (rsi - 50) / 25  # Centered at 0, +/-2 for extremes
            eng.append(norm_rsi)
        else:
            eng.append(0.0)
        
        # Feature 7: Options market sentiment indicator (enhanced)
        # Combines put-call ratio with synthetic short cost and implied volatility
        options_sentiment = put_call_ratio * synthetic_short_cost / max(implied_volatility, 1e-8)
        # Apply log transformation to handle extreme values better
        options_sentiment = np.log1p(options_sentiment) if options_sentiment > 0 else 0.0
        eng.append(options_sentiment)
        
        # Feature 8: Volume pressure with price direction
        # Enhanced version of Feature_14 which was important in previous iteration
        price_direction = 0.0
        if len(close_prices) >= 5:
            # Use 5-day price direction for more stability
            price_direction = 1.0 if close_prices[-1] > close_prices[-5] else -1.0
            vol_pressure = (avg_daily_volume / max(np.mean(close_prices[-5:]), 1e-8)) * price_direction
            # Normalize with tanh to handle extreme values
            vol_pressure = np.tanh(vol_pressure / 100)
            eng.append(vol_pressure)
        else:
            eng.append(0.0)
        
        # Feature 9: Implied volatility to historical volatility ratio (enhanced)
        # Important in previous iterations
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            # Apply log transformation with better scaling
            log_iv_hv = np.log1p(iv_hv_ratio) if iv_hv_ratio > 0 else 0.0
            eng.append(log_iv_hv)
        else:
            eng.append(0.0)
        
        # Feature 10: Short interest to volume ratio (normalized)
        # Enhanced with better normalization
        si_to_volume_norm = short_interest / max(avg_daily_volume * 5, 1e-8)  # 5-day equivalent
        # Apply log transformation for better distribution
        si_to_volume_norm = np.log1p(si_to_volume_norm)
        eng.append(si_to_volume_norm)
        
        # Feature 11: MACD signal line crossover indicator (enhanced)
        # Improved calculation with better EMA approximation
        if len(close_prices) >= 26:
            # Better EMA approximation with weighted average
            weights_12 = np.exp(np.linspace(-1, 0, 12))
            weights_12 = weights_12 / np.sum(weights_12)
            weights_26 = np.exp(np.linspace(-1, 0, 26))
            weights_26 = weights_26 / np.sum(weights_26)
            
            # Use available data points
            data_points = min(len(close_prices), 26)
            
            if data_points >= 12:
                ema12 = np.sum(close_prices[-12:] * weights_12[-12:])
            else:
                ema12 = np.mean(close_prices[-data_points:])
                
            if data_points >= 26:
                ema26 = np.sum(close_prices[-26:] * weights_26[-26:])
            else:
                ema26 = np.mean(close_prices[-data_points:])
                
            macd = ema12 - ema26
            
            # Normalize MACD by price level for better comparison across stocks
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 12: Price momentum with volatility adjustment
        # Enhanced version with better volatility calculation
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            
            # Calculate rolling volatility with better handling of edge cases
            if len(close_prices) >= 20:
                returns = np.diff(close_prices[-21:]) / np.maximum(close_prices[-21:-1], 1e-8)
                vol = np.std(returns) if len(returns) > 0 else 0.01
            else:
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                vol = np.std(returns) if len(returns) > 0 else 0.01
                
            # Momentum scaled by inverse volatility (higher weight to stable trends)
            vol_adj_momentum = momentum_10d / max(vol, 0.01)
            # Apply tanh to normalize
            vol_adj_momentum = np.tanh(vol_adj_momentum)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 13: Average True Range (ATR) - normalized
        # Improved calculation for volatility measurement
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                tr = max(
                    high_prices[-i] - low_prices[-i],
                    abs(high_prices[-i] - close_prices[-(i+1)]),
                    abs(low_prices[-i] - close_prices[-(i+1)])
                )
                true_ranges.append(tr)
            avg_tr = np.mean(true_ranges) if true_ranges else 0.0
            norm_atr = avg_tr / max(close_prices[-1], 1e-8)  # Normalized ATR
            eng.append(norm_atr)
        else:
            eng.append(0.0)
        
        # Feature 14: Short interest change rate (normalized)
        # Enhanced with better normalization
        if t > 0:
            prev_si = data[t-1, 0]
            si_change_rate = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Use sigmoid-like normalization to handle extreme values
            norm_si_change = np.tanh(si_change_rate * 5)
            eng.append(norm_si_change)
        else:
            eng.append(0.0)
        
        # Feature 15: Synthetic short cost momentum
        # Important in previous iterations
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            if prev_cost > 1e-8:
                synth_cost_momentum = (synthetic_short_cost / prev_cost) - 1.0
                # Apply tanh for better normalization
                synth_cost_momentum = np.tanh(synth_cost_momentum * 3)
        eng.append(synth_cost_momentum)
        
        # Feature 16: Price trend strength indicator
        # New composite feature combining price momentum and volume
        if len(close_prices) >= 10:
            # Calculate directional movement
            up_moves = 0
            down_moves = 0
            for i in range(1, 10):
                if close_prices[-i] > close_prices[-(i+1)]:
                    up_moves += 1
                elif close_prices[-i] < close_prices[-(i+1)]:
                    down_moves += 1
            
            # Trend strength from -1 (strong downtrend) to +1 (strong uptrend)
            trend_strength = (up_moves - down_moves) / 9.0
            
            # Weight by recent volume relative to average
            if avg_daily_volume > 1e-8:
                vol_weight = min(3.0, max(0.5, avg_daily_volume / max(np.mean(close_prices[-10:]), 1e-8)))
                trend_strength *= vol_weight
                # Normalize to [-1, 1] range
                trend_strength = np.clip(trend_strength, -1.0, 1.0)
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 17: Short squeeze risk score
        # New composite feature combining multiple risk factors
        squeeze_risk = days_to_cover * si_to_float
        if len(close_prices) >= 5:
            # Add price momentum component
            price_chg = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            # Higher risk when price is rising (potential squeeze)
            momentum_factor = 1.0 + max(0, price_chg * 2)
            squeeze_risk *= momentum_factor
            
            # Add volatility component
            vol_factor = 1.0 + min(2.0, implied_volatility)
            squeeze_risk *= vol_factor
            
            # Normalize with sigmoid-like function
            squeeze_risk = np.tanh(squeeze_risk / 5.0)
        eng.append(squeeze_risk)
        
        # Feature 18: Put-call ratio momentum
        # Enhanced version with better normalization
        put_call_momentum = 0.0
        if t > 0:
            prev_pc = data[t-1, 64]
            if prev_pc > 1e-8:
                put_call_momentum = (put_call_ratio / prev_pc) - 1.0
                # Apply tanh for better normalization
                put_call_momentum = np.tanh(put_call_momentum * 3)
        eng.append(put_call_momentum)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 884325.7880
RMSE: 1275382.2762
MAPE: 7.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0014, rank=1
   2. Feature_0_t3: importance=0.0010, rank=2
   3. Feature_20_t3: importance=0.0009, rank=3
   4. Feature_18_t3: importance=0.0009, rank=4
   5. Feature_15_t3: importance=0.0007, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.37%

ðŸ“ˆ Current best MAPE: 7.61%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract critical raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep the most important raw features based on previous iterations
        # Feature_0_t3 has consistently been a top important feature
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-related signals
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized)
        # Key metric for short squeeze potential, consistently important
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        # Apply log transformation for better distribution
        si_to_float_norm = np.log1p(si_to_float)
        eng.append(si_to_float_norm)
        
        # Feature 2: Short interest acceleration (second derivative)
        # Top important feature in previous iterations (Feature_13_t3)
        si_accel = 0.0
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # Calculate acceleration using log returns for better numerical stability
            si_t = max(si_t, 1e-8)
            si_t1 = max(si_t1, 1e-8)
            si_t2 = max(si_t2, 1e-8)
            
            mom_t = np.log(si_t / si_t1)
            mom_t1 = np.log(si_t1 / si_t2)
            
            si_accel = mom_t - mom_t1
            # Apply tanh to normalize extreme values
            si_accel = np.tanh(si_accel * 2)
        eng.append(si_accel)
        
        # Feature 3: Enhanced short squeeze indicator
        # Combines multiple factors that contribute to short squeeze potential
        # Feature_10_t3 was important in previous iterations
        squeeze_indicator = 0.0
        if len(close_prices) >= 5:
            # Calculate price momentum over 5 days
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Calculate volume ratio (current volume to average)
            vol_ratio = avg_daily_volume / max(np.mean(close_prices[-5:] * 0.01), 1e-8)
            
            # Combine factors with improved weighting
            squeeze_indicator = days_to_cover * si_to_float * (1 + max(price_momentum, 0)) * vol_ratio
            
            # Apply sigmoid-like normalization
            squeeze_indicator = np.tanh(squeeze_indicator * 0.5)
        eng.append(squeeze_indicator)
        
        # Feature 4: Short interest momentum with volatility adjustment
        # Enhanced version of previous high-importance feature
        si_vol_momentum = 0.0
        if t > 0:
            prev_si = max(data[t-1, 0], 1e-8)
            si_change = (short_interest / prev_si) - 1.0
            
            # Scale by implied volatility for better signal during volatile periods
            # Higher volatility means momentum is less reliable
            vol_adj = max(implied_volatility, 0.01)
            si_vol_momentum = si_change / vol_adj
            
            # Apply tanh for better normalization
            si_vol_momentum = np.tanh(si_vol_momentum * 2)
        eng.append(si_vol_momentum)
        
        # Feature 5: Improved Bollinger Band width (normalized)
        # Better indicator of volatility expansion/contraction
        bb_width = 0.0
        if len(close_prices) >= 10:
            ma_10 = np.mean(close_prices[-10:])
            std_10 = np.std(close_prices[-10:])
            bb_width = (2 * std_10) / max(ma_10, 1e-8)
            
            # Apply log transform with better scaling
            bb_width = np.log1p(bb_width * 5) / 2  # Scaled to typical range
        eng.append(bb_width)
        
        # Feature 6: Enhanced RSI with volume weighting
        # Improved calculation that incorporates volume information
        vol_weighted_rsi = 50.0  # Neutral default
        if len(close_prices) >= 14 and len(close_prices) == len(ohlc):
            # Calculate price changes
            diff = np.diff(close_prices[-15:])
            
            # Get corresponding volumes (use the last 14 days)
            vols = np.array([ohlc[-i-1, 0] * 0.01 for i in range(min(14, len(diff)))])
            vols = np.maximum(vols, 1e-8)  # Ensure no zeros
            
            # Weight the price changes by volume
            vol_weighted_diff = diff[-len(vols):] * vols
            
            # Calculate gains and losses
            gains = np.sum(np.maximum(vol_weighted_diff, 0))
            losses = np.sum(np.abs(np.minimum(vol_weighted_diff, 0)))
            
            # Calculate RSI
            if losses > 1e-8:
                rs = gains / losses
                vol_weighted_rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gains > 0:
                vol_weighted_rsi = 100.0
                
            # Transform to better capture extremes (-1 to 1 range)
            vol_weighted_rsi = (vol_weighted_rsi - 50) / 50
        eng.append(vol_weighted_rsi)
        
        # Feature 7: Options market sentiment indicator (enhanced)
        # Combines put-call ratio with synthetic short cost and implied volatility
        # Feature_15_t1 was important in previous iterations
        options_sentiment = 0.0
        if put_call_ratio > 0 and synthetic_short_cost > 0:
            # Higher values indicate bearish options sentiment
            options_sentiment = put_call_ratio * synthetic_short_cost / max(implied_volatility, 1e-8)
            
            # Apply log transformation with better scaling
            options_sentiment = np.log1p(options_sentiment) / 2
        eng.append(options_sentiment)
        
        # Feature 8: Volume pressure with directional strength
        # Enhanced version of Feature_14 which was important in previous iterations
        vol_pressure = 0.0
        if len(close_prices) >= 10:
            # Calculate directional strength over 10 days
            up_days = sum(1 for i in range(1, min(10, len(close_prices))) 
                          if close_prices[-i] > close_prices[-(i+1)])
            down_days = sum(1 for i in range(1, min(10, len(close_prices))) 
                           if close_prices[-i] < close_prices[-(i+1)])
            
            # Calculate directional strength (-1 to 1)
            dir_strength = (up_days - down_days) / max(up_days + down_days, 1)
            
            # Combine with volume
            vol_pressure = (avg_daily_volume / max(np.mean(close_prices[-10:]) * 0.01, 1e-8)) * dir_strength
            
            # Normalize with tanh
            vol_pressure = np.tanh(vol_pressure / 10)
        eng.append(vol_pressure)
        
        # Feature 9: Implied volatility to historical volatility ratio (enhanced)
        # Important for detecting options market expectations vs. realized volatility
        iv_hv_ratio = 1.0  # Neutral default
        if len(close_prices) >= 10:
            # Calculate historical volatility (annualized)
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            
            # Calculate ratio
            iv_hv_ratio = implied_volatility / max(hist_vol, 0.001)
            
            # Apply log transformation with better scaling
            iv_hv_ratio = np.log1p(iv_hv_ratio - 1) if iv_hv_ratio > 1 else -np.log1p(1 - iv_hv_ratio)
            
            # Normalize to typical range
            iv_hv_ratio = np.tanh(iv_hv_ratio)
        eng.append(iv_hv_ratio)
        
        # Feature 10: Short interest to volume ratio with trend adjustment
        # Enhanced with trend information
        si_vol_ratio = 0.0
        if avg_daily_volume > 0:
            # Base ratio
            si_vol_ratio = short_interest / max(avg_daily_volume * 5, 1e-8)
            
            # Add trend adjustment if we have enough data
            if t > 0 and len(close_prices) >= 5:
                prev_si = data[t-1, 0]
                si_trend = (short_interest / max(prev_si, 1e-8)) - 1.0
                
                # Price trend
                price_trend = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
                
                # Adjust ratio based on trends (higher when SI is rising but price is falling)
                trend_factor = (1 + max(0, si_trend)) * (1 - min(0, price_trend))
                si_vol_ratio *= trend_factor
            
            # Apply log transformation for better distribution
            si_vol_ratio = np.log1p(si_vol_ratio)
        eng.append(si_vol_ratio)
        
        # Feature 11: Improved MACD signal with better normalization
        # Enhanced calculation with better EMA approximation
        norm_macd = 0.0
        if len(close_prices) >= 26:
            # Calculate exponential weights
            alpha_12 = 2 / (12 + 1)
            alpha_26 = 2 / (26 + 1)
            
            # Calculate EMAs using available data
            ema12 = close_prices[-1]
            ema26 = close_prices[-1]
            
            # Calculate EMA-12
            for i in range(2, min(13, len(close_prices) + 1)):
                ema12 = close_prices[-i] * alpha_12 + ema12 * (1 - alpha_12)
                
            # Calculate EMA-26
            for i in range(2, min(27, len(close_prices) + 1)):
                ema26 = close_prices[-i] * alpha_26 + ema26 * (1 - alpha_26)
            
            # Calculate MACD
            macd = ema12 - ema26
            
            # Normalize by price level for better comparison
            norm_macd = macd / max(close_prices[-1], 1e-8)
            
            # Apply tanh for better scaling
            norm_macd = np.tanh(norm_macd * 20)
        eng.append(norm_macd)
        
        # Feature 12: Price momentum with volatility adjustment and trend strength
        # Enhanced version with better volatility calculation and trend information
        vol_adj_momentum = 0.0
        if len(close_prices) >= 10:
            # Calculate momentum
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            
            # Calculate volatility
            returns = np.diff(close_prices[-min(11, len(close_prices)):]) / np.maximum(close_prices[-min(11, len(close_prices)):-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            
            # Calculate trend consistency
            up_moves = sum(1 for r in returns if r > 0)
            down_moves = len(returns) - up_moves
            trend_consistency = abs(up_moves - down_moves) / max(len(returns), 1)
            
            # Adjust momentum by volatility and consistency
            vol_adj_momentum = momentum_10d * trend_consistency / max(vol, 0.01)
            
            # Apply tanh for normalization
            vol_adj_momentum = np.tanh(vol_adj_momentum * 2)
        eng.append(vol_adj_momentum)
        
        # Feature 13: Enhanced Average True Range (ATR) with price gap detection
        # Improved volatility measurement that captures overnight gaps
        enh_atr = 0.0
        if len(close_prices) >= 5:
            true_ranges = []
            gaps = []
            
            for i in range(1, min(5, len(close_prices))):
                # Standard TR calculation
                tr = max(
                    high_prices[-i] - low_prices[-i],
                    abs(high_prices[-i] - close_prices[-(i+1)]),
                    abs(low_prices[-i] - close_prices[-(i+1)])
                )
                true_ranges.append(tr)
                
                # Calculate overnight gap
                if i < len(close_prices) - 1:
                    gap = abs(open_prices[-i] - close_prices[-(i+1)])
                    gaps.append(gap)
            
            # Calculate enhanced ATR with gap emphasis
            avg_tr = np.mean(true_ranges) if true_ranges else 0.0
            avg_gap = np.mean(gaps) if gaps else 0.0
            
            # Combine TR and gap (giving more weight to gaps)
            enh_atr = (avg_tr + 2 * avg_gap) / 3
            
            # Normalize by price
            enh_atr = enh_atr / max(close_prices[-1], 1e-8)
            
            # Apply log transform for better distribution
            enh_atr = np.log1p(enh_atr * 10)
        eng.append(enh_atr)
        
        # Feature 14: Short interest change rate with acceleration
        # Enhanced with second-order effects
        si_change_rate = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change_rate = (short_interest - prev_si) / max(prev_si, 1e-8)
            
            # Add acceleration component if available
            if t > 1:
                prev_prev_si = data[t-2, 0]
                prev_change = (prev_si - prev_prev_si) / max(prev_prev_si, 1e-8)
                
                # Acceleration (change in change rate)
                accel = si_change_rate - prev_change
                
                # Combine rate and acceleration
                si_change_rate = si_change_rate + 0.5 * accel
            
            # Apply tanh for better normalization
            si_change_rate = np.tanh(si_change_rate * 3)
        eng.append(si_change_rate)
        
        # Feature 15: Synthetic short cost momentum with implied volatility adjustment
        # Important in previous iterations (Feature_15_t3)
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = max(data[t-1, 65], 1e-8)
            
            # Calculate basic momentum
            cost_change = (synthetic_short_cost / prev_cost) - 1.0
            
            # Adjust by implied volatility (more significant in low vol environments)
            iv_factor = 1.0 / max(implied_volatility, 0.05)
            synth_cost_momentum = cost_change * iv_factor
            
            # Apply tanh for normalization
            synth_cost_momentum = np.tanh(synth_cost_momentum * 2)
        eng.append(synth_cost_momentum)
        
        # Feature 16: Enhanced short squeeze risk score
        # New composite feature combining multiple risk factors with better weighting
        # Feature_19_t3 was important in previous iteration
        squeeze_risk = 0.0
        
        # Base risk from days to cover and short interest
        base_risk = days_to_cover * si_to_float
        
        if len(close_prices) >= 5:
            # Price momentum component (higher risk when price is rising)
            price_chg = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            momentum_factor = np.exp(max(0, price_chg * 3))  # Exponential scaling for sharp rises
            
            # Volatility component
            vol_factor = 1.0 + min(1.5, implied_volatility)
            
            # Volume surge component (higher risk with increasing volume)
            vol_surge = 1.0
            if t > 0 and data[t-1, 1] > 0:
                vol_surge = min(3.0, avg_daily_volume / max(data[t-1, 1], 1e-8))
            
            # Options component (higher risk with increasing put-call ratio)
            options_factor = 1.0
            if t > 0 and data[t-1, 64] > 0:
                pc_change = put_call_ratio / max(data[t-1, 64], 1e-8)
                options_factor = 1.0 + max(0, (pc_change - 1.0))
            
            # Combine all factors
            squeeze_risk = base_risk * momentum_factor * vol_factor * vol_surge * options_factor
            
            # Normalize with sigmoid-like function
            squeeze_risk = np.tanh(squeeze_risk / 3.0)
        eng.append(squeeze_risk)
        
        # Feature 17: Put-call ratio momentum with synthetic short cost interaction
        # Enhanced version that captures the interaction between options market and shorting costs
        pc_synth_interaction = 0.0
        if t > 0:
            prev_pc = max(data[t-1, 64], 1e-8)
            prev_synth = max(data[t-1, 65], 1e-8)
            
            # Calculate individual momentums
            pc_mom = (put_call_ratio / prev_pc) - 1.0
            synth_mom = (synthetic_short_cost / prev_synth) - 1.0
            
            # Create interaction term (higher when both are moving in same direction)
            pc_synth_interaction = pc_mom * synth_mom
            if pc_mom > 0 and synth_mom > 0:
                # Amplify when both are increasing (bearish signal)
                pc_synth_interaction *= 1.5
            
            # Apply tanh for normalization
            pc_synth_interaction = np.tanh(pc_synth_interaction * 2)
        eng.append(pc_synth_interaction)
        
        # Feature 18: Improved price trend strength with volume confirmation
        # Enhanced version of previous trend strength indicator
        price_vol_trend = 0.0
        if len(close_prices) >= 10:
            # Calculate price direction over multiple timeframes
            dir_5d = 1 if close_prices[-1] > close_prices[-min(5, len(close_prices))] else -1
            dir_10d = 1 if close_prices[-1] > close_prices[-min(10, len(close_prices))] else -1
            
            # Calculate volume trend
            recent_vol = avg_daily_volume
            if t > 0:
                prev_vol = data[t-1, 1]
                vol_trend = (recent_vol / max(prev_vol, 1e-8)) - 1.0
            else:
                vol_trend = 0.0
            
            # Combine price and volume trends
            # Stronger signal when price and volume move together
            price_vol_trend = (dir_5d + dir_10d) / 2  # Average direction
            
            # Adjust by volume trend (amplify when volume confirms price direction)
            if (price_vol_trend > 0 and vol_trend > 0) or (price_vol_trend < 0 and vol_trend < 0):
                price_vol_trend *= (1 + min(1, abs(vol_trend)))
            
            # Normalize to [-1, 1]
            price_vol_trend = np.clip(price_vol_trend, -1.0, 1.0)
        eng.append(price_vol_trend)
        
        # Feature 19: Enhanced short interest to float ratio with institutional ownership adjustment
        # Provides a better measure of the effective float available for shorting
        adj_si_float = si_to_float  # Start with basic ratio
        
        # Adjust based on options market data (synthetic proxy for institutional activity)
        if implied_volatility > 0 and put_call_ratio > 0:
            # Higher IV and put-call ratio often indicate higher institutional activity
            inst_proxy = implied_volatility * put_call_ratio
            
            # Scale the adjustment factor
            adj_factor = np.tanh(inst_proxy / 2) * 0.5 + 1.0  # Range: [0.5, 1.5]
            
            # Apply adjustment (higher factor means effectively lower float)
            adj_si_float *= adj_factor
            
            # Apply log transform for better distribution
            adj_si_float = np.log1p(adj_si_float)
        eng.append(adj_si_float)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 937107.0694
RMSE: 1312645.7480
MAPE: 8.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0016, rank=1
   2. Feature_24_t3: importance=0.0013, rank=2
   3. Feature_0_t3: importance=0.0013, rank=3
   4. Feature_20_t2: importance=0.0012, rank=4
   5. Feature_11_t3: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.85%

ðŸ“ˆ Current best MAPE: 7.61%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract critical raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep the most important raw features based on previous iterations
        # Short interest has consistently been a top important feature
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-related signals
        raw_keep.append(put_call_ratio)  # Important for market sentiment
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized)
        # Key metric for short squeeze potential, consistently important
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        si_to_float_norm = np.log1p(si_to_float * 100) / 5  # Better scaling
        eng.append(si_to_float_norm)
        
        # Feature 2: Short interest momentum (1st derivative)
        # Simplified from previous iterations to focus on the core signal
        si_momentum = 0.0
        if t > 0:
            prev_si = max(data[t-1, 0], 1e-8)
            si_momentum = np.log(short_interest / prev_si)
            # Apply tanh for better normalization
            si_momentum = np.tanh(si_momentum * 5)
        eng.append(si_momentum)
        
        # Feature 3: Short interest acceleration (2nd derivative)
        # Top important feature in previous iterations
        si_accel = 0.0
        if t >= 2:
            si_t = max(data[t, 0], 1e-8)
            si_t1 = max(data[t-1, 0], 1e-8)
            si_t2 = max(data[t-2, 0], 1e-8)
            
            mom_t = np.log(si_t / si_t1)
            mom_t1 = np.log(si_t1 / si_t2)
            
            si_accel = mom_t - mom_t1
            # Apply tanh with improved scaling
            si_accel = np.tanh(si_accel * 3)
        eng.append(si_accel)
        
        # Feature 4: Days to cover momentum
        # New feature focusing on changes in days to cover
        dtc_momentum = 0.0
        if t > 0:
            prev_dtc = max(data[t-1, 2], 1e-8)
            dtc_momentum = np.log(days_to_cover / prev_dtc)
            dtc_momentum = np.tanh(dtc_momentum * 3)
        eng.append(dtc_momentum)
        
        # Feature 5: Short squeeze indicator (improved)
        # Combines multiple factors that contribute to short squeeze potential
        squeeze_indicator = 0.0
        if len(close_prices) >= 5:
            # Calculate price momentum over 5 days
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Calculate volume ratio (current volume to average)
            vol_ratio = avg_daily_volume / max(np.mean(close_prices) * 0.01, 1e-8)
            
            # Combine factors with improved weighting
            squeeze_indicator = days_to_cover * si_to_float * (1 + max(price_momentum, 0)) * vol_ratio
            
            # Apply sigmoid-like normalization with better scaling
            squeeze_indicator = np.tanh(squeeze_indicator * 0.3)
        eng.append(squeeze_indicator)
        
        # Feature 6: Price volatility (normalized)
        # Improved calculation that better captures recent volatility
        norm_volatility = 0.0
        if len(close_prices) >= 5:
            # Calculate returns
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            
            # Calculate volatility and normalize by price level
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            norm_volatility = volatility / max(0.001, np.mean(returns) if len(returns) > 0 else 0.001)
            
            # Apply log transform with better scaling
            norm_volatility = np.log1p(norm_volatility * 10) / 3
        eng.append(norm_volatility)
        
        # Feature 7: Short interest to volume ratio (improved)
        # Key metric for liquidity of short positions
        si_vol_ratio = short_interest / max(avg_daily_volume * 5, 1e-8)
        si_vol_ratio = np.log1p(si_vol_ratio) / 2  # Better scaling
        eng.append(si_vol_ratio)
        
        # Feature 8: Options market pressure (improved)
        # Combines put-call ratio with synthetic short cost
        options_pressure = 0.0
        if put_call_ratio > 0 and synthetic_short_cost > 0:
            options_pressure = put_call_ratio * synthetic_short_cost / max(implied_volatility, 1e-8)
            options_pressure = np.tanh(options_pressure / 5)
        eng.append(options_pressure)
        
        # Feature 9: Price trend strength
        # Simplified from previous iterations to focus on the core signal
        price_trend = 0.0
        if len(close_prices) >= 10:
            # Calculate price direction over multiple timeframes
            dir_5d = 1 if close_prices[-1] > close_prices[-min(5, len(close_prices))] else -1
            dir_10d = 1 if close_prices[-1] > close_prices[-min(10, len(close_prices))] else -1
            
            # Combine with strength based on magnitude
            mag_5d = abs(close_prices[-1] / max(close_prices[-min(5, len(close_prices))], 1e-8) - 1.0)
            mag_10d = abs(close_prices[-1] / max(close_prices[-min(10, len(close_prices))], 1e-8) - 1.0)
            
            # Weight recent trend more heavily
            price_trend = (dir_5d * mag_5d * 0.7 + dir_10d * mag_10d * 0.3)
            price_trend = np.tanh(price_trend * 5)
        eng.append(price_trend)
        
        # Feature 10: Synthetic short cost momentum
        # Important in previous iterations
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = max(data[t-1, 65], 1e-8)
            synth_cost_momentum = np.log(synthetic_short_cost / prev_cost)
            synth_cost_momentum = np.tanh(synth_cost_momentum * 3)
        eng.append(synth_cost_momentum)
        
        # Feature 11: Implied volatility momentum
        # New feature focusing on changes in implied volatility
        iv_momentum = 0.0
        if t > 0:
            prev_iv = max(data[t-1, 66], 1e-8)
            iv_momentum = np.log(implied_volatility / prev_iv)
            iv_momentum = np.tanh(iv_momentum * 3)
        eng.append(iv_momentum)
        
        # Feature 12: Price to volatility ratio
        # Captures how much price movement is occurring relative to implied volatility
        price_vol_ratio = 0.0
        if len(close_prices) >= 5 and implied_volatility > 0:
            price_change = abs(close_prices[-1] / max(close_prices[-5], 1e-8) - 1.0)
            price_vol_ratio = price_change / max(implied_volatility, 1e-8)
            price_vol_ratio = np.tanh(price_vol_ratio * 10)
        eng.append(price_vol_ratio)
        
        # Feature 13: Short interest to days to cover ratio
        # New composite feature that captures relationship between SI and DTC
        si_dtc_ratio = np.log1p(short_interest / max(days_to_cover, 1e-8)) / 5
        eng.append(si_dtc_ratio)
        
        # Feature 14: Price gap detection
        # Captures overnight gaps which can be significant for short squeezes
        price_gaps = 0.0
        if len(open_prices) >= 5 and len(close_prices) >= 5:
            gaps = []
            for i in range(1, min(5, len(open_prices))):
                if i < len(close_prices):
                    gap = (open_prices[-i] - close_prices[-(i+1)]) / max(close_prices[-(i+1)], 1e-8)
                    gaps.append(gap)
            
            # Use the average of absolute gaps
            if gaps:
                price_gaps = np.mean(np.abs(gaps))
                # Apply log transform for better distribution
                price_gaps = np.log1p(price_gaps * 10) / 3
        eng.append(price_gaps)
        
        # Feature 15: Short interest to implied volatility ratio
        # New feature that captures relationship between short interest and expected volatility
        si_iv_ratio = np.log1p(short_interest / max(implied_volatility * 100, 1e-8)) / 3
        eng.append(si_iv_ratio)
        
        # Feature 16: Volume trend
        # Captures recent changes in trading volume
        volume_trend = 0.0
        if t > 0:
            prev_vol = max(data[t-1, 1], 1e-8)
            volume_trend = np.log(avg_daily_volume / prev_vol)
            volume_trend = np.tanh(volume_trend * 3)
        eng.append(volume_trend)
        
        # Feature 17: Put-call ratio momentum
        # Captures changes in options market sentiment
        pc_momentum = 0.0
        if t > 0:
            prev_pc = max(data[t-1, 64], 1e-8)
            pc_momentum = np.log(put_call_ratio / prev_pc)
            pc_momentum = np.tanh(pc_momentum * 3)
        eng.append(pc_momentum)
        
        # Feature 18: Enhanced RSI (Relative Strength Index)
        # Improved calculation with better handling of edge cases
        rsi = 50.0  # Neutral default
        if len(close_prices) >= 14:
            # Calculate price changes
            diff = np.diff(close_prices[-15:])
            
            # Calculate gains and losses
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            
            # Calculate RSI
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gains > 0:
                rsi = 100.0
                
            # Transform to better capture extremes (-1 to 1 range)
            rsi = (rsi - 50) / 50
        eng.append(rsi)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract critical raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep the most important raw features based on previous iterations
        # Short interest has consistently been a top important feature
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(synthetic_short_cost)  # High importance in previous iterations
        raw_keep.append(implied_volatility)  # Important for options-related signals
        raw_keep.append(put_call_ratio)  # Important for market sentiment
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized)
        # Key metric for short squeeze potential, consistently important
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        si_to_float_norm = np.log1p(si_to_float)
        eng.append(si_to_float_norm)
        
        # Feature 2: Short interest acceleration (second derivative)
        # Top important feature in previous iterations
        si_accel = 0.0
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # Calculate acceleration using log returns for better numerical stability
            si_t = max(si_t, 1e-8)
            si_t1 = max(si_t1, 1e-8)
            si_t2 = max(si_t2, 1e-8)
            
            mom_t = np.log(si_t / si_t1)
            mom_t1 = np.log(si_t1 / si_t2)
            
            si_accel = mom_t - mom_t1
            # Apply tanh to normalize extreme values
            si_accel = np.tanh(si_accel * 2)
        eng.append(si_accel)
        
        # Feature 3: Enhanced short squeeze indicator
        # Combines multiple factors that contribute to short squeeze potential
        squeeze_indicator = 0.0
        if len(close_prices) >= 5:
            # Calculate price momentum over 5 days
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Calculate volume ratio (current volume to average)
            vol_ratio = avg_daily_volume / max(np.mean(close_prices[-5:]) * 0.01, 1e-8)
            
            # Combine factors with improved weighting
            squeeze_indicator = days_to_cover * si_to_float * (1 + max(price_momentum, 0)) * vol_ratio
            
            # Apply sigmoid-like normalization
            squeeze_indicator = np.tanh(squeeze_indicator * 0.5)
        eng.append(squeeze_indicator)
        
        # Feature 4: Short interest momentum with volatility adjustment
        # Enhanced version of previous high-importance feature
        si_vol_momentum = 0.0
        if t > 0:
            prev_si = max(data[t-1, 0], 1e-8)
            si_change = (short_interest / prev_si) - 1.0
            
            # Scale by implied volatility for better signal during volatile periods
            vol_adj = max(implied_volatility, 0.01)
            si_vol_momentum = si_change / vol_adj
            
            # Apply tanh for better normalization
            si_vol_momentum = np.tanh(si_vol_momentum * 2)
        eng.append(si_vol_momentum)
        
        # Feature 5: Improved Bollinger Band width (normalized)
        # Better indicator of volatility expansion/contraction
        bb_width = 0.0
        if len(close_prices) >= 10:
            ma_10 = np.mean(close_prices[-10:])
            std_10 = np.std(close_prices[-10:])
            bb_width = (2 * std_10) / max(ma_10, 1e-8)
            
            # Apply log transform with better scaling
            bb_width = np.log1p(bb_width * 5) / 2
        eng.append(bb_width)
        
        # Feature 6: Enhanced RSI with volume weighting
        # Improved calculation that incorporates volume information
        vol_weighted_rsi = 50.0  # Neutral default
        if len(close_prices) >= 14:
            # Calculate price changes
            diff = np.diff(close_prices[-15:])
            
            # Get corresponding volumes (use the last 14 days)
            vols = np.ones(min(14, len(diff)))  # Default to 1 if no volume data
            
            # Weight the price changes by volume
            vol_weighted_diff = diff[-len(vols):] * vols
            
            # Calculate gains and losses
            gains = np.sum(np.maximum(vol_weighted_diff, 0))
            losses = np.sum(np.abs(np.minimum(vol_weighted_diff, 0)))
            
            # Calculate RSI
            if losses > 1e-8:
                rs = gains / losses
                vol_weighted_rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gains > 0:
                vol_weighted_rsi = 100.0
                
            # Transform to better capture extremes (-1 to 1 range)
            vol_weighted_rsi = (vol_weighted_rsi - 50) / 50
        eng.append(vol_weighted_rsi)
        
        # Feature 7: Options market sentiment indicator (enhanced)
        # Combines put-call ratio with synthetic short cost and implied volatility
        options_sentiment = 0.0
        if put_call_ratio > 0 and synthetic_short_cost > 0:
            # Higher values indicate bearish options sentiment
            options_sentiment = put_call_ratio * synthetic_short_cost / max(implied_volatility, 1e-8)
            
            # Apply log transformation with better scaling
            options_sentiment = np.log1p(options_sentiment) / 2
        eng.append(options_sentiment)
        
        # Feature 8: Volume pressure with directional strength
        # Enhanced version of previous important feature
        vol_pressure = 0.0
        if len(close_prices) >= 10:
            # Calculate directional strength over 10 days
            up_days = sum(1 for i in range(1, min(10, len(close_prices))) 
                          if close_prices[-i] > close_prices[-(i+1)])
            down_days = sum(1 for i in range(1, min(10, len(close_prices))) 
                           if close_prices[-i] < close_prices[-(i+1)])
            
            # Calculate directional strength (-1 to 1)
            dir_strength = (up_days - down_days) / max(up_days + down_days, 1)
            
            # Combine with volume
            vol_pressure = (avg_daily_volume / max(np.mean(close_prices[-10:]) * 0.01, 1e-8)) * dir_strength
            
            # Normalize with tanh
            vol_pressure = np.tanh(vol_pressure / 10)
        eng.append(vol_pressure)
        
        # Feature 9: Implied volatility to historical volatility ratio
        # Important for detecting options market expectations vs. realized volatility
        iv_hv_ratio = 1.0  # Neutral default
        if len(close_prices) >= 10:
            # Calculate historical volatility (annualized)
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            
            # Calculate ratio
            iv_hv_ratio = implied_volatility / max(hist_vol, 0.001)
            
            # Apply log transformation with better scaling
            iv_hv_ratio = np.log1p(iv_hv_ratio - 1) if iv_hv_ratio > 1 else -np.log1p(1 - iv_hv_ratio)
            
            # Normalize to typical range
            iv_hv_ratio = np.tanh(iv_hv_ratio)
        eng.append(iv_hv_ratio)
        
        # Feature 10: Short interest to volume ratio with trend adjustment
        # Enhanced with trend information
        si_vol_ratio = 0.0
        if avg_daily_volume > 0:
            # Base ratio
            si_vol_ratio = short_interest / max(avg_daily_volume * 5, 1e-8)
            
            # Add trend adjustment if we have enough data
            if t > 0 and len(close_prices) >= 5:
                prev_si = data[t-1, 0]
                si_trend = (short_interest / max(prev_si, 1e-8)) - 1.0
                
                # Price trend
                price_trend = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
                
                # Adjust ratio based on trends (higher when SI is rising but price is falling)
                trend_factor = (1 + max(0, si_trend)) * (1 - min(0, price_trend))
                si_vol_ratio *= trend_factor
            
            # Apply log transformation for better distribution
            si_vol_ratio = np.log1p(si_vol_ratio)
        eng.append(si_vol_ratio)
        
        # Feature 11: Improved MACD signal with better normalization
        # Enhanced calculation with better EMA approximation
        norm_macd = 0.0
        if len(close_prices) >= 26:
            # Calculate exponential weights
            alpha_12 = 2 / (12 + 1)
            alpha_26 = 2 / (26 + 1)
            
            # Calculate EMAs using available data
            ema12 = close_prices[-1]
            ema26 = close_prices[-1]
            
            # Calculate EMA-12
            for i in range(2, min(13, len(close_prices) + 1)):
                ema12 = close_prices[-i] * alpha_12 + ema12 * (1 - alpha_12)
                
            # Calculate EMA-26
            for i in range(2, min(27, len(close_prices) + 1)):
                ema26 = close_prices[-i] * alpha_26 + ema26 * (1 - alpha_26)
            
            # Calculate MACD
            macd = ema12 - ema26
            
            # Normalize by price level for better comparison
            norm_macd = macd / max(close_prices[-1], 1e-8)
            
            # Apply tanh for better scaling
            norm_macd = np.tanh(norm_macd * 20)
        eng.append(norm_macd)
        
        # Feature 12: Price momentum with volatility adjustment
        # Enhanced version with better volatility calculation
        vol_adj_momentum = 0.0
        if len(close_prices) >= 10:
            # Calculate momentum
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            
            # Calculate volatility
            returns = np.diff(close_prices[-min(11, len(close_prices)):]) / np.maximum(close_prices[-min(11, len(close_prices)):-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 0.01
            
            # Calculate trend consistency
            up_moves = sum(1 for r in returns if r > 0)
            down_moves = len(returns) - up_moves
            trend_consistency = abs(up_moves - down_moves) / max(len(returns), 1)
            
            # Adjust momentum by volatility and consistency
            vol_adj_momentum = momentum_10d * trend_consistency / max(vol, 0.01)
            
            # Apply tanh for normalization
            vol_adj_momentum = np.tanh(vol_adj_momentum * 2)
        eng.append(vol_adj_momentum)
        
        # Feature 13: Enhanced Average True Range (ATR) with price gap detection
        # Improved volatility measurement that captures overnight gaps
        enh_atr = 0.0
        if len(close_prices) >= 5:
            true_ranges = []
            
            for i in range(1, min(5, len(close_prices))):
                # Standard TR calculation
                tr = max(
                    high_prices[-i] - low_prices[-i],
                    abs(high_prices[-i] - close_prices[-(i+1)]),
                    abs(low_prices[-i] - close_prices[-(i+1)])
                )
                true_ranges.append(tr)
            
            # Calculate enhanced ATR
            avg_tr = np.mean(true_ranges) if true_ranges else 0.0
            
            # Normalize by price
            enh_atr = avg_tr / max(close_prices[-1], 1e-8)
            
            # Apply log transform for better distribution
            enh_atr = np.log1p(enh_atr * 10)
        eng.append(enh_atr)
        
        # Feature 14: Short interest change rate
        # Enhanced with second-order effects
        si_change_rate = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change_rate = (short_interest - prev_si) / max(prev_si, 1e-8)
            
            # Add acceleration component if available
            if t > 1:
                prev_prev_si = data[t-2, 0]
                prev_change = (prev_si - prev_prev_si) / max(prev_prev_si, 1e-8)
                
                # Acceleration (change in change rate)
                accel = si_change_rate - prev_change
                
                # Combine rate and acceleration
                si_change_rate = si_change_rate + 0.5 * accel
            
            # Apply tanh for better normalization
            si_change_rate = np.tanh(si_change_rate * 3)
        eng.append(si_change_rate)
        
        # Feature 15: Synthetic short cost momentum with implied volatility adjustment
        # Important in previous iterations
        synth_cost_momentum = 0.0
        if t > 0:
            prev_cost = max(data[t-1, 65], 1e-8)
            
            # Calculate basic momentum
            cost_change = (synthetic_short_cost / prev_cost) - 1.0
            
            # Adjust by implied volatility (more significant in low vol environments)
            iv_factor = 1.0 / max(implied_volatility, 0.05)
            synth_cost_momentum = cost_change * iv_factor
            
            # Apply tanh for normalization
            synth_cost_momentum = np.tanh(synth_cost_momentum * 2)
        eng.append(synth_cost_momentum)
        
        # Feature 16: Enhanced short squeeze risk score
        # New composite feature combining multiple risk factors with better weighting
        squeeze_risk = 0.0
        
        # Base risk from days to cover and short interest
        base_risk = days_to_cover * si_to_float
        
        if len(close_prices) >= 5:
            # Price momentum component (higher risk when price is rising)
            price_chg = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            momentum_factor = np.exp(max(0, price_chg * 3))  # Exponential scaling for sharp rises
            
            # Volatility component
            vol_factor = 1.0 + min(1.5, implied_volatility)
            
            # Volume surge component (higher risk with increasing volume)
            vol_surge = 1.0
            if t > 0 and data[t-1, 1] > 0:
                vol_surge = min(3.0, avg_daily_volume / max(data[t-1, 1], 1e-8))
            
            # Combine all factors
            squeeze_risk = base_risk * momentum_factor * vol_factor * vol_surge
            
            # Normalize with sigmoid-like function
            squeeze_risk = np.tanh(squeeze_risk / 3.0)
        eng.append(squeeze_risk)
        
        # Feature 17: Price trend strength with volume confirmation
        # Enhanced version of previous trend strength indicator
        price_vol_trend = 0.0
        if len(close_prices) >= 10:
            # Calculate price direction over multiple timeframes
            dir_5d = 1 if close_prices[-1] > close_prices[-min(5, len(close_prices))] else -1
            dir_10d = 1 if close_prices[-1] > close_prices[-min(10, len(close_prices))] else -1
            
            # Calculate volume trend
            recent_vol = avg_daily_volume
            if t > 0:
                prev_vol = data[t-1, 1]
                vol_trend = (recent_vol / max(prev_vol, 1e-8)) - 1.0
            else:
                vol_trend = 0.0
            
            # Combine price and volume trends
            # Stronger signal when price and volume move together
            price_vol_trend = (dir_5d + dir_10d) / 2  # Average direction
            
            # Adjust by volume trend (amplify when volume confirms price direction)
            if (price_vol_trend > 0 and vol_trend > 0) or (price_vol_trend < 0 and vol_trend < 0):
                price_vol_trend *= (1 + min(1, abs(vol_trend)))
            
            # Normalize to [-1, 1]
            price_vol_trend = np.clip(price_vol_trend, -1.0, 1.0)
        eng.append(price_vol_trend)
        
        # Feature 18: Enhanced short interest to float ratio with institutional ownership adjustment
        # Provides a better measure of the effective float available for shorting
        adj_si_float = si_to_float  # Start with basic ratio
        
        # Adjust based on options market data (synthetic proxy for institutional activity)
        if implied_volatility > 0 and put_call_ratio > 0:
            # Higher IV and put-call ratio often indicate higher institutional activity
            inst_proxy = implied_volatility * put_call_ratio
            
            # Scale the adjustment factor
            adj_factor = np.tanh(inst_proxy / 2) * 0.5 + 1.0  # Range: [0.5, 1.5]
            
            # Apply adjustment (higher factor means effectively lower float)
            adj_si_float *= adj_factor
            
            # Apply log transform for better distribution
            adj_si_float = np.log1p(adj_si_float)
        eng.append(adj_si_float)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 899855.6662
RMSE: 1273948.5840
MAPE: 8.10%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0016, rank=1
   2. Feature_19_t3: importance=0.0016, rank=2
   3. Feature_23_t3: importance=0.0015, rank=3
   4. Feature_12_t3: importance=0.0012, rank=4
   5. Feature_21_t2: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.48%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 829719.9631
RMSE: 1117329.8231
MAPE: 9.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0018, rank=1
   2. Feature_65_t2: importance=0.0016, rank=2
   3. Feature_1_t2: importance=0.0007, rank=3
   4. Feature_0_t2: importance=0.0007, rank=4
   5. Feature_67_t1: importance=0.0006, rank=5
   Baseline MAPE: 9.46%
   Baseline MAE: 829719.9631
   Baseline RMSE: 1117329.8231

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 719437.2713
RMSE: 998357.7889
MAPE: 9.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0019, rank=1
   2. Feature_10_t3: importance=0.0013, rank=2
   3. Feature_15_t3: importance=0.0011, rank=3
   4. Feature_12_t2: importance=0.0009, rank=4
   5. Feature_13_t1: importance=0.0008, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 9.55%
   MAE: 719437.2713
   RMSE: 998357.7889

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 9.46%
   Best Model MAPE: 9.55%
   Absolute Improvement: -0.09%
   Relative Improvement: -0.9%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  7.61            N/A                 
1          Iteration 1               8.06            -0.44%              
2          Iteration 2               7.94            -0.33%              
3          Iteration 3               7.99            -0.37%              
4          Iteration 4               8.46            -0.85%              
5          Iteration 5               8.10            -0.48%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 2 - MAPE: 7.94%
âœ… Saved EXTR results to cache/EXTR_iterative_results_enhanced.pkl
âœ… Summary report saved for EXTR

ðŸŽ‰ Process completed successfully for EXTR!

================================================================================
PROCESSING TICKER 11/15: SCSC
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for SCSC
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for SCSC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SCSC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 39481.4453
RMSE: 52362.7008
MAPE: 14.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0015, rank=1
   2. Feature_1_t0: importance=0.0011, rank=2
   3. Feature_2_t3: importance=0.0010, rank=3
   4. Feature_63_t0: importance=0.0009, rank=4
   5. Feature_65_t3: importance=0.0008, rank=5

ðŸ“Š Baseline Performance: MAPE = 14.98%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep track of raw features to retain and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0  # Safely handle potential index error
        
        # Keep essential raw features based on importance analysis
        raw_keep.append(short_interest)  # Feature_0
        raw_keep.append(avg_daily_volume)  # Feature_1
        raw_keep.append(days_to_cover)  # Feature_2
        
        # Keep only the most recent OHLC values instead of all 60
        raw_keep.append(close_prices[-1])  # Most recent close
        raw_keep.append(high_prices[-1])  # Most recent high
        raw_keep.append(low_prices[-1])  # Most recent low
        
        # Keep important options data
        raw_keep.append(put_call_ratio)  # Feature_64
        raw_keep.append(synthetic_short_cost)  # Feature_65
        raw_keep.append(implied_volatility)  # Feature_66
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Ratio (SI to Shares Outstanding)
        if shares_outstanding > 1e-8:
            si_ratio = short_interest / shares_outstanding
        else:
            si_ratio = 0
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        if avg_daily_volume > 1e-8:
            si_volume_ratio = short_interest / avg_daily_volume
        else:
            si_volume_ratio = 0
        eng.append(si_volume_ratio)
        
        # 3. Price Momentum (5-day)
        if len(close_prices) >= 5 and close_prices[0] > 1e-8:
            momentum_5d = (close_prices[-1] / close_prices[-5]) - 1
        else:
            momentum_5d = 0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        if len(close_prices) >= 10 and close_prices[-10] > 1e-8:
            momentum_10d = (close_prices[-1] / close_prices[-10]) - 1
        else:
            momentum_10d = 0
        eng.append(momentum_10d)
        
        # 5. Volatility (standard deviation of returns over 10 days)
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / close_prices[-11:-1]
            returns = np.nan_to_num(returns, nan=0.0, posinf=0.0, neginf=0.0)
            volatility = np.std(returns)
        else:
            volatility = 0
        eng.append(volatility)
        
        # 6. Average True Range (ATR) - volatility indicator
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(high_low, high_close, low_close))
            atr = np.mean(true_ranges) if true_ranges else 0
        else:
            atr = 0
        eng.append(atr)
        
        # 7. RSI (Relative Strength Index) - 14 period
        if len(close_prices) >= 15:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain[-14:])
            avg_loss = np.mean(loss[-14:])
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if avg_gain > 0 else 50
        else:
            rsi = 50
        eng.append(rsi)
        
        # 8. MACD Line (12-26 day EMA difference)
        if len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema_26 = np.mean(close_prices[-26:])
            macd_line = ema_12 - ema_26
        else:
            macd_line = 0
        eng.append(macd_line)
        
        # 9. Price to Moving Average Ratio (5-day)
        if len(close_prices) >= 5:
            ma_5 = np.mean(close_prices[-5:])
            if ma_5 > 1e-8:
                price_to_ma_5 = close_prices[-1] / ma_5
            else:
                price_to_ma_5 = 1
        else:
            price_to_ma_5 = 1
        eng.append(price_to_ma_5)
        
        # 10. Bollinger Band Width (volatility measure)
        if len(close_prices) >= 20:
            ma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            if ma_20 > 1e-8:
                bb_width = (2 * std_20) / ma_20  # Normalized by price level
            else:
                bb_width = 0
        else:
            bb_width = 0
        eng.append(bb_width)
        
        # 11. Options Implied Volatility to Historical Volatility Ratio
        if volatility > 1e-8:
            iv_hv_ratio = implied_volatility / volatility
        else:
            iv_hv_ratio = 1
        eng.append(iv_hv_ratio)
        
        # 12. Short Cost to Volatility Ratio
        if volatility > 1e-8:
            short_cost_vol_ratio = synthetic_short_cost / volatility
        else:
            short_cost_vol_ratio = 0
        eng.append(short_cost_vol_ratio)
        
        # 13. Short Interest Change Rate (if we have previous data)
        short_interest_change = 0
        if t > 0 and data[t-1, 0] > 1e-8:
            short_interest_change = (short_interest / data[t-1, 0]) - 1
        eng.append(short_interest_change)
        
        # 14. Put-Call Ratio to Volatility Ratio
        if implied_volatility > 1e-8:
            pc_vol_ratio = put_call_ratio / implied_volatility
        else:
            pc_vol_ratio = 0
        eng.append(pc_vol_ratio)
        
        # 15. Days to Cover Change Rate
        dtc_change = 0
        if t > 0 and data[t-1, 2] > 1e-8:
            dtc_change = (days_to_cover / data[t-1, 2]) - 1
        eng.append(dtc_change)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Clean up any NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (9,) (10,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep options data which showed high importance
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        raw_keep.append(put_call_ratio)
        raw_keep.append(synthetic_short_cost)
        raw_keep.append(implied_volatility)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
            
        # Feature 2: Price momentum (10-day)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
            
        # Feature 3: Volatility (standard deviation of returns over 10 days)
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)
            
        # Feature 4: Average True Range (ATR) - volatility indicator
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
            
        # Feature 5: Short Interest to Float Ratio
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 6: Short Interest to Volume Ratio
        short_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(short_to_volume)
        
        # Feature 7: RSI (Relative Strength Index)
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14.0
            avg_loss = loss / 14.0
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
            
        # Feature 8: Price Range Ratio (High-Low spread relative to close)
        if len(close_prices) >= 5:
            price_ranges = (high_prices[-5:] - low_prices[-5:]) / np.maximum(close_prices[-5:], 1e-8)
            price_range_ratio = np.mean(price_ranges)
            eng.append(price_range_ratio)
        else:
            eng.append(0.0)
            
        # Feature 9: Implied Volatility to Historical Volatility Ratio
        if len(close_prices) >= 10 and implied_volatility > 0:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252) if len(returns) > 0 else 1e-8
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)
            
        # Feature 10: Short Interest Momentum (change in short interest)
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
            
        # Feature 11: Volume Trend (ratio of current volume to average)
        if volume > 0:
            volume_trend = volume / max(avg_daily_volume, 1e-8)
            eng.append(volume_trend)
        else:
            eng.append(1.0)
            
        # Feature 12: MACD Line (12-day EMA minus 26-day EMA)
        if len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema_26 = np.mean(close_prices[-26:])
            macd = ema_12 - ema_26
            eng.append(macd)
        else:
            eng.append(0.0)
            
        # Feature 13: Bollinger Band Width
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            bb_width = (2 * std_20) / max(sma_20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
            
        # Feature 14: Put-Call Ratio Change
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
            
        # Feature 15: Short Cost to Volatility Ratio
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 16: Price to Volume Ratio
        if volume > 0:
            price_volume_ratio = close_prices[-1] / max(volume, 1e-8)
            eng.append(price_volume_ratio)
        else:
            eng.append(0.0)
            
        # Feature 17: Short Interest to Days to Cover Ratio
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 39893.3552
RMSE: 51534.0505
MAPE: 15.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0021, rank=1
   2. Feature_11_t3: importance=0.0017, rank=2
   3. Feature_10_t0: importance=0.0017, rank=3
   4. Feature_16_t3: importance=0.0017, rank=4
   5. Feature_14_t0: importance=0.0015, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.04%

ðŸ“ˆ Current best MAPE: 14.98%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features that showed high importance
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio (key metric for short squeeze potential)
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest to Volume Ratio (indicates how many days of volume needed to cover shorts)
        short_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(short_to_volume)
        
        # Feature 3: Short Interest Momentum (change in short interest)
        # This was among the top important features in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Price Momentum (5-day) - captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 5: Volatility (standard deviation of returns over 10 days)
        # Important for assessing risk and potential for large price movements
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)
        
        # Feature 6: RSI (Relative Strength Index) - momentum oscillator
        # Helps identify overbought/oversold conditions
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14.0 if gain != 0 else 1e-8
            avg_loss = loss / 14.0 if loss != 0 else 1e-8
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 7: Bollinger Band Width - volatility indicator
        # Wider bands indicate higher volatility
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            bb_width = (2 * std_20) / max(sma_20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 8: Put-Call Ratio Change - indicates changing market sentiment
        # This was among the top important features in previous iterations
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # Feature 9: Short Cost to Volatility Ratio - cost efficiency of shorting
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 10: Price Range Ratio (High-Low spread relative to close)
        # Indicates intraday volatility
        if len(close_prices) >= 5:
            price_ranges = (high_prices[-5:] - low_prices[-5:]) / np.maximum(close_prices[-5:], 1e-8)
            price_range_ratio = np.mean(price_ranges)
            eng.append(price_range_ratio)
        else:
            eng.append(0.0)
        
        # Feature 11: Average True Range (ATR) - volatility indicator
        # This was among the top important features in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
        
        # Feature 12: Short Interest to Days to Cover Ratio
        # Indicates relative difficulty of covering short positions
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 13: Implied Volatility to Historical Volatility Ratio
        # Indicates market expectations vs. realized volatility
        if len(close_prices) >= 10 and implied_volatility > 0:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252) if len(returns) > 0 else 1e-8
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)
        
        # Feature 14: Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            prev_change = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            short_interest_accel = current_change - prev_change
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 15: Price to Short Interest Ratio
        # Indicates relative valuation compared to short interest
        price_to_short = close_prices[-1] / max(short_interest, 1e-8)
        eng.append(price_to_short)
        
        # Feature 16: Synthetic Short Cost Change
        # Captures changing costs of shorting
        if t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8)) - 1.0
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # Feature 17: Price Gap Ratio
        # Captures overnight price jumps which can indicate changing sentiment
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            eng.append(gap)
        else:
            eng.append(0.0)
        
        # Feature 18: Normalized Short Interest
        # Short interest normalized by its recent range
        if t >= 5:
            short_history = [data[max(0, t-i), 0] for i in range(5)]
            short_min = min(short_history)
            short_max = max(short_history)
            short_range = max(short_max - short_min, 1e-8)
            norm_short = (short_interest - short_min) / short_range
            eng.append(norm_short)
        else:
            eng.append(0.5)  # Default to middle of range
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 38769.3617
RMSE: 51571.3994
MAPE: 14.43%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t1: importance=0.0014, rank=1
   2. Feature_1_t3: importance=0.0010, rank=2
   3. Feature_21_t3: importance=0.0010, rank=3
   4. Feature_11_t1: importance=0.0010, rank=4
   5. Feature_23_t0: importance=0.0008, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.55%

ðŸ“ˆ Current best MAPE: 14.43%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep essential raw features based on previous importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover was important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio (key metric for short squeeze potential)
        # This ratio is critical for identifying stocks with high short interest relative to available shares
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest Momentum (change in short interest)
        # This was among the top important features in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes - important for trend detection
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            prev_change = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            short_interest_accel = current_change - prev_change
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 4: Price Momentum (5-day) - captures recent price trend
        # Price momentum was highly significant in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 5: Price Momentum (10-day) - captures medium-term price trend
        # Adding a longer-term momentum to capture different time horizons
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
        
        # Feature 6: Average True Range (ATR) - volatility indicator
        # This was among the top important features in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
        
        # Feature 7: ATR to Price Ratio - normalized volatility
        # Normalizing ATR by price provides a relative volatility measure
        if len(close_prices) >= 2:
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 8: Short Cost to Volatility Ratio - cost efficiency of shorting
        # This ratio was significant in previous iterations
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 9: Synthetic Short Cost Change
        # Captures changing costs of shorting - important for predicting short interest changes
        if t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8)) - 1.0
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # Feature 10: Put-Call Ratio Change - indicates changing market sentiment
        # This was among the top important features in previous iterations
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # Feature 11: RSI (Relative Strength Index) - momentum oscillator
        # Helps identify overbought/oversold conditions
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14.0 if gain != 0 else 1e-8
            avg_loss = loss / 14.0 if loss != 0 else 1e-8
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 12: RSI Divergence - indicates potential trend reversals
        # New feature that captures divergence between price and RSI
        if t > 0 and len(close_prices) >= 14:
            prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3][-1]
            prev_delta = np.diff(data[t-1, 3:63].reshape(15, 4)[:, 3][-15:])
            prev_gain = np.sum(np.where(prev_delta > 0, prev_delta, 0))
            prev_loss = np.sum(np.where(prev_delta < 0, -prev_delta, 0))
            
            prev_avg_gain = prev_gain / 14.0 if prev_gain != 0 else 1e-8
            prev_avg_loss = prev_loss / 14.0 if prev_loss != 0 else 1e-8
            
            prev_rs = prev_avg_gain / max(prev_avg_loss, 1e-8)
            prev_rsi = 100.0 - (100.0 / (1.0 + prev_rs))
            
            price_change = (close_prices[-1] / max(prev_close, 1e-8)) - 1.0
            rsi_change = (rsi / max(prev_rsi, 1e-8)) - 1.0
            
            # Divergence occurs when price and RSI move in opposite directions
            rsi_divergence = price_change * rsi_change
            eng.append(rsi_divergence)
        else:
            eng.append(0.0)
        
        # Feature 13: Bollinger Band Width - volatility indicator
        # Wider bands indicate higher volatility
        if len(close_prices) >= 20:
            sma_20 = np.mean(close_prices[-20:])
            std_20 = np.std(close_prices[-20:])
            bb_width = (2 * std_20) / max(sma_20, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 14: Bollinger Band Position - relative price position
        # Indicates overbought/oversold conditions
        if len(close_prices) >= 20:
            upper_band = sma_20 + (2 * std_20)
            lower_band = sma_20 - (2 * std_20)
            band_range = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_range
            eng.append(bb_position)
        else:
            eng.append(0.5)  # Middle of the band
        
        # Feature 15: MACD Signal - trend-following momentum indicator
        # MACD was not previously used but is a powerful indicator for trend changes
        if len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema_26 = np.mean(close_prices[-26:])
            macd = ema_12 - ema_26
            
            # Normalize MACD by price to make it comparable across stocks
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 16: Price to Short Interest Ratio
        # Indicates relative valuation compared to short interest
        price_to_short = close_prices[-1] / max(short_interest, 1e-8)
        eng.append(price_to_short)
        
        # Feature 17: Normalized Short Interest
        # Short interest normalized by its recent range
        if t >= 5:
            short_history = [data[max(0, t-i), 0] for i in range(5)]
            short_min = min(short_history)
            short_max = max(short_history)
            short_range = max(short_max - short_min, 1e-8)
            norm_short = (short_interest - short_min) / short_range
            eng.append(norm_short)
        else:
            eng.append(0.5)  # Default to middle of range
        
        # Feature 18: Implied Volatility Change
        # Captures changing market expectations
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8)) - 1.0
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 39039.0922
RMSE: 51169.6942
MAPE: 14.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0017, rank=1
   2. Feature_7_t3: importance=0.0016, rank=2
   3. Feature_4_t3: importance=0.0012, rank=3
   4. Feature_12_t0: importance=0.0011, rank=4
   5. Feature_17_t3: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.03%

ðŸ“ˆ Current best MAPE: 14.43%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on previous importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover was important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio
        # Key metric for short squeeze potential - consistently important
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Normalized Short Interest
        # Short interest normalized by its recent range - helps identify extremes
        if t >= 3:
            short_history = [data[max(0, t-i), 0] for i in range(3)]
            short_min = min(short_history)
            short_max = max(short_history)
            short_range = max(short_max - short_min, 1e-8)
            norm_short = (short_interest - short_min) / short_range
            eng.append(norm_short)
        else:
            eng.append(0.5)
        
        # Feature 4: Price Momentum (5-day)
        # Short-term price trend - consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 5: Price Volatility (ATR-based)
        # Volatility indicator - high importance in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 6: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        # High importance in previous iterations
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 7: Synthetic Short Cost Change
        # Captures changing costs of shorting - important for predicting short interest
        if t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8)) - 1.0
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # Feature 8: Put-Call Ratio Change
        # Indicates changing market sentiment - high importance in previous iterations
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # Feature 9: RSI (Relative Strength Index)
        # Momentum oscillator - helps identify overbought/oversold conditions
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14.0 if gain != 0 else 1e-8
            avg_loss = loss / 14.0 if loss != 0 else 1e-8
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)
        
        # Feature 10: Volume Trend
        # Captures trend in trading volume - important for liquidity assessment
        if len(close_prices) >= 5:
            recent_volumes = [ohlc[-i-1, 3] * avg_daily_volume / max(close_prices[-i-1], 1e-8) for i in range(5)]
            volume_trend = np.mean(recent_volumes) / max(recent_volumes[-1], 1e-8) - 1.0
            eng.append(volume_trend)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        # Different from days to cover as it uses raw short interest
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 12: Implied Volatility Change
        # Captures changing market expectations - important for predicting short interest
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8)) - 1.0
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 13: Price to Short Interest Ratio
        # Indicates relative valuation compared to short interest
        price_to_short = close_prices[-1] / max(short_interest, 1e-8)
        eng.append(price_to_short)
        
        # Feature 14: Short Interest Acceleration
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            prev_change = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            short_interest_accel = current_change - prev_change
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 15: Bollinger Band Position
        # Indicates overbought/oversold conditions - useful for mean reversion
        if len(close_prices) >= 10:
            sma_10 = np.mean(close_prices[-10:])
            std_10 = np.std(close_prices[-10:])
            upper_band = sma_10 + (2 * std_10)
            lower_band = sma_10 - (2 * std_10)
            band_range = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_range
            eng.append(bb_position)
        else:
            eng.append(0.5)
        
        # Feature 16: Price Gap Analysis
        # Captures overnight gaps - significant price movements often indicate important news
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # Feature 17: Short Interest to Days to Cover Ratio
        # Combines two important metrics to create a more informative feature
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 18: Volatility-Adjusted Short Interest
        # Normalizes short interest by volatility for better cross-stock comparison
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 36263.3470
RMSE: 50208.4527
MAPE: 13.53%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 87

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0012, rank=1
   2. Feature_4_t3: importance=0.0012, rank=2
   3. Feature_13_t3: importance=0.0011, rank=3
   4. Feature_20_t3: importance=0.0010, rank=4
   5. Feature_7_t1: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.90%

ðŸ“ˆ Current best MAPE: 13.53%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on previous importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover was important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio
        # Key metric for short squeeze potential - consistently important
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Price Momentum (5-day)
        # Short-term price trend - consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 4: Price Volatility (ATR-based)
        # Volatility indicator - high importance in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 5: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        # High importance in previous iterations
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 6: Synthetic Short Cost Change
        # Captures changing costs of shorting - important for predicting short interest
        if t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8)) - 1.0
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # Feature 7: Put-Call Ratio Change
        # Indicates changing market sentiment - high importance in previous iterations
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # Feature 8: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        # Different from days to cover as it uses raw short interest
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 9: Implied Volatility Change
        # Captures changing market expectations - important for predicting short interest
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8)) - 1.0
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 10: Short Interest Acceleration
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            prev_change = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            short_interest_accel = current_change - prev_change
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Interest to Days to Cover Ratio
        # Combines two important metrics to create a more informative feature
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 12: Volatility-Adjusted Short Interest
        # Normalizes short interest by volatility for better cross-stock comparison
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 13: MACD Signal - NEW
        # Moving Average Convergence Divergence - powerful trend indicator
        if len(close_prices) >= 12:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices)  # Use all available data for longer EMA
            macd = ema12 - ema26
            # Normalize by price
            macd_norm = macd / max(close_prices[-1], 1e-8)
            eng.append(macd_norm)
        else:
            eng.append(0.0)
        
        # Feature 14: Short Interest to Price Ratio - NEW
        # Measures short interest relative to price - helps identify potential short squeezes
        si_price_ratio = short_interest / max(close_prices[-1], 1e-8)
        eng.append(si_price_ratio)
        
        # Feature 15: Price Range Volatility - NEW
        # Measures recent price volatility as (high-low)/close
        if len(close_prices) >= 5:
            recent_highs = high_prices[-5:]
            recent_lows = low_prices[-5:]
            max_high = np.max(recent_highs)
            min_low = np.min(recent_lows)
            price_range = (max_high - min_low) / max(close_prices[-1], 1e-8)
            eng.append(price_range)
        else:
            eng.append(0.0)
        
        # Feature 16: Short Interest Trend Strength - NEW
        # Measures consistency of short interest direction
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(3)]
            si_diffs = np.diff(si_history)
            # Count how many changes are in the same direction
            same_dir = sum(1 for i in range(len(si_diffs)-1) if si_diffs[i] * si_diffs[i+1] > 0)
            # Normalize to [-1, 1] range where 1 is strong trend, -1 is reversal
            trend_strength = same_dir / max(len(si_diffs)-1, 1e-8) * np.sign(si_diffs[-1])
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 17: Volume-Weighted Price Momentum - NEW
        # Combines price momentum with volume for stronger signal
        if len(close_prices) >= 5:
            recent_volumes = [data[t, 68] for _ in range(min(5, len(close_prices)))]
            recent_returns = [(close_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1.0 
                             for i in range(1, min(5, len(close_prices)))]
            
            # Ensure we have both volumes and returns
            if recent_returns and sum(recent_volumes) > 0:
                # Weight returns by volume
                vol_weighted_returns = sum(r * v for r, v in zip(recent_returns, recent_volumes)) / sum(recent_volumes)
                eng.append(vol_weighted_returns)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # Feature 18: Short Interest Relative to Historical Range - NEW
        # Identifies if current short interest is at extreme levels
        if t >= 5:
            si_history = [data[max(0, t-i), 0] for i in range(5)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(si_max - si_min, 1e-8)
            si_position = (short_interest - si_min) / si_range
            eng.append(si_position)
        else:
            eng.append(0.5)  # Middle of range if not enough history
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep essential raw features based on previous importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover was important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio
        # Key metric for short squeeze potential - consistently important
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Price Momentum (5-day)
        # Short-term price trend - consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        # High importance in previous iterations
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 5: Synthetic Short Cost Change
        # Captures changing costs of shorting - important for predicting short interest
        if t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8)) - 1.0
            eng.append(short_cost_change)
        else:
            eng.append(0.0)
        
        # Feature 6: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        # Different from days to cover as it uses raw short interest
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 7: Price to Short Interest Ratio
        # Indicates relative valuation compared to short interest
        price_to_short = close_prices[-1] / max(short_interest, 1e-8)
        eng.append(price_to_short)
        
        # Feature 8: Short Interest Acceleration
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            prev_change = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            short_interest_accel = current_change - prev_change
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 9: Volatility-Adjusted Short Interest
        # Normalizes short interest by volatility for better cross-stock comparison
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 10: Short Interest to Days to Cover Ratio
        # Combines two important metrics to create a more informative feature
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 11: Price Volatility (ATR-based)
        # Volatility indicator - high importance in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 12: Implied Volatility Change
        # Captures changing market expectations - important for predicting short interest
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8)) - 1.0
            eng.append(iv_change)
        else:
            eng.append(0.0)
        
        # Feature 13: Put-Call Ratio Change
        # Indicates changing market sentiment - high importance in previous iterations
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio / max(prev_put_call, 1e-8)) - 1.0
            eng.append(put_call_change)
        else:
            eng.append(0.0)
        
        # Feature 14: RSI (Relative Strength Index)
        # Momentum oscillator - helps identify overbought/oversold conditions
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14.0 if gain != 0 else 1e-8
            avg_loss = loss / 14.0 if loss != 0 else 1e-8
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)
        
        # Feature 15: Short Interest Trend Strength
        # Measures consistency of short interest direction over multiple periods
        if t >= 3:
            si_changes = []
            for i in range(1, min(4, t+1)):
                prev_si = data[t-i, 0]
                current_si = data[t-i+1, 0]
                si_changes.append(1 if current_si > prev_si else -1 if current_si < prev_si else 0)
            
            # Calculate trend strength (1.0 = perfect trend, 0.0 = no trend)
            trend_strength = abs(sum(si_changes)) / max(len(si_changes), 1e-8)
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 16: Price Gap Volatility
        # Measures the volatility of overnight gaps - significant for short interest prediction
        if len(open_prices) >= 5 and len(close_prices) >= 5:
            gaps = []
            for i in range(1, 5):
                if i < len(open_prices) and i < len(close_prices):
                    gap = abs((open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1.0)
                    gaps.append(gap)
            
            gap_volatility = np.std(gaps) if len(gaps) > 1 else 0.0
            eng.append(gap_volatility)
        else:
            eng.append(0.0)
        
        # Feature 17: Short Interest to Price Volatility Ratio
        # Relates short interest to price volatility - important for risk assessment
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            si_price_vol_ratio = short_interest / max(price_volatility, 1e-8)
            eng.append(si_price_vol_ratio)
        else:
            eng.append(0.0)
        
        # Feature 18: Normalized Days to Cover
        # Days to cover normalized by its recent history
        if t >= 3:
            dtc_history = [data[max(0, t-i), 2] for i in range(3)]
            dtc_min = min(dtc_history)
            dtc_max = max(dtc_history)
            dtc_range = max(dtc_max - dtc_min, 1e-8)
            norm_dtc = (days_to_cover - dtc_min) / dtc_range
            eng.append(norm_dtc)
        else:
            eng.append(0.5)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 38752.7969
RMSE: 50551.0584
MAPE: 14.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0012, rank=1
   2. Feature_8_t1: importance=0.0012, rank=2
   3. Feature_7_t3: importance=0.0011, rank=3
   4. Feature_17_t3: importance=0.0011, rank=4
   5. Feature_22_t2: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.02%

ðŸ“ˆ Current best MAPE: 13.53%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on previous importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Days to cover was important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short Interest to Float Ratio
        # Key metric for short squeeze potential - consistently important
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 2: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 3: Price Momentum (5-day)
        # Short-term price trend - consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        # High importance in previous iterations
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 5: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        # Different from days to cover as it uses raw short interest
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 6: Price to Short Interest Ratio
        # Indicates relative valuation compared to short interest
        price_to_short = close_prices[-1] / max(short_interest, 1e-8)
        eng.append(price_to_short)
        
        # Feature 7: Volatility-Adjusted Short Interest
        # Normalizes short interest by volatility for better cross-stock comparison
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 8: Short Interest to Days to Cover Ratio
        # Combines two important metrics to create a more informative feature
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 9: Price Volatility (ATR-based)
        # Volatility indicator - high importance in previous iterations
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 10: Short Interest to Price Volatility Ratio
        # Relates short interest to price volatility - important for risk assessment
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            si_price_vol_ratio = short_interest / max(price_volatility, 1e-8)
            eng.append(si_price_vol_ratio)
        else:
            eng.append(0.0)
        
        # Feature 11: Bollinger Band Position
        # Indicates where price is relative to its volatility bands
        # Useful for identifying potential reversals that could affect short interest
        if len(close_prices) >= 5:
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            upper_band = sma + (2 * std)
            lower_band = sma - (2 * std)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
            eng.append(bb_position)
        else:
            eng.append(0.5)  # Middle of the band
        
        # Feature 12: Short Interest Acceleration (improved calculation)
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            
            # Use percentage changes for more stable calculation
            prev_change_pct = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change_pct = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Acceleration is the change in the rate of change
            short_interest_accel = current_change_pct - prev_change_pct
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 13: Volume Pressure Indicator
        # Measures buying/selling pressure based on price movement and volume
        # High volume on up days indicates buying pressure, high volume on down days indicates selling pressure
        if len(close_prices) >= 5 and len(open_prices) >= 5:
            volume_pressure = 0
            for i in range(1, 5):
                # Determine if day was up or down
                day_change = close_prices[-i] - open_prices[-i]
                # Use volume as a proxy since we don't have actual volume
                day_volume = avg_daily_volume
                # Positive for up days, negative for down days, weighted by volume
                volume_pressure += np.sign(day_change) * day_volume
            
            # Normalize by total volume
            volume_pressure = volume_pressure / max(avg_daily_volume * 4, 1e-8)
            eng.append(volume_pressure)
        else:
            eng.append(0.0)
        
        # Feature 14: Short Interest Trend Consistency
        # Measures how consistently short interest has been moving in one direction
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(4)]
            si_changes = [np.sign(si_history[i] - si_history[i+1]) for i in range(3)]
            
            # Count how many changes are in the same direction
            same_direction = sum(1 for i in range(1, len(si_changes)) if si_changes[i] == si_changes[0])
            consistency = (same_direction + 1) / len(si_changes)  # +1 to count the first change
            eng.append(consistency)
        else:
            eng.append(0.5)
        
        # Feature 15: Normalized Short Interest
        # Short interest normalized by its recent history
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(4)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(si_max - si_min, 1e-8)
            norm_si = (short_interest - si_min) / si_range
            eng.append(norm_si)
        else:
            eng.append(0.5)
        
        # Feature 16: Price Gap Significance
        # Measures the significance of overnight price gaps
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            # Normalize by recent volatility
            recent_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8) if len(close_prices) >= 5 else 0.01
            gap_significance = gap / max(recent_volatility, 1e-8)
            eng.append(gap_significance)
        else:
            eng.append(0.0)
        
        # Feature 17: Short Interest to Put-Call Ratio
        # Relates short interest to options market sentiment
        si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
        eng.append(si_pc_ratio)
        
        # Feature 18: Composite Momentum Indicator
        # Combines price momentum across multiple timeframes
        if len(close_prices) >= 15:
            mom_3d = (close_prices[-1] / max(close_prices[-3], 1e-8)) - 1.0
            mom_7d = (close_prices[-1] / max(close_prices[-7], 1e-8)) - 1.0
            mom_15d = (close_prices[-1] / max(close_prices[-15], 1e-8)) - 1.0
            
            # Weight recent momentum more heavily
            composite_momentum = (3 * mom_3d + 2 * mom_7d + mom_15d) / 6
            eng.append(composite_momentum)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 37128.7082
RMSE: 50746.0536
MAPE: 13.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0010, rank=1
   2. Feature_17_t3: importance=0.0010, rank=2
   3. Feature_7_t2: importance=0.0010, rank=3
   4. Feature_8_t1: importance=0.0009, rank=4
   5. Feature_17_t1: importance=0.0009, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.34%

ðŸ“ˆ Current best MAPE: 13.53%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep essential raw features based on previous importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Consistently important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Volatility-Adjusted Short Interest
        # Consistently high importance in previous iterations (Feature_7)
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 2: Short Interest to Float Ratio
        # Key metric for short squeeze potential
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 3: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Interest to Put-Call Ratio
        # Consistently high importance in previous iterations (Feature_17)
        si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
        eng.append(si_pc_ratio)
        
        # Feature 5: Bollinger Band Position
        # Indicates where price is relative to its volatility bands
        # Useful for identifying potential reversals that could affect short interest
        if len(close_prices) >= 5:
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            upper_band = sma + (2 * std)
            lower_band = sma - (2 * std)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
            eng.append(bb_position)
        else:
            eng.append(0.5)  # Middle of the band
        
        # Feature 6: Short Interest to Days to Cover Ratio
        # Combines two important metrics to create a more informative feature
        # Consistently important in previous iterations (Feature_8)
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 7: RSI (Relative Strength Index)
        # New feature - momentum oscillator that measures speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 13
            avg_loss = loss / 13
            
            if avg_loss == 0:
                rsi = 100
            else:
                rs = avg_gain / max(avg_loss, 1e-8)
                rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI
        
        # Feature 8: Short Interest Acceleration
        # Second derivative of short interest - captures acceleration in short interest changes
        # Improved calculation based on previous iterations
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            
            # Use percentage changes for more stable calculation
            prev_change_pct = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change_pct = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Acceleration is the change in the rate of change
            short_interest_accel = current_change_pct - prev_change_pct
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 9: Price Momentum (5-day)
        # Short-term price trend - consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 10: MACD Signal
        # New feature - Moving Average Convergence Divergence
        # Trend-following momentum indicator showing relationship between two moving averages
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Normalize MACD by price level for better comparability
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 12: ATR-based Volatility
        # Improved calculation of Average True Range for volatility measurement
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 13: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 14: Normalized Short Interest
        # Short interest normalized by its recent history
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(4)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(si_max - si_min, 1e-8)
            norm_si = (short_interest - si_min) / si_range
            eng.append(norm_si)
        else:
            eng.append(0.5)
        
        # Feature 15: Price Gap Significance
        # Measures the significance of overnight price gaps
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            # Normalize by recent volatility
            recent_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8) if len(close_prices) >= 5 else 0.01
            gap_significance = gap / max(recent_volatility, 1e-8)
            eng.append(gap_significance)
        else:
            eng.append(0.0)
        
        # Feature 16: Stochastic Oscillator
        # New feature - momentum indicator comparing closing price to price range over time
        if len(close_prices) >= 14 and len(high_prices) >= 14 and len(low_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            
            range_hl = highest_high - lowest_low
            if range_hl == 0:
                stoch_k = 0.5
            else:
                stoch_k = (close_prices[-1] - lowest_low) / range_hl
            eng.append(stoch_k)
        else:
            eng.append(0.5)
        
        # Feature 17: Short Interest Trend Consistency
        # Measures how consistently short interest has been moving in one direction
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(4)]
            si_changes = [np.sign(si_history[i] - si_history[i+1]) for i in range(3)]
            
            # Count how many changes are in the same direction
            same_direction = sum(1 for i in range(1, len(si_changes)) if si_changes[i] == si_changes[0])
            consistency = (same_direction + 1) / len(si_changes)  # +1 to count the first change
            eng.append(consistency)
        else:
            eng.append(0.5)
        
        # Feature 18: Short Interest to Implied Volatility Ratio
        # New feature - relates short interest to market expectations of volatility
        si_iv_ratio = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_iv_ratio)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 7 (claude) (SVM)
==================================================
Training SVM model...

Iteration 7 (claude) Performance:
MAE: 35060.1937
RMSE: 47978.2459
MAPE: 13.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t1: importance=0.0012, rank=1
   2. Feature_8_t3: importance=0.0012, rank=2
   3. Feature_13_t1: importance=0.0011, rank=3
   4. Feature_14_t3: importance=0.0010, rank=4
   5. Feature_17_t3: importance=0.0010, rank=5
ðŸ’¾ Saved code for iteration 7 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.29%

ðŸ“ˆ Current best MAPE: 13.24%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 8
======================================================================
ðŸ¤– Calling Claude for iteration 8...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on previous importance analysis
        # These were consistently important in previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Consistently important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Volatility-Adjusted Short Interest
        # Consistently high importance in previous iterations (Feature_7)
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 2: Short Interest to Float Ratio
        # Key metric for short squeeze potential
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 3: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Interest to Put-Call Ratio
        # Consistently high importance in previous iterations (Feature_17)
        si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
        eng.append(si_pc_ratio)
        
        # Feature 5: Short Interest to Days to Cover Ratio
        # Combines two important metrics - consistently important in previous iterations (Feature_8)
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 6: RSI (Relative Strength Index) - Improved calculation
        # Momentum oscillator that measures speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / max(13, 1e-8)
            avg_loss = loss / max(13, 1e-8)
            
            if avg_loss < 1e-8:
                rsi = 100
            else:
                rs = avg_gain / max(avg_loss, 1e-8)
                rsi = 100 - (100 / (1 + rs))
            eng.append(rsi / 100)  # Normalize to [0,1]
        else:
            eng.append(0.5)  # Neutral RSI
        
        # Feature 7: Short Interest Acceleration - Improved calculation
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            
            # Use percentage changes for more stable calculation
            prev_change_pct = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change_pct = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Acceleration is the change in the rate of change
            short_interest_accel = current_change_pct - prev_change_pct
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 8: Price Momentum (5-day) - Consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 9: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 10: ATR-based Volatility - Improved calculation
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 12: Normalized Short Interest - Improved calculation
        # Short interest normalized by its recent history
        if t >= 3:
            si_history = [data[max(0, t-i), 0] for i in range(4)]
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(si_max - si_min, 1e-8)
            norm_si = (short_interest - si_min) / si_range
            eng.append(norm_si)
        else:
            eng.append(0.5)
        
        # Feature 13: Price Volatility Ratio (new feature)
        # Compares recent volatility to longer-term volatility
        if len(close_prices) >= 10:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_ratio = recent_vol / max(longer_vol, 1e-8)
            eng.append(vol_ratio)
        else:
            eng.append(1.0)  # Equal volatility
        
        # Feature 14: Short Interest Trend Strength (new feature)
        # Measures the strength and consistency of short interest trend
        if t >= 4:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            # Calculate linear regression slope using numpy's polyfit
            x = np.arange(len(si_history))
            slope, _ = np.polyfit(x, si_history, 1)
            # Normalize slope by average short interest
            norm_slope = slope / max(np.mean(si_history), 1e-8)
            eng.append(norm_slope)
        else:
            eng.append(0.0)
        
        # Feature 15: Short Interest Concentration (new feature)
        # Measures how concentrated short interest is relative to market cap
        market_cap = shares_outstanding * close_prices[-1]
        si_concentration = short_interest / max(market_cap, 1e-8) * 1000  # Scale for numerical stability
        eng.append(si_concentration)
        
        # Feature 16: Price Gap Significance (improved calculation)
        # Measures the significance of overnight price gaps
        if len(open_prices) >= 2 and len(close_prices) >= 2:
            gap = (open_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            # Normalize by recent volatility
            recent_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8) if len(close_prices) >= 5 else 0.01
            gap_significance = gap / max(recent_volatility, 1e-8)
            eng.append(gap_significance)
        else:
            eng.append(0.0)
        
        # Feature 17: Options Sentiment Indicator (new feature)
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 18: Short Interest Efficiency (new feature)
        # Measures how efficiently shorts are positioned relative to price movement
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            prev_short_interest = data[t-1, 0]
            si_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            # Negative correlation between price and SI changes indicates efficient shorting
            si_efficiency = -1 * si_change * price_change
            eng.append(si_efficiency)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 8 (claude) (SVM)
==================================================
Training SVM model...

Iteration 8 (claude) Performance:
MAE: 35161.0923
RMSE: 48794.8650
MAPE: 13.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0012, rank=1
   2. Feature_24_t1: importance=0.0011, rank=2
   3. Feature_4_t0: importance=0.0009, rank=3
   4. Feature_15_t0: importance=0.0009, rank=4
   5. Feature_15_t3: importance=0.0008, rank=5
ðŸ’¾ Saved code for iteration 8 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.05%

ðŸ“ˆ Current best MAPE: 13.24%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 9
======================================================================
ðŸ¤– Calling Claude for iteration 9...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep essential raw features based on previous importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Consistently important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Volatility-Adjusted Short Interest
        # Consistently high importance in previous iterations (Feature_7)
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 2: Short Interest to Float Ratio
        # Key metric for short squeeze potential
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 3: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Interest to Put-Call Ratio
        # Consistently high importance in previous iterations (Feature_17)
        si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
        eng.append(si_pc_ratio)
        
        # Feature 5: Short Interest to Days to Cover Ratio
        # Combines two important metrics - consistently important in previous iterations (Feature_8)
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 6: Bollinger Band Position - New feature
        # Measures where the current price is relative to its volatility bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std_dev = np.std(close_prices[-10:])
            upper_band = sma + (2 * std_dev)
            lower_band = sma - (2 * std_dev)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
            eng.append(bb_position)
        else:
            eng.append(0.5)  # Middle of the band
        
        # Feature 7: Short Interest Acceleration - Improved calculation
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            
            # Use percentage changes for more stable calculation
            prev_change_pct = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change_pct = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Acceleration is the change in the rate of change
            short_interest_accel = current_change_pct - prev_change_pct
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 8: Price Momentum (5-day) - Consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 9: Short Cost to Volatility Ratio
        # Measures cost efficiency of shorting relative to volatility
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(short_cost_vol_ratio)
        
        # Feature 10: ATR-based Volatility - Improved calculation
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Interest to Volume Ratio
        # Measures how many days of volume the short interest represents
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Feature 12: MACD Signal - New feature
        # Moving Average Convergence Divergence - trend-following momentum indicator
        if len(close_prices) >= 12:
            ema12 = np.mean(close_prices[-12:])  # Simple approximation of EMA
            ema26 = np.mean(close_prices[-min(26, len(close_prices)):])
            macd = ema12 - ema26
            # Normalize by price
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # Feature 13: Price Volatility Ratio (improved)
        # Compares recent volatility to longer-term volatility
        if len(close_prices) >= 10:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_ratio = recent_vol / max(longer_vol, 1e-8)
            eng.append(vol_ratio)
        else:
            eng.append(1.0)  # Equal volatility
        
        # Feature 14: Short Interest Trend Strength (improved)
        # Measures the strength and consistency of short interest trend
        if t >= 4:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            # Calculate linear regression slope using numpy's polyfit
            x = np.arange(len(si_history))
            slope, _ = np.polyfit(x, si_history, 1)
            # Normalize slope by average short interest
            norm_slope = slope / max(np.mean(si_history), 1e-8)
            eng.append(norm_slope)
        else:
            eng.append(0.0)
        
        # Feature 15: Short Interest Concentration (improved)
        # Measures how concentrated short interest is relative to market cap
        market_cap = shares_outstanding * close_prices[-1]
        si_concentration = short_interest / max(market_cap, 1e-8) * 1000  # Scale for numerical stability
        eng.append(si_concentration)
        
        # Feature 16: Stochastic Oscillator - New feature
        # Momentum indicator comparing closing price to price range over time
        if len(close_prices) >= 5:
            lowest_low = np.min(low_prices[-5:])
            highest_high = np.max(high_prices[-5:])
            price_range = max(highest_high - lowest_low, 1e-8)
            stoch_k = (close_prices[-1] - lowest_low) / price_range
            eng.append(stoch_k)
        else:
            eng.append(0.5)  # Middle value
        
        # Feature 17: Options Sentiment Indicator (improved)
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 18: Short Interest Efficiency (improved)
        # Measures how efficiently shorts are positioned relative to price movement
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            prev_short_interest = data[t-1, 0]
            si_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            # Negative correlation between price and SI changes indicates efficient shorting
            si_efficiency = -1 * si_change * price_change
            eng.append(si_efficiency)
        else:
            eng.append(0.0)
            
        # Feature 19: Normalized Volume Trend - New feature
        # Measures recent volume trend relative to average
        if len(close_prices) >= 5:
            recent_volumes = [data[t, 68] if data.shape[1] > 68 else avg_daily_volume for _ in range(5)]
            volume_trend = recent_volumes[-1] / max(np.mean(recent_volumes), 1e-8) - 1.0
            eng.append(volume_trend)
        else:
            eng.append(0.0)
            
        # Feature 20: Short Interest to Implied Volatility Ratio - New feature
        # Measures short interest relative to market's expectation of volatility
        si_iv_ratio = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_iv_ratio)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 9 (claude) (SVM)
==================================================
Training SVM model...

Iteration 9 (claude) Performance:
MAE: 35377.7177
RMSE: 49607.6291
MAPE: 13.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t1: importance=0.0020, rank=1
   2. Feature_12_t0: importance=0.0011, rank=2
   3. Feature_22_t1: importance=0.0011, rank=3
   4. Feature_24_t3: importance=0.0009, rank=4
   5. Feature_5_t3: importance=0.0009, rank=5
ðŸ’¾ Saved code for iteration 9 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: +0.01%

ðŸ“ˆ Current best MAPE: 13.24%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 10
======================================================================
ðŸ¤– Calling Claude for iteration 10...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume
        
        # Keep essential raw features based on previous importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Consistently important in previous iterations
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options put/call ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Volatility-Adjusted Short Interest
        # Consistently high importance in previous iterations (Feature_7)
        vol_adj_si = short_interest / max(implied_volatility, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 2: Short Interest to Float Ratio
        # Key metric for short squeeze potential
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # Feature 3: Short Interest Momentum (15-day change)
        # Captures trend in short interest - highly significant in previous iterations
        if t > 0:
            prev_short_interest = data[t-1, 0]
            short_interest_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            eng.append(short_interest_change)
        else:
            eng.append(0.0)
        
        # Feature 4: Short Interest to Put-Call Ratio
        # Consistently high importance in previous iterations (Feature_17)
        si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
        eng.append(si_pc_ratio)
        
        # Feature 5: Short Interest to Days to Cover Ratio
        # Combines two important metrics - consistently important in previous iterations (Feature_8)
        si_dtc_ratio = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_dtc_ratio)
        
        # Feature 6: Bollinger Band Position - Refined calculation
        # Measures where the current price is relative to its volatility bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std_dev = np.std(close_prices[-10:])
            upper_band = sma + (2 * std_dev)
            lower_band = sma - (2 * std_dev)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
            # Normalize to [-1, 1] range for better model interpretability
            bb_position = 2 * bb_position - 1
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # Feature 7: Short Interest Acceleration - Improved calculation
        # Second derivative of short interest - captures acceleration in short interest changes
        if t >= 2:
            prev_short_interest = data[t-1, 0]
            prev2_short_interest = data[t-2, 0]
            
            # Use percentage changes for more stable calculation
            prev_change_pct = (prev_short_interest / max(prev2_short_interest, 1e-8)) - 1.0
            current_change_pct = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Acceleration is the change in the rate of change
            short_interest_accel = current_change_pct - prev_change_pct
            # Apply sigmoid-like normalization to handle outliers
            short_interest_accel = np.tanh(short_interest_accel * 5)  # Scale factor 5 to maintain sensitivity
            eng.append(short_interest_accel)
        else:
            eng.append(0.0)
        
        # Feature 8: Price Momentum (5-day) - Consistently high importance
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            # Apply tanh to normalize extreme values
            momentum_5d = np.tanh(momentum_5d * 10)  # Scale factor 10 to maintain sensitivity
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 9: Short Cost to Volatility Ratio - Improved with normalization
        # Measures cost efficiency of shorting relative to volatility
        short_cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        # Normalize using log transformation to handle skewness
        short_cost_vol_ratio = np.log1p(short_cost_vol_ratio) if short_cost_vol_ratio > 0 else 0
        eng.append(short_cost_vol_ratio)
        
        # Feature 10: ATR-based Volatility - Improved calculation
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            # Normalize ATR by price for better comparability
            atr_price_ratio = atr / max(close_prices[-1], 1e-8)
            # Apply log transformation to handle skewness
            atr_price_ratio = np.log1p(atr_price_ratio * 100) / 5  # Scale for numerical stability
            eng.append(atr_price_ratio)
        else:
            eng.append(0.0)
        
        # Feature 11: Short Interest to Volume Ratio - Improved with normalization
        # Measures how many days of volume the short interest represents
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        # Apply log transformation to handle skewness
        si_volume_ratio = np.log1p(si_volume_ratio)
        eng.append(si_volume_ratio)
        
        # Feature 12: RSI (Relative Strength Index) - New feature
        # Momentum oscillator that measures the speed and change of price movements
        if len(close_prices) >= 14:
            # Calculate price changes
            delta = np.diff(close_prices[-14:])
            # Separate gains (up) from losses (down)
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            # Calculate RS and RSI
            rs = gain / max(loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            # Normalize to [0, 1]
            rsi_norm = rsi / 100
            eng.append(rsi_norm)
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # Feature 13: Price Volatility Ratio (improved)
        # Compares recent volatility to longer-term volatility
        if len(close_prices) >= 10:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_ratio = recent_vol / max(longer_vol, 1e-8)
            # Apply log transformation to handle skewness
            vol_ratio = np.log1p(vol_ratio)
            eng.append(vol_ratio)
        else:
            eng.append(0.0)
        
        # Feature 14: Short Interest Trend Strength (improved)
        # Measures the strength and consistency of short interest trend
        if t >= 4:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            # Calculate linear regression slope using numpy's polyfit
            x = np.arange(len(si_history))
            slope, _ = np.polyfit(x, si_history, 1)
            # Normalize slope by average short interest
            norm_slope = slope / max(np.mean(si_history), 1e-8)
            # Apply tanh to normalize extreme values
            norm_slope = np.tanh(norm_slope * 5)  # Scale factor 5 to maintain sensitivity
            eng.append(norm_slope)
        else:
            eng.append(0.0)
        
        # Feature 15: Short Interest Concentration (improved)
        # Measures how concentrated short interest is relative to market cap
        market_cap = shares_outstanding * close_prices[-1]
        si_concentration = short_interest / max(market_cap, 1e-8) * 1000  # Scale for numerical stability
        # Apply log transformation to handle skewness
        si_concentration = np.log1p(si_concentration)
        eng.append(si_concentration)
        
        # Feature 16: Stochastic Oscillator - Improved calculation
        # Momentum indicator comparing closing price to price range over time
        if len(close_prices) >= 5:
            lowest_low = np.min(low_prices[-5:])
            highest_high = np.max(high_prices[-5:])
            price_range = max(highest_high - lowest_low, 1e-8)
            stoch_k = (close_prices[-1] - lowest_low) / price_range
            # Calculate stoch_d (3-period moving average of stoch_k)
            if len(close_prices) >= 7:
                stoch_k_history = []
                for i in range(3):
                    if len(close_prices) > (5+i):
                        ll = np.min(low_prices[-(5+i):-i if i > 0 else None])
                        hh = np.max(high_prices[-(5+i):-i if i > 0 else None])
                        pr = max(hh - ll, 1e-8)
                        sk = (close_prices[-(i+1)] - ll) / pr
                        stoch_k_history.append(sk)
                    else:
                        stoch_k_history.append(stoch_k)
                stoch_d = np.mean(stoch_k_history)
                # Stochastic oscillator crossover signal
                stoch_signal = stoch_k - stoch_d
                eng.append(stoch_signal)
            else:
                eng.append(stoch_k - 0.5)  # Approximate signal
        else:
            eng.append(0.0)
        
        # Feature 17: Options Sentiment Indicator (improved)
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        # Apply log transformation to handle skewness
        options_sentiment = np.log1p(options_sentiment)
        eng.append(options_sentiment)
        
        # Feature 18: Short Interest Efficiency (improved)
        # Measures how efficiently shorts are positioned relative to price movement
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            prev_short_interest = data[t-1, 0]
            si_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            # Negative correlation between price and SI changes indicates efficient shorting
            si_efficiency = -1 * si_change * price_change
            # Apply tanh to normalize extreme values
            si_efficiency = np.tanh(si_efficiency * 10)  # Scale factor 10 to maintain sensitivity
            eng.append(si_efficiency)
        else:
            eng.append(0.0)
        
        # Feature 19: Normalized Volume Trend - Improved calculation
        # Measures recent volume trend relative to average
        if len(close_prices) >= 5:
            # Use volume data if available, otherwise use avg_daily_volume
            recent_volumes = []
            for i in range(min(5, len(close_prices))):
                if data.shape[1] > 68:
                    vol_idx = t - i if t - i >= 0 else t
                    recent_volumes.append(data[vol_idx, 68])
                else:
                    recent_volumes.append(avg_daily_volume)
            
            # Calculate volume trend using exponential weighting
            weights = np.exp(-np.arange(len(recent_volumes)) * 0.5)  # Exponential decay
            weighted_volumes = np.array(recent_volumes) * weights
            volume_trend = weighted_volumes[0] / max(np.mean(weighted_volumes), 1e-8) - 1.0
            # Apply tanh to normalize extreme values
            volume_trend = np.tanh(volume_trend * 5)  # Scale factor 5 to maintain sensitivity
            eng.append(volume_trend)
        else:
            eng.append(0.0)
        
        # Feature 20: Squeeze Indicator - New feature
        # Combines short interest, volume, and price volatility to detect potential short squeezes
        if len(close_prices) >= 5:
            price_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            squeeze_potential = short_to_float * days_to_cover * price_vol
            # Apply log transformation to handle skewness
            squeeze_potential = np.log1p(squeeze_potential * 100) / 5  # Scale for numerical stability
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 21: Short Interest to Synthetic Short Cost Ratio - New feature
        # Measures how short interest relates to the cost of creating synthetic shorts
        si_cost_ratio = short_interest / max(synthetic_short_cost, 1e-8)
        # Apply log transformation to handle skewness
        si_cost_ratio = np.log1p(si_cost_ratio)
        eng.append(si_cost_ratio)
        
        # Feature 22: Price to Moving Average Ratio - New feature
        # Measures current price relative to its moving average
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            price_ma_ratio = close_prices[-1] / max(ma10, 1e-8) - 1.0
            # Apply tanh to normalize extreme values
            price_ma_ratio = np.tanh(price_ma_ratio * 10)  # Scale factor 10 to maintain sensitivity
            eng.append(price_ma_ratio)
        else:
            eng.append(0.0)
        
        # Feature 23: Short Interest Volatility - New feature
        # Measures the volatility of short interest over time
        if t >= 4:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_mean = np.mean(si_history)
            si_std = np.std(si_history)
            si_volatility = si_std / max(si_mean, 1e-8)
            # Apply log transformation to handle skewness
            si_volatility = np.log1p(si_volatility * 10)  # Scale for numerical stability
            eng.append(si_volatility)
        else:
            eng.append(0.0)
        
        # Feature 24: Composite Momentum Indicator - New feature
        # Combines price momentum with volume and short interest trends
        if len(close_prices) >= 5 and t > 0:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            prev_short_interest = data[t-1, 0]
            si_momentum = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            
            # Combine signals: positive price momentum and decreasing short interest is bullish
            composite_momentum = price_momentum - si_momentum
            # Apply tanh to normalize extreme values
            composite_momentum = np.tanh(composite_momentum * 5)  # Scale factor 5 to maintain sensitivity
            eng.append(composite_momentum)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 10 (claude) (SVM)
==================================================
Training SVM model...

Iteration 10 (claude) Performance:
MAE: 35998.3233
RMSE: 49299.7327
MAPE: 13.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 85

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t1: importance=0.0011, rank=1
   2. Feature_24_t1: importance=0.0010, rank=2
   3. Feature_10_t1: importance=0.0009, rank=3
   4. Feature_16_t0: importance=0.0008, rank=4
   5. Feature_11_t0: importance=0.0008, rank=5
ðŸ’¾ Saved code for iteration 10 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.32%

ðŸ“ˆ Current best MAPE: 13.24%
ðŸ”„ Iterations without improvement: 3/5

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 147231.8427
RMSE: 227154.4157
MAPE: 10.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 120
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0019, rank=1
   2. Feature_65_t3: importance=0.0010, rank=2
   3. Feature_67_t1: importance=0.0010, rank=3
   4. Feature_64_t3: importance=0.0010, rank=4
   5. Feature_1_t0: importance=0.0010, rank=5
   Baseline MAPE: 10.16%
   Baseline MAE: 147231.8427
   Baseline RMSE: 227154.4157

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 171760.8424
RMSE: 250122.2745
MAPE: 11.89%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 79

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t1: importance=0.0013, rank=1
   2. Feature_22_t2: importance=0.0012, rank=2
   3. Feature_19_t0: importance=0.0011, rank=3
   4. Feature_10_t3: importance=0.0010, rank=4
   5. Feature_23_t3: importance=0.0009, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 11.89%
   MAE: 171760.8424
   RMSE: 250122.2745

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 10.16%
   Best Model MAPE: 11.89%
   Absolute Improvement: -1.72%
   Relative Improvement: -16.9%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  14.98           N/A                 
1          Iteration 1               15.02           -0.04%              
2          Iteration 2               14.43           +0.55%              
3          Iteration 3               14.40           +0.03%              
4          Iteration 4               13.53           +0.90%              
5          Iteration 5               14.55           -1.02%              
6          Iteration 6               13.87           -0.34%              
7          Iteration 7               13.24           +0.29%              
8          Iteration 8               13.29           -0.05%              
9          Iteration 9               13.23           +0.01%              
10         Iteration 10              13.56           -0.32%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 9 - MAPE: 13.23%
âœ… Saved SCSC results to cache/SCSC_iterative_results_enhanced.pkl
âœ… Summary report saved for SCSC

ðŸŽ‰ Process completed successfully for SCSC!

================================================================================
PROCESSING TICKER 12/15: SLG
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for SLG
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for SLG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SLG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 747701.8034
RMSE: 1126176.3567
MAPE: 5.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 147
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0002, rank=1
   2. Feature_2_t2: importance=0.0002, rank=2
   3. Feature_63_t0: importance=0.0001, rank=3
   4. Feature_67_t3: importance=0.0001, rank=4
   5. Feature_65_t1: importance=0.0001, rank=5

ðŸ“Š Baseline Performance: MAPE = 5.74%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (critical baseline features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep high importance features identified in previous iteration
        raw_keep.append(data[t, 67])  # shares_outstanding (Feature_67_t0)
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost (Feature_65_t1)
        raw_keep.append(data[t, 63])  # Last close price from OHLC
        raw_keep.append(data[t, 68])  # volume
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # 1. Short Interest Ratio (SI / Shares Outstanding)
        # Measures what percentage of total shares are sold short
        si_ratio = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        # Compares short interest to trading volume
        si_volume_ratio = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend
        price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1 if len(close_prices) >= 5 else 0
        eng.append(price_momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Captures medium-term price trend
        price_momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1 if len(close_prices) >= 10 else 0
        eng.append(price_momentum_10d)
        
        # 5. Volatility (standard deviation of returns over 10 days)
        # Measures price variability
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / close_prices[-11:-1]
            volatility = np.std(returns) if len(returns) > 0 else 0
        else:
            volatility = 0
        eng.append(volatility)
        
        # 6. Average True Range (ATR) - volatility indicator
        # Measures market volatility
        atr_sum = 0
        if len(high_prices) >= 2:
            for i in range(1, min(10, len(high_prices))):
                true_range = max(
                    high_prices[-i] - low_prices[-i],
                    abs(high_prices[-i] - close_prices[-i-1]),
                    abs(low_prices[-i] - close_prices[-i-1])
                )
                atr_sum += true_range
            atr = atr_sum / min(9, len(high_prices)-1)
        else:
            atr = 0
        eng.append(atr)
        
        # 7. RSI (Relative Strength Index) - 14 period
        # Momentum oscillator that measures speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            avg_gain = gain / 14
            avg_loss = loss / 14
            
            if avg_loss == 0:
                rsi = 100
            else:
                rs = avg_gain / max(avg_loss, 1e-8)
                rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Default value
        eng.append(rsi)
        
        # 8. Put-Call Ratio to Short Interest Ratio
        # Relates options market sentiment to short interest
        pc_si_ratio = data[t, 64] / max(si_ratio, 1e-8)
        eng.append(pc_si_ratio)
        
        # 9. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations to realized volatility
        iv_hv_ratio = data[t, 66] / max(volatility, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 10. Short Cost Efficiency
        # Measures cost efficiency of shorting
        short_cost_efficiency = data[t, 65] * data[t, 2]  # synthetic_short_cost * days_to_cover
        eng.append(short_cost_efficiency)
        
        # 11. VWAP (Volume Weighted Average Price) for last 5 days
        # Price that considers both price and volume
        if len(close_prices) >= 5 and len(data[t]) > 68:
            volumes = data[t, 68]  # Current volume
            vwap_sum = sum(close_prices[-5:] * volumes) if isinstance(volumes, np.ndarray) else sum(close_prices[-5:]) * volumes
            vwap = vwap_sum / max(sum(volumes) if isinstance(volumes, np.ndarray) else 5 * volumes, 1e-8)
        else:
            vwap = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else close_prices[-1] if len(close_prices) > 0 else 0
        eng.append(vwap)
        
        # 12. Price to VWAP Ratio
        # Indicates if current price is above/below average
        price_vwap_ratio = close_prices[-1] / max(vwap, 1e-8) if len(close_prices) > 0 else 1
        eng.append(price_vwap_ratio)
        
        # 13. Short Interest Change Rate
        # Rate of change in short interest
        if t > 0:
            si_change = (data[t, 0] / max(data[t-1, 0], 1e-8)) - 1
        else:
            si_change = 0
        eng.append(si_change)
        
        # 14. Bollinger Band Width
        # Measures volatility based on standard deviation
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
        else:
            bb_width = 0
        eng.append(bb_width)
        
        # 15. Short Interest to Implied Volatility Ratio
        # Relates short interest to market expectations
        si_iv_ratio = data[t, 0] / max(data[t, 66], 1e-8)
        eng.append(si_iv_ratio)
        
        # 16. Options Leverage Indicator
        # Combines options metrics to gauge potential leverage
        options_leverage = data[t, 64] * data[t, 66]  # put_call_ratio * implied_volatility
        eng.append(options_leverage)
        
        # 17. Short Squeeze Potential
        # Higher values indicate higher potential for a short squeeze
        short_squeeze_potential = si_ratio * data[t, 2] * volatility if volatility > 0 else si_ratio * data[t, 2]
        eng.append(short_squeeze_potential)
        
        # 18. Normalized Short Cost
        # Normalizes short cost by average daily volume
        norm_short_cost = data[t, 65] / max(data[t, 1], 1e-8)
        eng.append(norm_short_cost)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep important options and volume data based on feature importance analysis
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0  # Safely handle potential index error
        
        raw_keep.append(put_call_ratio)  # Options put/call volume ratio
        raw_keep.append(synthetic_short_cost)  # Synthetic short cost
        raw_keep.append(implied_volatility)  # Implied volatility
        raw_keep.append(shares_outstanding)  # Shares outstanding
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of shares outstanding
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Price momentum (5-day)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(momentum_5d)
        
        # Feature 4: Price momentum (10-day)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            eng.append(momentum_10d)
        
        # Feature 5: Average true range (ATR) - volatility measure
        if len(eng) < MAX_NEW and len(close_prices) >= 2:
            tr_values = []
            for i in range(1, len(close_prices)):
                high_low = high_prices[i] - low_prices[i]
                high_close_prev = abs(high_prices[i] - close_prices[i-1])
                low_close_prev = abs(low_prices[i] - close_prices[i-1])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            eng.append(atr)
        
        # Feature 6: Relative strength index (RSI)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain[-14:]) / 14
            avg_loss = np.sum(loss[-14:]) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # Feature 7: Bollinger Band Width
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        
        # Feature 8: Price to volume ratio
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            price_volume_ratio = close_prices[-1] / max(avg_daily_volume, 1e-8)
            eng.append(price_volume_ratio)
        
        # Feature 9: Volatility ratio (high-low range / close)
        if len(eng) < MAX_NEW and close_prices[-1] > 0:
            volatility_ratio = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(volatility_ratio)
        
        # Feature 10: Short interest growth rate (if we have previous data)
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 11: Implied volatility to historical volatility ratio
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            hist_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8) * np.sqrt(252)
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        
        # Feature 12: Short interest to synthetic short cost ratio
        if len(eng) < MAX_NEW and synthetic_short_cost > 0:
            si_cost_ratio = short_interest / max(synthetic_short_cost, 1e-8)
            eng.append(si_cost_ratio)
        
        # Feature 13: MACD line (12-26 day EMA difference)
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema_26 = np.mean(close_prices[-26:])
            macd = ema_12 - ema_26
            eng.append(macd)
        
        # Feature 14: Put-call ratio change
        if len(eng) < MAX_NEW and t > 0:
            prev_pc_ratio = data[t-1, 64]
            pc_ratio_change = (put_call_ratio / max(prev_pc_ratio, 1e-8)) - 1 if prev_pc_ratio > 0 else 0
            eng.append(pc_ratio_change)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 15: Short interest to put-call ratio
        if len(eng) < MAX_NEW and put_call_ratio > 0:
            si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
            eng.append(si_pc_ratio)
        
        # Feature 16: Normalized price level (current close / 15-day max)
        if len(eng) < MAX_NEW:
            price_max = np.max(close_prices)
            norm_price = close_prices[-1] / max(price_max, 1e-8)
            eng.append(norm_price)
        
        # Feature 17: Short squeeze potential (days to cover * volatility)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            vol_10d = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 787633.0851
RMSE: 1129163.3936
MAPE: 6.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t0: importance=0.0006, rank=1
   2. Feature_18_t2: importance=0.0003, rank=2
   3. Feature_15_t1: importance=0.0002, rank=3
   4. Feature_16_t1: importance=0.0002, rank=4
   5. Feature_19_t0: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.26%

ðŸ“ˆ Current best MAPE: 5.74%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback to avg_daily_volume if not available
        
        # Keep essential raw features based on importance analysis
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float (key metric for short squeeze potential)
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio (indicates how difficult covering shorts might be)
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Recent price trend (5-day) - short sellers often target downtrending stocks
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_trend_5d)
        
        # Feature 4: Volatility (10-day) - high volatility can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility_10d = np.std(returns) * np.sqrt(252)  # Annualized
            eng.append(volatility_10d)
        
        # Feature 5: Short interest growth rate (if we have previous data)
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 6: Short squeeze potential (days to cover * volatility)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        
        # Feature 7: Relative strength index (RSI) - oversold conditions can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Implied volatility to historical volatility ratio (options market sentiment)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        
        # Feature 9: Short interest to put-call ratio (relationship between options and short positions)
        if len(eng) < MAX_NEW and put_call_ratio > 0:
            si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
            eng.append(si_pc_ratio)
        
        # Feature 10: Price momentum with volume confirmation (stronger signal)
        if len(eng) < MAX_NEW and len(close_prices) >= 10 and avg_daily_volume > 0:
            price_change = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            volume_ratio = avg_daily_volume / max(np.mean(volume), 1e-8) if np.mean(volume) > 0 else 1.0
            momentum_vol_confirmed = price_change * volume_ratio
            eng.append(momentum_vol_confirmed)
        
        # Feature 11: Bollinger Band position (normalized price within volatility bands)
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - sma20) / max(std20 * 2, 1e-8)  # Normalized position within 2-sigma bands
            eng.append(bb_position)
        
        # Feature 12: Average True Range (ATR) - volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100  # ATR as percentage of price
            eng.append(atr_percent)
        
        # Feature 13: Short cost efficiency (short interest / synthetic short cost)
        if len(eng) < MAX_NEW and synthetic_short_cost > 0:
            short_cost_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
            eng.append(short_cost_efficiency)
        
        # Feature 14: Price gap analysis (overnight gaps can trigger short covering)
        if len(eng) < MAX_NEW and len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1
                overnight_gaps.append(gap)
            avg_gap = np.mean(overnight_gaps) if overnight_gaps else 0
            eng.append(avg_gap)
        
        # Feature 15: VWAP deviation (price relative to volume-weighted average price)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Simplified VWAP calculation using average volume
            vwap = np.sum(close_prices[-5:]) / 5  # Simple approximation
            vwap_deviation = (close_prices[-1] / max(vwap, 1e-8)) - 1
            eng.append(vwap_deviation)
        
        # Feature 16: Short interest acceleration (second derivative)
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 17: Composite momentum indicator (combines price, volume, and volatility)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            price_momentum = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            vol_ratio = implied_volatility / max(np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8), 1e-8)
            composite_momentum = price_momentum * (1 + vol_ratio * 0.1)  # Weighted combination
            eng.append(composite_momentum)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 778521.4840
RMSE: 1126683.1579
MAPE: 6.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0006, rank=1
   2. Feature_23_t1: importance=0.0004, rank=2
   3. Feature_15_t1: importance=0.0003, rank=3
   4. Feature_15_t0: importance=0.0003, rank=4
   5. Feature_19_t0: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.28%

ðŸ“ˆ Current best MAPE: 5.74%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume (required)
        raw_keep.append(short_interest)  # Short interest - primary target variable
        raw_keep.append(avg_daily_volume)  # Average daily volume - critical for liquidity assessment
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        
        # Add most recent OHLC values (last day)
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(high_prices[-1])  # Most recent high price
        raw_keep.append(low_prices[-1])  # Most recent low price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float (key metric for short squeeze potential)
        # This was one of the most important features in previous iterations
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio (indicates how difficult covering shorts might be)
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Recent price trend (5-day) - short sellers often target downtrending stocks
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_trend_5d)
        
        # Feature 4: Volatility (10-day) - high volatility can trigger short covering
        # This was identified as important in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility_10d = np.std(returns) * np.sqrt(252)  # Annualized
            eng.append(volatility_10d)
        
        # Feature 5: Short interest growth rate (if we have previous data)
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 6: Short squeeze potential (days to cover * volatility)
        # Combines two important metrics for predicting short interest changes
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        
        # Feature 7: Relative strength index (RSI) - oversold conditions can trigger short covering
        # Technical indicator that was significant in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Price momentum with volume confirmation (stronger signal)
        # Combines price movement with volume for a more robust signal
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            price_change = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            volume_ratio = avg_daily_volume / max(np.mean(avg_daily_volume), 1e-8) if np.mean(avg_daily_volume) > 0 else 1.0
            momentum_vol_confirmed = price_change * volume_ratio
            eng.append(momentum_vol_confirmed)
        
        # Feature 9: Bollinger Band position (normalized price within volatility bands)
        # Indicates potential mean reversion or trend continuation
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - sma20) / max(std20 * 2, 1e-8)  # Normalized position within 2-sigma bands
            eng.append(bb_position)
        
        # Feature 10: Average True Range (ATR) - volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100  # ATR as percentage of price
            eng.append(atr_percent)
        
        # Feature 11: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest growth
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 12: VWAP deviation (price relative to volume-weighted average price)
        # Important for institutional trading patterns
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Simplified VWAP calculation using average volume
            vwap = np.sum(close_prices[-5:]) / 5  # Simple approximation
            vwap_deviation = (close_prices[-1] / max(vwap, 1e-8)) - 1
            eng.append(vwap_deviation)
        
        # Feature 13: Price gap analysis (overnight gaps can trigger short covering)
        if len(eng) < MAX_NEW and len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1
                overnight_gaps.append(gap)
            avg_gap = np.mean(overnight_gaps) if overnight_gaps else 0
            eng.append(avg_gap)
        
        # Feature 14: Short interest to put-call ratio (relationship between options and short positions)
        # Combines options market sentiment with short interest
        if len(eng) < MAX_NEW and put_call_ratio > 0:
            si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
            eng.append(si_pc_ratio)
        
        # Feature 15: Price volatility ratio (recent vs longer-term)
        # Captures changing market conditions that might affect short interest
        if len(eng) < MAX_NEW and len(close_prices) >= 15:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices[-15:]) / max(np.mean(close_prices[-15:]), 1e-8)
            vol_ratio = recent_vol / max(longer_vol, 1e-8)
            eng.append(vol_ratio)
        
        # Feature 16: Moving Average Convergence Divergence (MACD) - trend and momentum indicator
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            eng.append(macd / max(close_prices[-1], 1e-8))  # Normalized MACD
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 789511.7997
RMSE: 1137672.4744
MAPE: 6.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t1: importance=0.0004, rank=1
   2. Feature_3_t2: importance=0.0003, rank=2
   3. Feature_15_t0: importance=0.0003, rank=3
   4. Feature_12_t1: importance=0.0002, rank=4
   5. Feature_22_t3: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.34%

ðŸ“ˆ Current best MAPE: 5.74%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume (required)
        raw_keep.append(short_interest)  # Short interest - primary target variable
        raw_keep.append(avg_daily_volume)  # Average daily volume - critical for liquidity assessment
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        
        # Add most recent OHLC values (last day)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float (key metric for short squeeze potential)
        # This was one of the most important features in previous iterations
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio (indicates how difficult covering shorts might be)
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Recent price trend (5-day) - short sellers often target downtrending stocks
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_trend_5d)
        
        # Feature 4: Volatility (10-day) - high volatility can trigger short covering
        # This was identified as important in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility_10d = np.std(returns) * np.sqrt(252)  # Annualized
            eng.append(volatility_10d)
        
        # Feature 5: Short interest growth rate (if we have previous data)
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 6: Price volatility ratio (recent vs longer-term)
        # Captures changing market conditions that might affect short interest
        # This was one of the top important features in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 15:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices[-15:]) / max(np.mean(close_prices[-15:]), 1e-8)
            vol_ratio = recent_vol / max(longer_vol, 1e-8)
            eng.append(vol_ratio)
        
        # Feature 7: Relative strength index (RSI) - oversold conditions can trigger short covering
        # Technical indicator that was significant in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Short squeeze potential (days to cover * volatility)
        # Combines two important metrics for predicting short interest changes
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        
        # Feature 9: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest growth
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 10: Bollinger Band position (normalized price within volatility bands)
        # Indicates potential mean reversion or trend continuation
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - sma20) / max(std20 * 2, 1e-8)  # Normalized position within 2-sigma bands
            eng.append(bb_position)
        
        # Feature 11: Implied volatility to historical volatility ratio
        # Compares market expectations with realized volatility
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        
        # Feature 12: Short interest to synthetic short cost ratio
        # Relationship between actual short positions and the cost of creating synthetic shorts
        if len(eng) < MAX_NEW and synthetic_short_cost > 0:
            si_synth_ratio = short_interest / max(synthetic_short_cost, 1e-8)
            eng.append(si_synth_ratio)
        
        # Feature 13: Average True Range (ATR) - volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100  # ATR as percentage of price
            eng.append(atr_percent)
        
        # Feature 14: Price gap analysis (overnight gaps can trigger short covering)
        if len(eng) < MAX_NEW and len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1
                overnight_gaps.append(gap)
            avg_gap = np.mean(overnight_gaps) if overnight_gaps else 0
            eng.append(avg_gap)
        
        # Feature 15: Normalized trading range (High-Low)/Close
        # Measure of intraday volatility
        if len(eng) < MAX_NEW:
            avg_range = np.mean((high_prices - low_prices) / np.maximum(close_prices, 1e-8))
            eng.append(avg_range)
        
        # Feature 16: Volume trend (5-day)
        # Increasing volume can indicate changing sentiment
        if len(eng) < MAX_NEW and t >= 5:
            vol_trend = []
            for i in range(5):
                if t-i >= 0:
                    vol_trend.append(data[t-i, 68])
            if vol_trend:
                vol_change = (vol_trend[0] / max(np.mean(vol_trend), 1e-8)) - 1
                eng.append(vol_change)
            else:
                eng.append(0.0)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 17: Put-call ratio trend
        # Change in options sentiment
        if len(eng) < MAX_NEW and t > 0:
            prev_pc_ratio = data[t-1, 64]
            pc_ratio_change = (put_call_ratio / max(prev_pc_ratio, 1e-8)) - 1 if prev_pc_ratio > 0 else 0
            eng.append(pc_ratio_change)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 18: Composite momentum indicator
        # Combines price and volume momentum
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            price_mom = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            if t >= 5:
                vol_data = []
                for i in range(5):
                    if t-i >= 0:
                        vol_data.append(data[t-i, 68])
                vol_mom = (vol_data[0] / max(np.mean(vol_data), 1e-8)) - 1 if vol_data else 0
            else:
                vol_mom = 0
            composite_mom = price_mom * (1 + vol_mom)
            eng.append(composite_mom)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 814073.1915
RMSE: 1150788.3882
MAPE: 6.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0003, rank=1
   2. Feature_20_t1: importance=0.0003, rank=2
   3. Feature_16_t0: importance=0.0002, rank=3
   4. Feature_17_t3: importance=0.0002, rank=4
   5. Feature_16_t3: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.48%

ðŸ“ˆ Current best MAPE: 5.74%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if t < data.shape[1] else 0
        
        # Keep essential raw features based on importance analysis from previous iterations
        raw_keep.append(short_interest)  # Always include short interest
        raw_keep.append(avg_daily_volume)  # Always include average volume
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float
        # Consistently high importance in previous iterations
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio
        # Key metric for assessing how difficult covering shorts might be
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Recent price momentum (5-day)
        # Short sellers often target stocks with negative momentum
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_momentum_5d)
        
        # Feature 4: Short interest growth rate (if we have previous data)
        # Rate of change in short interest is a key predictor
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 5: Normalized trading range (High-Low)/Close
        # Measure of intraday volatility - high importance in previous iterations
        if len(eng) < MAX_NEW:
            avg_range = np.mean((high_prices - low_prices) / np.maximum(close_prices, 1e-8))
            eng.append(avg_range)
        
        # Feature 6: Short squeeze potential (days to cover * volatility)
        # Combines two important metrics for predicting short interest changes
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 7: Relative strength index (RSI)
        # Technical indicator that was significant in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest growth
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 9: Implied volatility to historical volatility ratio
        # Compares market expectations with realized volatility
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        elif len(eng) < MAX_NEW:
            eng.append(1.0)  # Neutral ratio when insufficient data
        
        # Feature 10: Average True Range (ATR) as percentage of price
        # Volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100
            eng.append(atr_percent)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 11: Put-call ratio trend
        # Change in options sentiment - important for predicting short interest changes
        if len(eng) < MAX_NEW and t > 0:
            prev_pc_ratio = data[t-1, 64]
            pc_ratio_change = (put_call_ratio / max(prev_pc_ratio, 1e-8)) - 1 if prev_pc_ratio > 0 else 0
            eng.append(pc_ratio_change)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 12: MACD Signal Line Crossover
        # Technical indicator that can signal trend changes
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Signal line (9-day EMA of MACD)
            if t >= 9:
                macd_history = []
                for i in range(9):
                    if t-i >= 0 and len(data[t-i, 3:63].reshape(15, 4)[:, 3]) >= 26:
                        close_t = data[t-i, 3:63].reshape(15, 4)[:, 3]
                        ema12_t = np.mean(close_t[-12:])
                        ema26_t = np.mean(close_t[-26:])
                        macd_t = ema12_t - ema26_t
                        macd_history.append(macd_t)
                
                if len(macd_history) > 0:
                    signal = np.mean(macd_history)
                    macd_crossover = macd - signal
                    eng.append(macd_crossover)
                else:
                    eng.append(0.0)
            else:
                eng.append(0.0)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 13: Bollinger Band Width
        # Measure of volatility that can predict short covering events
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_width = (std20 * 2) / max(sma20, 1e-8)  # Normalized width
            eng.append(bb_width)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 14: Short interest to synthetic short cost efficiency
        # Relationship between actual short positions and synthetic short cost
        if len(eng) < MAX_NEW and synthetic_short_cost > 0:
            si_synth_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
            eng.append(si_synth_efficiency)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 15: Price gap significance
        # Large overnight gaps can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 2 and len(open_prices) >= 1:
            gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1
                gaps.append(abs(gap))  # Using absolute value to capture significance regardless of direction
            
            max_gap = max(gaps) if gaps else 0
            eng.append(max_gap)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 16: Volume-adjusted price momentum
        # Combines price movement with volume confirmation
        if len(eng) < MAX_NEW and len(close_prices) >= 5 and t >= 5:
            price_mom = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            
            vol_data = []
            for i in range(5):
                if t-i >= 0:
                    vol_data.append(data[t-i, 68] if t-i < data.shape[1] else 0)
            
            if vol_data and np.mean(vol_data) > 0:
                recent_vol = vol_data[0]
                avg_vol = np.mean(vol_data)
                vol_ratio = recent_vol / max(avg_vol, 1e-8)
                
                # Volume-weighted momentum
                vol_adj_mom = price_mom * vol_ratio
                eng.append(vol_adj_mom)
            else:
                eng.append(price_mom)  # Fall back to price momentum if volume data unavailable
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 17: Stochastic Oscillator
        # Technical indicator that can signal overbought/oversold conditions
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            
            if highest_high > lowest_low:
                k_percent = (close_prices[-1] - lowest_low) / max(highest_high - lowest_low, 1e-8) * 100
                eng.append(k_percent)
            else:
                eng.append(50.0)  # Neutral value
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Placeholder
        
        # Feature 18: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        if len(eng) < MAX_NEW and implied_volatility > 0:
            si_iv_ratio = short_interest / max(implied_volatility, 1e-8)
            eng.append(si_iv_ratio)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if t < data.shape[1] else 0.0
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and average volume (required)
        raw_keep.append(short_interest)  # Short interest - primary target variable
        raw_keep.append(avg_daily_volume)  # Average daily volume - critical for liquidity assessment
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        
        # Add most recent OHLC values (last day)
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float (key metric for short squeeze potential)
        # Consistently high importance in previous iterations
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        
        # Feature 2: Short interest to volume ratio (indicates how difficult covering shorts might be)
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # Feature 3: Recent price trend (5-day) - short sellers often target downtrending stocks
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_trend_5d)
        
        # Feature 4: Volatility (10-day) - high volatility can trigger short covering
        # Identified as important in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility_10d = np.std(returns) * np.sqrt(252)  # Annualized
            eng.append(volatility_10d)
        
        # Feature 5: Short interest growth rate (if we have previous data)
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder when previous data not available
        
        # Feature 6: Normalized trading range (High-Low)/Close
        # Measure of intraday volatility - consistently high importance in previous iterations
        if len(eng) < MAX_NEW:
            avg_range = np.mean((high_prices - low_prices) / np.maximum(close_prices, 1e-8))
            eng.append(avg_range)
        
        # Feature 7: Relative strength index (RSI) - oversold conditions can trigger short covering
        # Technical indicator that was significant in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Short squeeze potential (days to cover * volatility)
        # Combines two important metrics for predicting short interest changes
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        
        # Feature 9: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest growth
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 10: Bollinger Band position (normalized price within volatility bands)
        # Indicates potential mean reversion or trend continuation
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - sma20) / max(std20 * 2, 1e-8)  # Normalized position within 2-sigma bands
            eng.append(bb_position)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)  # Placeholder
        
        # Feature 11: Implied volatility to historical volatility ratio
        # Compares market expectations with realized volatility
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        
        # Feature 12: Average True Range (ATR) - volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100  # ATR as percentage of price
            eng.append(atr_percent)
        
        # Feature 13: MACD Signal - Momentum indicator that can predict trend changes
        # New feature that combines trend and momentum analysis
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Signal line (9-day EMA of MACD)
            if t >= 9:
                macd_history = []
                for i in range(9):
                    if t-i >= 0 and len(data[t-i, 3:63].reshape(15, 4)[:, 3]) >= 26:
                        prev_close = data[t-i, 3:63].reshape(15, 4)[:, 3]
                        prev_ema12 = np.mean(prev_close[-12:])
                        prev_ema26 = np.mean(prev_close[-26:])
                        macd_history.append(prev_ema12 - prev_ema26)
                
                if macd_history:
                    signal = np.mean(macd_history)
                    macd_histogram = macd - signal
                    eng.append(macd_histogram)
                else:
                    eng.append(0.0)
            else:
                eng.append(0.0)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 14: Price to moving average ratio (10-day)
        # Indicates overbought/oversold conditions
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            price_to_ma = close_prices[-1] / max(sma10, 1e-8)
            eng.append(price_to_ma)
        
        # Feature 15: Synthetic short cost to implied volatility ratio
        # Relationship between cost of shorting and expected volatility
        if len(eng) < MAX_NEW and implied_volatility > 0:
            cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(cost_vol_ratio)
        
        # Feature 16: Short interest to put-call ratio
        # Relationship between actual short positions and options market sentiment
        if len(eng) < MAX_NEW and put_call_ratio > 0:
            si_pc_ratio = short_interest / max(put_call_ratio, 1e-8)
            eng.append(si_pc_ratio)
        
        # Feature 17: Composite momentum indicator (price and volume)
        # Combines price and volume momentum - high importance in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            price_mom = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            vol_mom = 0
            if t >= 5:
                vol_data = []
                for i in range(5):
                    if t-i >= 0:
                        vol_data.append(data[t-i, 68] if t-i < data.shape[1] else 0)
                if vol_data and np.mean(vol_data) > 0:
                    vol_mom = (vol_data[0] / max(np.mean(vol_data), 1e-8)) - 1
            composite_mom = price_mom * (1 + vol_mom)
            eng.append(composite_mom)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 2/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep essential raw features based on importance analysis from previous iterations
        raw_keep.append(short_interest)  # Always include short interest (primary target)
        raw_keep.append(avg_daily_volume)  # Always include average volume (critical for liquidity)
        raw_keep.append(days_to_cover)  # Important for short squeeze potential
        raw_keep.append(shares_outstanding)  # Important for relative measures
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(synthetic_short_cost)  # Cost of shorting
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest as percentage of float
        # Consistently high importance in previous iterations
        if len(eng) < MAX_NEW and shares_outstanding > 0:
            si_percent = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percent)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 2: Short interest to volume ratio
        # Key metric for how difficult covering shorts might be
        if len(eng) < MAX_NEW and avg_daily_volume > 0:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 3: Recent price trend (5-day)
        # Short sellers often target downtrending stocks
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            price_trend_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            eng.append(price_trend_5d)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 4: Volatility (10-day)
        # High volatility can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility_10d = np.std(returns) * np.sqrt(252)  # Annualized
            eng.append(volatility_10d)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 5: Short interest growth rate
        # Rate of change in short interest is predictive of future changes
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_growth = (short_interest / max(prev_si, 1e-8)) - 1 if prev_si > 0 else 0
            eng.append(si_growth)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 6: Normalized trading range (High-Low)/Close
        # Measure of intraday volatility - consistently high importance
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            avg_range = np.mean((high_prices - low_prices) / np.maximum(close_prices, 1e-8))
            eng.append(avg_range)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 7: Relative strength index (RSI)
        # Technical indicator that was significant in previous iterations
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.sum(gain) / 14
            avg_loss = np.sum(loss) / 14
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral RSI when insufficient data
        
        # Feature 8: Short squeeze potential (days to cover * volatility)
        # Combines two important metrics for predicting short interest changes
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            vol_10d = np.std(returns) * np.sqrt(252)  # Annualized
            squeeze_potential = days_to_cover * vol_10d
            eng.append(squeeze_potential)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 9: Bollinger Band position
        # Indicates potential mean reversion or trend continuation
        if len(eng) < MAX_NEW and len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            bb_position = (close_prices[-1] - sma20) / max(std20 * 2, 1e-8)
            eng.append(bb_position)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 10: Implied volatility to historical volatility ratio
        # Compares market expectations with realized volatility
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        elif len(eng) < MAX_NEW:
            eng.append(1.0)  # Neutral value
        
        # Feature 11: Average True Range (ATR) as percentage of price
        # Volatility measure important for short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, min(14, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close_prev = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close_prev = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close_prev, low_close_prev)
                tr_values.append(tr)
            atr = sum(tr_values) / max(len(tr_values), 1)
            atr_percent = atr / max(close_prices[-1], 1e-8) * 100
            eng.append(atr_percent)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 12: Put-call ratio relative to its recent average
        # Normalized options sentiment indicator
        if len(eng) < MAX_NEW and t >= 5:
            pc_ratios = []
            for i in range(min(5, t+1)):
                pc_ratios.append(data[t-i, 64])
            avg_pc_ratio = np.mean(pc_ratios)
            pc_ratio_rel = put_call_ratio / max(avg_pc_ratio, 1e-8)
            eng.append(pc_ratio_rel)
        elif len(eng) < MAX_NEW:
            eng.append(1.0)  # Neutral value
        
        # Feature 13: Short interest to synthetic short cost efficiency
        # Relationship between actual short positions and synthetic short cost
        if len(eng) < MAX_NEW and synthetic_short_cost > 0:
            si_synth_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
            eng.append(si_synth_efficiency)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 14: Price momentum with volume confirmation
        # Combines price and volume trends for stronger signal
        if len(eng) < MAX_NEW and len(close_prices) >= 10 and t >= 5:
            price_mom = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            
            vol_data = []
            for i in range(min(5, t+1)):
                if data.shape[1] > 68:  # Check if volume data exists
                    vol_data.append(data[t-i, 68])
            
            if vol_data:
                vol_mom = (vol_data[0] / max(np.mean(vol_data), 1e-8)) - 1
                # Price momentum is more significant when confirmed by volume
                price_vol_confirm = price_mom * (1 + vol_mom)
                eng.append(price_vol_confirm)
            else:
                eng.append(price_mom)  # Fall back to price momentum only
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 15: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest growth
        if len(eng) < MAX_NEW and t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_previous = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_previous
            eng.append(si_acceleration)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 16: MACD-like indicator (difference between fast and slow EMAs)
        # Trend strength and potential reversal indicator
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = (ema12 / max(ema26, 1e-8)) - 1
            eng.append(macd)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 17: Gap analysis - significant overnight gaps can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 2 and len(open_prices) >= 1:
            overnight_gaps = []
            for i in range(1, min(5, len(open_prices))):
                gap = (open_prices[-i] / max(close_prices[-(i+1)], 1e-8)) - 1
                overnight_gaps.append(gap)
            avg_gap = np.mean(overnight_gaps) if overnight_gaps else 0
            eng.append(avg_gap)
        elif len(eng) < MAX_NEW:
            eng.append(0.0)
        
        # Feature 18: Stochastic oscillator - oversold conditions can trigger short covering
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            lowest_low = np.min(low_prices[-14:])
            highest_high = np.max(high_prices[-14:])
            k_percent = 100 * (close_prices[-1] - lowest_low) / max(highest_high - lowest_low, 1e-8)
            eng.append(k_percent)
        elif len(eng) < MAX_NEW:
            eng.append(50.0)  # Neutral value
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size by padding or truncating
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list)
    
    # Handle NaN and infinity values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 803670.3919
RMSE: 1145911.9773
MAPE: 6.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0004, rank=1
   2. Feature_16_t2: importance=0.0003, rank=2
   3. Feature_16_t0: importance=0.0003, rank=3
   4. Feature_16_t3: importance=0.0003, rank=4
   5. Feature_17_t1: importance=0.0002, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.41%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 472600.7003
RMSE: 583918.6526
MAPE: 5.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 101
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0003, rank=1
   2. Feature_65_t2: importance=0.0003, rank=2
   3. Feature_65_t1: importance=0.0003, rank=3
   4. Feature_67_t3: importance=0.0002, rank=4
   5. Feature_63_t0: importance=0.0002, rank=5
   Baseline MAPE: 5.69%
   Baseline MAE: 472600.7003
   Baseline RMSE: 583918.6526

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 500526.8156
RMSE: 626929.7164
MAPE: 5.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t0: importance=0.0006, rank=1
   2. Feature_18_t2: importance=0.0003, rank=2
   3. Feature_21_t2: importance=0.0002, rank=3
   4. Feature_22_t1: importance=0.0002, rank=4
   5. Feature_12_t0: importance=0.0002, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 5.82%
   MAE: 500526.8156
   RMSE: 626929.7164

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 5.69%
   Best Model MAPE: 5.82%
   Absolute Improvement: -0.13%
   Relative Improvement: -2.3%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  5.74            N/A                 
1          Iteration 1               6.00            -0.26%              
2          Iteration 2               6.02            -0.28%              
3          Iteration 3               6.08            -0.34%              
4          Iteration 4               6.22            -0.48%              
5          Iteration 5               6.15            -0.41%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 1 - MAPE: 6.00%
âœ… Saved SLG results to cache/SLG_iterative_results_enhanced.pkl
âœ… Summary report saved for SLG

ðŸŽ‰ Process completed successfully for SLG!

================================================================================
PROCESSING TICKER 13/15: HL
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for HL
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for HL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 1882310.5358
RMSE: 2367350.6551
MAPE: 10.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0007, rank=1
   2. Feature_1_t1: importance=0.0006, rank=2
   3. Feature_67_t3: importance=0.0005, rank=3
   4. Feature_63_t2: importance=0.0005, rank=4
   5. Feature_67_t0: importance=0.0004, rank=5

ðŸ“Š Baseline Performance: MAPE = 10.45%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume as they're directly relevant
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep high importance features from previous iteration
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost (Feature_65)
        raw_keep.append(data[t, 67])  # shares_outstanding (Feature_67)
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Add options data which showed importance
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        raw_keep.append(data[t, 68])  # volume
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Price momentum and volatility features
        if len(close_prices) > 1:
            # Price change rate over different periods
            price_change_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) if len(close_prices) >= 5 else 0
            price_change_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1) if len(close_prices) >= 10 else 0
            eng.append(price_change_5d)
            eng.append(price_change_10d)
            
            # Volatility: normalized price range
            recent_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8) if len(close_prices) >= 5 else 0
            eng.append(recent_volatility)
            
            # High-Low range relative to close price (recent price dispersion)
            avg_hl_range = np.mean(high_prices[-5:] - low_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8) if len(close_prices) >= 5 else 0
            eng.append(avg_hl_range)
        else:
            eng.extend([0, 0, 0, 0])  # Placeholders if not enough data
        
        # 2. Volume-based features
        volume = data[t, 68]
        avg_volume = data[t, 1]
        
        # Volume ratio: current volume to average
        volume_ratio = volume / max(avg_volume, 1e-8)
        eng.append(volume_ratio)
        
        # 3. Short interest relative metrics
        short_interest = data[t, 0]
        shares_outstanding = data[t, 67]
        
        # Short interest as percentage of shares outstanding
        short_pct_of_shares = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_pct_of_shares)
        
        # 4. Options-related features
        put_call_ratio = data[t, 64]
        implied_vol = data[t, 66]
        synthetic_short_cost = data[t, 65]
        
        # Synthetic short cost relative to implied volatility
        cost_vol_ratio = synthetic_short_cost / max(implied_vol, 1e-8)
        eng.append(cost_vol_ratio)
        
        # 5. Technical indicators
        if len(close_prices) >= 14:
            # RSI calculation
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            avg_gain = np.mean(gain[-14:])
            avg_loss = np.mean(loss[-14:])
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
            
            # MACD components
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-15:])  # Using all available data for longer EMA
            macd = ema12 - ema26
            eng.append(macd)
        else:
            eng.extend([0, 0])  # Placeholders if not enough data
        
        # 6. Combined metrics
        # Short interest to volume ratio (indicates days to cover trend)
        si_vol_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_vol_ratio)
        
        # Options sentiment vs price momentum
        options_price_sentiment = put_call_ratio * price_change_5d if len(close_prices) >= 5 else 0
        eng.append(options_price_sentiment)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Always keep short interest and average volume
        raw_keep = [data[t, 0], data[t, 1]]
        
        # Extract days to cover
        days_to_cover = data[t, 2]
        raw_keep.append(days_to_cover)
        
        # Extract OHLC data for the last 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Extract options and volume data
        options_put_call_ratio = data[t, 64]
        options_synthetic_short_cost = data[t, 65]  # High importance from previous iteration
        options_implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]  # High importance from previous iteration
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep high importance features from previous iteration
        raw_keep.append(options_synthetic_short_cost)
        raw_keep.append(shares_outstanding)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Price momentum and volatility features
        if len(close_prices) >= 5:
            # 5-day price momentum
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
            
            # 15-day price volatility (standard deviation / mean)
            mean_price = np.mean(close_prices)
            mean_price = max(abs(mean_price), 1e-8)
            price_volatility = np.std(close_prices) / mean_price
            eng.append(price_volatility)
        else:
            eng.extend([0.0, 0.0])  # Placeholders if not enough data
        
        # 2. Volume-based features
        if volume > 0:
            # Short interest to volume ratio
            si_volume_ratio = data[t, 0] / max(volume, 1e-8)
            eng.append(si_volume_ratio)
        else:
            eng.append(0.0)
        
        # 3. Options-related features
        # Normalized put-call ratio
        if options_put_call_ratio > 0:
            eng.append(options_put_call_ratio)
            
            # Interaction between put-call ratio and implied volatility
            pc_iv_interaction = options_put_call_ratio * options_implied_volatility
            eng.append(pc_iv_interaction)
        else:
            eng.extend([0.0, 0.0])
        
        # 4. Short interest relative to shares outstanding
        si_to_shares = data[t, 0] / max(shares_outstanding, 1e-8)
        eng.append(si_to_shares)
        
        # 5. Price range features
        if len(high_prices) > 0 and len(low_prices) > 0:
            # Average true range (ATR) - simplified version
            ranges = high_prices - low_prices
            atr = np.mean(ranges)
            eng.append(atr)
            
            # Normalized price range
            avg_price = np.mean(close_prices)
            avg_price = max(abs(avg_price), 1e-8)
            norm_range = atr / avg_price
            eng.append(norm_range)
        else:
            eng.extend([0.0, 0.0])
        
        # 6. Technical indicators
        if len(close_prices) >= 14:
            # RSI (Relative Strength Index) - simplified
            diff = np.diff(close_prices)
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if gains > 0 else 50.0
                
            eng.append(rsi)
            
            # MACD line (simple version - difference between 12 and 26 day EMAs)
            if len(close_prices) >= 26:
                ema12 = np.mean(close_prices[-12:])
                ema26 = np.mean(close_prices[-26:])
                macd = ema12 - ema26
                eng.append(macd)
            else:
                eng.append(0.0)
        else:
            eng.extend([0.0, 0.0])
        
        # 7. Short interest change rate
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 8. Synthetic short cost to implied volatility ratio
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = options_synthetic_short_cost / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 9. Days to cover to volume ratio
        if volume > 1e-8:
            dtc_volume_ratio = days_to_cover / volume
            eng.append(dtc_volume_ratio)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 1688358.9322
RMSE: 2158120.8726
MAPE: 9.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t0: importance=0.0012, rank=1
   2. Feature_14_t3: importance=0.0010, rank=2
   3. Feature_16_t2: importance=0.0007, rank=3
   4. Feature_4_t1: importance=0.0007, rank=4
   5. Feature_13_t1: importance=0.0007, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 1.15%

ðŸ“ˆ Current best MAPE: 9.30%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features
        raw_keep = [
            data[t, 0],  # short interest
            data[t, 1],  # average daily volume
            data[t, 2],  # days to cover
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent close price
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Keep high importance features from previous iterations
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest relative metrics (high importance in previous iterations)
        # Short interest to shares outstanding ratio
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # Short interest to average volume ratio
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 2. Price action features
        # Normalized price range over the past 15 days
        avg_price = np.mean(close_prices)
        avg_price = max(abs(avg_price), 1e-8)
        price_range = (np.max(high_prices) - np.min(low_prices)) / avg_price
        eng.append(price_range)
        
        # 3. Volatility features
        # Price volatility (normalized standard deviation)
        price_volatility = np.std(close_prices) / max(avg_price, 1e-8)
        eng.append(price_volatility)
        
        # 4. Momentum indicators
        # 5-day momentum (showed high importance)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 10-day momentum
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
        
        # 5. Technical indicators
        # RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            gains = np.sum(np.maximum(diff, 0))
            losses = np.sum(np.abs(np.minimum(diff, 0)))
            
            if losses > 1e-8:
                rs = gains / losses
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if gains > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Options-related features
        # Synthetic short cost to implied volatility ratio (high importance)
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # Put-call ratio (if available)
        if data.shape[1] > 64:
            options_put_call_ratio = data[t, 64]
            eng.append(options_put_call_ratio)
        else:
            eng.append(0.0)
        
        # 7. Volume dynamics
        # Volume trend (ratio of recent volume to average)
        if data.shape[1] > 68:
            recent_volume = data[t, 68]
            vol_ratio = recent_volume / max(data[t, 1], 1e-8)
            eng.append(vol_ratio)
        else:
            eng.append(1.0)  # Neutral value
        
        # 8. Short interest change metrics
        # Short interest change rate (if previous data available)
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 9. Bollinger Band position
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 5:
            sma = np.mean(close_prices)
            std = np.std(close_prices)
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 10. Days to cover relative to historical average
        # If we have previous timesteps, compare current days to cover with recent history
        if t > 0:
            prev_days_to_cover = []
            for i in range(max(0, t-3), t):
                prev_days_to_cover.append(data[i, 2])
            
            if prev_days_to_cover:
                avg_prev_dtc = np.mean(prev_days_to_cover)
                if avg_prev_dtc > 1e-8:
                    dtc_ratio = data[t, 2] / avg_prev_dtc
                    eng.append(dtc_ratio)
                else:
                    eng.append(1.0)
            else:
                eng.append(1.0)
        else:
            eng.append(1.0)
        
        # 11. MACD-like indicator (simplified)
        if len(close_prices) >= 12:
            ema_short = np.mean(close_prices[-5:])
            ema_long = np.mean(close_prices[-12:])
            
            if abs(ema_long) > 1e-8:
                macd_ratio = ema_short / ema_long - 1.0
                eng.append(macd_ratio)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 12. Interaction terms between important features
        # Short interest Ã— implied volatility interaction
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # Days to cover Ã— synthetic short cost interaction
        dtc_cost_interaction = data[t, 2] * data[t, 65]
        eng.append(dtc_cost_interaction)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 1765361.6871
RMSE: 2189685.7459
MAPE: 9.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0010, rank=1
   2. Feature_9_t3: importance=0.0006, rank=2
   3. Feature_5_t3: importance=0.0006, rank=3
   4. Feature_5_t0: importance=0.0005, rank=4
   5. Feature_10_t2: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.50%

ðŸ“ˆ Current best MAPE: 9.30%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features that showed high importance in previous iterations
        raw_keep = [
            data[t, 0],  # short interest (always keep)
            data[t, 1],  # average daily volume (always keep)
            data[t, 2],  # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance in iteration 0)
            data[t, 67],  # shares_outstanding (high importance in iteration 0)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest relative metrics (high importance in previous iterations)
        # Short interest to shares outstanding ratio (short % of float)
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio (improved version)
        # This was a top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # Improved to handle multiple lookback periods
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price trend features (showed high importance in iteration 1)
        # Recent price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. Volatility-adjusted price change
        # This normalizes price changes by volatility for better comparison
        if len(close_prices) >= 5:
            recent_std = np.std(close_prices[-5:])
            recent_std = max(recent_std, 1e-8)
            price_change = close_prices[-1] - close_prices[-5]
            vol_adj_change = price_change / recent_std
            eng.append(vol_adj_change)
        else:
            eng.append(0.0)
        
        # 6. RSI (Relative Strength Index) - showed high importance
        # Improved implementation with proper gain/loss calculation
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            avg_gain = np.mean(up_days)
            avg_loss = np.mean(down_days)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 7. Bollinger Band position (improved)
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 8. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 9. Put-call ratio (if available)
        # Important for gauging market sentiment
        if data.shape[1] > 64:
            options_put_call_ratio = data[t, 64]
            eng.append(options_put_call_ratio)
        else:
            eng.append(0.0)
        
        # 10. Days to cover trend
        # Compare current days to cover with recent history
        if t > 0:
            prev_days_to_cover = []
            for i in range(max(0, t-3), t):
                prev_days_to_cover.append(data[i, 2])
            
            if prev_days_to_cover:
                avg_prev_dtc = np.mean(prev_days_to_cover)
                if avg_prev_dtc > 1e-8:
                    dtc_ratio = data[t, 2] / avg_prev_dtc
                    eng.append(dtc_ratio)
                else:
                    eng.append(1.0)
            else:
                eng.append(1.0)
        else:
            eng.append(1.0)
        
        # 11. Price volatility (normalized)
        # Improved to use recent price action only
        recent_close = close_prices[-5:] if len(close_prices) >= 5 else close_prices
        avg_price = np.mean(recent_close)
        avg_price = max(abs(avg_price), 1e-8)
        price_volatility = np.std(recent_close) / avg_price
        eng.append(price_volatility)
        
        # 12. High-Low Range relative to price
        # Measures recent trading range normalized by price
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            avg_price = np.mean(close_prices[-5:])
            avg_price = max(abs(avg_price), 1e-8)
            
            norm_range = (recent_high - recent_low) / avg_price
            eng.append(norm_range)
        else:
            eng.append(0.0)
        
        # 13. Volume trend (ratio of recent volume to average)
        # Important for detecting unusual trading activity
        if data.shape[1] > 68:
            recent_volume = data[t, 68]
            vol_ratio = recent_volume / max(data[t, 1], 1e-8)
            eng.append(vol_ratio)
        else:
            # If volume not available, use a proxy from OHLC data
            price_range = np.mean(high_prices - low_prices)
            avg_range = np.mean(high_prices - low_prices)
            avg_range = max(avg_range, 1e-8)
            proxy_vol_ratio = price_range / avg_range
            eng.append(proxy_vol_ratio)
        
        # 14. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 15. MACD-like indicator (improved)
        if len(close_prices) >= 12:
            ema_short = np.mean(close_prices[-5:])
            ema_long = np.mean(close_prices[-12:])
            
            if abs(ema_long) > 1e-8:
                macd_ratio = (ema_short / ema_long) - 1.0
                eng.append(macd_ratio)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 16. Interaction terms between important features
        # Short interest Ã— implied volatility interaction
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 17. Days to cover Ã— synthetic short cost interaction
        # This was significant in previous iterations
        dtc_cost_interaction = data[t, 2] * data[t, 65]
        eng.append(dtc_cost_interaction)
        
        # 18. Short interest to put-call ratio
        # Measures alignment between options sentiment and short positions
        if data.shape[1] > 64:
            put_call = max(data[t, 64], 1e-8)
            si_to_pc = data[t, 0] / put_call
            eng.append(si_to_pc)
        else:
            eng.append(0.0)
        
        # 19. Stochastic oscillator (K%)
        # Measures current price relative to recent high-low range
        if len(high_prices) >= 14 and len(low_prices) >= 14:
            highest_high = np.max(high_prices[-14:])
            lowest_low = np.min(low_prices[-14:])
            range_hl = highest_high - lowest_low
            
            if range_hl > 1e-8:
                k_percent = (close_prices[-1] - lowest_low) / range_hl
                eng.append(k_percent)
            else:
                eng.append(0.5)
        else:
            eng.append(0.5)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 1796517.0018
RMSE: 2180548.9904
MAPE: 9.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_8_t2: importance=0.0007, rank=1
   2. Feature_4_t3: importance=0.0006, rank=2
   3. Feature_1_t1: importance=0.0005, rank=3
   4. Feature_24_t1: importance=0.0005, rank=4
   5. Feature_9_t1: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.55%

ðŸ“ˆ Current best MAPE: 9.30%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features that showed high importance in previous iterations
        raw_keep = [
            data[t, 0],  # short interest (always keep)
            data[t, 1],  # average daily volume (always keep)
            data[t, 2],  # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance in iteration 0)
            data[t, 67],  # shares_outstanding (high importance in iteration 0)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent OHLC values (compact representation)
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # This is a fundamental metric for short squeeze potential
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # This was a top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # Improved to handle multiple lookback periods
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. Volatility-adjusted price change
        # This normalizes price changes by volatility for better comparison
        if len(close_prices) >= 5:
            recent_std = np.std(close_prices[-5:])
            recent_std = max(recent_std, 1e-8)
            price_change = close_prices[-1] - close_prices[-5]
            vol_adj_change = price_change / recent_std
            eng.append(vol_adj_change)
        else:
            eng.append(0.0)
        
        # 6. RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            avg_gain = np.mean(up_days)
            avg_loss = np.mean(down_days)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 7. Bollinger Band position
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 8. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 9. Put-call ratio (if available)
        # Important for gauging market sentiment
        options_put_call_ratio = data[t, 64]
        eng.append(options_put_call_ratio)
        
        # 10. Days to cover trend
        # Compare current days to cover with recent history
        if t > 0:
            prev_days_to_cover = data[t-1, 2]
            if abs(prev_days_to_cover) > 1e-8:
                dtc_ratio = data[t, 2] / prev_days_to_cover
                eng.append(dtc_ratio)
            else:
                eng.append(1.0)
        else:
            eng.append(1.0)
        
        # 11. Price volatility (normalized)
        # Improved to use recent price action only
        recent_close = close_prices[-5:] if len(close_prices) >= 5 else close_prices
        avg_price = np.mean(recent_close)
        avg_price = max(abs(avg_price), 1e-8)
        price_volatility = np.std(recent_close) / avg_price
        eng.append(price_volatility)
        
        # 12. High-Low Range relative to price
        # Measures recent trading range normalized by price
        if len(high_prices) >= 5 and len(low_prices) >= 5:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            avg_price = np.mean(close_prices[-5:])
            avg_price = max(abs(avg_price), 1e-8)
            
            norm_range = (recent_high - recent_low) / avg_price
            eng.append(norm_range)
        else:
            eng.append(0.0)
        
        # 13. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 14. MACD-like indicator (improved)
        if len(close_prices) >= 12:
            ema_short = np.mean(close_prices[-5:])
            ema_long = np.mean(close_prices[-12:])
            
            if abs(ema_long) > 1e-8:
                macd_ratio = (ema_short / ema_long) - 1.0
                eng.append(macd_ratio)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 15. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 16. Interaction: Days to cover Ã— synthetic short cost
        # This was significant in previous iterations
        dtc_cost_interaction = data[t, 2] * data[t, 65]
        eng.append(dtc_cost_interaction)
        
        # 17. Short interest to put-call ratio
        # Measures alignment between options sentiment and short positions
        put_call = max(data[t, 64], 1e-8)
        si_to_pc = data[t, 0] / put_call
        eng.append(si_to_pc)
        
        # 18. Stochastic oscillator (K%)
        # Measures current price relative to recent high-low range
        if len(high_prices) >= 14 and len(low_prices) >= 14:
            highest_high = np.max(high_prices[-14:])
            lowest_low = np.min(low_prices[-14:])
            range_hl = highest_high - lowest_low
            
            if range_hl > 1e-8:
                k_percent = (close_prices[-1] - lowest_low) / range_hl
                eng.append(k_percent)
            else:
                eng.append(0.5)
        else:
            eng.append(0.5)
        
        # 19. NEW: Short interest acceleration (second derivative)
        # Captures the rate of change in short interest change
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            change_t = (si_t - si_t1) / max(abs(si_t1), 1e-8)
            change_t1 = (si_t1 - si_t2) / max(abs(si_t2), 1e-8)
            
            si_acceleration = change_t - change_t1
            eng.append(si_acceleration)
        else:
            eng.append(0.0)
            
        # 20. NEW: Normalized short interest relative to historical range
        # Provides context for current short interest level
        if t >= 5:
            historical_si = [data[i, 0] for i in range(max(0, t-5), t+1)]
            min_si = min(historical_si)
            max_si = max(historical_si)
            range_si = max_si - min_si
            
            if range_si > 1e-8:
                normalized_si = (data[t, 0] - min_si) / range_si
                eng.append(normalized_si)
            else:
                eng.append(0.5)
        else:
            eng.append(0.5)
            
        # 21. NEW: Composite momentum indicator
        # Combines price momentum with volume and volatility
        if len(close_prices) >= 5 and data[t, 1] > 1e-8:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_ratio = data[t, 68] / data[t, 1] if data.shape[1] > 68 else 1.0
            
            # Normalize to prevent extreme values
            vol_ratio = min(max(vol_ratio, 0.1), 10.0)
            
            # Composite indicator
            composite_momentum = price_momentum * np.sqrt(vol_ratio)
            eng.append(composite_momentum)
        else:
            eng.append(0.0)
            
        # 22. NEW: Short interest to price volatility ratio
        # Measures short interest relative to price volatility
        if price_volatility > 1e-8:
            si_to_volatility = data[t, 0] / price_volatility
            eng.append(si_to_volatility)
        else:
            eng.append(0.0)
            
        # 23. NEW: Options implied volatility skew
        # Approximation of volatility skew using available data
        iv = data[t, 66]
        pc_ratio = data[t, 64]
        iv_skew = iv * (pc_ratio - 1.0)
        eng.append(iv_skew)
        
        # 24. NEW: Normalized days to cover
        # Days to cover relative to its recent history
        if t >= 3:
            historical_dtc = [data[i, 2] for i in range(max(0, t-3), t+1)]
            avg_dtc = np.mean(historical_dtc)
            std_dtc = np.std(historical_dtc)
            
            if std_dtc > 1e-8:
                normalized_dtc = (data[t, 2] - avg_dtc) / std_dtc
                eng.append(normalized_dtc)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 1809169.3367
RMSE: 2183870.3041
MAPE: 9.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 94
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t0: importance=0.0006, rank=1
   2. Feature_19_t1: importance=0.0006, rank=2
   3. Feature_16_t1: importance=0.0005, rank=3
   4. Feature_24_t2: importance=0.0005, rank=4
   5. Feature_13_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.62%

ðŸ“ˆ Current best MAPE: 9.30%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features that showed high importance in previous iterations
        raw_keep = [
            data[t, 0],  # short interest (always keep)
            data[t, 1],  # average daily volume (always keep)
            data[t, 2],  # days to cover (high importance)
            data[t, 65], # options_synthetic_short_cost (high importance)
            data[t, 67], # shares_outstanding (high importance)
            data[t, 64], # options_put_call_volume_ratio (important for sentiment)
            data[t, 66]  # options_avg_implied_volatility (important for market expectations)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # This is a fundamental metric for short squeeze potential
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # This was a top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            avg_gain = np.mean(up_days)
            avg_loss = np.mean(down_days)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Bollinger Band position
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 7. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 8. Days to cover trend
        # Compare current days to cover with recent history
        if t > 0:
            prev_days_to_cover = data[t-1, 2]
            if abs(prev_days_to_cover) > 1e-8:
                dtc_ratio = data[t, 2] / prev_days_to_cover
                eng.append(dtc_ratio)
            else:
                eng.append(1.0)
        else:
            eng.append(1.0)
        
        # 9. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 10. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 11. Interaction: Days to cover Ã— synthetic short cost
        # This was significant in previous iterations
        dtc_cost_interaction = data[t, 2] * data[t, 65]
        eng.append(dtc_cost_interaction)
        
        # 12. Short interest to put-call ratio
        # Measures alignment between options sentiment and short positions
        put_call = max(data[t, 64], 1e-8)
        si_to_pc = data[t, 0] / put_call
        eng.append(si_to_pc)
        
        # 13. NEW: Normalized short interest relative to historical range
        # Provides context for current short interest level
        if t >= 5:
            historical_si = [data[i, 0] for i in range(max(0, t-5), t+1)]
            min_si = min(historical_si)
            max_si = max(historical_si)
            range_si = max_si - min_si
            
            if range_si > 1e-8:
                normalized_si = (data[t, 0] - min_si) / range_si
                eng.append(normalized_si)
            else:
                eng.append(0.5)
        else:
            eng.append(0.5)
        
        # 14. NEW: Volume Pressure Indicator
        # Combines volume with price direction to measure buying/selling pressure
        if len(close_prices) >= 3 and data.shape[1] > 68:
            price_change = close_prices[-1] - close_prices[-2]
            sign = 1 if price_change >= 0 else -1
            volume_ratio = data[t, 68] / max(data[t, 1], 1e-8)
            volume_pressure = sign * np.log1p(min(volume_ratio, 10))  # Log-scale and cap to prevent extremes
            eng.append(volume_pressure)
        else:
            eng.append(0.0)
        
        # 15. NEW: Short Interest Momentum Oscillator
        # Measures the momentum of short interest changes
        if t >= 3:
            si_values = np.array([data[max(0, t-i), 0] for i in range(3, -1, -1)])
            si_diff = np.diff(si_values)
            pos_changes = np.sum(np.where(si_diff > 0, si_diff, 0))
            neg_changes = np.sum(np.where(si_diff < 0, -si_diff, 0))
            
            if (pos_changes + neg_changes) > 1e-8:
                si_oscillator = (pos_changes - neg_changes) / (pos_changes + neg_changes)
                eng.append(si_oscillator)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 16. NEW: Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 17. NEW: Options Sentiment Divergence
        # Measures divergence between options sentiment and price action
        if len(close_prices) >= 5:
            price_trend = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            put_call_ratio = data[t, 64]
            # Positive when put-call ratio contradicts price trend
            sentiment_divergence = (put_call_ratio - 1.0) * (-1 * np.sign(price_trend))
            eng.append(sentiment_divergence)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 1634891.8005
RMSE: 2130621.0882
MAPE: 8.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 79

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0008, rank=1
   2. Feature_10_t2: importance=0.0006, rank=2
   3. Feature_12_t3: importance=0.0006, rank=3
   4. Feature_11_t2: importance=0.0006, rank=4
   5. Feature_14_t2: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.46%

ðŸ“ˆ Current best MAPE: 8.83%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features that showed high importance in previous iterations
        raw_keep = [
            data[t, 0],  # short interest (always keep)
            data[t, 1],  # average daily volume (always keep)
            data[t, 2],  # days to cover (high importance)
            data[t, 65], # options_synthetic_short_cost (high importance)
            data[t, 67], # shares_outstanding (high importance)
            data[t, 64], # options_put_call_volume_ratio (important for sentiment)
            data[t, 66]  # options_avg_implied_volatility (important for market expectations)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # This is a fundamental metric for short squeeze potential
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # This was a top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - showed high importance
        # Improved calculation with more accurate up/down day handling
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            # Use exponential moving average for smoother RSI
            alpha = 2/(14+1)
            if len(up_days) > 0 and len(down_days) > 0:
                avg_gain = np.mean(up_days[-min(14, len(up_days)):])
                avg_loss = np.mean(down_days[-min(14, len(down_days)):])
                
                if avg_loss > 1e-8:
                    rs = avg_gain / avg_loss
                    rsi = 100 - (100 / (1 + rs))
                else:
                    rsi = 100.0 if avg_gain > 0 else 50.0
                    
                eng.append(rsi / 100.0)  # Normalize to 0-1 range
            else:
                eng.append(0.5)  # Neutral RSI value
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Bollinger Band position - high importance in iteration 5
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 7. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 8. Days to cover trend - high importance in iteration 5
        # Compare current days to cover with recent history
        if t > 0:
            prev_days_to_cover = data[t-1, 2]
            if abs(prev_days_to_cover) > 1e-8:
                dtc_ratio = data[t, 2] / prev_days_to_cover
                eng.append(dtc_ratio)
            else:
                eng.append(1.0)
        else:
            eng.append(1.0)
        
        # 9. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 10. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 11. Volatility-Adjusted Short Interest - high importance in iteration 5
        # Normalizes short interest by price volatility
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 12. NEW: MACD Signal Line Crossover Indicator
        # Measures momentum shifts that might correlate with short covering
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            if t > 0 and len(close_prices) >= 27:
                prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3]
                prev_ema12 = np.mean(prev_close[-12:])
                prev_ema26 = np.mean(prev_close[-26:])
                prev_macd = prev_ema12 - prev_ema26
                
                # Signal line (9-day EMA of MACD)
                if t >= 9:
                    # Simplified signal calculation
                    signal = np.mean([data[max(0, t-i), 3:63].reshape(15, 4)[:, 3][-12:].mean() - 
                                     data[max(0, t-i), 3:63].reshape(15, 4)[:, 3][-26:].mean() 
                                     for i in range(9)])
                    
                    # MACD histogram (MACD - Signal)
                    macd_hist = macd - signal
                    
                    # Normalized MACD histogram
                    price_scale = max(close_prices[-1], 1e-8)
                    norm_macd_hist = macd_hist / price_scale
                    eng.append(norm_macd_hist)
                else:
                    eng.append(0.0)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 13. NEW: Short Interest Efficiency Ratio
        # Measures how effectively short positions are being utilized relative to price movement
        if t > 0 and len(close_prices) >= 2:
            price_change_pct = (close_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8 and abs(price_change_pct) > 1e-8:
                # Negative correlation expected between SI and price
                si_efficiency = (data[t, 0] - prev_si) / prev_si / abs(price_change_pct)
                # Cap extreme values
                si_efficiency = max(min(si_efficiency, 5.0), -5.0)
                eng.append(si_efficiency)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 14. NEW: Composite Short Squeeze Indicator
        # Combines multiple factors that contribute to short squeeze potential
        si_ratio = si_to_shares  # Short % of float
        vol_ratio = data[t, 1] / max(data[t, 68], 1e-8) if data.shape[1] > 68 else 1.0  # Vol ratio
        
        # Days to cover component
        dtc_component = np.tanh(data[t, 2] / 5.0)  # Normalize days to cover with tanh
        
        # Volatility component
        vol_component = data[t, 66] / 0.5  # Normalize IV (assuming 50% is typical)
        vol_component = min(vol_component, 3.0)  # Cap extreme values
        
        # Options sentiment
        options_sentiment = max(0, 2.0 - data[t, 64]) / 2.0  # Invert put/call ratio and normalize
        
        # Combine components with appropriate weights
        squeeze_indicator = (si_ratio * 0.4 + 
                            dtc_component * 0.3 + 
                            vol_component * 0.2 + 
                            options_sentiment * 0.1)
        
        # Normalize to 0-1 range
        squeeze_indicator = min(max(squeeze_indicator, 0.0), 1.0)
        eng.append(squeeze_indicator)
        
        # 15. NEW: Price Trend Strength
        # Measures the strength and consistency of recent price trends
        if len(close_prices) >= 10:
            # Calculate daily returns
            returns = np.diff(close_prices[-10:]) / close_prices[-11:-1]
            
            # Count positive and negative days
            pos_days = np.sum(returns > 0)
            neg_days = np.sum(returns < 0)
            
            # Calculate trend consistency
            consistency = abs(pos_days - neg_days) / 9.0  # Normalize to 0-1
            
            # Calculate trend direction
            direction = 1.0 if pos_days > neg_days else -1.0
            
            # Calculate average return magnitude
            avg_magnitude = np.mean(np.abs(returns))
            
            # Combine into trend strength indicator
            trend_strength = direction * consistency * min(avg_magnitude * 10, 3.0)  # Scale and cap
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # 16. NEW: Short Interest Acceleration
        # Measures the second derivative of short interest (acceleration)
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # First derivatives (velocities)
            v1 = (si_t - si_t1) / max(si_t1, 1e-8)
            v2 = (si_t1 - si_t2) / max(si_t2, 1e-8)
            
            # Second derivative (acceleration)
            acceleration = v1 - v2
            
            # Normalize and cap extreme values
            acceleration = max(min(acceleration, 2.0), -2.0)
            eng.append(acceleration)
        else:
            eng.append(0.0)
        
        # 17. NEW: Synthetic Short Cost Trend
        # Tracks changes in the cost of maintaining synthetic short positions
        if t > 0:
            prev_cost = data[t-1, 65]
            if abs(prev_cost) > 1e-8:
                cost_change = (data[t, 65] - prev_cost) / prev_cost
                # Cap extreme values
                cost_change = max(min(cost_change, 3.0), -3.0)
                eng.append(cost_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (9,) (10,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features based on importance from previous iterations
        raw_keep = [
            data[t, 0],   # short interest (always keep)
            data[t, 1],   # average daily volume (always keep)
            data[t, 2],   # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance)
            data[t, 67],  # shares_outstanding (high importance)
            data[t, 64],  # options_put_call_volume_ratio (important for sentiment)
            data[t, 66]   # options_avg_implied_volatility (important for market expectations)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # Consistently high importance in previous iterations
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # Top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            avg_gain = np.mean(up_days)
            avg_loss = np.mean(down_days)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Bollinger Band position
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 7. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 8. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 9. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 10. Short interest to put-call ratio
        # Measures alignment between options sentiment and short positions
        put_call = max(data[t, 64], 1e-8)
        si_to_pc = data[t, 0] / put_call
        eng.append(si_to_pc)
        
        # 11. Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility - high importance in iteration 5
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 12. NEW: Short Interest Acceleration
        # Measures the second derivative of short interest
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            first_diff_t = si_t - si_t1
            first_diff_t1 = si_t1 - si_t2
            
            acceleration = first_diff_t - first_diff_t1
            eng.append(acceleration)
        else:
            eng.append(0.0)
        
        # 13. NEW: Normalized Price Range
        # Measures recent price volatility as a ratio
        if len(close_prices) >= 5:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            recent_range = (recent_high - recent_low) / max(recent_low, 1e-8)
            eng.append(recent_range)
        else:
            eng.append(0.0)
        
        # 14. NEW: Short Interest Efficiency Ratio
        # Measures how efficiently short positions are being covered
        # Higher values indicate shorts are covering more efficiently
        if t > 0:
            si_change = data[t, 0] - data[t-1, 0]
            if si_change < 0:  # Short covering is happening
                avg_vol = data[t, 1]
                efficiency = abs(si_change) / max(avg_vol, 1e-8)
                eng.append(efficiency)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 15. NEW: Short Interest to Volatility Ratio
        # Measures short interest relative to market expectations of volatility
        si_to_iv = data[t, 0] / max(data[t, 66], 1e-8)
        eng.append(si_to_iv)
        
        # 16. NEW: Price Trend Strength
        # Measures the consistency of recent price direction
        if len(close_prices) >= 10:
            price_changes = np.diff(close_prices[-10:])
            pos_changes = np.sum(price_changes > 0)
            neg_changes = np.sum(price_changes < 0)
            trend_strength = abs(pos_changes - neg_changes) / 9.0  # Normalize to 0-1
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # 17. NEW: Composite Short Squeeze Indicator
        # Combines multiple factors that contribute to short squeeze potential
        if len(close_prices) >= 5:
            # Short interest to float ratio
            si_float = si_to_shares
            
            # Recent price momentum
            momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Days to cover
            dtc = data[t, 2]
            
            # Normalize each component to 0-1 range (simple approach)
            norm_si_float = min(si_float / 0.2, 1.0)  # Cap at 20% short interest
            norm_momentum = min(max(momentum / 0.1, 0.0), 1.0)  # +10% price move = 1.0
            norm_dtc = min(dtc / 10.0, 1.0)  # 10 days to cover = 1.0
            
            # Combine with weights
            squeeze_indicator = (0.4 * norm_si_float) + (0.4 * norm_momentum) + (0.2 * norm_dtc)
            eng.append(squeeze_indicator)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 1863167.0025
RMSE: 2247935.9750
MAPE: 10.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 94
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t3: importance=0.0006, rank=1
   2. Feature_4_t0: importance=0.0006, rank=2
   3. Feature_24_t2: importance=0.0005, rank=3
   4. Feature_24_t0: importance=0.0005, rank=4
   5. Feature_12_t2: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.28%

ðŸ“ˆ Current best MAPE: 8.83%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features based on importance from previous iterations
        # Focusing on the most consistently important features across iterations
        raw_keep = [
            data[t, 0],   # short interest (always keep)
            data[t, 1],   # average daily volume (always keep)
            data[t, 2],   # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance)
            data[t, 67],  # shares_outstanding (high importance)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # Consistently high importance across iterations
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # Top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1 and 5
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - consistently important
        # Improved implementation with proper smoothing
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            gains = np.where(diff > 0, diff, 0)
            losses = np.where(diff < 0, -diff, 0)
            
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Bollinger Band Width - measure of volatility
        # Replacing BB position with BB width which was more significant
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # 7. Options-related features - combining put-call ratio with implied volatility
        # Feature 14_t3 was top important in iteration 6
        put_call_ratio = data[t, 64]
        implied_vol = data[t, 66]
        options_sentiment = put_call_ratio * implied_vol
        eng.append(options_sentiment)
        
        # 8. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 9. Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility - high importance in iteration 5
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 10. Short Interest Acceleration - second derivative
        # Feature 12_t3 was important in iteration 5
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            first_diff_t = si_t - si_t1
            first_diff_t1 = si_t1 - si_t2
            
            acceleration = first_diff_t - first_diff_t1
            eng.append(acceleration)
        else:
            eng.append(0.0)
        
        # 11. MACD Signal - Momentum indicator
        # New feature based on successful technical indicator
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Normalize MACD by price level
            norm_macd = macd / max(close_prices[-1], 1e-8)
            eng.append(norm_macd)
        else:
            eng.append(0.0)
        
        # 12. Price Range Volatility - Feature 10_t2 was important in iteration 5
        if len(close_prices) >= 10:
            high_low_range = np.mean([high_prices[i] - low_prices[i] for i in range(-10, 0)])
            avg_price = np.mean(close_prices[-10:])
            range_volatility = high_low_range / max(avg_price, 1e-8)
            eng.append(range_volatility)
        else:
            eng.append(0.0)
        
        # 13. Short Interest Efficiency - Feature 4_t3 was top important in iteration 5
        # Improved implementation focusing on efficiency of short covering
        if t > 0:
            si_change = data[t, 0] - data[t-1, 0]
            volume_ratio = data[t, 68] / max(data[t, 1], 1e-8)  # Current volume to avg volume
            
            # Positive efficiency when shorts are covering (SI decreasing) with high volume
            if si_change < 0:
                efficiency = abs(si_change) * volume_ratio
            else:
                efficiency = -abs(si_change) * volume_ratio
                
            eng.append(efficiency)
        else:
            eng.append(0.0)
        
        # 14. Short Squeeze Potential - Feature 11_t2 was important in iteration 5
        # Combines multiple factors that contribute to short squeeze potential
        if len(close_prices) >= 5:
            # Short interest to float ratio
            si_float = si_to_shares
            
            # Recent price momentum
            momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Days to cover
            dtc = data[t, 2]
            
            # Implied volatility
            iv = data[t, 66]
            
            # Combine with weights - emphasizing momentum and days to cover
            squeeze_potential = (0.3 * si_float) + (0.3 * momentum) + (0.3 * dtc / 10.0) + (0.1 * iv)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # 15. Volume Trend - Feature 24_t2 was important in iteration 6
        # Measures recent volume trend relative to average
        if t >= 4:
            recent_volume = data[t, 68]
            avg_volume = data[t, 1]
            
            volume_trend = (recent_volume / max(avg_volume, 1e-8)) - 1.0
            eng.append(volume_trend)
        else:
            eng.append(0.0)
        
        # 16. Short Interest to Synthetic Short Cost Ratio
        # Combines two important metrics from raw features
        synthetic_cost = data[t, 65]
        si_to_cost = data[t, 0] / max(synthetic_cost, 1e-8)
        eng.append(si_to_cost)
        
        # 17. Price Trend Strength - Feature 14_t2 was important in iteration 5
        # Improved implementation focusing on consistency of price direction
        if len(close_prices) >= 10:
            price_changes = np.diff(close_prices[-10:])
            pos_changes = np.sum(price_changes > 0)
            neg_changes = np.sum(price_changes < 0)
            
            # Normalize to -1 to 1 range (negative = downtrend, positive = uptrend)
            trend_strength = (pos_changes - neg_changes) / 9.0
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # 18. Short Interest Concentration
        # New feature measuring how concentrated short interest is relative to market cap
        market_cap = data[t, 67] * close_prices[-1]
        si_concentration = (data[t, 0] * close_prices[-1]) / max(market_cap, 1e-8)
        eng.append(si_concentration)
        
        # 19. Composite Technical Indicator
        # Combines multiple technical signals into one feature
        if len(close_prices) >= 14:
            # Simple moving averages
            sma5 = np.mean(close_prices[-5:])
            sma10 = np.mean(close_prices[-10:])
            
            # SMA crossover signal (-1 to 1)
            sma_signal = (sma5 / max(sma10, 1e-8)) - 1.0
            
            # Use previously calculated RSI
            rsi_signal = (rsi / 100.0 - 0.5) * 2  # -1 to 1 range
            
            # Momentum
            mom_signal = momentum_5d
            
            # Combine signals
            composite = (0.4 * sma_signal) + (0.3 * rsi_signal) + (0.3 * mom_signal)
            eng.append(composite)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features based on importance from previous iterations
        # Focus on consistently high-importance features from iterations 1-6
        raw_keep = [
            data[t, 0],   # short interest (always keep)
            data[t, 1],   # average daily volume (always keep)
            data[t, 2],   # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance)
            data[t, 67],  # shares_outstanding (high importance)
            data[t, 64],  # options_put_call_volume_ratio (important for sentiment)
            data[t, 66]   # options_avg_implied_volatility (important for market expectations)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # Consistently high importance across iterations
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # Top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (data[t, 0] - prev_si) / max(abs(prev_si), 1e-8)
            eng.append(si_change)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1 and 5
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - showed high importance
        # Improved implementation with proper smoothing
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            gains = np.where(diff > 0, diff, 0)
            losses = np.where(diff < 0, -diff, 0)
            
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility - high importance in iteration 5
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 7. Short interest to days to cover ratio
        # Combines two important metrics
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 8. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 9. NEW: Exponential Short Interest Momentum
        # Gives more weight to recent changes in short interest
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # Weight recent changes more heavily
            exp_momentum = (0.7 * (si_t - si_t1) + 0.3 * (si_t1 - si_t2)) / max(si_t2, 1e-8)
            eng.append(exp_momentum)
        else:
            eng.append(0.0)
        
        # 10. NEW: Short Interest Concentration
        # Measures how concentrated short interest is relative to market cap
        market_cap_proxy = data[t, 67] * close_prices[-1]  # shares outstanding * price
        si_concentration = data[t, 0] * close_prices[-1] / max(market_cap_proxy, 1e-8)
        eng.append(si_concentration)
        
        # 11. NEW: Short Interest Velocity-Adjusted Volume Ratio
        # Combines short interest change rate with volume
        if t > 0:
            si_velocity = data[t, 0] - data[t-1, 0]
            volume_ratio = data[t, 68] / max(data[t, 1], 1e-8)  # current volume to avg volume
            velocity_vol = si_velocity * volume_ratio
            eng.append(velocity_vol)
        else:
            eng.append(0.0)
        
        # 12. NEW: Options Leverage Indicator
        # Measures potential leverage effect of options on short interest
        options_leverage = data[t, 64] * data[t, 66] / max(data[t, 65], 1e-8)
        eng.append(options_leverage)
        
        # 13. NEW: Price Trend Divergence
        # Measures divergence between price trend and short interest trend
        if t > 0 and len(close_prices) >= 5:
            price_trend = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            si_trend = (data[t, 0] / max(data[t-1, 0], 1e-8)) - 1.0
            trend_divergence = price_trend - si_trend
            eng.append(trend_divergence)
        else:
            eng.append(0.0)
        
        # 14. NEW: Normalized Short Interest Acceleration
        # Measures second derivative of short interest, normalized by level
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            first_diff_t = si_t - si_t1
            first_diff_t1 = si_t1 - si_t2
            
            acceleration = first_diff_t - first_diff_t1
            norm_acceleration = acceleration / max(si_t2, 1e-8)
            eng.append(norm_acceleration)
        else:
            eng.append(0.0)
        
        # 15. NEW: Short Squeeze Potential Index
        # Combines multiple factors that contribute to short squeeze potential
        # Refined from previous iterations with better normalization
        si_ratio = si_to_shares
        days_to_cover = data[t, 2]
        
        # Recent price momentum (if available)
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        else:
            price_momentum = 0.0
        
        # Implied volatility component
        iv_component = data[t, 66] / 0.5  # Normalize to typical IV range
        
        # Combine with appropriate weights
        squeeze_potential = (0.35 * min(si_ratio * 5, 1.0) + 
                            0.25 * min(days_to_cover / 8.0, 1.0) + 
                            0.25 * min(max(price_momentum * 10, 0.0), 1.0) + 
                            0.15 * min(iv_component, 1.0))
        eng.append(squeeze_potential)
        
        # 16. NEW: Relative Volume Pressure
        # Measures recent volume pressure relative to average volume
        if len(close_prices) >= 5:
            recent_volume = data[t, 68]  # Current volume
            avg_volume = data[t, 1]      # Average volume (past 15 days)
            rel_volume = recent_volume / max(avg_volume, 1e-8)
            eng.append(min(rel_volume, 5.0))  # Cap at 5x to avoid outliers
        else:
            eng.append(1.0)
        
        # 17. NEW: Short Interest Efficiency
        # Measures how efficiently short positions are being covered or increased
        if t > 0:
            si_change = data[t, 0] - data[t-1, 0]
            volume = data[t, 68]
            efficiency = abs(si_change) / max(volume, 1e-8)
            eng.append(min(efficiency * 1000, 1.0))  # Scale and cap
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 2/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features based on importance from previous iterations
        # Focusing on the most consistently important features across iterations
        raw_keep = [
            data[t, 0],   # short interest (always keep)
            data[t, 1],   # average daily volume (always keep)
            data[t, 2],   # days to cover (high importance)
            data[t, 65],  # options_synthetic_short_cost (high importance)
            data[t, 67],  # shares_outstanding (high importance)
            data[t, 64],  # options_put_call_volume_ratio (important for sentiment)
            data[t, 66]   # options_avg_implied_volatility (important for market expectations)
        ]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent close price (compact representation)
        raw_keep.append(close_prices[-1])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to shares outstanding ratio (short % of float)
        # Consistently high importance in previous iterations
        si_to_shares = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_shares)
        
        # 2. Short interest to average volume ratio
        # Top performer in iteration 1
        si_to_avg_vol = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_to_avg_vol)
        
        # 3. Short interest change rate (if previous data available)
        # High importance in previous iterations
        if t > 0:
            prev_si = data[t-1, 0]
            if abs(prev_si) > 1e-8:
                si_change = (data[t, 0] - prev_si) / prev_si
                eng.append(si_change)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 4. Price momentum (5-day) - showed high importance in iteration 1 and 5
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # 5. RSI (Relative Strength Index) - showed high importance
        if len(close_prices) >= 14:
            diff = np.diff(close_prices)
            up_days = np.where(diff > 0, diff, 0)
            down_days = np.where(diff < 0, abs(diff), 0)
            
            avg_gain = np.mean(up_days)
            avg_loss = np.mean(down_days)
            
            if avg_loss > 1e-8:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100.0 if avg_gain > 0 else 50.0
                
            eng.append(rsi / 100.0)  # Normalize to 0-1 range
        else:
            eng.append(0.5)  # Neutral RSI value
        
        # 6. Bollinger Band position - high importance in iteration 5
        # Where is the current price relative to Bollinger Bands?
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                bb_position = (close_prices[-1] - sma) / (2 * std)
                eng.append(bb_position)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 7. Options-related features (high importance in iteration 0)
        # Synthetic short cost to implied volatility ratio
        options_implied_volatility = data[t, 66]
        if options_implied_volatility > 1e-8:
            cost_to_iv_ratio = data[t, 65] / options_implied_volatility
            eng.append(cost_to_iv_ratio)
        else:
            eng.append(0.0)
        
        # 8. Short interest to days to cover ratio - important in iteration 5
        si_to_dtc = data[t, 0] / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # 9. Interaction: Short interest Ã— implied volatility
        # This was significant in previous iterations
        si_iv_interaction = data[t, 0] * data[t, 66]
        eng.append(si_iv_interaction)
        
        # 10. Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility - high importance in iteration 5
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = data[t, 0] * (1 + price_volatility)
            eng.append(vol_adj_si)
        else:
            eng.append(data[t, 0])
        
        # 11. NEW: MACD Signal Line Crossover Indicator
        # MACD is a trend-following momentum indicator
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Signal line (9-day EMA of MACD)
            if t >= 9:
                macd_history = []
                for i in range(9):
                    if t-i >= 0 and t-i < lookback_window:
                        prev_ohlc = data[t-i, 3:63].reshape(15, 4)
                        prev_close = prev_ohlc[:, 3]
                        if len(prev_close) >= 26:
                            prev_ema12 = np.mean(prev_close[-12:])
                            prev_ema26 = np.mean(prev_close[-26:])
                            macd_history.append(prev_ema12 - prev_ema26)
                
                if len(macd_history) > 0:
                    signal = np.mean(macd_history)
                    # MACD crossover signal (normalized)
                    crossover = (macd - signal) / max(abs(signal), 1e-8)
                    eng.append(crossover)
                else:
                    eng.append(0.0)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 12. NEW: Normalized Volume Trend
        # Measures recent volume trend relative to average
        volume = data[t, 68]
        avg_volume = data[t, 1]
        if avg_volume > 1e-8:
            vol_trend = volume / avg_volume - 1.0
            eng.append(vol_trend)
        else:
            eng.append(0.0)
        
        # 13. NEW: Short Interest Momentum Oscillator
        # Measures the rate of change in short interest momentum
        if t >= 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # First differences
            diff1 = si_t - si_t1
            diff2 = si_t1 - si_t2
            
            # Normalize the momentum oscillator
            if abs(si_t2) > 1e-8:
                si_osc = (diff1 - diff2) / si_t2
                eng.append(si_osc)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 14. NEW: Price-Volume Divergence
        # Measures divergence between price and volume movements
        if t > 0 and len(close_prices) >= 2:
            prev_close = data[t-1, 3:63].reshape(15, 4)[:, 3][-1]
            price_change = (close_prices[-1] / max(prev_close, 1e-8)) - 1.0
            
            prev_volume = data[t-1, 68]
            volume_change = (data[t, 68] / max(prev_volume, 1e-8)) - 1.0
            
            # Divergence: positive when price and volume move in opposite directions
            divergence = price_change * (-1 * volume_change)
            eng.append(divergence)
        else:
            eng.append(0.0)
        
        # 15. NEW: Short Interest Efficiency
        # Measures how efficiently short positions are being covered relative to volume
        if t > 0:
            si_change = data[t, 0] - data[t-1, 0]
            if si_change < 0:  # Short covering is happening
                efficiency = abs(si_change) / max(data[t, 68], 1e-8)  # Using actual volume instead of avg
                eng.append(efficiency)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 16. NEW: Composite Short Squeeze Risk Indicator
        # Combines multiple factors that contribute to short squeeze potential
        # High short interest, high days to cover, recent price momentum, and high implied volatility
        si_percentile = min(data[t, 0] / 1000000, 1.0)  # Normalize SI (assuming 1M+ is high)
        dtc_percentile = min(data[t, 2] / 10.0, 1.0)    # 10+ days to cover is high
        
        # Recent price momentum (if available)
        if len(close_prices) >= 5:
            momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            momentum_percentile = min(max(momentum / 0.1, 0.0), 1.0)  # +10% move = 1.0
        else:
            momentum_percentile = 0.0
        
        # Implied volatility component
        iv_percentile = min(data[t, 66] / 0.5, 1.0)  # 50% IV is considered high
        
        # Combine with weights
        squeeze_risk = (0.35 * si_percentile) + (0.25 * dtc_percentile) + 
                       (0.25 * momentum_percentile) + (0.15 * iv_percentile)
        eng.append(squeeze_risk)
        
        # 17. NEW: Options Sentiment Indicator
        # Combines put-call ratio with implied volatility to gauge market sentiment
        put_call_ratio = data[t, 64]
        iv = data[t, 66]
        
        # Normalize put-call ratio (typically 0.5-1.5 range)
        norm_pc = min(max((put_call_ratio - 0.7) / 0.8, 0.0), 1.0)
        
        # Normalize IV (typically 0.1-0.5 range)
        norm_iv = min(iv / 0.5, 1.0)
        
        # Bearish sentiment increases with both put-call ratio and IV
        options_sentiment = (0.6 * norm_pc) + (0.4 * norm_iv)
        eng.append(options_sentiment)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle NaN, inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: invalid syntax (<string>, line 228)
âš ï¸ Function execution failed (attempt 3/3)
âš ï¸ All function execution attempts failed, using fallback

ðŸ”§ Applying feature selection using fallback function...
Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 1: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 1 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 1}), (1, 'Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 1}), (2, 'Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 1})]
ðŸ”„ Retrying... (1/5)
Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 2: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 2 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 2}), (1, 'Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 2}), (2, 'Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 2})]
ðŸ”„ Retrying... (2/5)
Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 3: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 3 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 3}), (1, 'Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 3}), (2, 'Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 3})]
ðŸ”„ Retrying... (3/5)
Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 4: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 4 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 4}), (1, 'Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 4}), (2, 'Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 4})]
ðŸ”„ Retrying... (4/5)
Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 36 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 37 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 38 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 39 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 40 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 41 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 42 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 43 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 44 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 45 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 46 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 47 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 48 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 49 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 50 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 51 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 52 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 53 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 54 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 55 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 56 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 57 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 58 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 59 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 60 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 61 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 62 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 63 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 64 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 65 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 66 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 67 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 68 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 69 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 70 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 71 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 72 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 73 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 74 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 75 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 76 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 77 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 78 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 79 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 80 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 81 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 82 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 83 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 84 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 85 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 86 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 87 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 88 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 89 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 90 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 91 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 92 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 93 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 94 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 95 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 96 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 97 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 98 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 99 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 100 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 101 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 102 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 103 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 104 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 105 in attempt 5: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 5 failed with error: High error rate: 106/106 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 5}), (1, 'Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 5}), (2, 'Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 5})]
âš ï¸ All 5 attempts failed, using fallback function
ðŸ†˜ Using fallback feature selection...
Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 1: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 1: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 1 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 1}), (1, 'Error processing sample 1 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 1}), (2, 'Error processing sample 2 in attempt 1: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 1})]
ðŸ”„ Retrying... (1/5)
Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 2: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 2: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 2 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 2}), (1, 'Error processing sample 1 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 2}), (2, 'Error processing sample 2 in attempt 2: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 2})]
ðŸ”„ Retrying... (2/5)
Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 3: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 3: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 3 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 3}), (1, 'Error processing sample 1 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 3}), (2, 'Error processing sample 2 in attempt 3: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 3})]
ðŸ”„ Retrying... (3/5)
Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 4: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 4: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 4 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 4}), (1, 'Error processing sample 1 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 4}), (2, 'Error processing sample 2 in attempt 4: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 4})]
ðŸ”„ Retrying... (4/5)
Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 3 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 4 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 5 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 6 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 7 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 8 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 9 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 10 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 11 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 12 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 13 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 14 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 15 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 16 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 17 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 18 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 19 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 20 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 21 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 22 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 23 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 24 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 25 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 26 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 27 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 28 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 29 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 30 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 31 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 32 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 33 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 34 in attempt 5: cannot reshape array of size 66 into shape (15,4)
Error processing sample 35 in attempt 5: cannot reshape array of size 66 into shape (15,4)
âŒ Attempt 5 failed with error: High error rate: 36/36 samples failed. Sample errors: [(0, 'Error processing sample 0 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 0, shape: (4, 68)', 'attempt': 5}), (1, 'Error processing sample 1 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 1, shape: (4, 68)', 'attempt': 5}), (2, 'Error processing sample 2 in attempt 5: cannot reshape array of size 66 into shape (15,4)', {'error_type': 'ValueError', 'error_message': 'cannot reshape array of size 66 into shape (15,4)', 'sample_info': 'Sample 2, shape: (4, 68)', 'attempt': 5})]
âš ï¸ All 5 attempts failed, using fallback function
ðŸ†˜ Using fallback feature selection...
Training data shape: (106, 4, 68) -> (106, 4, 15)
Validation data shape: (36, 4, 68) -> (36, 4, 15)
âš ï¸ Total errors encountered: 720
  Error 1: ValueError - cannot reshape array of size 66 into shape (15,4)
  Error 2: ValueError - cannot reshape array of size 66 into shape (15,4)
  Error 3: ValueError - cannot reshape array of size 66 into shape (15,4)

==================================================
Training Iteration 7 (fallback) (SVM)
==================================================
Training SVM model...

Iteration 7 (fallback) Performance:
MAE: 2229539.5395
RMSE: 2746771.6419
MAPE: 11.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 60
   â€¢ Important features (top 10%): 20
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0042, rank=1
   2. Feature_1_t3: importance=0.0018, rank=2
   3. Feature_1_t0: importance=0.0013, rank=3
   4. Feature_0_t2: importance=0.0012, rank=4
   5. Feature_0_t3: importance=0.0012, rank=5
ðŸ“Š No significant improvement. Change: -2.91%

ðŸ“ˆ Current best MAPE: 8.83%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 8
======================================================================
ðŸ¤– Calling Claude for iteration 8...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Constructs features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Essential raw features to keep
        raw_keep = [
            data[t, 0],  # short interest
            data[t, 1],  # average daily volume
            data[t, 2],  # days to cover
            data[t, 64], # options_put_call_volume_ratio
            data[t, 65], # options_synthetic_short_cost - consistently high importance
            data[t, 67], # shares_outstanding - consistently high importance
            data[t, 68]  # volume
        ]
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Add most recent OHLC values (last day)
        raw_keep.append(close_prices[-1])  # Most recent close
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short Interest Ratio (SI/Shares Outstanding)
        # This was consistently important in previous iterations
        si_ratio = data[t, 0] / max(abs(data[t, 67]), 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Interest to Volume Ratio
        # Measures how much of the trading volume is related to short interest
        si_volume_ratio = data[t, 0] / max(abs(data[t, 1]), 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(abs(close_prices[-5]), 1e-8)) - 1
        else:
            momentum_5d = 0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Captures medium-term price trend
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(abs(close_prices[-10]), 1e-8)) - 1
        else:
            momentum_10d = 0
        eng.append(momentum_10d)
        
        # 5. Volatility (5-day)
        # Measures recent price volatility
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(abs(close_prices[-6:-1]), 1e-8)
            volatility_5d = np.std(returns) if len(returns) > 0 else 0
        else:
            volatility_5d = 0
        eng.append(volatility_5d)
        
        # 6. Average True Range (ATR) - 5-day
        # Measures volatility incorporating gaps
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(6, len(close_prices))):
                tr1 = high_prices[-i] - low_prices[-i]
                tr2 = abs(high_prices[-i] - close_prices[-(i+1)])
                tr3 = abs(low_prices[-i] - close_prices[-(i+1)])
                true_ranges.append(max(tr1, tr2, tr3))
            atr_5d = np.mean(true_ranges) if true_ranges else 0
        else:
            atr_5d = 0
        eng.append(atr_5d)
        
        # 7. RSI (Relative Strength Index) - 14-day
        # Momentum oscillator measuring speed and change of price movements
        if len(close_prices) >= 15:
            diff = np.diff(close_prices)
            gains = np.where(diff > 0, diff, 0)
            losses = np.where(diff < 0, -diff, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            if avg_loss == 0:
                rsi = 100
            else:
                rs = avg_gain / max(avg_loss, 1e-8)
                rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral value when not enough data
        eng.append(rsi)
        
        # 8. Options-based Synthetic Short Interest Indicator
        # Combines put/call ratio with synthetic short cost
        options_indicator = data[t, 64] * data[t, 65]
        eng.append(options_indicator)
        
        # 9. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations vs. realized volatility
        if volatility_5d > 0:
            iv_hv_ratio = data[t, 66] / max(volatility_5d, 1e-8)
        else:
            iv_hv_ratio = 1.0
        eng.append(iv_hv_ratio)
        
        # 10. Short Interest Change Rate
        # Measures the rate of change in short interest
        if t > 0:
            si_change = (data[t, 0] / max(abs(data[t-1, 0]), 1e-8)) - 1
        else:
            si_change = 0
        eng.append(si_change)
        
        # 11. Volume Trend (5-day)
        # Captures recent volume trend
        if len(close_prices) >= 5:
            recent_volumes = data[t-4:t+1, 68] if t >= 4 else data[:t+1, 68]
            if len(recent_volumes) >= 2:
                volume_trend = np.polyfit(np.arange(len(recent_volumes)), recent_volumes, 1)[0]
            else:
                volume_trend = 0
        else:
            volume_trend = 0
        eng.append(volume_trend)
        
        # 12. Price Range Ratio (High/Low)
        # Measures the trading range as a ratio
        price_range_ratio = np.mean(high_prices[-5:] / np.maximum(abs(low_prices[-5:]), 1e-8)) if len(high_prices) >= 5 else 1
        eng.append(price_range_ratio)
        
        # 13. Short Cost Efficiency
        # Measures how efficiently shorts are deployed relative to price
        short_cost_efficiency = data[t, 65] / max(abs(close_prices[-1]), 1e-8)
        eng.append(short_cost_efficiency)
        
        # 14. Normalized Short Interest
        # Short interest normalized by recent trading volume
        norm_si = data[t, 0] / max(abs(np.mean(data[t-5:t+1, 68]) if t >= 5 else np.mean(data[:t+1, 68])), 1e-8)
        eng.append(norm_si)
        
        # 15. Options Implied Direction
        # Combines put/call ratio with implied volatility to predict direction
        options_direction = data[t, 64] * data[t, 66]
        eng.append(options_direction)
        
        # 16. Short Interest to Float Ratio
        # Measures how much of the available float is being shorted
        si_float_ratio = data[t, 0] / max(abs(data[t, 67]), 1e-8)
        eng.append(si_float_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Clean up any NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Constructs features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    # Constants
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Keep essential raw features
        raw_keep = [
            data[t, 0],  # short interest
            data[t, 1],  # average daily volume
            data[t, 2],  # days to cover
            data[t, 64],  # options_put_call_volume_ratio
            data[t, 65],  # options_synthetic_short_cost
            data[t, 66],  # options_avg_implied_volatility
            data[t, 67],  # shares_outstanding
            data[t, 68] if data.shape[1] > 68 else 0.0,  # volume (if available)
        ]
        
        # Extract OHLC data for the last 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Calculate the maximum number of new features we can add
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        eng = []
        
        # 1. Recent price momentum (last 5 days vs previous 5 days)
        recent_momentum = (np.mean(close_prices[-5:]) / max(np.mean(close_prices[-10:-5]), 1e-8)) - 1.0
        eng.append(recent_momentum)
        
        # 2. Volatility ratio (recent vs longer-term)
        recent_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
        longer_volatility = np.std(close_prices) / max(np.mean(close_prices), 1e-8)
        vol_ratio = recent_volatility / max(longer_volatility, 1e-8)
        eng.append(vol_ratio)
        
        # 3. Short interest to float ratio
        short_interest = data[t, 0]
        shares_outstanding = data[t, 67]
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # 4. Average true range (ATR) - volatility indicator
        true_ranges = []
        for i in range(1, len(close_prices)):
            high_low = high_prices[i] - low_prices[i]
            high_close_prev = abs(high_prices[i] - close_prices[i-1])
            low_close_prev = abs(low_prices[i] - close_prices[i-1])
            true_range = max(high_low, high_close_prev, low_close_prev)
            true_ranges.append(true_range)
        atr = np.mean(true_ranges) if true_ranges else 0.0
        eng.append(atr)
        
        # 5. Normalized ATR (ATR / Close price)
        norm_atr = atr / max(close_prices[-1], 1e-8)
        eng.append(norm_atr)
        
        # 6. RSI (Relative Strength Index) - 14 period
        delta = np.diff(close_prices)
        gain = np.where(delta > 0, delta, 0)
        loss = np.where(delta < 0, -delta, 0)
        avg_gain = np.mean(gain) if len(gain) > 0 else 0
        avg_loss = np.mean(loss) if len(loss) > 0 else 0
        rs = avg_gain / max(avg_loss, 1e-8)
        rsi = 100 - (100 / (1 + rs))
        eng.append(rsi)
        
        # 7. Short interest change rate
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 8. Volume-weighted price ratio (VWAP / Close)
        volume = data[t, 68] if data.shape[1] > 68 else 1.0
        vwap = np.sum(close_prices * volume) / max(np.sum(volume), 1e-8)
        vwap_ratio = vwap / max(close_prices[-1], 1e-8)
        eng.append(vwap_ratio)
        
        # 9. Options implied volatility to historical volatility ratio
        implied_vol = data[t, 66]
        hist_vol = np.std(close_prices) / max(np.mean(close_prices), 1e-8)
        iv_hv_ratio = implied_vol / max(hist_vol, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 10. Price to moving average ratio (5-day)
        ma5 = np.mean(close_prices[-5:])
        price_to_ma5 = close_prices[-1] / max(ma5, 1e-8)
        eng.append(price_to_ma5)
        
        # 11. Price to moving average ratio (15-day)
        ma15 = np.mean(close_prices)
        price_to_ma15 = close_prices[-1] / max(ma15, 1e-8)
        eng.append(price_to_ma15)
        
        # 12. Bollinger Band Width
        middle_band = ma15
        std_dev = np.std(close_prices)
        bb_width = (2 * std_dev) / max(middle_band, 1e-8)
        eng.append(bb_width)
        
        # 13. Bollinger Band Position
        upper_band = middle_band + (2 * std_dev)
        lower_band = middle_band - (2 * std_dev)
        bb_range = max(upper_band - lower_band, 1e-8)
        bb_position = (close_prices[-1] - lower_band) / bb_range
        eng.append(bb_position)
        
        # 14. Short interest to volume ratio
        avg_volume = data[t, 1]
        si_to_volume = short_interest / max(avg_volume, 1e-8)
        eng.append(si_to_volume)
        
        # 15. Put-call ratio change
        put_call_ratio = data[t, 64]
        put_call_change = 0.0
        if t > 0:
            prev_put_call = data[t-1, 64]
            put_call_change = (put_call_ratio - prev_put_call) / max(prev_put_call, 1e-8)
        eng.append(put_call_change)
        
        # 16. Synthetic short cost to implied volatility ratio
        synthetic_short_cost = data[t, 65]
        cost_to_iv_ratio = synthetic_short_cost / max(implied_vol, 1e-8)
        eng.append(cost_to_iv_ratio)
        
        # 17. MACD Line (12-day EMA - 26-day EMA)
        # Using simple approximation since we only have 15 days
        ema12 = np.mean(close_prices[-min(12, len(close_prices)):])
        ema26 = np.mean(close_prices)
        macd_line = ema12 - ema26
        eng.append(macd_line)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN and infinity values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 8 (claude) (SVM)
==================================================
Training SVM model...

Iteration 8 (claude) Performance:
MAE: 1734674.4053
RMSE: 2194046.8707
MAPE: 9.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 94
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0007, rank=1
   2. Feature_9_t0: importance=0.0007, rank=2
   3. Feature_8_t1: importance=0.0005, rank=3
   4. Feature_14_t2: importance=0.0005, rank=4
   5. Feature_12_t1: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 8 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.74%

ðŸ“ˆ Current best MAPE: 8.83%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 9
======================================================================
ðŸ¤– Calling Claude for iteration 9...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Constructs features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    # Constants
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Extract OHLC data for the last 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep essential raw features based on importance analysis
        raw_keep = [
            data[t, 0],   # short interest - consistently important
            data[t, 1],   # average daily volume - consistently important
            data[t, 2],   # days to cover - key metric for short interest dynamics
            data[t, 65],  # options_synthetic_short_cost - high importance in iteration 0
            data[t, 66],  # options_avg_implied_volatility - important for market sentiment
            data[t, 67],  # shares_outstanding - important for normalization
            close_prices[-1]  # Most recent closing price
        ]
        
        # Calculate the maximum number of new features we can add
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        eng = []
        
        # 1. Short interest to float ratio - consistently important in previous iterations
        short_interest = data[t, 0]
        shares_outstanding = data[t, 67]
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # 2. Short interest to volume ratio - high importance in iteration 8
        avg_volume = data[t, 1]
        si_to_volume = short_interest / max(avg_volume, 1e-8)
        eng.append(si_to_volume)
        
        # 3. Short interest change rate - temporal dynamics
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 4. Recent price momentum (5-day) - important in iterations 1 and 5
        recent_momentum = 0.0
        if len(close_prices) >= 5:
            recent_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(recent_momentum)
        
        # 5. Volatility ratio (recent vs longer-term) - refined from previous iterations
        recent_volatility = np.std(close_prices[-5:]) if len(close_prices) >= 5 else 0.0
        longer_volatility = np.std(close_prices)
        vol_ratio = recent_volatility / max(longer_volatility, 1e-8)
        eng.append(vol_ratio)
        
        # 6. Average true range (ATR) - volatility indicator, important in iteration 5
        true_ranges = []
        for i in range(1, len(close_prices)):
            high_low = high_prices[i] - low_prices[i]
            high_close_prev = abs(high_prices[i] - close_prices[i-1])
            low_close_prev = abs(low_prices[i] - close_prices[i-1])
            true_range = max(high_low, high_close_prev, low_close_prev)
            true_ranges.append(true_range)
        atr = np.mean(true_ranges) if true_ranges else 0.0
        eng.append(atr)
        
        # 7. Normalized ATR (ATR / Close price) - volatility relative to price
        norm_atr = atr / max(close_prices[-1], 1e-8)
        eng.append(norm_atr)
        
        # 8. RSI (Relative Strength Index) - momentum indicator
        delta = np.diff(close_prices)
        gain = np.where(delta > 0, delta, 0)
        loss = np.where(delta < 0, -delta, 0)
        avg_gain = np.mean(gain) if len(gain) > 0 else 0
        avg_loss = np.mean(loss) if len(loss) > 0 else 0
        rs = avg_gain / max(avg_loss, 1e-8)
        rsi = 100 - (100 / (1 + rs))
        eng.append(rsi)
        
        # 9. Options implied volatility to historical volatility ratio - important in iteration 8
        implied_vol = data[t, 66]
        hist_vol = np.std(close_prices) / max(np.mean(close_prices), 1e-8)
        iv_hv_ratio = implied_vol / max(hist_vol, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 10. Bollinger Band Position - price relative to volatility bands
        ma15 = np.mean(close_prices)
        std_dev = np.std(close_prices)
        upper_band = ma15 + (2 * std_dev)
        lower_band = ma15 - (2 * std_dev)
        bb_range = max(upper_band - lower_band, 1e-8)
        bb_position = (close_prices[-1] - lower_band) / bb_range
        eng.append(bb_position)
        
        # 11. Put-call ratio - important for market sentiment
        put_call_ratio = data[t, 64]
        eng.append(put_call_ratio)
        
        # 12. Synthetic short cost to implied volatility ratio - important in iteration 8
        synthetic_short_cost = data[t, 65]
        cost_to_iv_ratio = synthetic_short_cost / max(implied_vol, 1e-8)
        eng.append(cost_to_iv_ratio)
        
        # 13. Price trend strength - directional movement
        price_changes = np.diff(close_prices)
        up_moves = np.sum(price_changes > 0)
        down_moves = np.sum(price_changes < 0)
        trend_strength = (up_moves - down_moves) / max(len(price_changes), 1)
        eng.append(trend_strength)
        
        # 14. Price acceleration - rate of change of momentum
        if len(close_prices) >= 3:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            acceleration = np.diff(returns)
            mean_accel = np.mean(acceleration) if len(acceleration) > 0 else 0.0
            eng.append(mean_accel)
        else:
            eng.append(0.0)
        
        # 15. High-Low range relative to price - volatility measure
        avg_hl_range = np.mean(high_prices - low_prices) if len(high_prices) > 0 else 0.0
        avg_price = np.mean(close_prices) if len(close_prices) > 0 else 1.0
        rel_hl_range = avg_hl_range / max(avg_price, 1e-8)
        eng.append(rel_hl_range)
        
        # 16. Days to cover change - temporal dynamics of short interest
        dtc_change = 0.0
        if t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (data[t, 2] - prev_dtc) / max(prev_dtc, 1e-8)
        eng.append(dtc_change)
        
        # 17. Volume trend - directional movement in volume
        if data.shape[1] > 68:  # If volume data is available
            volume_trend = 0.0
            if t > 0 and data[t-1, 68] > 0:
                volume_trend = (data[t, 68] - data[t-1, 68]) / max(data[t-1, 68], 1e-8)
            eng.append(volume_trend)
        else:
            eng.append(0.0)
        
        # 18. Short interest to days to cover ratio - relationship between key metrics
        si_to_dtc = short_interest / max(data[t, 2], 1e-8)
        eng.append(si_to_dtc)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN and infinity values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 9 (claude) (SVM)
==================================================
Training SVM model...

Iteration 9 (claude) Performance:
MAE: 1808508.0469
RMSE: 2294442.9413
MAPE: 9.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t1: importance=0.0005, rank=1
   2. Feature_16_t1: importance=0.0005, rank=2
   3. Feature_10_t1: importance=0.0005, rank=3
   4. Feature_9_t2: importance=0.0005, rank=4
   5. Feature_5_t3: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 9 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.06%

ðŸ“ˆ Current best MAPE: 8.83%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 10
======================================================================
ðŸ¤– Calling Claude for iteration 10...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    """
    Constructs features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    # Constants
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Extract OHLC data for the last 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep essential raw features based on importance analysis
        # Focus on the most consistently important features from previous iterations
        raw_keep = [
            data[t, 0],   # short interest - consistently important
            data[t, 1],   # average daily volume - consistently important
            data[t, 2],   # days to cover - key metric for short interest dynamics
            data[t, 65],  # options_synthetic_short_cost - high importance
            data[t, 66],  # options_avg_implied_volatility - important for market sentiment
            data[t, 67],  # shares_outstanding - important for normalization
            close_prices[-1]  # Most recent closing price
        ]
        
        # Calculate the maximum number of new features we can add
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        eng = []
        
        # 1. Short interest to float ratio - consistently important in previous iterations
        short_interest = data[t, 0]
        shares_outstanding = data[t, 67]
        short_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(short_to_float)
        
        # 2. Short interest to volume ratio - high importance in iteration 8
        avg_volume = data[t, 1]
        si_to_volume = short_interest / max(avg_volume, 1e-8)
        eng.append(si_to_volume)
        
        # 3. Short interest change rate - temporal dynamics
        # Improved to use a more robust calculation
        si_change = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 4. Recent price momentum (5-day) - important in iterations 1 and 5
        # Using log returns for better statistical properties
        recent_momentum = 0.0
        if len(close_prices) >= 5:
            recent_momentum = np.log(max(close_prices[-1], 1e-8) / max(close_prices[-5], 1e-8))
        eng.append(recent_momentum)
        
        # 5. Volatility ratio (recent vs longer-term) - refined from previous iterations
        # Using log returns for volatility calculation
        log_returns = np.diff(np.log(np.maximum(close_prices, 1e-8)))
        recent_vol = np.std(log_returns[-4:]) if len(log_returns) >= 4 else 0.0
        overall_vol = np.std(log_returns) if len(log_returns) > 0 else 0.0
        vol_ratio = recent_vol / max(overall_vol, 1e-8)
        eng.append(vol_ratio)
        
        # 6. Average true range (ATR) - volatility indicator, important in iteration 5
        # Normalized by price level for better comparability
        true_ranges = []
        for i in range(1, len(close_prices)):
            high_low = high_prices[i] - low_prices[i]
            high_close_prev = abs(high_prices[i] - close_prices[i-1])
            low_close_prev = abs(low_prices[i] - close_prices[i-1])
            true_range = max(high_low, high_close_prev, low_close_prev)
            true_ranges.append(true_range)
        atr = np.mean(true_ranges) if true_ranges else 0.0
        norm_atr = atr / max(close_prices[-1], 1e-8)
        eng.append(norm_atr)
        
        # 7. RSI (Relative Strength Index) - momentum indicator
        # Using exponential weighting to emphasize recent price movements
        delta = np.diff(close_prices)
        gain = np.where(delta > 0, delta, 0)
        loss = np.where(delta < 0, -delta, 0)
        
        # Apply exponential weights if we have enough data points
        if len(gain) >= 5:
            weights = np.exp(np.linspace(-1, 0, len(gain)))
            weights = weights / np.sum(weights)
            avg_gain = np.sum(gain * weights)
            avg_loss = np.sum(loss * weights)
        else:
            avg_gain = np.mean(gain) if len(gain) > 0 else 0
            avg_loss = np.mean(loss) if len(loss) > 0 else 0
            
        rs = avg_gain / max(avg_loss, 1e-8)
        rsi = 100 - (100 / (1 + rs))
        eng.append(rsi)
        
        # 8. Options implied volatility to historical volatility ratio
        # Important in iteration 8, refined calculation
        implied_vol = data[t, 66]
        hist_vol = overall_vol * np.sqrt(252)  # Annualized historical volatility
        iv_hv_ratio = implied_vol / max(hist_vol, 1e-8)
        eng.append(iv_hv_ratio)
        
        # 9. Bollinger Band Width - measure of volatility
        # More stable than BB position and captures volatility regime
        ma15 = np.mean(close_prices)
        std_dev = np.std(close_prices)
        bb_width = (2 * std_dev) / max(ma15, 1e-8)
        eng.append(bb_width)
        
        # 10. Put-call ratio exponentially weighted
        # Emphasizing recent options sentiment
        put_call_ratio = data[t, 64]
        weighted_pcr = put_call_ratio
        if t > 0:
            prev_pcr = data[t-1, 64]
            weighted_pcr = 0.7 * put_call_ratio + 0.3 * prev_pcr
        eng.append(weighted_pcr)
        
        # 11. Synthetic short cost to implied volatility ratio
        # Important in iteration 8, refined calculation
        synthetic_short_cost = data[t, 65]
        cost_to_iv_ratio = synthetic_short_cost / max(implied_vol, 1e-8)
        eng.append(cost_to_iv_ratio)
        
        # 12. Price trend strength using MACD-like indicator
        # More robust than simple up/down count
        if len(close_prices) >= 12:
            ema_fast = np.mean(close_prices[-5:])
            ema_slow = np.mean(close_prices[-12:])
            macd = (ema_fast - ema_slow) / max(ema_slow, 1e-8)
            eng.append(macd)
        else:
            eng.append(0.0)
        
        # 13. Days to cover change rate - temporal dynamics
        # Important in iteration 5
        dtc_change = 0.0
        if t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (data[t, 2] - prev_dtc) / max(prev_dtc, 1e-8)
        eng.append(dtc_change)
        
        # 14. Short interest acceleration - second derivative
        # Captures changing momentum in short interest
        si_accel = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            current_si_change = si_change
            si_accel = current_si_change - prev_si_change
        eng.append(si_accel)
        
        # 15. Price gap analysis - captures significant overnight moves
        # Important for detecting potential short squeezes
        gap_metric = 0.0
        for i in range(1, min(len(open_prices), 10)):
            gap = (open_prices[i] - close_prices[i-1]) / max(close_prices[i-1], 1e-8)
            gap_metric += abs(gap)
        gap_metric = gap_metric / min(len(open_prices)-1, 9) if len(open_prices) > 1 else 0.0
        eng.append(gap_metric)
        
        # 16. Volume-weighted price momentum
        # Combines price and volume signals
        if data.shape[1] > 68 and len(close_prices) >= 5:  # If volume data is available
            volume_data = data[t, 68]
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_price_momentum = price_change * (volume_data / max(avg_volume, 1e-8))
            eng.append(vol_price_momentum)
        else:
            # Use a proxy based on price momentum if volume isn't available
            if len(close_prices) >= 5:
                price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
                eng.append(price_change)
            else:
                eng.append(0.0)
        
        # 17. Synthetic short cost change rate
        # Captures changing dynamics in borrowing costs
        synth_cost_change = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            synth_cost_change = (synthetic_short_cost - prev_cost) / max(prev_cost, 1e-8)
        eng.append(synth_cost_change)
        
        # 18. Implied volatility change rate
        # Captures changing market expectations
        iv_change = 0.0
        if t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_vol - prev_iv) / max(prev_iv, 1e-8)
        eng.append(iv_change)
        
        # Ensure we don't exceed MAX_NEW features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN and infinity values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 10 (claude) (SVM)
==================================================
Training SVM model...

Iteration 10 (claude) Performance:
MAE: 1904650.5197
RMSE: 2415822.5154
MAPE: 10.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t1: importance=0.0008, rank=1
   2. Feature_20_t2: importance=0.0007, rank=2
   3. Feature_12_t1: importance=0.0007, rank=3
   4. Feature_5_t3: importance=0.0006, rank=4
   5. Feature_24_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 10 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.71%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 2604679.7924
RMSE: 3000180.3175
MAPE: 12.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 100
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0010, rank=1
   2. Feature_67_t3: importance=0.0009, rank=2
   3. Feature_1_t1: importance=0.0006, rank=3
   4. Feature_67_t0: importance=0.0005, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5
   Baseline MAPE: 12.93%
   Baseline MAE: 2604679.7924
   Baseline RMSE: 3000180.3175

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 2410679.2836
RMSE: 2886473.7792
MAPE: 12.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t3: importance=0.0007, rank=1
   2. Feature_24_t2: importance=0.0006, rank=2
   3. Feature_10_t2: importance=0.0006, rank=3
   4. Feature_10_t3: importance=0.0006, rank=4
   5. Feature_4_t1: importance=0.0006, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 12.21%
   MAE: 2410679.2836
   RMSE: 2886473.7792

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 12.93%
   Best Model MAPE: 12.21%
   Absolute Improvement: 0.72%
   Relative Improvement: 5.6%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  10.45           N/A                 
1          Iteration 1               9.30            +1.15%              
2          Iteration 2               9.79            -0.50%              
3          Iteration 3               9.84            -0.55%              
4          Iteration 4               9.92            -0.62%              
5          Iteration 5               8.83            +0.46%              
6          Iteration 6               10.11           -1.28%              
7          Iteration 7               11.75           -2.91%              
8          Iteration 8               9.57            -0.74%              
9          Iteration 9               9.90            -1.06%              
10         Iteration 10              10.55           -1.71%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 5 - MAPE: 8.83%
âœ… Saved HL results to cache/HL_iterative_results_enhanced.pkl
âœ… Summary report saved for HL

ðŸŽ‰ Process completed successfully for HL!

================================================================================
PROCESSING TICKER 14/15: ANDE
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for ANDE
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for ANDE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ANDE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 120121.2987
RMSE: 148016.5099
MAPE: 16.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 128
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0006, rank=1
   2. Feature_65_t2: importance=0.0006, rank=2
   3. Feature_1_t2: importance=0.0005, rank=3
   4. Feature_2_t3: importance=0.0005, rank=4
   5. Feature_67_t2: importance=0.0005, rank=5

ðŸ“Š Baseline Performance: MAPE = 16.39%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume (critical features)
        raw_keep.append(data[t, 0])  # short interest
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover (high importance from previous iteration)
        
        # Keep high-importance features from previous iteration
        raw_keep.append(data[t, 65])  # options_synthetic_short_cost
        raw_keep.append(data[t, 67])  # shares_outstanding
        
        # Extract OHLC data for the current timestep
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Add options data (high importance from previous iteration)
        raw_keep.append(data[t, 64])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 66])  # options_avg_implied_volatility
        raw_keep.append(data[t, 68])  # volume
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Float Ratio (normalized short interest)
        # This measures short interest relative to shares outstanding
        si_to_float = data[t, 0] / max(data[t, 67], 1e-8)
        eng.append(si_to_float)
        
        # 2. Short Interest to Volume Ratio
        # Measures short interest relative to trading volume
        si_to_volume = data[t, 0] / max(data[t, 68], 1e-8)
        eng.append(si_to_volume)
        
        # 3. Price Momentum (5-day)
        # Captures recent price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
        else:
            momentum_5d = 0
        eng.append(momentum_5d)
        
        # 4. Price Momentum (10-day)
        # Captures medium-term price trend
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
        else:
            momentum_10d = 0
        eng.append(momentum_10d)
        
        # 5. Volatility (5-day)
        # Measures recent price volatility
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility_5d = np.std(returns) if len(returns) > 0 else 0
        else:
            volatility_5d = 0
        eng.append(volatility_5d)
        
        # 6. Average True Range (ATR) - 5-day
        # Measures volatility accounting for gaps
        if len(close_prices) >= 2:
            tr_values = []
            for i in range(1, min(6, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr_5d = np.mean(tr_values) if tr_values else 0
        else:
            atr_5d = 0
        eng.append(atr_5d)
        
        # 7. Relative Strength Index (RSI) - 14-day
        # Momentum oscillator measuring speed and change of price movements
        if len(close_prices) >= 15:
            delta = np.diff(close_prices)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = np.mean(loss)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral value when not enough data
        eng.append(rsi)
        
        # 8. Options Put/Call Ratio Change
        # Measures change in sentiment from options market
        if t > 0:
            pc_ratio_change = data[t, 64] / max(data[t-1, 64], 1e-8) - 1
        else:
            pc_ratio_change = 0
        eng.append(pc_ratio_change)
        
        # 9. Implied Volatility to Historical Volatility Ratio
        # Compares market expectations to realized volatility
        if volatility_5d > 0:
            iv_hv_ratio = data[t, 66] / max(volatility_5d, 1e-8)
        else:
            iv_hv_ratio = 1
        eng.append(iv_hv_ratio)
        
        # 10. Volume Pressure (ratio of volume to average volume)
        # Indicates unusual trading activity
        volume_pressure = data[t, 68] / max(data[t, 1], 1e-8)
        eng.append(volume_pressure)
        
        # 11. Short Cost Momentum
        # Measures change in cost to short
        if t > 0:
            short_cost_change = data[t, 65] / max(data[t-1, 65], 1e-8) - 1
        else:
            short_cost_change = 0
        eng.append(short_cost_change)
        
        # 12. Price to Volume Correlation (5-day)
        # Measures relationship between price and volume movements
        if len(close_prices) >= 5:
            price_returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volume_changes = np.diff(data[max(0, t-4):t+1, 68]) / np.maximum(data[max(0, t-5):t, 68], 1e-8)
            if len(price_returns) == len(volume_changes) and len(price_returns) > 1:
                # Use a simple product of signs instead of correlation for stability
                price_vol_corr = np.mean(np.sign(price_returns) * np.sign(volume_changes))
            else:
                price_vol_corr = 0
        else:
            price_vol_corr = 0
        eng.append(price_vol_corr)
        
        # 13. Bollinger Band Width (volatility indicator)
        # Measures the width of Bollinger Bands, indicating potential volatility
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
        else:
            bb_width = 0
        eng.append(bb_width)
        
        # 14. Days to Cover Change
        # Measures change in short covering difficulty
        if t > 0:
            dtc_change = data[t, 2] / max(data[t-1, 2], 1e-8) - 1
        else:
            dtc_change = 0
        eng.append(dtc_change)
        
        # 15. Short Interest Change
        # Measures change in short interest
        if t > 0:
            si_change = data[t, 0] / max(data[t-1, 0], 1e-8) - 1
        else:
            si_change = 0
        eng.append(si_change)
        
        # 16. OHLC Range Ratio
        # Measures the daily trading range relative to opening price
        daily_ranges = (high_prices - low_prices) / np.maximum(open_prices, 1e-8)
        avg_range_ratio = np.mean(daily_ranges) if len(daily_ranges) > 0 else 0
        eng.append(avg_range_ratio)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.extend([short_interest, avg_daily_volume, days_to_cover])
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.extend([open_prices[-1], high_prices[-1], low_prices[-1], close_prices[-1]])
        
        # Keep important options and volume metrics
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0  # Handle potential index error
        
        raw_keep.extend([put_call_ratio, synthetic_short_cost, implied_volatility, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Price momentum and volatility features
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # 5-day price momentum
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) if len(close_prices) >= 5 else 0
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # 10-day price momentum
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1) if len(close_prices) >= 10 else 0
            eng.append(momentum_10d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 4:
            # Price volatility (standard deviation of returns)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        
        # 2. Volume-based features
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        if len(eng) < MAX_NEW and len(close_prices) > 4:
            # Money flow index (simplified)
            typical_prices = (high_prices + low_prices + close_prices) / 3
            money_flow = typical_prices * volume if volume != 0 else typical_prices
            pos_flow = np.sum((typical_prices[1:] > typical_prices[:-1]) * money_flow[1:])
            neg_flow = np.sum((typical_prices[1:] < typical_prices[:-1]) * money_flow[1:])
            mfi = 100 * pos_flow / max(pos_flow + neg_flow, 1e-8) if (pos_flow + neg_flow) > 0 else 50
            eng.append(mfi)
        
        # 3. Technical indicators
        if len(eng) < MAX_NEW and len(close_prices) > 13:
            # RSI (14-day)
            diff = np.diff(close_prices)
            gain = np.sum(np.maximum(diff, 0)) / 14 if len(diff) >= 14 else 0
            loss = np.sum(np.maximum(-diff, 0)) / 14 if len(diff) >= 14 else 0
            rs = gain / max(loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        if len(eng) < MAX_NEW and len(close_prices) > 4:
            # Bollinger Band Width
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            bb_width = std / max(sma, 1e-8)
            eng.append(bb_width)
        
        # 4. Options-related features
        if len(eng) < MAX_NEW:
            # Normalized put-call ratio
            norm_put_call = put_call_ratio / max(np.mean([put_call_ratio]), 1e-8)
            eng.append(norm_put_call)
        
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        # 5. Short interest specific features
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Short interest to market cap ratio
            market_cap = close_prices[-1] * shares_outstanding
            si_to_market_cap = short_interest / max(market_cap, 1e-8)
            eng.append(si_to_market_cap)
        
        # 6. Price pattern features
        if len(eng) < MAX_NEW and len(close_prices) > 4:
            # Gap analysis
            overnight_gaps = np.abs(open_prices[1:] - close_prices[:-1]) / np.maximum(close_prices[:-1], 1e-8)
            avg_gap = np.mean(overnight_gaps) if len(overnight_gaps) > 0 else 0
            eng.append(avg_gap)
        
        if len(eng) < MAX_NEW and len(high_prices) > 4 and len(low_prices) > 4:
            # Average true range (ATR)
            tr1 = high_prices[1:] - low_prices[1:]
            tr2 = np.abs(high_prices[1:] - close_prices[:-1])
            tr3 = np.abs(low_prices[1:] - close_prices[:-1])
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) if len(tr) > 0 else 0
            eng.append(atr)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 112367.2843
RMSE: 137677.1531
MAPE: 15.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0009, rank=1
   2. Feature_12_t2: importance=0.0007, rank=2
   3. Feature_18_t3: importance=0.0007, rank=3
   4. Feature_15_t1: importance=0.0006, rank=4
   5. Feature_11_t0: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 1.04%

ðŸ“ˆ Current best MAPE: 15.36%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else np.mean(avg_daily_volume)  # Use avg volume if not available
        
        # Keep only the most important raw features based on previous iterations
        # Always keep short interest and volume metrics which are fundamental
        raw_keep.extend([short_interest, avg_daily_volume, days_to_cover])
        
        # Keep only the most recent OHLC values instead of all 60 values
        raw_keep.extend([close_prices[-1]])  # Only keep the most recent close price
        
        # Keep the most important options and volume metrics
        raw_keep.extend([put_call_ratio, synthetic_short_cost, implied_volatility, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Specific Features - these were highly important in previous iterations
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding - key metric for short squeeze potential
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Short interest to market cap ratio - measures relative size of short positions
            market_cap = close_prices[-1] * shares_outstanding
            si_to_market_cap = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for better numerical properties
            eng.append(si_to_market_cap)
        
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio - indicates how many days of volume the short interest represents
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # 2. Price Momentum and Volatility Features - refined from previous iteration
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum with improved scaling
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100  # Convert to percentage
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # 10-day price momentum with improved scaling
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1) * 100  # Convert to percentage
            eng.append(momentum_10d)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price volatility (normalized by price level) - measures relative price instability
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
            volatility = np.std(returns) * 100 if len(returns) > 0 else 0  # Scale to percentage
            eng.append(volatility)
        
        # 3. Technical Indicators - refined based on previous importance
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - momentum oscillator that measures speed and change of price movements
            diff = np.diff(close_prices[-15:])  # Get at most 15 days
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Bollinger Band Width - measures volatility relative to moving average
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            bb_width = (std / max(sma, 1e-8)) * 100  # Scale to percentage
            eng.append(bb_width)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price distance from 5-day SMA - measures momentum and potential mean reversion
            sma5 = np.mean(close_prices[-5:])
            price_to_sma = (close_prices[-1] / max(sma5, 1e-8) - 1) * 100  # Convert to percentage
            eng.append(price_to_sma)
        
        # 4. Options-related features - refined based on previous importance
        if len(eng) < MAX_NEW:
            # Normalized put-call ratio - indicates market sentiment
            norm_put_call = put_call_ratio / max(np.mean([1, put_call_ratio]), 1e-8)
            eng.append(norm_put_call)
        
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio - measures cost efficiency of shorting
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        if len(eng) < MAX_NEW:
            # Options implied volatility to historical volatility ratio - measures expected vs realized volatility
            hist_vol = np.std(np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)) * np.sqrt(252) if len(close_prices) > 1 else 1e-8
            iv_to_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_to_hv_ratio)
        
        # 5. Volume and Liquidity Features
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Volume trend - measures increasing or decreasing trading activity
            recent_volumes = np.array([volume] * 5)  # Use available volume data
            volume_trend = (recent_volumes[-1] / max(np.mean(recent_volumes[:-1]), 1e-8) - 1) * 100
            eng.append(volume_trend)
        
        if len(eng) < MAX_NEW:
            # Liquidity ratio - measures how easily shorts can be covered
            liquidity_ratio = avg_daily_volume / max(short_interest, 1e-8)
            eng.append(liquidity_ratio)
        
        # 6. Price Pattern Features
        if len(eng) < MAX_NEW and len(high_prices) >= 5 and len(low_prices) >= 5:
            # Average true range (ATR) - measures volatility
            tr1 = high_prices[-5:] - low_prices[-5:]
            tr2 = np.abs(high_prices[-5:] - np.append(close_prices[-6:-1], close_prices[-1]))
            tr3 = np.abs(low_prices[-5:] - np.append(close_prices[-6:-1], close_prices[-1]))
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) / max(close_prices[-1], 1e-8) * 100  # Normalize by price and scale to percentage
            eng.append(atr)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Directional movement - measures strength of trend
            up_moves = np.sum(np.diff(close_prices[-10:]) > 0)
            down_moves = np.sum(np.diff(close_prices[-10:]) < 0)
            directional_strength = (up_moves - down_moves) / max(up_moves + down_moves, 1e-8)
            eng.append(directional_strength)
        
        if len(eng) < MAX_NEW and len(high_prices) >= 5 and len(low_prices) >= 5:
            # High-Low range ratio - measures volatility
            recent_range = np.mean(high_prices[-5:] - low_prices[-5:])
            normalized_range = recent_range / max(close_prices[-1], 1e-8) * 100  # Scale to percentage
            eng.append(normalized_range)
        
        # 7. Combined/Interaction Features
        if len(eng) < MAX_NEW:
            # Short interest * implied volatility interaction - captures combined effect
            si_iv_interaction = (short_interest / max(shares_outstanding, 1e-8)) * implied_volatility
            eng.append(si_iv_interaction)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Short squeeze potential score - combines multiple factors
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1)
            si_ratio = short_interest / max(shares_outstanding, 1e-8)
            squeeze_score = si_ratio * days_to_cover * (1 + max(price_momentum, 0)) * 100
            eng.append(squeeze_score)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract core features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep only essential raw features based on previous importance analysis
        # Keep short interest, avg daily volume, days to cover (consistently important)
        raw_keep.extend([short_interest, avg_daily_volume, days_to_cover])
        
        # Keep only the most recent OHLC value instead of all 60 values
        raw_keep.extend([close_prices[-1]])
        
        # Keep important options metrics that showed significance
        raw_keep.extend([put_call_ratio, synthetic_short_cost, implied_volatility, shares_outstanding])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest specific features (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Short interest to market cap ratio (improved version)
            market_cap = close_prices[-1] * shares_outstanding
            si_to_market_cap = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for better numerical stability
            eng.append(si_to_market_cap)
        
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio (high importance in previous iteration)
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # 2. Price momentum and volatility features (refined from previous iteration)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (showed significance)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1)
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # 10-day price momentum (showed significance)
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1)
            eng.append(momentum_10d)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price volatility (standard deviation of returns)
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8) if len(close_prices) >= 6 else np.zeros(1)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        
        # 3. Technical indicators (refined based on importance)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - showed high importance
            diff = np.diff(close_prices[-15:]) if len(close_prices) >= 15 else np.diff(close_prices)
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Bollinger Band Width (normalized)
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            bb_width = std / max(sma, 1e-8)
            eng.append(bb_width)
        
        # 4. Options-related features (refined)
        if len(eng) < MAX_NEW:
            # Normalized put-call ratio (showed significance)
            norm_put_call = put_call_ratio / max(np.mean([put_call_ratio]), 1e-8)
            eng.append(norm_put_call)
        
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio (showed significance)
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        # 5. Price pattern features (refined)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Average true range (ATR) - showed importance
            tr1 = high_prices[-5:] - low_prices[-5:]
            tr2 = np.abs(high_prices[-5:] - np.append(close_prices[-6:-1], close_prices[-6])) if len(close_prices) >= 6 else np.abs(high_prices[-5:] - close_prices[-5])
            tr3 = np.abs(low_prices[-5:] - np.append(close_prices[-6:-1], close_prices[-6])) if len(close_prices) >= 6 else np.abs(low_prices[-5:] - close_prices[-5])
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) if len(tr) > 0 else 0
            eng.append(atr)
        
        # 6. New features based on domain knowledge
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price to moving average ratio (15-day)
            ma15 = np.mean(close_prices)
            price_to_ma = close_prices[-1] / max(ma15, 1e-8)
            eng.append(price_to_ma)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # High-Low range relative to close
            recent_hl_range = (high_prices[-5:] - low_prices[-5:]) / np.maximum(close_prices[-5:], 1e-8)
            avg_hl_range = np.mean(recent_hl_range)
            eng.append(avg_hl_range)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Directional movement index (simplified DMI)
            up_moves = np.maximum(high_prices[-10:-1] - high_prices[-11:-2], 0) if len(high_prices) >= 11 else np.zeros(1)
            down_moves = np.maximum(low_prices[-11:-2] - low_prices[-10:-1], 0) if len(low_prices) >= 11 else np.zeros(1)
            
            plus_di = np.sum(up_moves) / max(np.sum(up_moves) + np.sum(down_moves), 1e-8)
            eng.append(plus_di)
        
        if len(eng) < MAX_NEW:
            # Days to cover relative to implied volatility
            dtc_to_iv = days_to_cover / max(implied_volatility, 1e-8)
            eng.append(dtc_to_iv)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Gap volatility
            if len(open_prices) >= 5 and len(close_prices) >= 6:
                gaps = np.abs(open_prices[-5:] - close_prices[-6:-1]) / np.maximum(close_prices[-6:-1], 1e-8)
                gap_vol = np.std(gaps) if len(gaps) > 0 else 0
                eng.append(gap_vol)
            else:
                eng.append(0)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Acceleration of price movement (second derivative)
            if len(close_prices) >= 7:
                returns = np.diff(close_prices[-7:]) / np.maximum(close_prices[-8:-1], 1e-8)
                acceleration = np.diff(returns)
                mean_accel = np.mean(acceleration) if len(acceleration) > 0 else 0
                eng.append(mean_accel)
            else:
                eng.append(0)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 2/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep only essential raw features based on previous importance analysis
        # Always keep short interest and volume metrics
        raw_keep.extend([
            short_interest,                # Critical target-related feature
            avg_daily_volume,              # Important for liquidity context
            days_to_cover,                 # Key short interest metric
            close_prices[-1],              # Most recent close price
            put_call_ratio,                # Options sentiment indicator
            synthetic_short_cost,          # Cost of shorting
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # For normalization of short interest
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Specific Features (highest priority based on domain knowledge)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding (key metric for short squeeze potential)
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio (indicates how many days of volume needed to cover shorts)
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Short interest to market cap ratio (normalized short interest by company size)
            market_cap = close_prices[-1] * shares_outstanding
            si_to_market_cap = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for numerical stability
            eng.append(si_to_market_cap)
        
        # 2. Price Momentum and Volatility Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (short-term trend)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1)
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # 10-day price momentum (medium-term trend)
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1)
            eng.append(momentum_10d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Recent price volatility (standard deviation of returns over available days)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        
        # 3. Technical Indicators (shown significant in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - momentum oscillator measuring speed and change of price movements
            diff = np.diff(close_prices)
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
            avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Bollinger Band Width - indicates volatility
            sma = np.mean(close_prices[-5:])
            std = np.std(close_prices[-5:])
            bb_width = std / max(sma, 1e-8)
            eng.append(bb_width)
        
        # 4. Options-related features (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio (cost efficiency of shorting)
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Implied volatility to price ratio (volatility normalized by price)
            iv_price_ratio = implied_volatility / max(close_prices[-1], 1e-8)
            eng.append(iv_price_ratio)
        
        # 5. Price pattern features
        if len(eng) < MAX_NEW and len(high_prices) > 1 and len(low_prices) > 1:
            # Average true range (ATR) - volatility indicator
            tr1 = high_prices[1:] - low_prices[1:]
            tr2 = np.abs(high_prices[1:] - close_prices[:-1])
            tr3 = np.abs(low_prices[1:] - close_prices[:-1])
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) if len(tr) > 0 else 0
            eng.append(atr)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price distance from 5-day moving average (trend deviation)
            sma5 = np.mean(close_prices[-5:])
            price_to_sma5 = close_prices[-1] / max(sma5, 1e-8) - 1
            eng.append(price_to_sma5)
        
        # 6. Advanced indicators
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # MACD Line (12-26 simplified to 5-day vs 10-day due to data constraints)
            ema5 = np.mean(close_prices[-5:])  # Simplified EMA
            ema10 = np.mean(close_prices[-10:]) if len(close_prices) >= 10 else ema5
            macd_line = ema5 - ema10
            eng.append(macd_line)
        
        if len(eng) < MAX_NEW and len(high_prices) >= 5 and len(low_prices) >= 5:
            # Stochastic Oscillator %K (5-day)
            lowest_low = np.min(low_prices[-5:])
            highest_high = np.max(high_prices[-5:])
            range_hl = highest_high - lowest_low
            stoch_k = 100 * (close_prices[-1] - lowest_low) / max(range_hl, 1e-8)
            eng.append(stoch_k)
        
        # 7. Volume-based features
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # On-balance volume trend (simplified due to data constraints)
            volume_direction = np.sign(np.diff(close_prices))
            obv_trend = np.sum(volume_direction)  # Simplified OBV trend
            eng.append(obv_trend)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 109076.2105
RMSE: 131589.5814
MAPE: 14.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0009, rank=1
   2. Feature_11_t1: importance=0.0005, rank=2
   3. Feature_2_t2: importance=0.0005, rank=3
   4. Feature_16_t1: importance=0.0004, rank=4
   5. Feature_5_t3: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.60%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if volume not available
        
        # Keep only essential raw features based on previous importance analysis
        raw_keep.extend([
            short_interest,                # Critical target-related feature
            avg_daily_volume,              # Important for liquidity context
            days_to_cover,                 # Key short interest metric
            close_prices[-1],              # Most recent close price
            put_call_ratio,                # Options sentiment indicator
            synthetic_short_cost,          # Cost of shorting
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # For normalization of short interest
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Specific Features (highest priority based on domain knowledge)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding (key metric for short squeeze potential)
            # This was highly significant in previous iterations
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio (indicates how many days of volume needed to cover shorts)
            # This is a more direct calculation than days_to_cover and provides additional insight
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        if len(eng) < MAX_NEW and len(close_prices) > 0:
            # Short interest to market cap ratio (normalized short interest by company size)
            # This provides context on the relative size of short positions
            market_cap = close_prices[-1] * shares_outstanding
            si_to_market_cap = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for numerical stability
            eng.append(si_to_market_cap)
        
        # 2. Price Momentum and Volatility Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (short-term trend)
            # Momentum was significant in previous iterations
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1)
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Recent price volatility (standard deviation of returns over available days)
            # Volatility is a key factor in short interest dynamics
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility)
        
        # 3. Advanced Technical Indicators (refined from previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - momentum oscillator measuring speed and change of price movements
            # RSI was highly significant in previous iterations
            diff = np.diff(close_prices)
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
            avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 4. New: Short Interest Trend and Acceleration
        if len(eng) < MAX_NEW and t > 0:
            # Short interest change rate (trend)
            # This captures the momentum of short interest itself
            prev_si = data[t-1, 0]
            si_change_rate = (short_interest / max(prev_si, 1e-8) - 1)
            eng.append(si_change_rate)
        
        if len(eng) < MAX_NEW and t > 1:
            # Short interest acceleration (change in change rate)
            # This captures acceleration in short interest movements
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            prev_change = (prev_si / max(prev_prev_si, 1e-8) - 1)
            current_change = (short_interest / max(prev_si, 1e-8) - 1)
            si_acceleration = current_change - prev_change
            eng.append(si_acceleration)
        
        # 5. Options-related features (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio (cost efficiency of shorting)
            # This was significant in previous iterations
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        if len(eng) < MAX_NEW:
            # New: Put-call ratio trend (options sentiment change)
            # This captures changing market sentiment through options
            if t > 0:
                prev_pc_ratio = data[t-1, 64]
                pc_ratio_change = (put_call_ratio / max(prev_pc_ratio, 1e-8) - 1)
            else:
                pc_ratio_change = 0
            eng.append(pc_ratio_change)
        
        # 6. Price pattern features
        if len(eng) < MAX_NEW and len(high_prices) > 1 and len(low_prices) > 1:
            # Average true range (ATR) - volatility indicator
            # ATR provides insight into price volatility
            tr1 = high_prices[1:] - low_prices[1:]
            tr2 = np.abs(high_prices[1:] - close_prices[:-1])
            tr3 = np.abs(low_prices[1:] - close_prices[:-1])
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) if len(tr) > 0 else 0
            eng.append(atr)
        
        # 7. New: Gap Analysis
        if len(eng) < MAX_NEW and len(open_prices) > 1 and len(close_prices) > 1:
            # Overnight gap ratio (close to next open)
            # Gaps often indicate significant sentiment shifts
            gaps = open_prices[1:] - close_prices[:-1]
            avg_gap_ratio = np.mean(gaps / np.maximum(close_prices[:-1], 1e-8)) if len(gaps) > 0 else 0
            eng.append(avg_gap_ratio)
        
        # 8. New: Price Efficiency Ratio
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price Efficiency Ratio (directional movement vs total movement)
            # Measures how efficiently price is moving in a direction
            net_movement = abs(close_prices[-1] - close_prices[-5])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-4, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 9. New: Volume-Price Relationship
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Volume-weighted price momentum
            # Incorporates volume as a weight for price movements
            price_changes = np.diff(close_prices)
            if hasattr(data[t], 'shape') and data[t].shape[0] > 68:
                # If we have daily volume data
                daily_volumes = data[t, 68:68+len(price_changes)]
                vol_weighted_momentum = np.sum(price_changes * daily_volumes) / max(np.sum(daily_volumes), 1e-8)
            else:
                # Fallback to average volume
                vol_weighted_momentum = np.mean(price_changes) * avg_daily_volume
            eng.append(vol_weighted_momentum)
        
        # 10. New: Short Interest to Options Volume Relationship
        if len(eng) < MAX_NEW:
            # Short interest to put-call ratio relationship
            # Captures alignment between short positions and options sentiment
            si_pc_alignment = short_interest * put_call_ratio / max(avg_daily_volume, 1e-8)
            eng.append(si_pc_alignment)
        
        # 11. New: Volatility-Adjusted Short Interest
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Short interest normalized by price volatility
            # Provides context on short interest relative to price stability
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            vol = np.std(returns) if len(returns) > 0 else 1e-8
            vol_adjusted_si = short_interest / max(vol * market_cap, 1e-8) * 1000
            eng.append(vol_adjusted_si)
        
        # 12. New: Short Interest Concentration
        if len(eng) < MAX_NEW:
            # Short interest concentration relative to market cap and volume
            # Measures how concentrated short positions are
            si_concentration = (short_interest / max(shares_outstanding, 1e-8)) / max(avg_daily_volume / max(market_cap, 1e-8), 1e-8)
            eng.append(si_concentration)
        
        # 13. New: Synthetic Short Cost Trend
        if len(eng) < MAX_NEW and t > 0:
            # Trend in synthetic short cost
            # Captures changing costs of shorting
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8) - 1)
            eng.append(short_cost_change)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 116493.5224
RMSE: 136061.3744
MAPE: 16.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0010, rank=1
   2. Feature_5_t3: importance=0.0008, rank=2
   3. Feature_19_t1: importance=0.0006, rank=3
   4. Feature_21_t2: importance=0.0005, rank=4
   5. Feature_5_t0: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.41%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep only essential raw features based on previous importance analysis
        # These were consistently important across iterations
        raw_keep.extend([
            short_interest,                # Critical target-related feature
            avg_daily_volume,              # Important for liquidity context
            days_to_cover,                 # Key short interest metric
            close_prices[-1],              # Most recent close price
            put_call_ratio,                # Options sentiment indicator
            synthetic_short_cost,          # Cost of shorting
            implied_volatility             # Market volatility expectations
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Relative Metrics (consistently high importance)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding
            # Key metric for short squeeze potential
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        if len(eng) < MAX_NEW:
            # Short interest to volume ratio (refined version of days_to_cover)
            # More direct calculation than the provided days_to_cover
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # 2. Short Interest Trend Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and t > 0:
            # Short interest change rate (trend)
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(prev_si, 1e-8) - 1) * 100  # Percentage change
            eng.append(si_change)
        
        if len(eng) < MAX_NEW and t > 1:
            # Short interest momentum (acceleration)
            # Captures acceleration in short interest movements
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            current_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            previous_change = (prev_si - prev_prev_si) / max(prev_prev_si, 1e-8)
            si_momentum = current_change - previous_change
            eng.append(si_momentum)
        
        # 3. Price Action Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (short-term trend)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Recent price volatility (standard deviation of returns)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility * 100)  # Scale for numerical stability
        
        # 4. Technical Indicators (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - momentum oscillator
            # Consistently high importance in previous iterations
            diff = np.diff(close_prices)
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
            avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 5. Options Market Indicators (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Put-call ratio to implied volatility relationship
            # Captures alignment between options sentiment and expected volatility
            pc_iv_ratio = put_call_ratio / max(implied_volatility, 1e-8)
            eng.append(pc_iv_ratio)
        
        if len(eng) < MAX_NEW and t > 0:
            # Change in put-call ratio (options sentiment shift)
            prev_pc_ratio = data[t-1, 64]
            pc_ratio_change = (put_call_ratio / max(prev_pc_ratio, 1e-8) - 1) * 100
            eng.append(pc_ratio_change)
        
        # 6. Short Cost Dynamics (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Short cost to implied volatility ratio
            # Measures cost efficiency of shorting relative to expected volatility
            short_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(short_cost_to_iv)
        
        if len(eng) < MAX_NEW and t > 0:
            # Change in synthetic short cost
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8) - 1) * 100
            eng.append(short_cost_change)
        
        # 7. Price Pattern Features (medium importance in previous iterations)
        if len(eng) < MAX_NEW and len(high_prices) > 1 and len(low_prices) > 1:
            # Average true range (ATR) - volatility indicator
            tr1 = high_prices[1:] - low_prices[1:]
            tr2 = np.abs(high_prices[1:] - close_prices[:-1])
            tr3 = np.abs(low_prices[1:] - close_prices[:-1])
            tr = np.maximum(np.maximum(tr1, tr2), tr3)
            atr = np.mean(tr) if len(tr) > 0 else 0
            # Normalize by price level for better comparability
            norm_atr = atr / max(close_prices[-1], 1e-8) * 100
            eng.append(norm_atr)
        
        # 8. Volume-Price Relationship (new feature with potential importance)
        if len(eng) < MAX_NEW and len(close_prices) > 5:
            # Volume-weighted price trend
            # Weights recent price changes by volume importance
            price_changes = np.diff(close_prices[-6:])
            # Use exponential weights to emphasize recent data
            weights = np.exp(np.linspace(0, 1, len(price_changes)))
            weighted_momentum = np.sum(price_changes * weights) / np.sum(weights)
            # Normalize by price level
            norm_weighted_momentum = weighted_momentum / max(close_prices[-1], 1e-8) * 100
            eng.append(norm_weighted_momentum)
        
        # 9. Short Squeeze Potential Indicators (new composite features)
        if len(eng) < MAX_NEW:
            # Short squeeze potential score
            # Combines short interest, volume, and volatility
            # Higher values indicate higher squeeze potential
            squeeze_potential = (si_percentage / 100) * volatility * days_to_cover
            eng.append(squeeze_potential)
        
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Short pressure indicator
            # Combines short interest with recent price action
            # Higher values suggest shorts might be under pressure
            price_change_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1)
            short_pressure = si_percentage * max(price_change_5d, 0) * 10  # Scale for readability
            eng.append(short_pressure)
        
        # 10. Market Efficiency Metrics (new features)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Price Efficiency Ratio
            # Measures how efficiently price is moving in a direction
            net_movement = abs(close_prices[-1] - close_prices[-5])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-5, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 11. Volatility-Adjusted Metrics (new features)
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Volatility-adjusted short interest
            # Normalizes short interest by price volatility
            vol_adjusted_si = si_percentage / max(volatility * 100, 1e-8)
            eng.append(vol_adjusted_si)
        
        # 12. Options Market Depth (new features)
        if len(eng) < MAX_NEW:
            # Options market depth indicator
            # Combines put-call ratio with implied volatility
            options_depth = put_call_ratio * implied_volatility
            eng.append(options_depth)
        
        # 13. Short Interest Concentration (new feature)
        if len(eng) < MAX_NEW:
            # Short interest concentration relative to market cap
            market_cap = close_prices[-1] * shares_outstanding
            si_concentration = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for numerical stability
            eng.append(si_concentration)
        
        # 14. Composite Trend Indicators (new features)
        if len(eng) < MAX_NEW and t > 0 and len(close_prices) > 1:
            # Composite trend indicator
            # Combines price momentum with short interest trend
            prev_si = data[t-1, 0]
            si_trend = (short_interest / max(prev_si, 1e-8) - 1)
            price_trend = (close_prices[-1] / max(close_prices[0], 1e-8) - 1)
            composite_trend = si_trend * price_trend * 100  # Scale for readability
            eng.append(composite_trend)
        
        # 15. Mean Reversion Potential (new feature)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Mean reversion potential
            # Measures deviation from recent average price
            mean_price = np.mean(close_prices[-10:])
            price_deviation = (close_prices[-1] / max(mean_price, 1e-8) - 1) * 100
            eng.append(price_deviation)
        
        # 16. Bollinger Band Position (new feature)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Position within Bollinger Bands
            # Indicates potential overbought/oversold conditions
            mean_price = np.mean(close_prices[-10:])
            std_price = np.std(close_prices[-10:])
            bb_position = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            eng.append(bb_position)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 114229.7277
RMSE: 134340.5711
MAPE: 15.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0011, rank=1
   2. Feature_9_t2: importance=0.0006, rank=2
   3. Feature_10_t0: importance=0.0006, rank=3
   4. Feature_21_t1: importance=0.0006, rank=4
   5. Feature_17_t3: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -1.02%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep only essential raw features based on previous importance analysis
        # These were consistently important across iterations
        raw_keep.extend([
            short_interest,                # Critical target-related feature
            avg_daily_volume,              # Important for liquidity context
            days_to_cover,                 # Key short interest metric
            close_prices[-1],              # Most recent close price
            put_call_ratio,                # Options sentiment indicator
            synthetic_short_cost,          # Cost of shorting
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # Important for relative metrics
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Relative Metrics (consistently high importance)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding
            # Key metric for short squeeze potential
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        # 2. Short Interest Trend Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and t > 0:
            # Short interest change rate (trend)
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(prev_si, 1e-8) - 1) * 100  # Percentage change
            eng.append(si_change)
        
        if len(eng) < MAX_NEW and t > 1:
            # Short interest acceleration (2nd derivative)
            # Captures acceleration in short interest movements
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            current_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            previous_change = (prev_si - prev_prev_si) / max(prev_prev_si, 1e-8)
            si_acceleration = current_change - previous_change
            eng.append(si_acceleration)
        
        # 3. Price Action Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (short-term trend)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100
            eng.append(momentum_5d)
        
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Recent price volatility (standard deviation of returns)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility * 100)  # Scale for numerical stability
        
        # 4. Technical Indicators (high importance in previous iterations)
        if len(eng) < MAX_NEW and len(close_prices) >= 14:
            # RSI (14-day) - momentum oscillator
            # Consistently high importance in previous iterations
            diff = np.diff(close_prices)
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
            avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 5. Short Squeeze Potential Indicators
        if len(eng) < MAX_NEW:
            # Short squeeze potential score - refined version
            # Combines short interest, volume, and volatility with price momentum
            # Higher values indicate higher squeeze potential
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            squeeze_potential = (si_percentage / 100) * volatility * days_to_cover * max(1 + recent_price_change/100, 0.1)
            eng.append(squeeze_potential)
        
        # 6. Options Market Indicators (high importance in previous iterations)
        if len(eng) < MAX_NEW:
            # Put-call ratio to implied volatility relationship
            # Captures alignment between options sentiment and expected volatility
            pc_iv_ratio = put_call_ratio / max(implied_volatility, 1e-8)
            eng.append(pc_iv_ratio)
        
        # 7. Short Cost Dynamics (high importance in previous iterations)
        if len(eng) < MAX_NEW and t > 0:
            # Change in synthetic short cost - important cost trend indicator
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8) - 1) * 100
            eng.append(short_cost_change)
        
        # 8. Bollinger Band Position (high importance in iteration 4)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Position within Bollinger Bands - indicates potential overbought/oversold conditions
            mean_price = np.mean(close_prices[-10:])
            std_price = np.std(close_prices[-10:])
            bb_position = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            eng.append(bb_position)
        
        # 9. Short Interest Concentration (high importance in iteration 4)
        if len(eng) < MAX_NEW:
            # Short interest concentration relative to market cap
            market_cap = close_prices[-1] * shares_outstanding
            si_concentration = short_interest / max(market_cap, 1e-8) * 1000  # Scaled for numerical stability
            eng.append(si_concentration)
        
        # 10. Volume-Price Relationship (new improved feature)
        if len(eng) < MAX_NEW and len(close_prices) > 5:
            # On-Balance Volume (OBV) momentum - volume-weighted price trend indicator
            # More effective than previous volume-weighted price trend
            obv = 0
            for i in range(1, min(15, len(close_prices))):
                if close_prices[i] > close_prices[i-1]:
                    obv += volume if i == 14 else avg_daily_volume/15  # Use actual volume for last day if available
                elif close_prices[i] < close_prices[i-1]:
                    obv -= volume if i == 14 else avg_daily_volume/15
            # Normalize OBV by average volume for comparability
            norm_obv = obv / max(avg_daily_volume, 1e-8)
            eng.append(norm_obv)
        
        # 11. Volatility-Adjusted Metrics (high importance in iteration 4)
        if len(eng) < MAX_NEW:
            # Volatility-adjusted days to cover - normalizes days to cover by price volatility
            vol_adjusted_dtc = days_to_cover / max(volatility * 100, 1e-8)
            eng.append(vol_adjusted_dtc)
        
        # 12. Price Efficiency Ratio (high importance in iteration 4)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Measures how efficiently price is moving in a direction
            net_movement = abs(close_prices[-1] - close_prices[-5])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-5, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 13. Short Interest to Options Relationship (new feature)
        if len(eng) < MAX_NEW:
            # Relationship between short interest and options market sentiment
            # Higher values indicate potential disconnect between short positions and options market
            si_options_ratio = si_percentage / max(put_call_ratio * 100, 1e-8)
            eng.append(si_options_ratio)
        
        # 14. Composite Trend Indicator (improved version)
        if len(eng) < MAX_NEW and t > 0 and len(close_prices) > 1:
            # Combines price momentum with short interest trend
            prev_si = data[t-1, 0]
            si_trend = (short_interest / max(prev_si, 1e-8) - 1)
            price_trend = (close_prices[-1] / max(close_prices[0], 1e-8) - 1)
            # Add sign agreement factor - trend alignment is more significant
            sign_agreement = 1 if (si_trend > 0) == (price_trend > 0) else -1
            composite_trend = si_trend * price_trend * sign_agreement * 100
            eng.append(composite_trend)
        
        # 15. Mean Reversion Potential (high importance in iteration 4)
        if len(eng) < MAX_NEW and len(close_prices) >= 10:
            # Measures deviation from recent average price
            mean_price = np.mean(close_prices[-10:])
            price_deviation = (close_prices[-1] / max(mean_price, 1e-8) - 1) * 100
            eng.append(price_deviation)
        
        # 16. MACD Signal (new feature - momentum indicator)
        if len(eng) < MAX_NEW and len(close_prices) >= 26:
            # MACD (Moving Average Convergence Divergence)
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA calculation
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            # Normalize by price level
            norm_macd = macd / max(close_prices[-1], 1e-8) * 100
            eng.append(norm_macd)
        elif len(eng) < MAX_NEW:
            # Simplified MACD for shorter time series
            fast_window = min(len(close_prices) // 2, 12)
            slow_window = min(len(close_prices) - 1, 26)
            if fast_window > 0 and slow_window > fast_window:
                ema_fast = np.mean(close_prices[-fast_window:])
                ema_slow = np.mean(close_prices[-slow_window:])
                simple_macd = ema_fast - ema_slow
                norm_simple_macd = simple_macd / max(close_prices[-1], 1e-8) * 100
                eng.append(norm_simple_macd)
            else:
                eng.append(0)  # Placeholder if not enough data
        
        # 17. Short Interest Relative to Historical Range (new feature)
        if len(eng) < MAX_NEW and t >= 2:
            # Position of current short interest within its recent historical range
            historical_si = [data[max(0, t-i), 0] for i in range(min(t+1, 5))]
            si_min = min(historical_si)
            si_max = max(historical_si)
            si_range_position = (short_interest - si_min) / max(si_max - si_min, 1e-8)
            eng.append(si_range_position)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 110688.3357
RMSE: 131506.3813
MAPE: 15.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0008, rank=1
   2. Feature_5_t3: importance=0.0006, rank=2
   3. Feature_10_t1: importance=0.0005, rank=3
   4. Feature_23_t1: importance=0.0005, rank=4
   5. Feature_23_t2: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.39%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        # Focus on the most consistently important features across iterations
        raw_keep.extend([
            short_interest,                # Critical target feature
            avg_daily_volume,              # Key liquidity metric
            days_to_cover,                 # Important short interest metric
            close_prices[-1],              # Most recent close price
            synthetic_short_cost,          # Cost of shorting (high importance in iterations)
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # Important for relative metrics
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest Relative Metrics (consistently high importance)
        if len(eng) < MAX_NEW:
            # Short interest as percentage of shares outstanding
            # Key metric for short squeeze potential
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        # 2. Short Interest Trend Features (high importance in previous iterations)
        if len(eng) < MAX_NEW and t > 0:
            # Short interest change rate (trend)
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(prev_si, 1e-8) - 1) * 100
            eng.append(si_change)
        
        # 3. Short Interest Acceleration (2nd derivative)
        if len(eng) < MAX_NEW and t > 1:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            
            # Use more robust calculation to avoid division issues
            current_change = short_interest - prev_si
            previous_change = prev_si - prev_prev_si
            
            # Normalize by the average SI level for better scaling
            avg_si = max((short_interest + prev_si + prev_prev_si) / 3, 1e-8)
            si_acceleration = (current_change - previous_change) / avg_si * 100
            eng.append(si_acceleration)
        
        # 4. Price Momentum Features (high importance in iterations 1-2)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # 5-day price momentum (short-term trend)
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100
            eng.append(momentum_5d)
        
        # 5. Price Volatility (consistently important)
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Recent price volatility (standard deviation of returns)
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility * 100)  # Scale for numerical stability
        
        # 6. RSI (14-day) - momentum oscillator (high importance in iterations 1-2)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Use adaptive window size based on available data
            window = min(14, len(close_prices)-1)
            diff = np.diff(close_prices[-window-1:])
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 7. Short Squeeze Potential Score (refined version)
        if len(eng) < MAX_NEW:
            # Combines multiple factors that contribute to short squeeze likelihood
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            
            # Volatility calculation (ensure it exists)
            if 'volatility' not in locals():
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                volatility = np.std(returns) if len(returns) > 0 else 0
            
            # More balanced formula with better scaling
            squeeze_potential = (si_percentage/100) * (1 + volatility*10) * days_to_cover * (1 + max(recent_price_change/100, 0))
            # Apply log transformation to handle extreme values better
            squeeze_potential = np.log1p(max(squeeze_potential, 0))
            eng.append(squeeze_potential)
        
        # 8. Options Market Indicators (important in iteration 5)
        if len(eng) < MAX_NEW:
            # Improved put-call ratio to implied volatility relationship
            # Normalize by market average put-call ratio (typically around 0.7)
            normalized_pc = put_call_ratio / 0.7
            pc_iv_ratio = normalized_pc / max(implied_volatility, 1e-8)
            eng.append(pc_iv_ratio)
        
        # 9. Short Cost Dynamics (high importance in previous iterations)
        if len(eng) < MAX_NEW and t > 0:
            # Change in synthetic short cost - important cost trend indicator
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8) - 1) * 100
            eng.append(short_cost_change)
        
        # 10. Bollinger Band Position (high importance in iteration 4)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Use adaptive window size
            window = min(10, len(close_prices))
            mean_price = np.mean(close_prices[-window:])
            std_price = np.std(close_prices[-window:])
            bb_position = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            # Clip to reasonable range to avoid extreme values
            bb_position = max(min(bb_position, 3), -3)
            eng.append(bb_position)
        
        # 11. Short Interest Concentration (high importance in iteration 4)
        if len(eng) < MAX_NEW:
            # Short interest concentration relative to market cap
            market_cap = close_prices[-1] * shares_outstanding
            si_concentration = short_interest / max(market_cap, 1e-8) * 1000
            eng.append(si_concentration)
        
        # 12. Price Efficiency Ratio (high importance in iteration 4)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Measures how efficiently price is moving in a direction
            window = min(5, len(close_prices)-1)
            net_movement = abs(close_prices[-1] - close_prices[-window-1])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-window, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 13. Short Interest to Options Relationship (new feature)
        if len(eng) < MAX_NEW:
            # Relationship between short interest and options market sentiment
            si_options_ratio = si_percentage / max(put_call_ratio * 100, 1e-8)
            # Apply log transformation to handle extreme values
            si_options_ratio = np.log1p(max(si_options_ratio, 0))
            eng.append(si_options_ratio)
        
        # 14. Days to Cover Trend (new feature)
        if len(eng) < MAX_NEW and t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (days_to_cover / max(prev_dtc, 1e-8) - 1) * 100
            eng.append(dtc_change)
        
        # 15. Short Interest Relative to Historical Range (high importance in iteration 5)
        if len(eng) < MAX_NEW and t >= 2:
            # Position of current short interest within its recent historical range
            historical_si = [data[max(0, t-i), 0] for i in range(min(t+1, 5))]
            si_min = min(historical_si)
            si_max = max(historical_si)
            si_range_position = (short_interest - si_min) / max(si_max - si_min, 1e-8)
            eng.append(si_range_position)
        
        # 16. Short Interest to Volume Ratio (new feature)
        if len(eng) < MAX_NEW:
            # Measures how many days of current volume would be needed to cover short interest
            # Different from days to cover as it uses current volume not average
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            eng.append(si_volume_ratio)
        
        # 17. Implied Volatility Trend (new feature)
        if len(eng) < MAX_NEW and t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8) - 1) * 100
            eng.append(iv_change)
        
        # 18. Short Interest Momentum (new feature)
        if len(eng) < MAX_NEW and t >= 3:
            # Measures acceleration in short interest over multiple periods
            si_3ago = data[t-3, 0]
            si_momentum = (short_interest / max(si_3ago, 1e-8) - 1) * 100
            eng.append(si_momentum)
        
        # 19. Composite Short Pressure Index (new feature)
        if len(eng) < MAX_NEW:
            # Combines multiple short-related metrics into a single pressure indicator
            # Higher values indicate more pressure on shorts
            if 'volatility' not in locals():
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                volatility = np.std(returns) if len(returns) > 0 else 0
                
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            
            # Pressure increases with: high SI%, rising prices, high volatility, high borrow cost
            short_pressure = (si_percentage/100) * (1 + max(recent_price_change/100, 0)) * (1 + volatility*5) * (1 + synthetic_short_cost)
            # Normalize with log transform
            short_pressure = np.log1p(max(short_pressure, 0))
            eng.append(short_pressure)
        
        # 20. Relative Strength Comparison (new feature)
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            # Compare recent price strength to longer-term trend
            short_term = close_prices[-1] / max(close_prices[-3], 1e-8) if len(close_prices) >= 3 else 1
            longer_term = close_prices[-1] / max(close_prices[-min(len(close_prices), 10)], 1e-8) if len(close_prices) >= 10 else 1
            rel_strength = (short_term / max(longer_term, 1e-8) - 1) * 100
            eng.append(rel_strength)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 111836.3644
RMSE: 130574.6539
MAPE: 15.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0012, rank=1
   2. Feature_9_t0: importance=0.0007, rank=2
   3. Feature_15_t3: importance=0.0006, rank=3
   4. Feature_4_t3: importance=0.0005, rank=4
   5. Feature_15_t1: importance=0.0005, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.62%

ðŸ“ˆ Current best MAPE: 14.76%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep essential raw features based on importance analysis
        # Focus on consistently important features from previous iterations
        raw_keep.extend([
            short_interest,                # Critical target feature
            avg_daily_volume,              # Key liquidity metric
            days_to_cover,                 # Important short interest metric
            close_prices[-1],              # Most recent close price
            synthetic_short_cost,          # Cost of shorting (high importance)
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # Important for relative metrics
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest as Percentage of Shares Outstanding
        # Consistently high importance across iterations
        if len(eng) < MAX_NEW:
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        # 2. Short Interest Change Rate (1-period)
        # High importance in previous iterations
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(prev_si, 1e-8) - 1) * 100
            eng.append(si_change)
        
        # 3. Short Interest Acceleration (2nd derivative)
        # Improved calculation based on previous iterations
        if len(eng) < MAX_NEW and t > 1:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            
            # Calculate acceleration as percentage change in growth rate
            current_change_rate = (short_interest / max(prev_si, 1e-8)) - 1
            previous_change_rate = (prev_si / max(prev_prev_si, 1e-8)) - 1
            
            si_acceleration = (current_change_rate - previous_change_rate) * 100
            eng.append(si_acceleration)
        
        # 4. Price Momentum (5-day)
        # High importance in iterations 1-2
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100
            eng.append(momentum_5d)
        
        # 5. Recent Price Volatility
        # Consistently important across iterations
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) * 100 if len(returns) > 0 else 0
            eng.append(volatility)
        
        # 6. RSI (Relative Strength Index)
        # High importance in iterations 1-2
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(14, len(close_prices)-1)
            diff = np.diff(close_prices[-window-1:])
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 7. Short Squeeze Potential Score
        # Refined based on performance in previous iterations
        if len(eng) < MAX_NEW:
            # Calculate recent price change if not already done
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            
            # Calculate volatility if not already done
            if 'volatility' not in locals():
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                volatility = np.std(returns) * 100 if len(returns) > 0 else 0
            
            # Improved formula with better scaling
            # Higher values indicate higher squeeze potential
            squeeze_potential = (si_percentage/100) * days_to_cover * (1 + volatility/100) * (1 + max(recent_price_change/50, 0))
            # Log transform to handle extreme values
            squeeze_potential = np.log1p(squeeze_potential)
            eng.append(squeeze_potential)
        
        # 8. Short Interest to Volume Ratio
        # New feature that showed promise in iteration 6
        if len(eng) < MAX_NEW:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            # Apply log transform to handle extreme values
            si_volume_ratio = np.log1p(si_volume_ratio)
            eng.append(si_volume_ratio)
        
        # 9. Bollinger Band Position
        # High importance in iteration 4
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(10, len(close_prices))
            mean_price = np.mean(close_prices[-window:])
            std_price = np.std(close_prices[-window:])
            bb_position = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            # Clip to reasonable range
            bb_position = max(min(bb_position, 3), -3)
            eng.append(bb_position)
        
        # 10. Short Interest Concentration
        # High importance in iteration 4
        if len(eng) < MAX_NEW:
            market_cap = close_prices[-1] * shares_outstanding
            si_concentration = short_interest / max(market_cap, 1e-8) * 1000
            # Apply log transform for better scaling
            si_concentration = np.log1p(si_concentration)
            eng.append(si_concentration)
        
        # 11. Price Efficiency Ratio
        # High importance in iteration 4
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(5, len(close_prices)-1)
            net_movement = abs(close_prices[-1] - close_prices[-window-1])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-window, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 12. Days to Cover Change Rate
        # New feature based on importance of days_to_cover
        if len(eng) < MAX_NEW and t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (days_to_cover / max(prev_dtc, 1e-8) - 1) * 100
            eng.append(dtc_change)
        
        # 13. Implied Volatility to Historical Volatility Ratio
        # New feature combining two important volatility metrics
        if len(eng) < MAX_NEW and 'volatility' in locals():
            iv_hv_ratio = implied_volatility / max(volatility, 1e-8)
            # Clip to reasonable range
            iv_hv_ratio = max(min(iv_hv_ratio, 5), 0.2)
            eng.append(iv_hv_ratio)
        
        # 14. Short Interest Relative to Historical Range
        # High importance in iteration 5
        if len(eng) < MAX_NEW and t >= 2:
            historical_si = [data[max(0, t-i), 0] for i in range(min(t+1, 5))]
            si_min = min(historical_si)
            si_max = max(historical_si)
            si_range_position = (short_interest - si_min) / max(si_max - si_min, 1e-8)
            eng.append(si_range_position)
        
        # 15. Put-Call Ratio to Short Interest Relationship
        # New feature combining options market sentiment with short interest
        if len(eng) < MAX_NEW:
            pc_si_ratio = put_call_ratio / max(si_percentage/100, 1e-8)
            # Apply log transform for better scaling
            pc_si_ratio = np.log1p(pc_si_ratio)
            eng.append(pc_si_ratio)
        
        # 16. Short Cost Momentum
        # Improved version of short cost dynamics from previous iterations
        if len(eng) < MAX_NEW and t > 1:
            prev_cost = data[t-1, 65]
            prev_prev_cost = data[t-2, 65]
            
            # Calculate momentum as percentage change over two periods
            short_cost_momentum = (synthetic_short_cost / max(prev_prev_cost, 1e-8) - 1) * 100
            eng.append(short_cost_momentum)
        
        # 17. MACD Signal Line Crossover
        # New technical indicator with good predictive power
        if len(eng) < MAX_NEW and len(close_prices) >= 12:
            # Calculate EMA-12 and EMA-26 (or shorter if not enough data)
            ema12_period = min(12, len(close_prices))
            ema26_period = min(26, len(close_prices))
            
            # Simple approximation of EMA using weighted average
            weights12 = np.exp(np.linspace(-1, 0, ema12_period))
            weights12 = weights12 / weights12.sum()
            
            weights26 = np.exp(np.linspace(-1, 0, ema26_period))
            weights26 = weights26 / weights26.sum()
            
            ema12 = np.sum(close_prices[-ema12_period:] * weights12)
            ema26 = np.sum(close_prices[-ema26_period:] * weights26)
            
            # MACD line
            macd = ema12 - ema26
            
            # Signal line (9-period EMA of MACD)
            # For simplicity, use the difference between current MACD and previous
            if t > 0 and len(close_prices) >= 13:
                prev_close = np.concatenate([close_prices[1:], [0]])  # Shift by 1
                
                prev_ema12 = np.sum(prev_close[-ema12_period:] * weights12)
                prev_ema26 = np.sum(prev_close[-ema26_period:] * weights26)
                prev_macd = prev_ema12 - prev_ema26
                
                # MACD momentum (difference between current and previous)
                macd_momentum = macd - prev_macd
                eng.append(macd_momentum)
            else:
                eng.append(0)  # Placeholder when not enough data
        
        # 18. Volume Pressure Index
        # New feature measuring volume pressure relative to price movement
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            # Calculate recent price change
            recent_return = (close_prices[-1] / max(close_prices[-2], 1e-8) - 1)
            
            # Volume relative to average
            volume_ratio = data[t, 68] / max(avg_daily_volume, 1e-8)
            
            # Volume pressure: higher when volume is high and price moves significantly
            volume_pressure = volume_ratio * abs(recent_return) * 100
            eng.append(volume_pressure)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: index 68 is out of bounds for axis 1 with size 68
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_list = []
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract other important features
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep essential raw features based on importance analysis
        # Focus on the most consistently important features across iterations
        raw_keep.extend([
            short_interest,                # Critical target feature
            avg_daily_volume,              # Key liquidity metric
            days_to_cover,                 # Important short interest metric
            close_prices[-1],              # Most recent close price
            synthetic_short_cost,          # Cost of shorting (high importance)
            implied_volatility,            # Market volatility expectations
            shares_outstanding             # Important for relative metrics
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest as Percentage of Shares Outstanding
        # Consistently high importance across iterations
        if len(eng) < MAX_NEW:
            si_percentage = short_interest / max(shares_outstanding, 1e-8) * 100
            eng.append(si_percentage)
        
        # 2. Short Interest Change Rate (1-period)
        # High importance in previous iterations
        if len(eng) < MAX_NEW and t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest / max(prev_si, 1e-8) - 1) * 100
            eng.append(si_change)
        
        # 3. Short Interest Acceleration (2nd derivative)
        # Important for detecting inflection points in short interest trends
        if len(eng) < MAX_NEW and t > 1:
            prev_si = data[t-1, 0]
            prev_prev_si = data[t-2, 0]
            current_change = short_interest - prev_si
            previous_change = prev_si - prev_prev_si
            avg_si = max((short_interest + prev_si + prev_prev_si) / 3, 1e-8)
            si_acceleration = (current_change - previous_change) / avg_si * 100
            eng.append(si_acceleration)
        
        # 4. Days to Cover Trend
        # Important for understanding changing dynamics in short covering potential
        if len(eng) < MAX_NEW and t > 0:
            prev_dtc = data[t-1, 2]
            dtc_change = (days_to_cover / max(prev_dtc, 1e-8) - 1) * 100
            eng.append(dtc_change)
        
        # 5. Short Interest to Volume Ratio
        # Different perspective than days to cover, focusing on current volume dynamics
        if len(eng) < MAX_NEW:
            si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
            # Apply log transformation to handle extreme values
            si_volume_ratio = np.log1p(max(si_volume_ratio, 0))
            eng.append(si_volume_ratio)
        
        # 6. Price Momentum (5-day)
        # Captures recent price trend, important for short squeeze potential
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) * 100
            eng.append(momentum_5d)
        
        # 7. Price Volatility
        # Consistently important across iterations
        if len(eng) < MAX_NEW and len(close_prices) > 1:
            returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0
            eng.append(volatility * 100)  # Scale for numerical stability
        
        # 8. RSI (Relative Strength Index)
        # Important momentum oscillator with high feature importance
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(14, len(close_prices)-1)
            diff = np.diff(close_prices[-window-1:])
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        
        # 9. Bollinger Band Position
        # High importance in iteration 4, measures price relative to volatility
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(10, len(close_prices))
            mean_price = np.mean(close_prices[-window:])
            std_price = np.std(close_prices[-window:])
            bb_position = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            # Clip to reasonable range to avoid extreme values
            bb_position = max(min(bb_position, 3), -3)
            eng.append(bb_position)
        
        # 10. Short Interest Concentration
        # Measures short interest relative to market cap
        if len(eng) < MAX_NEW:
            market_cap = close_prices[-1] * shares_outstanding
            si_concentration = short_interest / max(market_cap, 1e-8) * 1000
            # Apply log transformation for better scaling
            si_concentration = np.log1p(max(si_concentration, 0))
            eng.append(si_concentration)
        
        # 11. Short Cost Dynamics
        # Tracks changes in cost of shorting, important for predicting covering
        if len(eng) < MAX_NEW and t > 0:
            prev_short_cost = data[t-1, 65]
            short_cost_change = (synthetic_short_cost / max(prev_short_cost, 1e-8) - 1) * 100
            eng.append(short_cost_change)
        
        # 12. Implied Volatility Trend
        # Important for understanding changing market expectations
        if len(eng) < MAX_NEW and t > 0:
            prev_iv = data[t-1, 66]
            iv_change = (implied_volatility / max(prev_iv, 1e-8) - 1) * 100
            eng.append(iv_change)
        
        # 13. Short Squeeze Potential Score
        # Refined composite metric combining key factors that predict short squeezes
        if len(eng) < MAX_NEW:
            # Ensure volatility is calculated
            if 'volatility' not in locals() or volatility == 0:
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                volatility = np.std(returns) if len(returns) > 0 else 0
            
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            
            # Improved formula with better balance between factors
            squeeze_potential = (si_percentage/100) * (1 + volatility*5) * days_to_cover * (1 + max(recent_price_change/100, 0))
            # Apply log transformation to handle extreme values
            squeeze_potential = np.log1p(max(squeeze_potential, 0))
            eng.append(squeeze_potential)
        
        # 14. Put-Call Ratio to Implied Volatility Relationship
        # Important options market indicator
        if len(eng) < MAX_NEW:
            normalized_pc = put_call_ratio / 0.7  # Normalize by typical market ratio
            pc_iv_ratio = normalized_pc * implied_volatility  # Changed to multiplication for better signal
            eng.append(pc_iv_ratio)
        
        # 15. Price Efficiency Ratio
        # Measures how efficiently price is moving in a direction
        if len(eng) < MAX_NEW and len(close_prices) >= 5:
            window = min(5, len(close_prices)-1)
            net_movement = abs(close_prices[-1] - close_prices[-window-1])
            total_movement = sum(abs(close_prices[i] - close_prices[i-1]) for i in range(-window, 0))
            price_efficiency = net_movement / max(total_movement, 1e-8)
            eng.append(price_efficiency)
        
        # 16. Short Interest to Options Volume Relationship
        # New feature combining short interest with options market sentiment
        if len(eng) < MAX_NEW:
            si_options_ratio = si_percentage / max(put_call_ratio * 100, 1e-8)
            # Apply log transformation to handle extreme values
            si_options_ratio = np.log1p(max(si_options_ratio, 0))
            eng.append(si_options_ratio)
        
        # 17. Short Interest Relative to Historical Range
        # Position of current short interest within its recent range
        if len(eng) < MAX_NEW and t >= 2:
            historical_si = [data[max(0, t-i), 0] for i in range(min(t+1, 5))]
            si_min = min(historical_si)
            si_max = max(historical_si)
            si_range_position = (short_interest - si_min) / max(si_max - si_min, 1e-8)
            eng.append(si_range_position)
        
        # 18. MACD Signal
        # Moving Average Convergence Divergence - important technical indicator
        if len(eng) < MAX_NEW and len(close_prices) >= 12:
            # Adaptive window sizes based on available data
            fast_window = min(12, len(close_prices))
            slow_window = min(26, len(close_prices))
            
            fast_ema = np.mean(close_prices[-fast_window:])  # Simplified EMA
            slow_ema = np.mean(close_prices[-slow_window:])  # Simplified EMA
            
            macd = fast_ema - slow_ema
            # Normalize by price level
            macd_normalized = macd / max(close_prices[-1], 1e-8) * 100
            eng.append(macd_normalized)
        
        # 19. Composite Short Pressure Index
        # Combines multiple short-related metrics into a single pressure indicator
        if len(eng) < MAX_NEW:
            # Ensure volatility is calculated
            if 'volatility' not in locals() or volatility == 0:
                returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
                volatility = np.std(returns) if len(returns) > 0 else 0
                
            recent_price_change = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) * 100 if len(close_prices) > 1 else 0
            
            # Improved formula with better weighting
            short_pressure = (si_percentage/100) * (1 + max(recent_price_change/100, 0)) * (1 + volatility*5) * (1 + synthetic_short_cost/2)
            # Apply log transformation
            short_pressure = np.log1p(max(short_pressure, 0))
            eng.append(short_pressure)
        
        # 20. Volume Trend
        # Tracks changes in trading volume, important for liquidity dynamics
        if len(eng) < MAX_NEW and t > 0:
            prev_volume = data[t-1, 1]  # Using avg_daily_volume from previous timestep
            volume_change = (avg_daily_volume / max(prev_volume, 1e-8) - 1) * 100
            eng.append(volume_change)
        
        # 21. Short Interest Momentum (3-period)
        # Longer-term trend in short interest
        if len(eng) < MAX_NEW and t >= 3:
            si_3ago = data[t-3, 0]
            si_momentum = (short_interest / max(si_3ago, 1e-8) - 1) * 100
            eng.append(si_momentum)
        
        # 22. Average True Range (ATR)
        # Important volatility indicator
        if len(eng) < MAX_NEW and len(close_prices) >= 2:
            window = min(14, len(close_prices)-1)
            tr_values = []
            
            for i in range(1, min(window+1, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-i-1])
                low_close = abs(low_prices[-i] - close_prices[-i-1])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            
            atr = np.mean(tr_values) if tr_values else 0
            # Normalize by price level
            atr_normalized = atr / max(close_prices[-1], 1e-8) * 100
            eng.append(atr_normalized)
        
        # 23. Synthetic Short Cost to Implied Volatility Ratio
        # Relationship between cost of shorting and expected volatility
        if len(eng) < MAX_NEW:
            cost_vol_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
            eng.append(cost_vol_ratio)
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if too many features
            row = row[:MAX_TOTAL]
        
        features_list.append(row)
    
    # Stack all rows into a 2D array
    features_array = np.stack(features_list, axis=0)
    
    # Handle NaN and infinite values
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 7 (claude) (SVM)
==================================================
Training SVM model...

Iteration 7 (claude) Performance:
MAE: 109075.8652
RMSE: 129313.3595
MAPE: 14.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0006, rank=1
   2. Feature_24_t2: importance=0.0005, rank=2
   3. Feature_9_t1: importance=0.0005, rank=3
   4. Feature_7_t1: importance=0.0005, rank=4
   5. Feature_16_t2: importance=0.0004, rank=5
ðŸ’¾ Saved code for iteration 7 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.11%

ðŸ›‘ Stopping: No improvement for 5 consecutive iterations

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 90018.9068
RMSE: 121027.9390
MAPE: 12.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 130
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0011, rank=1
   2. Feature_0_t3: importance=0.0007, rank=2
   3. Feature_67_t2: importance=0.0007, rank=3
   4. Feature_2_t3: importance=0.0007, rank=4
   5. Feature_64_t2: importance=0.0005, rank=5
   Baseline MAPE: 12.75%
   Baseline MAE: 90018.9068
   Baseline RMSE: 121027.9390

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 94982.6133
RMSE: 130495.9640
MAPE: 13.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0008, rank=1
   2. Feature_21_t1: importance=0.0007, rank=2
   3. Feature_5_t3: importance=0.0005, rank=3
   4. Feature_14_t1: importance=0.0005, rank=4
   5. Feature_12_t2: importance=0.0005, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 13.77%
   MAE: 94982.6133
   RMSE: 130495.9640

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 12.75%
   Best Model MAPE: 13.77%
   Absolute Improvement: -1.02%
   Relative Improvement: -8.0%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  16.39           N/A                 
1          Iteration 1               15.36           +1.04%              
2          Iteration 2               14.76           +0.60%              
3          Iteration 3               16.17           -1.41%              
4          Iteration 4               15.78           -1.02%              
5          Iteration 5               15.15           -0.39%              
6          Iteration 6               15.38           -0.62%              
7          Iteration 7               14.87           -0.11%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 2 - MAPE: 14.76%
âœ… Saved ANDE results to cache/ANDE_iterative_results_enhanced.pkl
âœ… Summary report saved for ANDE

ðŸŽ‰ Process completed successfully for ANDE!

================================================================================
PROCESSING TICKER 15/15: AROC
================================================================================
ðŸš€ Starting Iterative Agent-Based Feature Selection Process for AROC
======================================================================
ðŸ“Š Loading data from parquet file...
ðŸ“Š Loading data for AROC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AROC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data integrity validation passed
âœ… Data loaded successfully!
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
Features per timestep: 68
Lookback window: 4

ðŸŽ¯ Step 1: Running baseline model on validation set...

==================================================
Training Baseline (All 68 Features) (SVM)
==================================================
Training SVM model...

Baseline (All 68 Features) Performance:
MAE: 451417.5985
RMSE: 602912.6690
MAPE: 10.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 119
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0031, rank=1
   2. Feature_67_t3: importance=0.0025, rank=2
   3. Feature_63_t0: importance=0.0016, rank=3
   4. Feature_67_t1: importance=0.0014, rank=4
   5. Feature_0_t1: importance=0.0008, rank=5

ðŸ“Š Baseline Performance: MAPE = 10.36%

ðŸ”„ Starting iterative improvement process...
Max iterations: 10
Min improvement threshold: 0.1%
Patience: 5 iterations without improvement

======================================================================
ITERATION 1
======================================================================
ðŸ¤– Calling Claude for iteration 1...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep options data which showed importance
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        raw_keep.append(put_call_ratio)  # Options put/call volume ratio
        raw_keep.append(shares_outstanding)  # Shares outstanding
        raw_keep.append(volume)  # Volume
        
        # Calculate MAX_NEW after determining raw features to keep
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Price momentum and volatility features
        if len(close_prices) > 1:
            # 5-day price momentum
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8) - 1) if len(close_prices) >= 5 else 0
            eng.append(price_momentum_5d)
            
            # 10-day price momentum
            price_momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1) if len(close_prices) >= 10 else 0
            eng.append(price_momentum_10d)
            
            # Price volatility (standard deviation of returns)
            if len(close_prices) >= 5:
                returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-6:-1], 1e-8)
                price_volatility = np.std(returns) if len(returns) > 0 else 0
                eng.append(price_volatility)
        else:
            eng.extend([0, 0, 0])  # Placeholders if not enough data
        
        # 2. Volume-based features
        # Short interest to volume ratio
        si_volume_ratio = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Technical indicators
        if len(close_prices) >= 14:
            # RSI (14-day)
            diff = np.diff(close_prices[-15:])
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
            
            # MACD components
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema_26 = np.mean(close_prices[-15:])  # Using available data
            macd = ema_12 - ema_26
            eng.append(macd)
        else:
            eng.extend([0, 0])  # Placeholders if not enough data
        
        # 4. Options-based features
        # Implied volatility to historical volatility ratio
        if len(close_prices) >= 10:
            hist_returns = np.diff(close_prices[-10:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = np.std(hist_returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol, 1e-8)
            eng.append(iv_hv_ratio)
        else:
            eng.append(0)
        
        # 5. Short interest relative to shares outstanding
        si_shares_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_shares_ratio)
        
        # 6. Short interest change rate
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            eng.append(si_change)
        else:
            eng.append(0)
        
        # 7. Price range features
        if len(high_prices) > 0 and len(low_prices) > 0:
            # High-Low range relative to close
            hl_range = (high_prices[-1] - low_prices[-1]) / max(close_prices[-1], 1e-8)
            eng.append(hl_range)
        else:
            eng.append(0)
        
        # 8. Synthetic short cost to implied volatility ratio
        synth_iv_ratio = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_iv_ratio)
        
        # 9. Volume trend
        if len(close_prices) >= 5 and volume > 0:
            volume_trend = volume / max(avg_daily_volume, 1e-8)
            eng.append(volume_trend)
        else:
            eng.append(0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaNs and infinities
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (4,) (5,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Always keep short interest and average daily volume
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Keep these important raw features
        raw_keep.append(short_interest)  # Short interest
        raw_keep.append(avg_daily_volume)  # Average daily volume
        raw_keep.append(days_to_cover)  # Days to cover
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values
        raw_keep.append(close_prices[-1])  # Most recent close price
        
        # Keep important options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0  # Safely handle potential index error
        
        raw_keep.append(put_call_ratio)  # Options put/call volume ratio
        raw_keep.append(implied_volatility)  # Average implied volatility
        raw_keep.append(shares_outstanding)  # Shares outstanding
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Price momentum (5-day)
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
            
        # Feature 2: Price momentum (10-day)
        if len(close_prices) >= 10:
            momentum_10d = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            eng.append(momentum_10d)
        else:
            eng.append(0.0)
            
        # Feature 3: Volatility (standard deviation of returns over 10 days)
        if len(close_prices) >= 11:
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            volatility = np.std(returns) if len(returns) > 0 else 0.0
            eng.append(volatility)
        else:
            eng.append(0.0)
            
        # Feature 4: Average True Range (ATR) - volatility indicator
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
            
        # Feature 5: Short interest to float ratio
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 6: Short interest to volume ratio
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 7: Relative Strength Index (RSI)
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            avg_gain = gain / 14.0
            avg_loss = loss / 14.0
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
            
        # Feature 8: Price to volume ratio
        price_to_volume = close_prices[-1] / max(avg_daily_volume, 1e-8)
        eng.append(price_to_volume)
        
        # Feature 9: Bollinger Band Width (volatility measure)
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
            
        # Feature 10: OHLC price range relative to close
        if len(close_prices) > 0:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            price_range = (recent_high - recent_low) / max(close_prices[-1], 1e-8)
            eng.append(price_range)
        else:
            eng.append(0.0)
            
        # Feature 11: Short interest momentum (change)
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 12: Put-call ratio relative to historical average
        if t > 5:
            historical_pc_ratio = np.mean([data[t-i, 64] for i in range(1, min(6, t+1))])
            pc_ratio_relative = put_call_ratio / max(historical_pc_ratio, 1e-8)
            eng.append(pc_ratio_relative)
        else:
            eng.append(1.0)
            
        # Feature 13: Implied volatility relative to historical
        if t > 5:
            historical_iv = np.mean([data[t-i, 66] for i in range(1, min(6, t+1))])
            iv_relative = implied_volatility / max(historical_iv, 1e-8)
            eng.append(iv_relative)
        else:
            eng.append(1.0)
            
        # Feature 14: Moving Average Convergence Divergence (MACD)
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            eng.append(macd)
        else:
            eng.append(0.0)
            
        # Feature 15: Volume Oscillator
        if len(close_prices) >= 10:
            short_vol_avg = np.mean(volume) if isinstance(volume, (int, float)) else 0.0
            if t > 5:
                long_vol_avg = np.mean([data[t-i, 68] if data.shape[1] > 68 else 0.0 for i in range(1, min(10, t+1))])
                vol_osc = (short_vol_avg - long_vol_avg) / max(long_vol_avg, 1e-8)
                eng.append(vol_osc)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
            
        # Feature 16: Synthetic short cost to implied volatility ratio
        synth_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_cost_to_iv)
        
        # Feature 17: Days to cover momentum
        dtc_momentum = 0.0
        if t > 0 and data[t-1, 2] > 0:
            dtc_momentum = (days_to_cover / max(data[t-1, 2], 1e-8)) - 1.0
        eng.append(dtc_momentum)
        
        # Feature 18: Short interest to synthetic short cost ratio
        si_to_synth_cost = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_to_synth_cost)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
            
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
            
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 1 (claude) (SVM)
==================================================
Training SVM model...

Iteration 1 (claude) Performance:
MAE: 440979.1953
RMSE: 528179.5622
MAPE: 10.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0045, rank=1
   2. Feature_6_t3: importance=0.0031, rank=2
   3. Feature_24_t1: importance=0.0024, rank=3
   4. Feature_8_t2: importance=0.0020, rank=4
   5. Feature_11_t1: importance=0.0017, rank=5
ðŸ’¾ Saved code for iteration 1 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.13%

ðŸ“ˆ Current best MAPE: 10.23%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 2
======================================================================
ðŸ¤– Calling Claude for iteration 2...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback to avg_daily_volume if needed
        
        # Keep high-importance raw features based on previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for calculating relative metrics
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (high importance in previous iterations)
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (high importance in previous iterations)
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (improved calculation)
        # Measures the rate of change in short interest, critical for prediction
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 4: Days to cover momentum (improved calculation)
        # Measures the rate of change in days to cover
        dtc_momentum = 0.0
        if t > 0 and data[t-1, 2] > 0:
            dtc_momentum = (days_to_cover / max(data[t-1, 2], 1e-8)) - 1.0
        eng.append(dtc_momentum)
        
        # Feature 5: Price momentum (5-day) - refined from previous iteration
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 6: Relative Strength Index (RSI) - refined calculation
        # Momentum oscillator that measures the speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 7: Bollinger Band Width (volatility measure) - refined calculation
        # Measures volatility by calculating the distance between upper and lower bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 8: Average True Range (ATR) - volatility indicator
        # Measures market volatility by decomposing the entire range of an asset price
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
        
        # Feature 9: Synthetic short cost to implied volatility ratio
        # Relationship between cost of shorting and expected volatility
        synth_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_cost_to_iv)
        
        # Feature 10: Short interest to synthetic short cost ratio
        # Compares actual short interest to the cost of creating synthetic shorts
        si_to_synth_cost = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_to_synth_cost)
        
        # Feature 11: Put-call ratio momentum
        # Measures the change in options sentiment
        pc_momentum = 0.0
        if t > 0 and data[t-1, 64] > 0:
            pc_momentum = (put_call_ratio / max(data[t-1, 64], 1e-8)) - 1.0
        eng.append(pc_momentum)
        
        # Feature 12: Implied volatility momentum
        # Measures the change in market's expectation of volatility
        iv_momentum = 0.0
        if t > 0 and data[t-1, 66] > 0:
            iv_momentum = (implied_volatility / max(data[t-1, 66], 1e-8)) - 1.0
        eng.append(iv_momentum)
        
        # Feature 13: OHLC price range relative to close (improved calculation)
        # Measures recent price volatility relative to current price
        if len(close_prices) > 0:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            price_range = (recent_high - recent_low) / max(close_prices[-1], 1e-8)
            eng.append(price_range)
        else:
            eng.append(0.0)
        
        # Feature 14: Volume Oscillator (improved calculation)
        # Measures the relationship between short-term and long-term volume
        if t > 5 and len(close_prices) >= 10:
            short_vol_avg = np.mean([data[max(t-i, 0), 1] for i in range(5)])  # 5-day avg volume
            long_vol_avg = np.mean([data[max(t-i, 0), 1] for i in range(10)])  # 10-day avg volume
            vol_osc = (short_vol_avg - long_vol_avg) / max(long_vol_avg, 1e-8)
            eng.append(vol_osc)
        else:
            eng.append(0.0)
        
        # Feature 15: Short interest to days to cover ratio
        # Relationship between short interest and the time needed to cover
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # Feature 16: Price to implied volatility ratio
        # Relationship between price and expected volatility
        price_to_iv = close_prices[-1] / max(implied_volatility, 1e-8)
        eng.append(price_to_iv)
        
        # Feature 17: Short squeeze potential indicator
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            squeeze_potential = si_to_float * days_to_cover * max(0, price_momentum)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 18: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(si_zscore)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 2 (claude) (SVM)
==================================================
Training SVM model...

Iteration 2 (claude) Performance:
MAE: 443653.2034
RMSE: 532062.9417
MAPE: 10.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0043, rank=1
   2. Feature_21_t3: importance=0.0041, rank=2
   3. Feature_16_t1: importance=0.0018, rank=3
   4. Feature_5_t3: importance=0.0014, rank=4
   5. Feature_6_t0: importance=0.0014, rank=5
ðŸ’¾ Saved code for iteration 2 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.06%

ðŸ“ˆ Current best MAPE: 10.23%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 3
======================================================================
ðŸ¤– Calling Claude for iteration 3...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep high-importance raw features based on previous iterations
        raw_keep.append(short_interest)  # Always keep short interest
        raw_keep.append(avg_daily_volume)  # Always keep average daily volume
        raw_keep.append(days_to_cover)  # Important for short interest prediction
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for calculating relative metrics
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (consistently high importance)
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (improved with exponential weighting)
        # Measures the rate of change in short interest with more weight on recent changes
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            # Apply exponential weighting if we have more history
            if t > 1 and data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
                si_momentum = 0.7 * si_momentum + 0.3 * prev_momentum
        eng.append(si_momentum)
        
        # Feature 4: Relative volume - current volume compared to recent average
        # Indicates unusual trading activity which often precedes short interest changes
        if len(close_prices) >= 5:
            recent_avg_volume = np.mean([data[max(t-i, 0), 1] for i in range(5)])
            rel_volume = avg_daily_volume / max(recent_avg_volume, 1e-8)
            eng.append(rel_volume)
        else:
            eng.append(1.0)  # Neutral value
        
        # Feature 5: Price trend strength (ADX-inspired)
        # Measures the strength of a price trend regardless of direction
        if len(close_prices) >= 10:
            up_moves = np.zeros(9)
            down_moves = np.zeros(9)
            
            for i in range(9):
                price_diff = close_prices[-(i+1)] - close_prices[-(i+2)]
                up_moves[i] = max(price_diff, 0)
                down_moves[i] = max(-price_diff, 0)
            
            avg_up = np.mean(up_moves)
            avg_down = np.mean(down_moves)
            
            if avg_up + avg_down > 0:
                trend_strength = abs(avg_up - avg_down) / (avg_up + avg_down)
            else:
                trend_strength = 0.0
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # Feature 6: Short squeeze risk indicator (refined)
        # Combines short interest, days to cover, and price momentum
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            # Higher values indicate higher squeeze potential
            squeeze_risk = si_to_float * days_to_cover * (1 + max(0, price_momentum))
            eng.append(squeeze_risk)
        else:
            eng.append(0.0)
        
        # Feature 7: Options-based sentiment indicator
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility / 100.0
        eng.append(options_sentiment)
        
        # Feature 8: Short interest utilization rate
        # Percentage of available shares being shorted
        si_utilization = short_interest / max(shares_outstanding, 1e-8) * 100.0
        eng.append(si_utilization)
        
        # Feature 9: Volatility-adjusted price momentum
        # Price momentum normalized by volatility
        if len(close_prices) >= 10:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            price_std = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_momentum = price_momentum / max(price_std, 1e-8)
            eng.append(vol_adj_momentum)
        else:
            eng.append(0.0)
        
        # Feature 10: Short cost efficiency
        # Relationship between synthetic short cost and days to cover
        short_cost_efficiency = synthetic_short_cost * days_to_cover
        eng.append(short_cost_efficiency)
        
        # Feature 11: Normalized price range (improved calculation)
        # Recent price volatility normalized by price level
        if len(close_prices) >= 5:
            recent_high = np.max(high_prices[-5:])
            recent_low = np.min(low_prices[-5:])
            avg_price = np.mean(close_prices[-5:])
            norm_range = (recent_high - recent_low) / max(avg_price, 1e-8)
            eng.append(norm_range)
        else:
            eng.append(0.0)
        
        # Feature 12: Short interest acceleration
        # Second derivative of short interest - rate of change of momentum
        si_acceleration = 0.0
        if t > 1 and data[t-1, 0] > 0 and data[t-2, 0] > 0:
            current_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            previous_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_acceleration = current_momentum - previous_momentum
        eng.append(si_acceleration)
        
        # Feature 13: Implied volatility to historical volatility ratio
        # Compares market expectations to realized volatility
        if len(close_prices) >= 10:
            returns = np.diff(close_prices[-10:]) / close_prices[-11:-1]
            hist_vol = np.std(returns) * np.sqrt(252)  # Annualized
            iv_hv_ratio = implied_volatility / max(hist_vol * 100, 1e-8)  # IV is typically in percentage
            eng.append(iv_hv_ratio)
        else:
            eng.append(1.0)  # Neutral value
        
        # Feature 14: Money Flow Index (MFI) - volume-weighted RSI
        # Indicates buying/selling pressure with volume consideration
        if len(close_prices) >= 14:
            typical_prices = (high_prices[-14:] + low_prices[-14:] + close_prices[-14:]) / 3
            money_flow = typical_prices * avg_daily_volume
            
            delta = np.diff(typical_prices)
            pos_flow = np.sum(money_flow[1:][delta > 0])
            neg_flow = np.sum(money_flow[1:][delta < 0])
            
            if neg_flow > 0:
                money_ratio = pos_flow / max(neg_flow, 1e-8)
                mfi = 100 - (100 / (1 + money_ratio))
            else:
                mfi = 100.0
            eng.append(mfi / 100.0)  # Normalize to 0-1
        else:
            eng.append(0.5)  # Neutral value
        
        # Feature 15: Short interest to options volume ratio
        # Relationship between actual shorts and options activity
        options_volume_proxy = put_call_ratio * avg_daily_volume  # Approximate options volume
        si_to_options = short_interest / max(options_volume_proxy, 1e-8)
        eng.append(si_to_options)
        
        # Feature 16: Composite momentum indicator
        # Combines price, volume, and short interest momentum
        if t > 0 and len(close_prices) >= 5:
            price_mom = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            volume_mom = (avg_daily_volume / max(data[max(t-1, 0), 1], 1e-8)) - 1.0
            si_mom = (short_interest / max(data[max(t-1, 0), 0], 1e-8)) - 1.0
            
            # Weight the components based on their predictive power
            composite_mom = 0.5 * si_mom + 0.3 * price_mom + 0.2 * volume_mom
            eng.append(composite_mom)
        else:
            eng.append(0.0)
        
        # Feature 17: Liquidity-adjusted days to cover
        # Days to cover adjusted for market liquidity conditions
        if t > 5:
            avg_volume_trend = np.mean([data[max(t-i, 0), 1] for i in range(5)]) / max(np.mean([data[max(t-i-5, 0), 1] for i in range(5)]), 1e-8)
            liq_adj_dtc = days_to_cover * (2 - min(avg_volume_trend, 1.5))  # Adjust up if volume decreasing, down if increasing
            eng.append(liq_adj_dtc)
        else:
            eng.append(days_to_cover)
        
        # Feature 18: Short interest z-score (refined calculation)
        # Measures how extreme current short interest is relative to recent history
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
            # Clip extreme values for stability
            si_zscore = max(min(si_zscore, 3.0), -3.0)
            eng.append(si_zscore)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âŒ Error executing generated code: Function failed test execution: operands could not be broadcast together with shapes (9,) (10,) 
âš ï¸ Function execution failed (attempt 1/3)
ðŸ”„ Retrying function execution with error feedback...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data - ensure proper indexing
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data - ensure proper indexing
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep high-importance raw features based on previous iterations
        # Always include short interest and avg volume (required)
        raw_keep.append(short_interest)  # Short interest - critical target-related feature
        raw_keep.append(avg_daily_volume)  # Average daily volume - consistently high importance
        raw_keep.append(days_to_cover)  # Days to cover - key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for calculating relative metrics
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (consistently high importance)
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (improved calculation)
        # Measures the rate of change in short interest
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 4: Days to cover momentum
        # Measures the rate of change in days to cover
        dtc_momentum = 0.0
        if t > 0 and data[t-1, 2] > 0:
            dtc_momentum = (days_to_cover / max(data[t-1, 2], 1e-8)) - 1.0
        eng.append(dtc_momentum)
        
        # Feature 5: Price momentum (5-day)
        # Captures medium-term price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 6: Relative Strength Index (RSI)
        # Momentum oscillator that measures the speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 7: Bollinger Band Width (volatility measure)
        # Measures volatility by calculating the distance between upper and lower bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 8: Average True Range (ATR) - volatility indicator
        # Measures market volatility
        if len(close_prices) >= 2:
            true_ranges = []
            for i in range(1, min(10, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                true_range = max(high_low, high_close, low_close)
                true_ranges.append(true_range)
            atr = np.mean(true_ranges) if true_ranges else 0.0
            eng.append(atr)
        else:
            eng.append(0.0)
        
        # Feature 9: Synthetic short cost to implied volatility ratio
        # Relationship between cost of shorting and expected volatility
        synth_cost_to_iv = synthetic_short_cost / max(implied_volatility, 1e-8)
        eng.append(synth_cost_to_iv)
        
        # Feature 10: Short interest to synthetic short cost ratio
        # Compares actual short interest to the cost of creating synthetic shorts
        si_to_synth_cost = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_to_synth_cost)
        
        # Feature 11: Put-call ratio momentum
        # Measures the change in options sentiment
        pc_momentum = 0.0
        if t > 0 and data[t-1, 64] > 0:
            pc_momentum = (put_call_ratio / max(data[t-1, 64], 1e-8)) - 1.0
        eng.append(pc_momentum)
        
        # Feature 12: Implied volatility momentum
        # Measures the change in market's expectation of volatility
        iv_momentum = 0.0
        if t > 0 and data[t-1, 66] > 0:
            iv_momentum = (implied_volatility / max(data[t-1, 66], 1e-8)) - 1.0
        eng.append(iv_momentum)
        
        # Feature 13: OHLC price range relative to close
        # Measures recent price volatility relative to current price
        if len(close_prices) > 0:
            recent_high = np.max(high_prices[-5:]) if len(high_prices) >= 5 else high_prices[-1]
            recent_low = np.min(low_prices[-5:]) if len(low_prices) >= 5 else low_prices[-1]
            price_range = (recent_high - recent_low) / max(close_prices[-1], 1e-8)
            eng.append(price_range)
        else:
            eng.append(0.0)
        
        # Feature 14: Short interest to days to cover ratio
        # Relationship between short interest and the time needed to cover
        si_to_dtc = short_interest / max(days_to_cover, 1e-8)
        eng.append(si_to_dtc)
        
        # Feature 15: Price to implied volatility ratio
        # Relationship between price and expected volatility
        if len(close_prices) > 0:
            price_to_iv = close_prices[-1] / max(implied_volatility, 1e-8)
            eng.append(price_to_iv)
        else:
            eng.append(0.0)
        
        # Feature 16: Short squeeze potential indicator
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            squeeze_potential = si_to_float * days_to_cover * max(0, price_momentum)
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 17: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(si_zscore)
        else:
            eng.append(0.0)
        
        # Feature 18: Price volatility ratio
        # Compares recent volatility to longer-term volatility
        if len(close_prices) >= 10:
            recent_std = np.std(close_prices[-5:])
            longer_std = np.std(close_prices[-10:])
            vol_ratio = recent_std / max(longer_std, 1e-8)
            eng.append(vol_ratio)
        else:
            eng.append(1.0)  # Neutral value
            
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 3 (claude) (SVM)
==================================================
Training SVM model...

Iteration 3 (claude) Performance:
MAE: 444537.7555
RMSE: 531597.6592
MAPE: 10.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0052, rank=1
   2. Feature_1_t3: importance=0.0038, rank=2
   3. Feature_6_t3: importance=0.0016, rank=3
   4. Feature_5_t3: importance=0.0014, rank=4
   5. Feature_6_t0: importance=0.0013, rank=5
ðŸ’¾ Saved code for iteration 3 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.09%

ðŸ“ˆ Current best MAPE: 10.23%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 4
======================================================================
ðŸ¤– Calling Claude for iteration 4...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep high-importance raw features based on previous iterations
        # Always include short interest and avg volume (required)
        raw_keep.append(short_interest)  # Short interest - critical target-related feature
        raw_keep.append(avg_daily_volume)  # Average daily volume - consistently high importance
        raw_keep.append(days_to_cover)  # Days to cover - key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (consistently high importance)
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (improved calculation)
        # Measures the rate of change in short interest
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 4: Days to cover momentum
        # Measures the rate of change in days to cover
        dtc_momentum = 0.0
        if t > 0 and data[t-1, 2] > 0:
            dtc_momentum = (days_to_cover / max(data[t-1, 2], 1e-8)) - 1.0
        eng.append(dtc_momentum)
        
        # Feature 5: Price momentum (5-day)
        # Captures medium-term price trend
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 6: Relative Strength Index (RSI)
        # Momentum oscillator that measures the speed and change of price movements
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 7: Bollinger Band Width (volatility measure)
        # Measures volatility by calculating the distance between upper and lower bands
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            bb_width = (2 * std) / max(sma, 1e-8)
            eng.append(bb_width)
        else:
            eng.append(0.0)
        
        # Feature 8: Short squeeze potential indicator (refined)
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum))
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 9: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(si_zscore)
        else:
            eng.append(0.0)
        
        # Feature 10: Short interest acceleration
        # Second derivative of short interest - captures acceleration/deceleration
        si_accel = 0.0
        if t >= 2:
            prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 11: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Feature 12: Volatility-adjusted short interest
        # Short interest normalized by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_si = short_interest / max(price_vol, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 13: Short interest trend strength
        # Measures the consistency of short interest changes
        si_trend_strength = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_changes = np.sum(si_diff > 0)
            neg_changes = np.sum(si_diff < 0)
            si_trend_strength = (pos_changes - neg_changes) / max(len(si_diff), 1e-8)
        eng.append(si_trend_strength)
        
        # Feature 14: Price to short interest correlation
        # Measures relationship between price movements and short interest
        price_si_corr = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            historical_close = np.array([data[max(t-i, 0), 3:63].reshape(15, 4)[-1, 3] for i in range(5)])
            
            # Calculate correlation manually to avoid numerical issues
            si_mean = np.mean(historical_si)
            close_mean = np.mean(historical_close)
            
            numerator = np.sum((historical_si - si_mean) * (historical_close - close_mean))
            si_var = np.sum((historical_si - si_mean)**2)
            close_var = np.sum((historical_close - close_mean)**2)
            
            denominator = np.sqrt(max(si_var, 1e-8) * max(close_var, 1e-8))
            price_si_corr = numerator / max(denominator, 1e-8)
        eng.append(price_si_corr)
        
        # Feature 15: Options sentiment indicator
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 16: Short interest coverage ratio
        # Ratio of short interest to average daily volume, adjusted by volatility
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            coverage_ratio = (short_interest / max(avg_daily_volume, 1e-8)) * (1 + price_vol)
            eng.append(coverage_ratio)
        else:
            eng.append(short_interest / max(avg_daily_volume, 1e-8))
        
        # Feature 17: Price trend strength
        # Measures the consistency of price movements
        price_trend = 0.0
        if len(close_prices) >= 10:
            price_diff = np.diff(close_prices[-10:])
            pos_changes = np.sum(price_diff > 0)
            neg_changes = np.sum(price_diff < 0)
            price_trend = (pos_changes - neg_changes) / max(len(price_diff), 1e-8)
        eng.append(price_trend)
        
        # Feature 18: Short interest relative to historical range
        # Measures current short interest position within its historical range
        si_range_position = 0.5  # Default to middle
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_min = np.min(historical_si)
            si_max = np.max(historical_si)
            si_range = si_max - si_min
            if si_range > 0:
                si_range_position = (short_interest - si_min) / max(si_range, 1e-8)
        eng.append(si_range_position)
        
        # Feature 19: Composite momentum indicator
        # Combines price, volume, and short interest momentum
        composite_momentum = 0.0
        if t > 0 and len(close_prices) >= 5:
            price_mom = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            vol_mom = (avg_daily_volume / max(data[t-1, 1], 1e-8)) - 1.0
            composite_momentum = price_mom - si_momentum + vol_mom
        eng.append(composite_momentum)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 4 (claude) (SVM)
==================================================
Training SVM model...

Iteration 4 (claude) Performance:
MAE: 449959.4026
RMSE: 529880.1299
MAPE: 10.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0055, rank=1
   2. Feature_24_t3: importance=0.0026, rank=2
   3. Feature_16_t1: importance=0.0022, rank=3
   4. Feature_16_t2: importance=0.0018, rank=4
   5. Feature_6_t1: importance=0.0018, rank=5
ðŸ’¾ Saved code for iteration 4 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.15%

ðŸ“ˆ Current best MAPE: 10.23%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 5
======================================================================
ðŸ¤– Calling Claude for iteration 5...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0.0
        
        # Keep high-importance raw features based on previous iterations
        # Always include short interest and avg volume (required)
        raw_keep.append(short_interest)  # Short interest - critical target-related feature
        raw_keep.append(avg_daily_volume)  # Average daily volume - consistently high importance
        raw_keep.append(days_to_cover)  # Days to cover - key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for calculating float-related metrics
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        # Measures what percentage of available shares are sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (consistently high importance)
        # Indicates how many days of average volume the short interest represents
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (1-period)
        # Measures the rate of change in short interest - key predictor in previous iterations
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 4: Short interest momentum (2-period)
        # Longer-term trend in short interest changes
        si_momentum_2p = 0.0
        if t > 1 and data[t-2, 0] > 0:
            si_momentum_2p = (short_interest / max(data[t-2, 0], 1e-8)) - 1.0
        eng.append(si_momentum_2p)
        
        # Feature 5: Price momentum (5-day)
        # Medium-term price trend - consistently important in previous iterations
        if len(close_prices) >= 5:
            momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            eng.append(momentum_5d)
        else:
            eng.append(0.0)
        
        # Feature 6: Relative Strength Index (RSI)
        # Momentum oscillator that measures the speed and change of price movements
        # Consistently high importance in previous iterations
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # Feature 7: Short squeeze potential indicator
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        # Refined based on performance in previous iterations
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum))
            eng.append(squeeze_potential)
        else:
            eng.append(0.0)
        
        # Feature 8: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
            eng.append(si_zscore)
        else:
            eng.append(0.0)
        
        # Feature 9: Short interest acceleration
        # Second derivative of short interest - captures acceleration/deceleration
        si_accel = 0.0
        if t >= 2:
            prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 10: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Feature 11: Volatility-adjusted short interest
        # Short interest normalized by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_si = short_interest / max(price_vol, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 12: Options sentiment indicator
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 13: Short interest coverage ratio
        # Ratio of short interest to average daily volume, adjusted by volatility
        # Consistently high importance in previous iterations
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            coverage_ratio = (short_interest / max(avg_daily_volume, 1e-8)) * (1 + price_vol)
            eng.append(coverage_ratio)
        else:
            eng.append(short_interest / max(avg_daily_volume, 1e-8))
        
        # Feature 14: Average True Range (ATR) - Volatility measure
        # More robust volatility measure than simple standard deviation
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, 5):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values)
            # Normalize by price level
            atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 15: MACD Signal Line Crossover
        # Momentum indicator that shows relationship between two moving averages
        macd_signal = 0.0
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            # Signal line (9-day EMA of MACD)
            if t >= 9:
                macd_history = []
                for i in range(9):
                    if t-i >= 0 and len(data[t-i, 3:63].reshape(15, 4)[:, 3]) >= 26:
                        hist_close = data[t-i, 3:63].reshape(15, 4)[:, 3]
                        hist_ema12 = np.mean(hist_close[-12:])
                        hist_ema26 = np.mean(hist_close[-26:])
                        macd_history.append(hist_ema12 - hist_ema26)
                    else:
                        macd_history.append(0)
                signal_line = np.mean(macd_history)
                macd_signal = macd - signal_line
        eng.append(macd_signal)
        
        # Feature 16: On-Balance Volume (OBV) momentum
        # Volume-based indicator that relates volume flow to price changes
        obv_momentum = 0.0
        if t > 0 and len(close_prices) >= 5:
            current_obv = 0
            prev_obv = 0
            
            # Calculate current OBV
            for i in range(5):
                if i < len(close_prices) - 1:
                    volume_val = data[t, 68] if data.shape[1] > 68 else avg_daily_volume
                    if close_prices[-(i+1)] > close_prices[-(i+2)]:
                        current_obv += volume_val
                    elif close_prices[-(i+1)] < close_prices[-(i+2)]:
                        current_obv -= volume_val
            
            # Calculate previous OBV
            if t > 0:
                prev_close_prices = data[t-1, 3:63].reshape(15, 4)[:, 3]
                for i in range(5):
                    if i < len(prev_close_prices) - 1:
                        volume_val = data[t-1, 68] if data.shape[1] > 68 else data[t-1, 1]
                        if prev_close_prices[-(i+1)] > prev_close_prices[-(i+2)]:
                            prev_obv += volume_val
                        elif prev_close_prices[-(i+1)] < prev_close_prices[-(i+2)]:
                            prev_obv -= volume_val
            
            # Calculate OBV momentum
            if prev_obv != 0:
                obv_momentum = (current_obv / max(abs(prev_obv), 1e-8)) - 1.0
        eng.append(obv_momentum)
        
        # Feature 17: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 18: Short interest to synthetic short cost ratio
        # Measures relationship between actual short positions and the cost of synthetic shorts
        si_to_synthetic = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_to_synthetic)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 5 (claude) (SVM)
==================================================
Training SVM model...

Iteration 5 (claude) Performance:
MAE: 453014.5797
RMSE: 539098.5389
MAPE: 10.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0056, rank=1
   2. Feature_1_t0: importance=0.0029, rank=2
   3. Feature_6_t3: importance=0.0027, rank=3
   4. Feature_6_t0: importance=0.0023, rank=4
   5. Feature_16_t1: importance=0.0019, rank=5
ðŸ’¾ Saved code for iteration 5 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.24%

ðŸ“ˆ Current best MAPE: 10.23%
ðŸ”„ Iterations without improvement: 4/5

======================================================================
ITERATION 6
======================================================================
ðŸ¤– Calling Claude for iteration 6...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep consistently high-importance raw features based on previous iterations
        raw_keep.append(short_interest)  # Short interest - critical target-related feature
        raw_keep.append(avg_daily_volume)  # Average daily volume - consistently high importance
        raw_keep.append(days_to_cover)  # Days to cover - key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for calculating float-related metrics
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        # Measures what percentage of available shares are sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest to volume ratio (consistently high importance)
        # Indicates how many days of average volume the short interest represents
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 3: Short interest momentum (1-period)
        # Measures the rate of change in short interest - key predictor in previous iterations
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 4: Price momentum (5-day)
        # Medium-term price trend - consistently important in previous iterations
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # Feature 5: Relative Strength Index (RSI)
        # Momentum oscillator that measures the speed and change of price movements
        # Consistently high importance in previous iterations
        rsi = 50.0  # Neutral RSI value
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
        eng.append(rsi)
        
        # Feature 6: Short squeeze potential indicator
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        # Refined based on performance in previous iterations
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum_5d))
        eng.append(squeeze_potential)
        
        # Feature 7: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        si_zscore = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
        eng.append(si_zscore)
        
        # Feature 8: Short interest acceleration
        # Second derivative of short interest - captures acceleration/deceleration
        si_accel = 0.0
        if t >= 2:
            prev_momentum = 0.0
            if data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 9: Volatility-adjusted short interest
        # Short interest normalized by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_si = short_interest / max(price_vol, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 10: Options sentiment indicator
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 11: Average True Range (ATR) - Volatility measure
        # More robust volatility measure than simple standard deviation
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            if tr_values:
                atr = np.mean(tr_values)
                # Normalize by price level
                atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 12: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 13: Price volatility ratio
        # Compares recent volatility to longer-term volatility
        price_vol_ratio = 1.0
        if len(close_prices) >= 15:
            recent_vol = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            longer_vol = np.std(close_prices) / max(np.mean(close_prices), 1e-8)
            price_vol_ratio = recent_vol / max(longer_vol, 1e-8)
        eng.append(price_vol_ratio)
        
        # Feature 14: Short interest to synthetic short cost efficiency
        # Measures relationship between actual short positions and the cost of synthetic shorts
        si_synthetic_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synthetic_efficiency)
        
        # Feature 15: Bollinger Band position
        # Indicates where the current price is relative to its volatility bands
        bb_position = 0.5  # Middle of the band
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            upper_band = sma + (2 * std)
            lower_band = sma - (2 * std)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
        eng.append(bb_position)
        
        # Feature 16: Short interest relative to historical range
        # Measures where current short interest falls within its historical range
        si_range_position = 0.5  # Middle of range
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_min = np.min(historical_si)
            si_max = np.max(historical_si)
            si_range = max(si_max - si_min, 1e-8)
            si_range_position = (short_interest - si_min) / si_range
        eng.append(si_range_position)
        
        # Feature 17: Price gap significance
        # Measures the significance of overnight price gaps
        gap_significance = 0.0
        if len(close_prices) >= 2 and len(open_prices) >= 1:
            gap = open_prices[-1] - close_prices[-2]
            gap_significance = gap / max(close_prices[-2], 1e-8)
        eng.append(gap_significance)
        
        # Feature 18: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 6 (claude) (SVM)
==================================================
Training SVM model...

Iteration 6 (claude) Performance:
MAE: 436463.3155
RMSE: 523257.6304
MAPE: 10.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0041, rank=1
   2. Feature_24_t1: importance=0.0030, rank=2
   3. Feature_20_t1: importance=0.0016, rank=3
   4. Feature_6_t0: importance=0.0015, rank=4
   5. Feature_24_t3: importance=0.0015, rank=5
ðŸ’¾ Saved code for iteration 6 to iteration_codes dictionary
ðŸŽ‰ IMPROVEMENT! MAPE improved by 0.14%

ðŸ“ˆ Current best MAPE: 10.08%
ðŸ”„ Iterations without improvement: 0/5

======================================================================
ITERATION 7
======================================================================
ðŸ¤– Calling Claude for iteration 7...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_daily_volume  # Fallback if not available
        
        # Keep consistently high-importance raw features based on previous iterations
        # Feature 1_t3 has been consistently the most important across iterations
        raw_keep.append(short_interest)  # Critical target-related feature
        raw_keep.append(avg_daily_volume)  # Consistently high importance
        raw_keep.append(days_to_cover)  # Key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (consistently high importance)
        # Measures what percentage of available shares are sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest momentum (1-period)
        # Measures the rate of change in short interest - key predictor in previous iterations
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 3: Short interest acceleration - second derivative
        # Captures acceleration/deceleration in short interest changes
        si_accel = 0.0
        if t >= 2:
            prev_momentum = 0.0
            if data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 4: Short squeeze potential indicator (refined)
        # Combines short interest, volume, and price momentum to gauge squeeze potential
        # Refined based on performance in previous iterations
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum))
        eng.append(squeeze_potential)
        
        # Feature 5: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        si_zscore = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
        eng.append(si_zscore)
        
        # Feature 6: Short interest relative to historical range
        # Measures where current short interest falls within its historical range
        # Feature 16_t1 and 16_t2 were consistently important in previous iterations
        si_range_position = 0.5  # Middle of range
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_min = np.min(historical_si)
            si_max = np.max(historical_si)
            si_range = max(si_max - si_min, 1e-8)
            si_range_position = (short_interest - si_min) / si_range
        eng.append(si_range_position)
        
        # Feature 7: Relative Strength Index (RSI) - refined calculation
        # Momentum oscillator that measures the speed and change of price movements
        rsi = 50.0  # Neutral RSI value
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gains = np.where(delta > 0, delta, 0)
            losses = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 1e-8
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100.0 - (100.0 / (1.0 + rs))
        eng.append(rsi)
        
        # Feature 8: Price momentum (5-day) - Feature 24_t1 and 24_t3 were important
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # Feature 9: Short interest to volume ratio (refined)
        # Indicates how many days of average volume the short interest represents
        # Adjusted to use more recent volume data
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 10: Options sentiment indicator (refined)
        # Combines put-call ratio with implied volatility - Feature 20_t1 and 20_t3 were important
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 11: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 12: Bollinger Band position (refined)
        # Indicates where the current price is relative to its volatility bands
        bb_position = 0.5  # Middle of the band
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            upper_band = sma + (2 * std)
            lower_band = sma - (2 * std)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
        eng.append(bb_position)
        
        # Feature 13: Average True Range (ATR) - Volatility measure
        # More robust volatility measure than simple standard deviation
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            if tr_values:
                atr = np.mean(tr_values)
                # Normalize by price level
                atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 14: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Feature 15: MACD Signal Line Crossover
        # Momentum indicator that shows the relationship between two moving averages
        macd_signal = 0.0
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            signal = np.mean(close_prices[-9:])  # Simplified signal line
            macd_signal = macd - signal
        eng.append(macd_signal)
        
        # Feature 16: Short interest change relative to price change
        # Measures how short interest changes correlate with price changes
        si_price_correlation = 0.0
        if t >= 3:
            si_change = 0.0
            if data[t-1, 0] > 0:
                si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            
            price_change = 0.0
            if len(close_prices) >= 2:
                price_change = (close_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            
            # Avoid division by zero
            if abs(price_change) > 1e-8:
                si_price_correlation = si_change / price_change
            else:
                si_price_correlation = si_change * 100  # Large value if price unchanged but SI changed
        eng.append(si_price_correlation)
        
        # Feature 17: Short interest to synthetic short cost efficiency (refined)
        # Measures relationship between actual short positions and the cost of synthetic shorts
        si_synthetic_efficiency = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synthetic_efficiency)
        
        # Feature 18: Volatility-adjusted short interest (refined)
        # Short interest normalized by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_si = short_interest / max(price_vol, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 19: Short interest concentration
        # Measures how concentrated short interest is relative to total market cap
        si_concentration = short_interest / max(shares_outstanding * close_prices[-1], 1e-8)
        eng.append(si_concentration)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 7 (claude) (SVM)
==================================================
Training SVM model...

Iteration 7 (claude) Performance:
MAE: 447057.4857
RMSE: 529082.4073
MAPE: 10.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0055, rank=1
   2. Feature_22_t1: importance=0.0024, rank=2
   3. Feature_19_t1: importance=0.0021, rank=3
   4. Feature_5_t2: importance=0.0018, rank=4
   5. Feature_19_t2: importance=0.0016, rank=5
ðŸ’¾ Saved code for iteration 7 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.26%

ðŸ“ˆ Current best MAPE: 10.08%
ðŸ”„ Iterations without improvement: 1/5

======================================================================
ITERATION 8
======================================================================
ðŸ¤– Calling Claude for iteration 8...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Always keep these critical raw features (consistently high importance across iterations)
        raw_keep.append(short_interest)  # Target-related feature
        raw_keep.append(avg_daily_volume)  # Consistently high importance (Feature_1_t3)
        raw_keep.append(days_to_cover)  # Key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio
        # Consistently high importance - measures percentage of available shares sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest momentum (1-period)
        # Key predictor in previous iterations - measures rate of change in short interest
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 3: Short interest acceleration - second derivative
        # Captures acceleration/deceleration in short interest changes
        si_accel = 0.0
        if t >= 2:
            prev_momentum = 0.0
            if data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 4: Improved short squeeze potential indicator
        # Combines short interest, volume, days to cover, and price momentum
        # Refined based on performance in previous iterations
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            volume_ratio = avg_daily_volume / max(np.mean(close_prices[-5:]) * shares_outstanding, 1e-8)
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum)) * volume_ratio
        eng.append(squeeze_potential)
        
        # Feature 5: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        si_zscore = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
        eng.append(si_zscore)
        
        # Feature 6: Short interest relative to historical range
        # Feature 16_t1 and 16_t2 were consistently important in previous iterations
        si_range_position = 0.5
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_min = np.min(historical_si)
            si_max = np.max(historical_si)
            si_range = max(si_max - si_min, 1e-8)
            si_range_position = (short_interest - si_min) / si_range
        eng.append(si_range_position)
        
        # Feature 7: Price momentum (5-day) - Feature 24_t1 and 24_t3 were important
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # Feature 8: Options sentiment indicator (refined)
        # Combines put-call ratio with implied volatility - Feature 20_t1 and 20_t3 were important
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 9: Short interest to volume ratio (refined)
        # Indicates how many days of average volume the short interest represents
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 10: Short interest change relative to price change
        # Measures how short interest changes correlate with price changes
        si_price_correlation = 0.0
        if t >= 3 and len(close_prices) >= 2:
            si_change = 0.0
            if data[t-1, 0] > 0:
                si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            
            price_change = (close_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            
            # Avoid division by zero
            if abs(price_change) > 1e-8:
                si_price_correlation = si_change / price_change
            else:
                si_price_correlation = si_change * 100
        eng.append(si_price_correlation)
        
        # Feature 11: Volatility-adjusted short interest
        # Short interest normalized by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 10:
            price_vol = np.std(close_prices[-10:]) / max(np.mean(close_prices[-10:]), 1e-8)
            vol_adj_si = short_interest / max(price_vol, 1e-8)
        eng.append(vol_adj_si)
        
        # Feature 12: Short interest concentration
        # Measures how concentrated short interest is relative to total market cap
        si_concentration = short_interest / max(shares_outstanding * close_prices[-1], 1e-8)
        eng.append(si_concentration)
        
        # Feature 13: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 14: Bollinger Band position for short interest
        # Indicates where the current short interest is relative to its volatility bands
        si_bb_position = 0.5
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_sma = np.mean(historical_si)
            si_std = np.std(historical_si)
            upper_band = si_sma + (2 * si_std)
            lower_band = max(si_sma - (2 * si_std), 0)  # Short interest can't be negative
            band_width = max(upper_band - lower_band, 1e-8)
            si_bb_position = (short_interest - lower_band) / band_width
        eng.append(si_bb_position)
        
        # Feature 15: Short interest to synthetic short cost ratio
        # Measures relationship between actual short positions and the cost of synthetic shorts
        si_synthetic_ratio = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synthetic_ratio)
        
        # Feature 16: Price volatility (ATR-based)
        # More robust volatility measure than simple standard deviation
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            if tr_values:
                atr = np.mean(tr_values)
                # Normalize by price level
                atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 17: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Feature 18: Short interest weighted by options sentiment
        # Combines short interest with options market sentiment
        si_options_weighted = short_interest * put_call_ratio / max(1.0, 1e-8)
        eng.append(si_options_weighted)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 8 (claude) (SVM)
==================================================
Training SVM model...

Iteration 8 (claude) Performance:
MAE: 462473.8408
RMSE: 550045.4399
MAPE: 10.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0049, rank=1
   2. Feature_23_t1: importance=0.0028, rank=2
   3. Feature_0_t1: importance=0.0024, rank=3
   4. Feature_6_t3: importance=0.0024, rank=4
   5. Feature_21_t1: importance=0.0022, rank=5
ðŸ’¾ Saved code for iteration 8 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.50%

ðŸ“ˆ Current best MAPE: 10.08%
ðŸ”„ Iterations without improvement: 2/5

======================================================================
ITERATION 9
======================================================================
ðŸ¤– Calling Claude for iteration 9...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else 0
        
        # Keep critical raw features based on importance analysis across iterations
        # Feature_1_t3 (avg_daily_volume) has been consistently the most important feature
        raw_keep.append(short_interest)  # Target variable
        raw_keep.append(avg_daily_volume)  # Consistently highest importance
        raw_keep.append(days_to_cover)  # Key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # Consistently high importance - measures percentage of available shares sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest momentum (1-period)
        # Key predictor in previous iterations - measures rate of change in short interest
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 3: Improved short squeeze potential indicator
        # Combines short interest, volume, days to cover, and price momentum
        # Feature_6_t0 and Feature_6_t3 were consistently important
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            volume_ratio = avg_daily_volume / max(np.mean(close_prices[-5:]) * shares_outstanding, 1e-8)
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum)) * volume_ratio
        eng.append(squeeze_potential)
        
        # Feature 4: Short interest relative to historical range (percentile)
        # Feature_16_t1 and Feature_16_t2 were consistently important in previous iterations
        si_range_position = 0.5
        if t >= 10:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(10)])
            si_min = np.min(historical_si)
            si_max = np.max(historical_si)
            si_range = max(si_max - si_min, 1e-8)
            si_range_position = (short_interest - si_min) / si_range
        eng.append(si_range_position)
        
        # Feature 5: Price momentum (5-day) - Feature_24_t1 and Feature_24_t3 were important
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # Feature 6: Options sentiment indicator (refined)
        # Combines put-call ratio with implied volatility - Feature_20_t1 and Feature_20_t3 were important
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 7: Short interest to volume ratio (refined)
        # Similar to days to cover but with different normalization
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 8: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        si_zscore = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si)
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
        eng.append(si_zscore)
        
        # Feature 9: Short interest concentration
        # Measures how concentrated short interest is relative to total market cap
        # Feature_22_t1 was important in iteration 7
        si_concentration = short_interest / max(shares_outstanding * close_prices[-1], 1e-8)
        eng.append(si_concentration)
        
        # Feature 10: Relative Strength Index (RSI) - price momentum indicator
        # New feature to capture price strength/weakness, which can influence short covering
        rsi = 50.0  # Neutral value as default
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss > 0:
                rs = gain / max(loss, 1e-8)
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gain > 0:
                rsi = 100.0
        eng.append(rsi)
        
        # Feature 11: Short interest change vs. price change ratio
        # Measures how short interest responds to price changes
        # Feature_19_t1 and Feature_19_t2 were important in iteration 7
        si_price_ratio = 0.0
        if t > 0 and len(close_prices) >= 2:
            price_change = (close_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            
            if abs(price_change) > 1e-8:
                si_price_ratio = si_change / price_change
            else:
                si_price_ratio = si_change * 100.0
        eng.append(si_price_ratio)
        
        # Feature 12: Bollinger Band position for price
        # Indicates where the current price is relative to its volatility bands
        # New feature to capture potential mean reversion or breakout scenarios
        bb_position = 0.5  # Middle of the band as default
        if len(close_prices) >= 20:
            sma = np.mean(close_prices[-20:])
            std = np.std(close_prices[-20:])
            upper_band = sma + (2 * std)
            lower_band = sma - (2 * std)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
        eng.append(bb_position)
        
        # Feature 13: Short interest acceleration - second derivative
        # Captures acceleration/deceleration in short interest changes
        si_accel = 0.0
        if t >= 2:
            prev_momentum = 0.0
            if data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 14: Price volatility (ATR-based)
        # More robust volatility measure than simple standard deviation
        # Feature_5_t2 and Feature_5_t3 were important in iteration 7
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            if tr_values:
                atr = np.mean(tr_values)
                # Normalize by price level
                atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 15: MACD Signal - trend strength and direction indicator
        # New feature to capture trend strength which affects short covering decisions
        macd = 0.0
        macd_signal = 0.0
        macd_hist = 0.0
        if len(close_prices) >= 26:
            ema12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema26 = np.mean(close_prices[-26:])
            macd = ema12 - ema26
            
            if len(close_prices) >= 35:  # Need 26 + 9 points for signal line
                # Use last 9 MACD values (simplified)
                macd_values = []
                for i in range(9):
                    idx = t - i
                    if idx >= 0:
                        ema12_past = np.mean(data[idx, 3:63].reshape(15, 4)[:, 3][-12:])
                        ema26_past = np.mean(data[idx, 3:63].reshape(15, 4)[:, 3][-26:])
                        macd_values.append(ema12_past - ema26_past)
                
                if macd_values:
                    macd_signal = np.mean(macd_values)
                    macd_hist = macd - macd_signal
        eng.append(macd_hist)
        
        # Feature 16: Short interest to synthetic short cost ratio
        # Measures relationship between actual short positions and the cost of synthetic shorts
        # Feature_23_t1 was important in iteration 8
        si_synthetic_ratio = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synthetic_ratio)
        
        # Feature 17: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 18: Volume Oscillator - volume trend indicator
        # New feature to capture changes in trading volume which can precede short covering
        vol_osc = 0.0
        if len(close_prices) >= 10:
            # Use price data as a proxy for volume trend
            short_vol = np.mean(close_prices[-5:])
            long_vol = np.mean(close_prices[-10:])
            vol_osc = (short_vol / max(long_vol, 1e-8)) - 1.0
        eng.append(vol_osc)
        
        # Feature 19: Short interest to implied volatility ratio
        # Relationship between short positions and expected volatility
        si_to_iv = short_interest / max(implied_volatility, 1e-8)
        eng.append(si_to_iv)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 9 (claude) (SVM)
==================================================
Training SVM model...

Iteration 9 (claude) Performance:
MAE: 445455.7784
RMSE: 534067.5491
MAPE: 10.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0055, rank=1
   2. Feature_24_t0: importance=0.0016, rank=2
   3. Feature_5_t3: importance=0.0014, rank=3
   4. Feature_24_t3: importance=0.0014, rank=4
   5. Feature_4_t1: importance=0.0013, rank=5
ðŸ’¾ Saved code for iteration 9 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.24%

ðŸ“ˆ Current best MAPE: 10.08%
ðŸ”„ Iterations without improvement: 3/5

======================================================================
ITERATION 10
======================================================================
ðŸ¤– Calling Claude for iteration 10...
âœ… Claude response received!

ðŸ“ Claude's Response:
--------------------------------------------------
```python
def construct_features(data):
    RAW_DIM = 68
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features_array = []
    
    for t in range(lookback_window):
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_daily_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        
        # Keep critical raw features based on importance analysis
        # Feature_1_t3 (avg_daily_volume) has been consistently the most important feature
        raw_keep.append(short_interest)  # Target variable
        raw_keep.append(avg_daily_volume)  # Consistently highest importance
        raw_keep.append(days_to_cover)  # Key short interest metric
        raw_keep.append(close_prices[-1])  # Most recent close price
        raw_keep.append(put_call_ratio)  # Options sentiment indicator
        raw_keep.append(implied_volatility)  # Market expectation of volatility
        raw_keep.append(shares_outstanding)  # Important for normalization
        
        # Calculate MAX_NEW after raw_keep is formed
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Feature 1: Short interest to float ratio (normalized short interest)
        # Consistently high importance - measures percentage of available shares sold short
        si_to_float = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_to_float)
        
        # Feature 2: Short interest momentum (1-period)
        # Key predictor in previous iterations - measures rate of change in short interest
        si_momentum = 0.0
        if t > 0 and data[t-1, 0] > 0:
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # Feature 3: Improved short squeeze potential indicator
        # Combines short interest, volume, days to cover, and price momentum
        # Feature_6_t0 and Feature_6_t3 were consistently important
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            volume_ratio = avg_daily_volume / max(np.mean(close_prices[-5:]) * shares_outstanding, 1e-8)
            squeeze_potential = si_to_float * days_to_cover * (1 + max(0, price_momentum)) * volume_ratio
        eng.append(squeeze_potential)
        
        # Feature 4: Price momentum (5-day) - Feature_24_t1 and Feature_24_t3 were important
        # Consistently high importance across iterations
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # Feature 5: Options sentiment indicator (refined)
        # Combines put-call ratio with implied volatility - Feature_20_t1 and Feature_20_t3 were important
        options_sentiment = put_call_ratio * implied_volatility
        eng.append(options_sentiment)
        
        # Feature 6: Short interest to volume ratio (refined)
        # Similar to days to cover but with different normalization
        si_to_volume = short_interest / max(avg_daily_volume, 1e-8)
        eng.append(si_to_volume)
        
        # Feature 7: Normalized short interest (z-score)
        # Measures how extreme the current short interest is compared to recent history
        si_zscore = 0.0
        if t >= 5:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_mean = np.mean(historical_si)
            si_std = np.std(historical_si) if len(historical_si) > 1 else 1.0
            si_zscore = (short_interest - si_mean) / max(si_std, 1e-8)
        eng.append(si_zscore)
        
        # Feature 8: Short interest concentration
        # Measures how concentrated short interest is relative to total market cap
        # Feature_22_t1 was important in iteration 7
        si_concentration = short_interest / max(shares_outstanding * close_prices[-1], 1e-8)
        eng.append(si_concentration)
        
        # Feature 9: Relative Strength Index (RSI) - price momentum indicator
        # New feature to capture price strength/weakness, which can influence short covering
        rsi = 50.0  # Neutral value as default
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss > 0:
                rs = gain / max(loss, 1e-8)
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gain > 0:
                rsi = 100.0
        eng.append(rsi)
        
        # Feature 10: Short interest change vs. price change ratio
        # Measures how short interest responds to price changes
        # Feature_19_t1 and Feature_19_t2 were important in iteration 7
        si_price_ratio = 0.0
        if t > 0 and len(close_prices) >= 2:
            price_change = (close_prices[-1] / max(close_prices[-2], 1e-8)) - 1.0
            si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            
            if abs(price_change) > 1e-8:
                si_price_ratio = si_change / price_change
            else:
                si_price_ratio = si_change * 100.0
        eng.append(si_price_ratio)
        
        # Feature 11: Price volatility (ATR-based)
        # More robust volatility measure than simple standard deviation
        # Feature_5_t2 and Feature_5_t3 were important in iteration 7
        atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else 0)
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            if tr_values:
                atr = np.mean(tr_values)
                # Normalize by price level
                atr = atr / max(close_prices[-1], 1e-8)
        eng.append(atr)
        
        # Feature 12: Short interest to synthetic short cost ratio
        # Measures relationship between actual short positions and the cost of synthetic shorts
        # Feature_23_t1 was important in iteration 8
        si_synthetic_ratio = short_interest / max(synthetic_short_cost, 1e-8)
        eng.append(si_synthetic_ratio)
        
        # Feature 13: Short interest trend consistency
        # Measures the consistency of short interest changes (up or down)
        si_trend_consistency = 0.0
        if t >= 4:
            historical_si = np.array([data[max(t-i, 0), 0] for i in range(5)])
            si_diff = np.diff(historical_si)
            pos_count = np.sum(si_diff > 0)
            neg_count = np.sum(si_diff < 0)
            si_trend_consistency = abs(pos_count - neg_count) / max(len(si_diff), 1e-8)
        eng.append(si_trend_consistency)
        
        # Feature 14: Short interest acceleration - second derivative
        # Captures acceleration/deceleration in short interest changes
        si_accel = 0.0
        if t >= 2:
            prev_momentum = 0.0
            if data[t-2, 0] > 0:
                prev_momentum = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            si_accel = si_momentum - prev_momentum
        eng.append(si_accel)
        
        # Feature 15: Bollinger Band Width - volatility measure
        # New feature to capture market volatility which affects short covering decisions
        bb_width = 0.0
        if len(close_prices) >= 20:
            sma = np.mean(close_prices[-20:])
            std = np.std(close_prices[-20:])
            bb_width = (2 * std) / max(sma, 1e-8)  # Normalized by price level
        eng.append(bb_width)
        
        # Feature 16: Short interest to implied volatility ratio weighted by put-call ratio
        # Enhanced version of Feature 19 from previous iteration
        si_iv_pc_ratio = si_to_float * put_call_ratio / max(implied_volatility, 1e-8)
        eng.append(si_iv_pc_ratio)
        
        # Feature 17: Short interest change rate relative to historical average
        # New feature to capture unusual changes in short interest
        si_change_vs_avg = 0.0
        if t >= 5:
            historical_si_changes = []
            for i in range(1, min(5, t+1)):
                if data[t-i, 0] > 0 and data[t-i-1, 0] > 0:
                    change = (data[t-i, 0] / data[t-i-1, 0]) - 1.0
                    historical_si_changes.append(change)
            
            if historical_si_changes:
                avg_change = np.mean(historical_si_changes)
                if t > 0 and data[t-1, 0] > 0:
                    current_change = (short_interest / data[t-1, 0]) - 1.0
                    si_change_vs_avg = current_change / max(abs(avg_change), 1e-8)
        eng.append(si_change_vs_avg)
        
        # Feature 18: Composite momentum indicator
        # Combines price momentum with volume and short interest trends
        # New feature to capture multiple momentum factors in one indicator
        composite_momentum = 0.0
        if len(close_prices) >= 10 and t > 0:
            price_mom = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1.0
            volume_mom = (avg_daily_volume / max(data[t-1, 1], 1e-8)) - 1.0
            si_mom = si_momentum
            
            # Weight factors based on their typical impact
            composite_momentum = 0.5 * price_mom + 0.3 * volume_mom + 0.2 * si_mom
        eng.append(composite_momentum)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features_array.append(row)
    
    # Stack all rows into a 2D array
    result = np.stack(features_array, axis=0)
    
    # Handle NaN and infinity values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```
--------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Function executed successfully and passed validation!

ðŸ”§ Applying feature selection using claude function...
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Training data shape: (106, 4, 68) -> (106, 4, 25)
Validation data shape: (36, 4, 68) -> (36, 4, 25)

==================================================
Training Iteration 10 (claude) (SVM)
==================================================
Training SVM model...

Iteration 10 (claude) Performance:
MAE: 439010.0694
RMSE: 529019.0979
MAPE: 10.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0042, rank=1
   2. Feature_9_t3: importance=0.0040, rank=2
   3. Feature_6_t3: importance=0.0030, rank=3
   4. Feature_8_t1: importance=0.0029, rank=4
   5. Feature_24_t3: importance=0.0028, rank=5
ðŸ’¾ Saved code for iteration 10 to iteration_codes dictionary
ðŸ“Š No significant improvement. Change: -0.04%

ðŸ“ˆ Current best MAPE: 10.08%
ðŸ”„ Iterations without improvement: 4/5

ðŸŽ¯ Final Evaluation on Test Set (Unseen Data)
======================================================================
Using train+validation data for final model training to maximize data usage
Combined train+val data shape: (142, 4, 68)
Test data shape: (36, 4, 68)

ðŸ“Š BASELINE EVALUATION (Raw Features, Train+Val â†’ Test):

==================================================
Training Baseline Final (Raw Features) (SVM)
==================================================
Training SVM model...

Baseline Final (Raw Features) Performance:
MAE: 463507.1854
RMSE: 641947.2016
MAPE: 9.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 134
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0019, rank=1
   2. Feature_63_t0: importance=0.0013, rank=2
   3. Feature_0_t1: importance=0.0010, rank=3
   4. Feature_66_t1: importance=0.0009, rank=4
   5. Feature_67_t0: importance=0.0008, rank=5
   Baseline MAPE: 9.21%
   Baseline MAE: 463507.1854
   Baseline RMSE: 641947.2016

ðŸ”§ BEST MODEL EVALUATION (Processed Features, Train+Val â†’ Test):
Applying best feature engineering to all data...
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Processed train+val shape: (142, 4, 25)
Processed test shape: (36, 4, 25)

==================================================
Training Best Model Final (Processed Features) (SVM)
==================================================
Training SVM model...

Best Model Final (Processed Features) Performance:
MAE: 511548.2037
RMSE: 661702.9470
MAPE: 9.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0033, rank=1
   2. Feature_24_t1: importance=0.0021, rank=2
   3. Feature_24_t3: importance=0.0018, rank=3
   4. Feature_6_t0: importance=0.0016, rank=4
   5. Feature_20_t1: importance=0.0014, rank=5

ðŸ“Š Best Model Test Set Performance:
   MAPE: 9.78%
   MAE: 511548.2037
   RMSE: 661702.9470

ðŸŽ¯ IMPROVEMENT OVER BASELINE:
   Baseline MAPE: 9.21%
   Best Model MAPE: 9.78%
   Absolute Improvement: -0.57%
   Relative Improvement: -6.2%

======================================================================
ITERATION PERFORMANCE SUMMARY
======================================================================

ðŸ“Š VALIDATION MAPE TREND:
--------------------------------------------------------------------------------
Iteration  Model                     Validation MAPE Improvement from Last
--------------------------------------------------------------------------------
0          Baseline                  10.36           N/A                 
1          Iteration 1               10.23           +0.13%              
2          Iteration 2               10.29           -0.06%              
3          Iteration 3               10.31           -0.09%              
4          Iteration 4               10.38           -0.15%              
5          Iteration 5               10.46           -0.24%              
6          Iteration 6               10.08           +0.14%              
7          Iteration 7               10.35           -0.26%              
8          Iteration 8               10.58           -0.50%              
9          Iteration 9               10.33           -0.24%              
10         Iteration 10              10.13           -0.04%              
--------------------------------------------------------------------------------
ðŸ† Best: Iteration 6 - MAPE: 10.08%
âœ… Saved AROC results to cache/AROC_iterative_results_enhanced.pkl
âœ… Summary report saved for AROC

ðŸŽ‰ Process completed successfully for AROC!

================================================================================
GENERATING UNIVERSAL FEATURE ENGINEERING CODE
================================================================================
Successfully processed 12 tickers: ABCB, EIG, FSS, ABM, IART, SRPT, EXTR, SCSC, SLG, HL, ANDE, AROC

ðŸ¤– Calling Claude to generate universal feature engineering code...
âœ… Universal code response received!

ðŸ“ Claude's Universal Feature Engineering Code:
------------------------------------------------------------
# Universal Feature Engineering for Short Interest Prediction

I've synthesized a universal feature construction function that captures the strongest predictive signals across all tickers while maintaining a compact feature set. This implementation balances feature importance, non-redundancy, and computational efficiency.

```python
def construct_features(data):
    """
    Universal feature engineering function for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 68+)
        
    Returns:
        numpy array of shape (lookback_window, 25)
    """
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options and volume data
        put_call_ratio = data[t, 64]
        synthetic_short_cost = data[t, 65]
        implied_volatility = data[t, 66]
        shares_outstanding = data[t, 67]
        volume = data[t, 68] if data.shape[1] > 68 else avg_volume  # Fallback if not available
        
        # Keep essential raw features (consistently important across all tickers)
        raw_keep.append(short_interest)       # Always keep short interest (target-related)
        raw_keep.append(avg_volume)           # Always keep volume (consistently important)
        raw_keep.append(days_to_cover)        # Important for short squeeze potential
        raw_keep.append(close_prices[-1])     # Most recent close price
        raw_keep.append(synthetic_short_cost) # High importance in multiple tickers
        raw_keep.append(implied_volatility)   # Important for options-based signals
        raw_keep.append(shares_outstanding)   # Important for normalization
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # --- ENGINEERED FEATURES ---
        
        # 1. Short Interest to Float Ratio (universally important)
        # Measures what percentage of available shares are sold short
        si_float_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short Interest to Volume Ratio (universally important)
        # Indicates how many days of average volume the short interest represents
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 3. Short Interest Momentum (change rate)
        # Captures trend in short interest - highly significant across tickers
        si_momentum = 0.0
        if t > 0:
            prev_si = data[t-1, 0]
            si_momentum = (short_interest / max(prev_si, 1e-8)) - 1.0
        eng.append(si_momentum)
        
        # 4. Short Interest Acceleration (second derivative)
        # Captures acceleration in short interest changes
        si_acceleration = 0.0
        if t >= 2:
            prev_si_change = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1.0
            current_si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1.0
            si_acceleration = current_si_change - prev_si_change
        eng.append(si_acceleration)
        
        # 5. Price Momentum (5-day)
        # Captures recent price direction - high importance across tickers
        price_momentum_5d = 0.0
        if len(close_prices) >= 5:
            price_momentum_5d = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
        eng.append(price_momentum_5d)
        
        # 6. RSI (Relative Strength Index)
        # Momentum oscillator - consistently important across tickers
        rsi = 50.0  # Default to neutral
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-14:])
            gain = np.sum(np.where(delta > 0, delta, 0))
            loss = np.sum(np.where(delta < 0, -delta, 0))
            
            if loss > 1e-8:
                rs = gain / loss
                rsi = 100.0 - (100.0 / (1.0 + rs))
            elif gain > 0:
                rsi = 100.0
        eng.append(rsi / 100.0)  # Normalize to [0,1]
        
        # 7. Short Squeeze Potential Indicator
        # Combines multiple factors that contribute to short squeeze potential
        squeeze_potential = 0.0
        if len(close_prices) >= 5:
            # Price momentum component (only consider positive momentum for squeeze)
            price_factor = max(0, price_momentum_5d)
            
            # Combine factors with appropriate weights
            squeeze_potential = si_float_ratio * days_to_cover * (1 + price_factor)
            
            # Apply non-linear transformation to emphasize extreme values
            squeeze_potential = np.tanh(squeeze_potential)  # Bound between -1 and 1
        eng.append(squeeze_potential)
        
        # 8. Volatility-Adjusted Short Interest
        # Normalizes short interest by price volatility
        vol_adj_si = 0.0
        if len(close_prices) >= 5:
            returns = np.diff(close_prices[-5:]) / np.maximum(close_prices[-5:-1], 1e-8)
            price_volatility = np.std(returns) if len(returns) > 0 else 0.01
            vol_adj_si = si_float_ratio / max(price_volatility, 0.01)
            vol_adj_si = np.tanh(vol_adj_si)  # Bound extreme values
        eng.append(vol_adj_si)
        
        # 9. Bollinger Band Position
        # Where is the current price relative to its volatility bands?
        bb_position = 0.5  # Default to middle
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            if std > 1e-8:
                upper_band = sma + (2 * std)
                lower_band = sma - (2 * std)
                band_range = upper_band - lower_band
                bb_position = (close_prices[-1] - lower_band) / max(band_range, 1e-8)
                bb_position = min(max(bb_position, 0), 1)  # Ensure in [0,1]
        eng.append(bb_position)
        
        # 10. Options Sentiment Indicator
        # Combines put-call ratio with implied volatility
        options_sentiment = put_call_ratio * implied_volatility
        options_sentiment = np.tanh(options_sentiment)  # Bound extreme values
        eng.append(options_sentiment)
        
        # 11. Short Interest to Implied Volatility Ratio
        # Relates short interest to market's expectation of volatility
        si_iv_ratio = short_interest / max(implied_volatility, 1e-8)
        si_iv_ratio = np.tanh(np.log1p(si_iv_ratio))  # Log transform and bound
        eng.append(si_iv_ratio)
        
        # 12. Short Interest Divergence from Price Trend
        # Captures when short interest moves contrary to price (potential reversal signal)
        si_price_divergence = 0.0
        if t > 0 and len(close_prices) >= 5:
            si_change = (short_interest - data[t-1, 0]) / max(data[t-1, 0], 1e-8)
            price_change = (close_prices[-1] - close_prices[-5]) / max(close_prices[-5], 1e-8)
            
            # Divergence occurs when signs differ (one positive, one negative)
            si_price_divergence = -np.tanh(si_change * price_change * 5)  # Negative to highlight divergence
        eng.append(si_price_divergence)
        
        # 13. Composite Short Pressure Indicator
        # Combines multiple short-related metrics
        short_pressure = (si_float_ratio + si_volume_ratio + (synthetic_short_cost / max(close_prices[-1], 1e-8))) / 3
        short_pressure = np.tanh(short_pressure * 3)  # Bound extreme values
        eng.append(short_pressure)
        
        # 14. ATR-based Volatility (normalized)
        # More robust volatility measure than simple standard deviation
        norm_atr = 0.0
        if len(close_prices) >= 5:
            tr_values = []
            for i in range(1, min(5, len(close_prices))):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)] if i+1 < len(close_prices) else close_prices[-i])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values) if tr_values else 0.0
            # Normalize ATR by price for better comparability
            norm_atr = atr / max(close_prices[-1], 1e-8)
        eng.append(norm_atr)
        
        # 15. Short Interest Efficiency
        # Measures how efficiently shorts are positioned relative to price movement
        si_efficiency = 0.0
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            prev_short_interest = data[t-1, 0]
            si_change = (short_interest / max(prev_short_interest, 1e-8)) - 1.0
            # Negative correlation between price and SI changes indicates efficient shorting
            si_efficiency = -1 * si_change * price_change
            si_efficiency = np.tanh(si_efficiency * 3)  # Bound extreme values
        eng.append(si_efficiency)
        
        # 16. MACD Signal (simplified)
        # Trend-following momentum indicator
        macd_signal = 0.0
        if len(close_prices) >= 12:
            ema12 = np.mean(close_prices[-12:])  # Simple approximation of EMA
            ema26 = np.mean(close_prices[-min(26, len(close_prices)):])
            macd = ema12 - ema26
            # Normalize by price
            macd_signal = macd / max(close_prices[-1], 1e-8)
        eng.append(macd_signal)
        
        # 17. Short Interest Concentration
        # Measures how concentrated short interest is relative to float and days to cover
        si_concentration = si_float_ratio * days_to_cover
        si_concentration = np.tanh(si_concentration)  # Bound extreme values
        eng.append(si_concentration)
        
        # 18. Synthetic Short Cost Momentum
        # Captures changes in the cost of shorting
        cost_momentum = 0.0
        if t > 0:
            prev_cost = data[t-1, 65]
            cost_momentum = (synthetic_short_cost / max(prev_cost, 1e-8)) - 1.0
            cost_momentum = np.tanh(cost_momentum * 5)  # Bound extreme values
        eng.append(cost_momentum)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

## Key Design Principles

1. **Essential Raw Features**: I've kept the most consistently important raw features across all tickers, including short interest, volume metrics, recent price, and key options data.

2. **Universal Predictive Signals**: The engineered features capture the strongest predictive signals that appeared consistently across multiple tickers:
   - Short interest relative to float and volume
   - Short interest momentum and acceleration
   - Price momentum and technical indicators
   - Options market sentiment
   - Volatility-adjusted metrics

3. **Robust Normalization**: All features use robust normalization techniques to handle extreme values and ensure numerical stability:
   - Division by maximum of value or small epsilon (1e-8)
   - Tanh transformations to bound extreme values
   - Log transformations for highly skewed distributions

4. **Non-Redundancy**: Features are designed to capture different aspects of short interest dynamics without excessive correlation.

5. **Computational Efficiency**: The implementation is optimized for speed with fallback mechanisms when data is limited.

This universal feature engineering function should provide strong predictive performance across a wide range of tickers while maintaining a compact, interpretable feature set.
------------------------------------------------------------
âœ… Successfully Extracted Function Code!
Test output shape: (4, 25)
âœ… Universal function extracted and validated successfully!
ðŸ’¾ Universal code saved to: cache/universal_feature_engineering_code.py
ðŸ’¾ Comprehensive results saved to: cache/comprehensive_multi_ticker_results.pkl

================================================================================
ITERATIVE PROCESS SUMMARY
================================================================================

ABCB:
  Best MAPE: 8.26%
  Improvement: 1.66%
  Feature count: 25
  Iterations: 6

EIG:
  Best MAPE: 14.67%
  Improvement: 0.99%
  Feature count: 25
  Iterations: 8

FSS:
  Best MAPE: 12.60%
  Improvement: -0.83%
  Feature count: 25
  Iterations: 5

ABM:
  Best MAPE: 14.63%
  Improvement: 0.02%
  Feature count: 25
  Iterations: 5

IART:
  Best MAPE: 9.27%
  Improvement: 0.10%
  Feature count: 25
  Iterations: 4

SRPT:
  Best MAPE: 8.21%
  Improvement: 0.99%
  Feature count: 25
  Iterations: 9

EXTR:
  Best MAPE: 7.94%
  Improvement: -0.33%
  Feature count: 25
  Iterations: 5

SCSC:
  Best MAPE: 13.23%
  Improvement: 1.75%
  Feature count: 25
  Iterations: 10

SLG:
  Best MAPE: 6.00%
  Improvement: -0.26%
  Feature count: 25
  Iterations: 5

HL:
  Best MAPE: 8.83%
  Improvement: 1.61%
  Feature count: 25
  Iterations: 9

ANDE:
  Best MAPE: 14.76%
  Improvement: 1.63%
  Feature count: 25
  Iterations: 7

AROC:
  Best MAPE: 10.08%
  Improvement: 0.27%
  Feature count: 25
  Iterations: 10

================================================================================
STARTING VALIDATION PHASE
================================================================================
Testing universal feature engineering on all available tickers...

================================================================================
TESTING UNIVERSAL FEATURE ENGINEERING ON MULTIPLE TICKERS
================================================================================
Testing on 464 tickers: AAP, AAT, ABCB, ABG, ABM, ABR, ACAD, ACHC, ACIW, ACLS, ADMA, ADNT, ADUS, AEIS, AEO, AGO, AGYS, AHH, AIN, AIR, AKR, AL, ALEX, ALG, ALGT, ALKS, ALRM, AMN, AMPH, AMSF, AMWD, ANDE, ANGI, ANIP, AOSL, APAM, APLE, APOG, ARCB, ARI, AROC, ARR, ARWR, ASIX, ASTE, ATEN, ATGE, AVA, AWI, AWR, AXL, AZZ, BANC, BANF, BANR, BCC, BCPC, BDN, BFS, BHE, BJRI, BKE, BKU, BL, BLFS, BLMN, BMI, BOH, BOOT, BOX, BRC, BTU, BWA, BXMT, CABO, CAKE, CAL, CALM, CALX, CARG, CARS, CASH, CATY, CBRL, CBU, CC, CCOI, CCS, CE, CENT, CENTA, CENX, CEVA, CFFN, CHCO, CHEF, CLB, CNK, CNMD, CNS, CNXN, COHU, COLL, CORT, CPF, CPK, CPRX, CRI, CRK, CRVL, CSGS, CTRE, CTS, CUBI, CVBF, CVCO, CVI, CWT, CXW, CZR, DAN, DCOM, DEA, DEI, DFIN, DGII, DIOD, DLX, DNOW, DORM, DRH, DVAX, DXC, DXPE, DY, EAT, ECPG, EFC, EGBN, EIG, ENPH, ENR, ENVA, EPC, ESE, ETSY, EVTC, EXPI, EXTR, EYE, EZPW, FBK, FBNC, FBP, FCF, FCPT, FDP, FELE, FFBC, FHB, FIZZ, FMC, FORM, FOXF, FRPT, FSS, FUL, FULT, FUN, FWRD, GBX, GDEN, GEO, GES, GFF, GIII, GKOS, GNL, GNW, GOGO, GOLF, GPI, GRBK, GTY, GVA, HAFC, HASI, HBI, HCC, HCI, HCSG, HELE, HFWA, HI, HIW, HL, HLIT, HLX, HMN, HNI, HOPE, HP, HSII, HSTM, HTH, HTLD, HUBG, HWKN, HZO, IAC, IART, IBP, ICHR, ICUI, IDCC, IIIN, IIPR, INDB, INN, INSW, INVA, IOSP, IPAR, ITGR, ITRI, JBGS, JBLU, JBSS, JJSF, JOE, KAI, KALU, KAR, KFY, KLIC, KMT, KN, KOP, KREF, KRYS, KSS, KW, KWR, LCII, LEG, LGIH, LGND, LKFN, LMAT, LNC, LNN, LPG, LQDT, LRN, LTC, LXP, LZB, MAC, MAN, MARA, MATW, MATX, MC, MCRI, MCY, MD, MDU, MGEE, MGPI, MHO, MKTX, MMI, MMSI, MNRO, MPW, MRCY, MRTN, MSEX, MTH, MTRN, MTX, MWA, MXL, MYGN, MYRG, NAVI, NBHC, NBTB, NEO, NEOG, NGVT, NHC, NMIH, NOG, NPK, NPO, NSIT, NTCT, NWBI, NWL, NWN, NX, NXRT, OFG, OI, OII, OMCL, OSIS, OTTR, OUT, OXM, PAHC, PARR, PATK, PBH, PBI, PCRX, PDFS, PEB, PENN, PFBC, PFS, PI, PINC, PJT, PLAB, PLAY, PLUS, PLXS, PMT, POWL, PRA, PRAA, PRGS, PRK, PRLB, PSMT, PTEN, PTGX, PZZA, QDEL, QNST, QRVO, QTWO, RDN, RDNT, RES, REX, RGR, RHI, RHP, RNST, ROCK, ROG, RUN, RUSHA, RWT, SAFE, SABR, SAFT, SAH, SANM, SBCF, SBH, SBSI, SCHL, SCL, SCSC, SCVL, SEDG, SEE, SEM, SFBS, SFNC, SHAK, SHEN, SHO, SHOO, SIG, SKT, SKY, SKYW, SLG, SM, SMP, SMPL, SMTC, SNDR, SPSC, SPXC, SRPT, SSTK, STAA, STBA, STC, STRA, STRL, SUPN, SXC, SXI, SXT, TBBK, TDC, TDS, TDW, TFX, TGNA, TGTX, THRM, THS, TILE, TMP, TNC, TNDM, TPH, TR, TRIP, TRMK, TRN, TRNO, TRST, TRUP, TTMI, TWI, TWO, UCTT, UE, UFCS, UFPT, UHT, UNF, UNFI, UNIT, URBN, USNA, USPH, UTL, UVV, VBTX, VCEL, VCYT, VECO, VIAV, VICR, VIRT, VRTS, VSAT, VSH, WABC, WAFD, WD, WDFC, WEN, WERN, WGO, WOR, WRLD, WSC, WSFS, WSR, WWW, XHR, XNCR, YELP

============================================================
TESTING TICKER 1/464: AAP
============================================================
ðŸ“Š Loading data for AAP...
ðŸ“Š Loading data for AAP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing AAP: 'AAP'

============================================================
TESTING TICKER 2/464: AAT
============================================================
ðŸ“Š Loading data for AAT...
ðŸ“Š Loading data for AAT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AAT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AAT...

==================================================
Training Baseline AAT (SVM)
==================================================
Training SVM model...

Baseline AAT Performance:
MAE: 156601.4522
RMSE: 207983.1785
MAPE: 15.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0023, rank=1
   2. Feature_64_t1: importance=0.0018, rank=2
   3. Feature_2_t3: importance=0.0014, rank=3
   4. Feature_0_t3: importance=0.0014, rank=4
   5. Feature_65_t1: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for AAT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AAT...

==================================================
Training Enhanced AAT (SVM)
==================================================
Training SVM model...

Enhanced AAT Performance:
MAE: 145149.2343
RMSE: 187872.5575
MAPE: 14.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0011, rank=1
   2. Feature_16_t1: importance=0.0011, rank=2
   3. Feature_9_t3: importance=0.0011, rank=3
   4. Feature_0_t3: importance=0.0010, rank=4
   5. Feature_23_t3: importance=0.0010, rank=5

ðŸ“Š AAT Results:
  Baseline MAPE: 15.57%
  Enhanced MAPE: 14.06%
  MAPE Improvement: +1.51% (+9.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 3/464: ABCB
============================================================
ðŸ“Š Loading data for ABCB...
ðŸ“Š Loading data for ABCB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABCB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ABCB...

==================================================
Training Baseline ABCB (SVM)
==================================================
Training SVM model...

Baseline ABCB Performance:
MAE: 127805.4723
RMSE: 173206.5231
MAPE: 12.32%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 136
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0006, rank=1
   2. Feature_0_t3: importance=0.0005, rank=2
   3. Feature_0_t1: importance=0.0002, rank=3
   4. Feature_1_t3: importance=0.0002, rank=4
   5. Feature_2_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for ABCB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ABCB...

==================================================
Training Enhanced ABCB (SVM)
==================================================
Training SVM model...

Enhanced ABCB Performance:
MAE: 128428.2463
RMSE: 156826.6442
MAPE: 12.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0004, rank=1
   2. Feature_6_t3: importance=0.0003, rank=2
   3. Feature_13_t3: importance=0.0002, rank=3
   4. Feature_12_t1: importance=0.0002, rank=4
   5. Feature_0_t3: importance=0.0002, rank=5

ðŸ“Š ABCB Results:
  Baseline MAPE: 12.32%
  Enhanced MAPE: 12.02%
  MAPE Improvement: +0.31% (+2.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 4/464: ABG
============================================================
ðŸ“Š Loading data for ABG...
ðŸ“Š Loading data for ABG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ABG...

==================================================
Training Baseline ABG (SVM)
==================================================
Training SVM model...

Baseline ABG Performance:
MAE: 83217.8374
RMSE: 111613.7824
MAPE: 4.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 30
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t2: importance=0.0001, rank=1
   2. Feature_2_t0: importance=0.0001, rank=2
   3. Feature_64_t3: importance=0.0001, rank=3
   4. Feature_1_t1: importance=0.0001, rank=4
   5. Feature_66_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for ABG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ABG...

==================================================
Training Enhanced ABG (SVM)
==================================================
Training SVM model...

Enhanced ABG Performance:
MAE: 81502.1334
RMSE: 106880.1336
MAPE: 4.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t2: importance=0.0001, rank=1
   2. Feature_12_t3: importance=0.0001, rank=2
   3. Feature_10_t3: importance=0.0001, rank=3
   4. Feature_22_t1: importance=0.0000, rank=4
   5. Feature_16_t0: importance=0.0000, rank=5

ðŸ“Š ABG Results:
  Baseline MAPE: 4.91%
  Enhanced MAPE: 4.74%
  MAPE Improvement: +0.17% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 5/464: ABM
============================================================
ðŸ“Š Loading data for ABM...
ðŸ“Š Loading data for ABM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ABM...

==================================================
Training Baseline ABM (SVM)
==================================================
Training SVM model...

Baseline ABM Performance:
MAE: 208682.8330
RMSE: 270310.4956
MAPE: 12.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 112
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t0: importance=0.0008, rank=1
   2. Feature_65_t0: importance=0.0007, rank=2
   3. Feature_66_t3: importance=0.0007, rank=3
   4. Feature_1_t2: importance=0.0007, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ABM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ABM...

==================================================
Training Enhanced ABM (SVM)
==================================================
Training SVM model...

Enhanced ABM Performance:
MAE: 249366.7385
RMSE: 305181.7260
MAPE: 14.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0015, rank=1
   2. Feature_21_t3: importance=0.0010, rank=2
   3. Feature_23_t0: importance=0.0009, rank=3
   4. Feature_13_t2: importance=0.0008, rank=4
   5. Feature_13_t0: importance=0.0007, rank=5

ðŸ“Š ABM Results:
  Baseline MAPE: 12.59%
  Enhanced MAPE: 14.74%
  MAPE Improvement: -2.15% (-17.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 6/464: ABR
============================================================
ðŸ“Š Loading data for ABR...
ðŸ“Š Loading data for ABR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ABR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ABR...

==================================================
Training Baseline ABR (SVM)
==================================================
Training SVM model...

Baseline ABR Performance:
MAE: 3020790.5040
RMSE: 3991362.6773
MAPE: 4.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 131
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0013, rank=1
   2. Feature_67_t3: importance=0.0010, rank=2
   3. Feature_65_t3: importance=0.0007, rank=3
   4. Feature_2_t0: importance=0.0007, rank=4
   5. Feature_2_t1: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for ABR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ABR...

==================================================
Training Enhanced ABR (SVM)
==================================================
Training SVM model...

Enhanced ABR Performance:
MAE: 4718363.0211
RMSE: 5173031.1234
MAPE: 7.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0013, rank=1
   2. Feature_21_t2: importance=0.0011, rank=2
   3. Feature_6_t3: importance=0.0010, rank=3
   4. Feature_15_t1: importance=0.0009, rank=4
   5. Feature_16_t0: importance=0.0007, rank=5

ðŸ“Š ABR Results:
  Baseline MAPE: 4.80%
  Enhanced MAPE: 7.50%
  MAPE Improvement: -2.70% (-56.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 7/464: ACAD
============================================================
ðŸ“Š Loading data for ACAD...
ðŸ“Š Loading data for ACAD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing ACAD: 'ACAD'

============================================================
TESTING TICKER 8/464: ACHC
============================================================
ðŸ“Š Loading data for ACHC...
ðŸ“Š Loading data for ACHC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ACHC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ACHC...

==================================================
Training Baseline ACHC (SVM)
==================================================
Training SVM model...

Baseline ACHC Performance:
MAE: 454729.0036
RMSE: 636306.2862
MAPE: 8.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 32

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0004, rank=1
   2. Feature_1_t3: importance=0.0001, rank=2
   3. Feature_67_t3: importance=0.0001, rank=3
   4. Feature_64_t0: importance=0.0001, rank=4
   5. Feature_2_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for ACHC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ACHC...

==================================================
Training Enhanced ACHC (SVM)
==================================================
Training SVM model...

Enhanced ACHC Performance:
MAE: 468193.1428
RMSE: 640382.2865
MAPE: 8.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0003, rank=1
   2. Feature_10_t3: importance=0.0002, rank=2
   3. Feature_12_t2: importance=0.0001, rank=3
   4. Feature_15_t3: importance=0.0001, rank=4
   5. Feature_8_t2: importance=0.0001, rank=5

ðŸ“Š ACHC Results:
  Baseline MAPE: 8.13%
  Enhanced MAPE: 8.16%
  MAPE Improvement: -0.04% (-0.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 9/464: ACIW
============================================================
ðŸ“Š Loading data for ACIW...
ðŸ“Š Loading data for ACIW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ACIW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ACIW...

==================================================
Training Baseline ACIW (SVM)
==================================================
Training SVM model...

Baseline ACIW Performance:
MAE: 154331.1485
RMSE: 195359.4258
MAPE: 6.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 131
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0010, rank=1
   2. Feature_63_t1: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0003, rank=3
   4. Feature_66_t3: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ACIW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ACIW...

==================================================
Training Enhanced ACIW (SVM)
==================================================
Training SVM model...

Enhanced ACIW Performance:
MAE: 139074.8980
RMSE: 173960.2581
MAPE: 5.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0012, rank=1
   2. Feature_21_t1: importance=0.0004, rank=2
   3. Feature_1_t1: importance=0.0003, rank=3
   4. Feature_18_t1: importance=0.0003, rank=4
   5. Feature_21_t3: importance=0.0003, rank=5

ðŸ“Š ACIW Results:
  Baseline MAPE: 6.29%
  Enhanced MAPE: 5.68%
  MAPE Improvement: +0.61% (+9.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 10/464: ACLS
============================================================
ðŸ“Š Loading data for ACLS...
ðŸ“Š Loading data for ACLS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ACLS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ACLS...

==================================================
Training Baseline ACLS (SVM)
==================================================
Training SVM model...

Baseline ACLS Performance:
MAE: 270270.9890
RMSE: 470148.2258
MAPE: 9.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0009, rank=1
   2. Feature_2_t1: importance=0.0008, rank=2
   3. Feature_2_t3: importance=0.0005, rank=3
   4. Feature_66_t1: importance=0.0005, rank=4
   5. Feature_63_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ACLS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ACLS...

==================================================
Training Enhanced ACLS (SVM)
==================================================
Training SVM model...

Enhanced ACLS Performance:
MAE: 244276.9588
RMSE: 479128.6892
MAPE: 7.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t1: importance=0.0007, rank=1
   2. Feature_15_t1: importance=0.0007, rank=2
   3. Feature_5_t2: importance=0.0006, rank=3
   4. Feature_5_t3: importance=0.0006, rank=4
   5. Feature_12_t3: importance=0.0005, rank=5

ðŸ“Š ACLS Results:
  Baseline MAPE: 9.23%
  Enhanced MAPE: 7.68%
  MAPE Improvement: +1.55% (+16.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 11/464: ADMA
============================================================
ðŸ“Š Loading data for ADMA...
ðŸ“Š Loading data for ADMA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing ADMA: 'ADMA'

============================================================
TESTING TICKER 12/464: ADNT
============================================================
ðŸ“Š Loading data for ADNT...
ðŸ“Š Loading data for ADNT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing ADNT: 'ADNT'

============================================================
TESTING TICKER 13/464: ADUS
============================================================
ðŸ“Š Loading data for ADUS...
ðŸ“Š Loading data for ADUS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ADUS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ADUS...

==================================================
Training Baseline ADUS (SVM)
==================================================
Training SVM model...

Baseline ADUS Performance:
MAE: 50848.2026
RMSE: 62842.0149
MAPE: 10.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 105
   â€¢ Highly important features (top 5%): 17

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0026, rank=1
   2. Feature_67_t3: importance=0.0021, rank=2
   3. Feature_1_t3: importance=0.0011, rank=3
   4. Feature_0_t3: importance=0.0011, rank=4
   5. Feature_65_t0: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for ADUS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ADUS...

==================================================
Training Enhanced ADUS (SVM)
==================================================
Training SVM model...

Enhanced ADUS Performance:
MAE: 59032.2681
RMSE: 73991.8805
MAPE: 12.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0033, rank=1
   2. Feature_14_t3: importance=0.0021, rank=2
   3. Feature_19_t3: importance=0.0021, rank=3
   4. Feature_7_t1: importance=0.0017, rank=4
   5. Feature_1_t3: importance=0.0016, rank=5

ðŸ“Š ADUS Results:
  Baseline MAPE: 10.50%
  Enhanced MAPE: 12.18%
  MAPE Improvement: -1.68% (-16.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 14/464: AEIS
============================================================
ðŸ“Š Loading data for AEIS...
ðŸ“Š Loading data for AEIS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AEIS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AEIS...

==================================================
Training Baseline AEIS (SVM)
==================================================
Training SVM model...

Baseline AEIS Performance:
MAE: 195737.7572
RMSE: 242096.3690
MAPE: 7.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0008, rank=1
   2. Feature_65_t2: importance=0.0006, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_64_t2: importance=0.0004, rank=4
   5. Feature_0_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for AEIS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AEIS...

==================================================
Training Enhanced AEIS (SVM)
==================================================
Training SVM model...

Enhanced AEIS Performance:
MAE: 120902.7426
RMSE: 157086.7767
MAPE: 4.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t2: importance=0.0005, rank=1
   2. Feature_7_t0: importance=0.0004, rank=2
   3. Feature_15_t1: importance=0.0004, rank=3
   4. Feature_12_t1: importance=0.0003, rank=4
   5. Feature_6_t0: importance=0.0003, rank=5

ðŸ“Š AEIS Results:
  Baseline MAPE: 7.86%
  Enhanced MAPE: 4.84%
  MAPE Improvement: +3.02% (+38.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 15/464: AEO
============================================================
ðŸ“Š Loading data for AEO...
ðŸ“Š Loading data for AEO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AEO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AEO...

==================================================
Training Baseline AEO (SVM)
==================================================
Training SVM model...

Baseline AEO Performance:
MAE: 1884320.7141
RMSE: 2495213.5027
MAPE: 10.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 100
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0008, rank=1
   2. Feature_65_t1: importance=0.0005, rank=2
   3. Feature_0_t3: importance=0.0004, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_65_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AEO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AEO...

==================================================
Training Enhanced AEO (SVM)
==================================================
Training SVM model...

Enhanced AEO Performance:
MAE: 1868835.3826
RMSE: 2428195.2238
MAPE: 10.89%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t2: importance=0.0011, rank=1
   2. Feature_21_t2: importance=0.0006, rank=2
   3. Feature_11_t3: importance=0.0005, rank=3
   4. Feature_15_t2: importance=0.0005, rank=4
   5. Feature_11_t2: importance=0.0004, rank=5

ðŸ“Š AEO Results:
  Baseline MAPE: 10.98%
  Enhanced MAPE: 10.89%
  MAPE Improvement: +0.09% (+0.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 16/464: AGO
============================================================
ðŸ“Š Loading data for AGO...
ðŸ“Š Loading data for AGO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AGO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AGO...

==================================================
Training Baseline AGO (SVM)
==================================================
Training SVM model...

Baseline AGO Performance:
MAE: 125203.3683
RMSE: 171336.1741
MAPE: 13.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 197
   â€¢ Highly important features (top 5%): 106

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0011, rank=1
   2. Feature_64_t2: importance=0.0007, rank=2
   3. Feature_67_t2: importance=0.0006, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_2_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for AGO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AGO...

==================================================
Training Enhanced AGO (SVM)
==================================================
Training SVM model...

Enhanced AGO Performance:
MAE: 105067.0412
RMSE: 145890.7408
MAPE: 10.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0058, rank=1
   2. Feature_3_t3: importance=0.0014, rank=2
   3. Feature_3_t2: importance=0.0012, rank=3
   4. Feature_22_t1: importance=0.0011, rank=4
   5. Feature_22_t3: importance=0.0011, rank=5

ðŸ“Š AGO Results:
  Baseline MAPE: 13.17%
  Enhanced MAPE: 10.26%
  MAPE Improvement: +2.90% (+22.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 17/464: AGYS
============================================================
ðŸ“Š Loading data for AGYS...
ðŸ“Š Loading data for AGYS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AGYS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AGYS...

==================================================
Training Baseline AGYS (SVM)
==================================================
Training SVM model...

Baseline AGYS Performance:
MAE: 86614.8340
RMSE: 114968.1411
MAPE: 11.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0019, rank=1
   2. Feature_63_t1: importance=0.0008, rank=2
   3. Feature_1_t1: importance=0.0007, rank=3
   4. Feature_1_t0: importance=0.0006, rank=4
   5. Feature_64_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AGYS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AGYS...

==================================================
Training Enhanced AGYS (SVM)
==================================================
Training SVM model...

Enhanced AGYS Performance:
MAE: 91778.0370
RMSE: 120179.1524
MAPE: 12.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0006, rank=1
   2. Feature_18_t3: importance=0.0005, rank=2
   3. Feature_7_t2: importance=0.0004, rank=3
   4. Feature_11_t2: importance=0.0004, rank=4
   5. Feature_8_t1: importance=0.0004, rank=5

ðŸ“Š AGYS Results:
  Baseline MAPE: 11.99%
  Enhanced MAPE: 12.29%
  MAPE Improvement: -0.30% (-2.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 18/464: AHH
============================================================
ðŸ“Š Loading data for AHH...
ðŸ“Š Loading data for AHH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AHH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AHH...

==================================================
Training Baseline AHH (SVM)
==================================================
Training SVM model...

Baseline AHH Performance:
MAE: 229467.1756
RMSE: 316773.9571
MAPE: 15.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 164
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0024, rank=1
   2. Feature_2_t1: importance=0.0017, rank=2
   3. Feature_63_t3: importance=0.0015, rank=3
   4. Feature_1_t3: importance=0.0013, rank=4
   5. Feature_65_t0: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for AHH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AHH...

==================================================
Training Enhanced AHH (SVM)
==================================================
Training SVM model...

Enhanced AHH Performance:
MAE: 205337.6740
RMSE: 299551.3469
MAPE: 15.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0023, rank=1
   2. Feature_6_t2: importance=0.0022, rank=2
   3. Feature_11_t0: importance=0.0019, rank=3
   4. Feature_4_t2: importance=0.0017, rank=4
   5. Feature_17_t1: importance=0.0015, rank=5

ðŸ“Š AHH Results:
  Baseline MAPE: 15.36%
  Enhanced MAPE: 15.24%
  MAPE Improvement: +0.12% (+0.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 19/464: AIN
============================================================
ðŸ“Š Loading data for AIN...
ðŸ“Š Loading data for AIN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing AIN: 'AIN'

============================================================
TESTING TICKER 20/464: AIR
============================================================
ðŸ“Š Loading data for AIR...
ðŸ“Š Loading data for AIR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AIR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AIR...

==================================================
Training Baseline AIR (SVM)
==================================================
Training SVM model...

Baseline AIR Performance:
MAE: 126455.7756
RMSE: 148040.6755
MAPE: 16.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 121
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0006, rank=1
   2. Feature_64_t2: importance=0.0005, rank=2
   3. Feature_66_t3: importance=0.0005, rank=3
   4. Feature_2_t0: importance=0.0004, rank=4
   5. Feature_64_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AIR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AIR...

==================================================
Training Enhanced AIR (SVM)
==================================================
Training SVM model...

Enhanced AIR Performance:
MAE: 94880.7295
RMSE: 115408.7115
MAPE: 12.54%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0006, rank=1
   2. Feature_23_t2: importance=0.0006, rank=2
   3. Feature_19_t2: importance=0.0005, rank=3
   4. Feature_6_t1: importance=0.0005, rank=4
   5. Feature_5_t3: importance=0.0004, rank=5

ðŸ“Š AIR Results:
  Baseline MAPE: 16.94%
  Enhanced MAPE: 12.54%
  MAPE Improvement: +4.40% (+26.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 21/464: AKR
============================================================
ðŸ“Š Loading data for AKR...
ðŸ“Š Loading data for AKR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AKR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AKR...

==================================================
Training Baseline AKR (SVM)
==================================================
Training SVM model...

Baseline AKR Performance:
MAE: 1267482.4772
RMSE: 2223616.9628
MAPE: 21.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 121
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0009, rank=1
   2. Feature_0_t3: importance=0.0009, rank=2
   3. Feature_2_t0: importance=0.0006, rank=3
   4. Feature_67_t2: importance=0.0005, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for AKR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AKR...

==================================================
Training Enhanced AKR (SVM)
==================================================
Training SVM model...

Enhanced AKR Performance:
MAE: 1182415.1304
RMSE: 2206863.3404
MAPE: 20.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0015, rank=1
   2. Feature_12_t1: importance=0.0012, rank=2
   3. Feature_2_t0: importance=0.0011, rank=3
   4. Feature_11_t0: importance=0.0009, rank=4
   5. Feature_9_t3: importance=0.0009, rank=5

ðŸ“Š AKR Results:
  Baseline MAPE: 21.80%
  Enhanced MAPE: 20.85%
  MAPE Improvement: +0.95% (+4.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 22/464: AL
============================================================
ðŸ“Š Loading data for AL...
ðŸ“Š Loading data for AL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AL...

==================================================
Training Baseline AL (SVM)
==================================================
Training SVM model...

Baseline AL Performance:
MAE: 420451.3421
RMSE: 634737.3431
MAPE: 12.81%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 169
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_1_t3: importance=0.0006, rank=2
   3. Feature_67_t0: importance=0.0004, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_65_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AL...

==================================================
Training Enhanced AL (SVM)
==================================================
Training SVM model...

Enhanced AL Performance:
MAE: 406534.5392
RMSE: 615762.8533
MAPE: 12.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t2: importance=0.0012, rank=1
   2. Feature_9_t3: importance=0.0008, rank=2
   3. Feature_13_t1: importance=0.0007, rank=3
   4. Feature_13_t2: importance=0.0006, rank=4
   5. Feature_6_t2: importance=0.0006, rank=5

ðŸ“Š AL Results:
  Baseline MAPE: 12.81%
  Enhanced MAPE: 12.33%
  MAPE Improvement: +0.47% (+3.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 23/464: ALEX
============================================================
ðŸ“Š Loading data for ALEX...
ðŸ“Š Loading data for ALEX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ALEX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ALEX...

==================================================
Training Baseline ALEX (SVM)
==================================================
Training SVM model...

Baseline ALEX Performance:
MAE: 79353.9779
RMSE: 98784.7513
MAPE: 12.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 206
   â€¢ Highly important features (top 5%): 109

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0013, rank=1
   2. Feature_65_t3: importance=0.0011, rank=2
   3. Feature_64_t3: importance=0.0011, rank=3
   4. Feature_2_t2: importance=0.0010, rank=4
   5. Feature_67_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for ALEX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ALEX...

==================================================
Training Enhanced ALEX (SVM)
==================================================
Training SVM model...

Enhanced ALEX Performance:
MAE: 73032.4012
RMSE: 98390.1725
MAPE: 12.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t3: importance=0.0017, rank=1
   2. Feature_24_t2: importance=0.0017, rank=2
   3. Feature_9_t1: importance=0.0014, rank=3
   4. Feature_19_t2: importance=0.0013, rank=4
   5. Feature_15_t3: importance=0.0012, rank=5

ðŸ“Š ALEX Results:
  Baseline MAPE: 12.78%
  Enhanced MAPE: 12.72%
  MAPE Improvement: +0.06% (+0.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 24/464: ALG
============================================================
ðŸ“Š Loading data for ALG...
ðŸ“Š Loading data for ALG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ALG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ALG...

==================================================
Training Baseline ALG (SVM)
==================================================
Training SVM model...

Baseline ALG Performance:
MAE: 28163.7126
RMSE: 35396.5111
MAPE: 8.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 178
   â€¢ Highly important features (top 5%): 115

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_0_t3: importance=0.0009, rank=2
   3. Feature_1_t3: importance=0.0006, rank=3
   4. Feature_0_t1: importance=0.0005, rank=4
   5. Feature_65_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ALG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ALG...

==================================================
Training Enhanced ALG (SVM)
==================================================
Training SVM model...

Enhanced ALG Performance:
MAE: 29220.9825
RMSE: 33341.6036
MAPE: 9.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0010, rank=1
   2. Feature_23_t0: importance=0.0009, rank=2
   3. Feature_23_t2: importance=0.0008, rank=3
   4. Feature_24_t1: importance=0.0008, rank=4
   5. Feature_12_t0: importance=0.0007, rank=5

ðŸ“Š ALG Results:
  Baseline MAPE: 8.35%
  Enhanced MAPE: 9.67%
  MAPE Improvement: -1.33% (-15.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 25/464: ALGT
============================================================
ðŸ“Š Loading data for ALGT...
ðŸ“Š Loading data for ALGT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ALGT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ALGT...

==================================================
Training Baseline ALGT (SVM)
==================================================
Training SVM model...

Baseline ALGT Performance:
MAE: 157105.2648
RMSE: 184788.3188
MAPE: 12.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 159
   â€¢ Highly important features (top 5%): 88

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0024, rank=1
   2. Feature_67_t1: importance=0.0014, rank=2
   3. Feature_0_t3: importance=0.0013, rank=3
   4. Feature_65_t0: importance=0.0009, rank=4
   5. Feature_65_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for ALGT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ALGT...

==================================================
Training Enhanced ALGT (SVM)
==================================================
Training SVM model...

Enhanced ALGT Performance:
MAE: 153992.2218
RMSE: 192757.6105
MAPE: 11.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t3: importance=0.0062, rank=1
   2. Feature_9_t3: importance=0.0051, rank=2
   3. Feature_4_t2: importance=0.0026, rank=3
   4. Feature_24_t1: importance=0.0025, rank=4
   5. Feature_10_t3: importance=0.0024, rank=5

ðŸ“Š ALGT Results:
  Baseline MAPE: 12.07%
  Enhanced MAPE: 11.90%
  MAPE Improvement: +0.17% (+1.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 26/464: ALKS
============================================================
ðŸ“Š Loading data for ALKS...
ðŸ“Š Loading data for ALKS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ALKS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ALKS...

==================================================
Training Baseline ALKS (SVM)
==================================================
Training SVM model...

Baseline ALKS Performance:
MAE: 1120140.6330
RMSE: 1376919.5801
MAPE: 8.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0005, rank=2
   3. Feature_2_t2: importance=0.0005, rank=3
   4. Feature_65_t3: importance=0.0005, rank=4
   5. Feature_67_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for ALKS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ALKS...

==================================================
Training Enhanced ALKS (SVM)
==================================================
Training SVM model...

Enhanced ALKS Performance:
MAE: 1169767.6997
RMSE: 1415787.9442
MAPE: 8.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0005, rank=1
   2. Feature_23_t3: importance=0.0005, rank=2
   3. Feature_19_t3: importance=0.0005, rank=3
   4. Feature_6_t3: importance=0.0005, rank=4
   5. Feature_1_t3: importance=0.0003, rank=5

ðŸ“Š ALKS Results:
  Baseline MAPE: 8.21%
  Enhanced MAPE: 8.77%
  MAPE Improvement: -0.55% (-6.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 27/464: ALRM
============================================================
ðŸ“Š Loading data for ALRM...
ðŸ“Š Loading data for ALRM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ALRM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ALRM...

==================================================
Training Baseline ALRM (SVM)
==================================================
Training SVM model...

Baseline ALRM Performance:
MAE: 146680.3223
RMSE: 221840.0207
MAPE: 7.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 47
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0002, rank=1
   2. Feature_2_t0: importance=0.0001, rank=2
   3. Feature_65_t3: importance=0.0001, rank=3
   4. Feature_1_t2: importance=0.0001, rank=4
   5. Feature_64_t3: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for ALRM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ALRM...

==================================================
Training Enhanced ALRM (SVM)
==================================================
Training SVM model...

Enhanced ALRM Performance:
MAE: 135659.0608
RMSE: 220083.7509
MAPE: 6.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 40
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0004, rank=1
   2. Feature_6_t0: importance=0.0002, rank=2
   3. Feature_21_t3: importance=0.0002, rank=3
   4. Feature_18_t3: importance=0.0001, rank=4
   5. Feature_19_t3: importance=0.0001, rank=5

ðŸ“Š ALRM Results:
  Baseline MAPE: 7.25%
  Enhanced MAPE: 6.70%
  MAPE Improvement: +0.55% (+7.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 28/464: AMN
============================================================
ðŸ“Š Loading data for AMN...
ðŸ“Š Loading data for AMN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AMN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AMN...

==================================================
Training Baseline AMN (SVM)
==================================================
Training SVM model...

Baseline AMN Performance:
MAE: 604307.0098
RMSE: 743698.0031
MAPE: 13.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 151
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0005, rank=2
   3. Feature_63_t2: importance=0.0004, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_64_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AMN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AMN...

==================================================
Training Enhanced AMN (SVM)
==================================================
Training SVM model...

Enhanced AMN Performance:
MAE: 416914.4585
RMSE: 490780.9282
MAPE: 9.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t2: importance=0.0025, rank=1
   2. Feature_19_t2: importance=0.0021, rank=2
   3. Feature_23_t2: importance=0.0019, rank=3
   4. Feature_13_t2: importance=0.0015, rank=4
   5. Feature_20_t2: importance=0.0014, rank=5

ðŸ“Š AMN Results:
  Baseline MAPE: 13.26%
  Enhanced MAPE: 9.22%
  MAPE Improvement: +4.05% (+30.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 29/464: AMPH
============================================================
ðŸ“Š Loading data for AMPH...
ðŸ“Š Loading data for AMPH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AMPH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AMPH...

==================================================
Training Baseline AMPH (SVM)
==================================================
Training SVM model...

Baseline AMPH Performance:
MAE: 205451.4842
RMSE: 271908.2186
MAPE: 5.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 196
   â€¢ Highly important features (top 5%): 164

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t1: importance=0.0002, rank=1
   2. Feature_13_t1: importance=0.0002, rank=2
   3. Feature_7_t1: importance=0.0002, rank=3
   4. Feature_20_t1: importance=0.0002, rank=4
   5. Feature_9_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for AMPH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AMPH...

==================================================
Training Enhanced AMPH (SVM)
==================================================
Training SVM model...

Enhanced AMPH Performance:
MAE: 267776.5727
RMSE: 332910.2672
MAPE: 6.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 44
   â€¢ Highly important features (top 5%): 30

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t1: importance=0.0009, rank=1
   2. Feature_23_t3: importance=0.0008, rank=2
   3. Feature_19_t1: importance=0.0003, rank=3
   4. Feature_3_t1: importance=0.0002, rank=4
   5. Feature_11_t1: importance=0.0002, rank=5

ðŸ“Š AMPH Results:
  Baseline MAPE: 5.26%
  Enhanced MAPE: 6.87%
  MAPE Improvement: -1.61% (-30.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 30/464: AMSF
============================================================
ðŸ“Š Loading data for AMSF...
ðŸ“Š Loading data for AMSF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AMSF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AMSF...

==================================================
Training Baseline AMSF (SVM)
==================================================
Training SVM model...

Baseline AMSF Performance:
MAE: 33704.8851
RMSE: 46926.3239
MAPE: 15.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 212
   â€¢ Highly important features (top 5%): 98

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_65_t3: importance=0.0007, rank=2
   3. Feature_65_t1: importance=0.0005, rank=3
   4. Feature_1_t0: importance=0.0005, rank=4
   5. Feature_64_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for AMSF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AMSF...

==================================================
Training Enhanced AMSF (SVM)
==================================================
Training SVM model...

Enhanced AMSF Performance:
MAE: 34552.5564
RMSE: 48424.9643
MAPE: 17.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0015, rank=1
   2. Feature_16_t0: importance=0.0009, rank=2
   3. Feature_22_t2: importance=0.0009, rank=3
   4. Feature_23_t3: importance=0.0008, rank=4
   5. Feature_13_t3: importance=0.0008, rank=5

ðŸ“Š AMSF Results:
  Baseline MAPE: 15.86%
  Enhanced MAPE: 17.09%
  MAPE Improvement: -1.23% (-7.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 31/464: AMWD
============================================================
ðŸ“Š Loading data for AMWD...
ðŸ“Š Loading data for AMWD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AMWD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AMWD...

==================================================
Training Baseline AMWD (SVM)
==================================================
Training SVM model...

Baseline AMWD Performance:
MAE: 48708.4933
RMSE: 67273.6119
MAPE: 11.34%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 172
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0013, rank=1
   2. Feature_63_t0: importance=0.0010, rank=2
   3. Feature_2_t2: importance=0.0010, rank=3
   4. Feature_65_t2: importance=0.0007, rank=4
   5. Feature_67_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for AMWD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AMWD...

==================================================
Training Enhanced AMWD (SVM)
==================================================
Training SVM model...

Enhanced AMWD Performance:
MAE: 62117.8363
RMSE: 86470.1145
MAPE: 15.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0011, rank=1
   2. Feature_20_t2: importance=0.0011, rank=2
   3. Feature_2_t2: importance=0.0009, rank=3
   4. Feature_11_t3: importance=0.0008, rank=4
   5. Feature_11_t2: importance=0.0008, rank=5

ðŸ“Š AMWD Results:
  Baseline MAPE: 11.34%
  Enhanced MAPE: 15.16%
  MAPE Improvement: -3.82% (-33.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 32/464: ANDE
============================================================
ðŸ“Š Loading data for ANDE...
ðŸ“Š Loading data for ANDE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ANDE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ANDE...

==================================================
Training Baseline ANDE (SVM)
==================================================
Training SVM model...

Baseline ANDE Performance:
MAE: 90018.9068
RMSE: 121027.9390
MAPE: 12.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 118
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0011, rank=1
   2. Feature_1_t2: importance=0.0010, rank=2
   3. Feature_64_t2: importance=0.0009, rank=3
   4. Feature_65_t3: importance=0.0008, rank=4
   5. Feature_67_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for ANDE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ANDE...

==================================================
Training Enhanced ANDE (SVM)
==================================================
Training SVM model...

Enhanced ANDE Performance:
MAE: 96117.2968
RMSE: 132962.7201
MAPE: 14.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0009, rank=1
   2. Feature_12_t3: importance=0.0009, rank=2
   3. Feature_23_t2: importance=0.0008, rank=3
   4. Feature_4_t3: importance=0.0006, rank=4
   5. Feature_12_t2: importance=0.0005, rank=5

ðŸ“Š ANDE Results:
  Baseline MAPE: 12.75%
  Enhanced MAPE: 14.04%
  MAPE Improvement: -1.29% (-10.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 33/464: ANGI
============================================================
ðŸ“Š Loading data for ANGI...
ðŸ“Š Loading data for ANGI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ANGI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ANGI...

==================================================
Training Baseline ANGI (SVM)
==================================================
Training SVM model...

Baseline ANGI Performance:
MAE: 857163.0641
RMSE: 1909078.4489
MAPE: 26.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 121
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0007, rank=1
   2. Feature_63_t3: importance=0.0006, rank=2
   3. Feature_63_t2: importance=0.0006, rank=3
   4. Feature_67_t3: importance=0.0006, rank=4
   5. Feature_66_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ANGI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ANGI...

==================================================
Training Enhanced ANGI (SVM)
==================================================
Training SVM model...

Enhanced ANGI Performance:
MAE: 975171.1361
RMSE: 2196237.4698
MAPE: 31.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0008, rank=1
   2. Feature_5_t3: importance=0.0007, rank=2
   3. Feature_1_t2: importance=0.0007, rank=3
   4. Feature_9_t3: importance=0.0006, rank=4
   5. Feature_8_t2: importance=0.0006, rank=5

ðŸ“Š ANGI Results:
  Baseline MAPE: 26.85%
  Enhanced MAPE: 31.18%
  MAPE Improvement: -4.33% (-16.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 34/464: ANIP
============================================================
ðŸ“Š Loading data for ANIP...
ðŸ“Š Loading data for ANIP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ANIP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ANIP...

==================================================
Training Baseline ANIP (SVM)
==================================================
Training SVM model...

Baseline ANIP Performance:
MAE: 195995.5714
RMSE: 326288.0333
MAPE: 13.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 153
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0028, rank=1
   2. Feature_65_t3: importance=0.0004, rank=2
   3. Feature_0_t3: importance=0.0004, rank=3
   4. Feature_2_t0: importance=0.0003, rank=4
   5. Feature_0_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ANIP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ANIP...

==================================================
Training Enhanced ANIP (SVM)
==================================================
Training SVM model...

Enhanced ANIP Performance:
MAE: 171098.6187
RMSE: 307563.7055
MAPE: 13.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0012, rank=1
   2. Feature_23_t3: importance=0.0012, rank=2
   3. Feature_6_t3: importance=0.0009, rank=3
   4. Feature_24_t2: importance=0.0006, rank=4
   5. Feature_16_t2: importance=0.0006, rank=5

ðŸ“Š ANIP Results:
  Baseline MAPE: 13.90%
  Enhanced MAPE: 13.01%
  MAPE Improvement: +0.88% (+6.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 35/464: AOSL
============================================================
ðŸ“Š Loading data for AOSL...
ðŸ“Š Loading data for AOSL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AOSL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AOSL...

==================================================
Training Baseline AOSL (SVM)
==================================================
Training SVM model...

Baseline AOSL Performance:
MAE: 115467.4632
RMSE: 169015.8597
MAPE: 8.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 45
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0019, rank=1
   2. Feature_2_t0: importance=0.0010, rank=2
   3. Feature_1_t2: importance=0.0005, rank=3
   4. Feature_67_t1: importance=0.0004, rank=4
   5. Feature_65_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for AOSL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AOSL...

==================================================
Training Enhanced AOSL (SVM)
==================================================
Training SVM model...

Enhanced AOSL Performance:
MAE: 143172.8338
RMSE: 194255.2493
MAPE: 10.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0014, rank=1
   2. Feature_14_t3: importance=0.0014, rank=2
   3. Feature_19_t3: importance=0.0010, rank=3
   4. Feature_6_t3: importance=0.0009, rank=4
   5. Feature_22_t1: importance=0.0008, rank=5

ðŸ“Š AOSL Results:
  Baseline MAPE: 8.18%
  Enhanced MAPE: 10.59%
  MAPE Improvement: -2.41% (-29.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 36/464: APAM
============================================================
ðŸ“Š Loading data for APAM...
ðŸ“Š Loading data for APAM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for APAM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for APAM...

==================================================
Training Baseline APAM (SVM)
==================================================
Training SVM model...

Baseline APAM Performance:
MAE: 192827.8202
RMSE: 238423.0765
MAPE: 7.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 132
   â€¢ Highly important features (top 5%): 94

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0011, rank=1
   2. Feature_67_t2: importance=0.0009, rank=2
   3. Feature_0_t2: importance=0.0007, rank=3
   4. Feature_1_t3: importance=0.0006, rank=4
   5. Feature_65_t2: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for APAM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for APAM...

==================================================
Training Enhanced APAM (SVM)
==================================================
Training SVM model...

Enhanced APAM Performance:
MAE: 230770.6113
RMSE: 293034.1925
MAPE: 8.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t2: importance=0.0011, rank=1
   2. Feature_9_t1: importance=0.0009, rank=2
   3. Feature_4_t1: importance=0.0009, rank=3
   4. Feature_23_t2: importance=0.0008, rank=4
   5. Feature_4_t3: importance=0.0008, rank=5

ðŸ“Š APAM Results:
  Baseline MAPE: 7.07%
  Enhanced MAPE: 8.41%
  MAPE Improvement: -1.34% (-19.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 37/464: APLE
============================================================
ðŸ“Š Loading data for APLE...
ðŸ“Š Loading data for APLE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing APLE: 'APLE'

============================================================
TESTING TICKER 38/464: APOG
============================================================
ðŸ“Š Loading data for APOG...
ðŸ“Š Loading data for APOG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for APOG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for APOG...

==================================================
Training Baseline APOG (SVM)
==================================================
Training SVM model...

Baseline APOG Performance:
MAE: 92441.6030
RMSE: 114526.0378
MAPE: 11.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 25

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0007, rank=1
   2. Feature_2_t0: importance=0.0007, rank=2
   3. Feature_64_t0: importance=0.0005, rank=3
   4. Feature_65_t0: importance=0.0004, rank=4
   5. Feature_67_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for APOG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for APOG...

==================================================
Training Enhanced APOG (SVM)
==================================================
Training SVM model...

Enhanced APOG Performance:
MAE: 84709.8259
RMSE: 103862.3604
MAPE: 11.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0011, rank=1
   2. Feature_22_t3: importance=0.0009, rank=2
   3. Feature_15_t3: importance=0.0007, rank=3
   4. Feature_22_t2: importance=0.0007, rank=4
   5. Feature_10_t3: importance=0.0007, rank=5

ðŸ“Š APOG Results:
  Baseline MAPE: 11.76%
  Enhanced MAPE: 11.05%
  MAPE Improvement: +0.71% (+6.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 39/464: ARCB
============================================================
ðŸ“Š Loading data for ARCB...
ðŸ“Š Loading data for ARCB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ARCB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ARCB...

==================================================
Training Baseline ARCB (SVM)
==================================================
Training SVM model...

Baseline ARCB Performance:
MAE: 127296.4902
RMSE: 155267.9304
MAPE: 9.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0016, rank=1
   2. Feature_63_t3: importance=0.0010, rank=2
   3. Feature_1_t3: importance=0.0008, rank=3
   4. Feature_65_t0: importance=0.0007, rank=4
   5. Feature_63_t1: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for ARCB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ARCB...

==================================================
Training Enhanced ARCB (SVM)
==================================================
Training SVM model...

Enhanced ARCB Performance:
MAE: 118020.7709
RMSE: 148728.0457
MAPE: 8.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0011, rank=1
   2. Feature_1_t0: importance=0.0010, rank=2
   3. Feature_15_t0: importance=0.0009, rank=3
   4. Feature_4_t1: importance=0.0009, rank=4
   5. Feature_22_t3: importance=0.0007, rank=5

ðŸ“Š ARCB Results:
  Baseline MAPE: 9.21%
  Enhanced MAPE: 8.71%
  MAPE Improvement: +0.50% (+5.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 40/464: ARI
============================================================
ðŸ“Š Loading data for ARI...
ðŸ“Š Loading data for ARI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ARI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ARI...

==================================================
Training Baseline ARI (SVM)
==================================================
Training SVM model...

Baseline ARI Performance:
MAE: 340009.6854
RMSE: 432333.0196
MAPE: 8.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 117
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0007, rank=1
   2. Feature_2_t2: importance=0.0007, rank=2
   3. Feature_67_t0: importance=0.0006, rank=3
   4. Feature_65_t0: importance=0.0006, rank=4
   5. Feature_67_t2: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for ARI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ARI...

==================================================
Training Enhanced ARI (SVM)
==================================================
Training SVM model...

Enhanced ARI Performance:
MAE: 358659.6974
RMSE: 474097.1786
MAPE: 9.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0007, rank=1
   2. Feature_11_t3: importance=0.0006, rank=2
   3. Feature_6_t2: importance=0.0006, rank=3
   4. Feature_7_t2: importance=0.0006, rank=4
   5. Feature_2_t2: importance=0.0005, rank=5

ðŸ“Š ARI Results:
  Baseline MAPE: 8.75%
  Enhanced MAPE: 9.12%
  MAPE Improvement: -0.37% (-4.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 41/464: AROC
============================================================
ðŸ“Š Loading data for AROC...
ðŸ“Š Loading data for AROC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AROC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AROC...

==================================================
Training Baseline AROC (SVM)
==================================================
Training SVM model...

Baseline AROC Performance:
MAE: 463507.1854
RMSE: 641947.2016
MAPE: 9.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 105
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0023, rank=1
   2. Feature_63_t0: importance=0.0017, rank=2
   3. Feature_67_t3: importance=0.0016, rank=3
   4. Feature_63_t3: importance=0.0010, rank=4
   5. Feature_66_t1: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for AROC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AROC...

==================================================
Training Enhanced AROC (SVM)
==================================================
Training SVM model...

Enhanced AROC Performance:
MAE: 467214.5371
RMSE: 623096.0459
MAPE: 9.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0035, rank=1
   2. Feature_1_t3: importance=0.0026, rank=2
   3. Feature_17_t1: importance=0.0017, rank=3
   4. Feature_17_t3: importance=0.0016, rank=4
   5. Feature_6_t0: importance=0.0016, rank=5

ðŸ“Š AROC Results:
  Baseline MAPE: 9.21%
  Enhanced MAPE: 9.12%
  MAPE Improvement: +0.09% (+1.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 42/464: ARR
============================================================
ðŸ“Š Loading data for ARR...
ðŸ“Š Loading data for ARR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ARR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ARR...

==================================================
Training Baseline ARR (SVM)
==================================================
Training SVM model...

Baseline ARR Performance:
MAE: 896584.0565
RMSE: 1251009.3826
MAPE: 16.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 142
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0054, rank=1
   2. Feature_67_t2: importance=0.0050, rank=2
   3. Feature_65_t2: importance=0.0017, rank=3
   4. Feature_65_t3: importance=0.0016, rank=4
   5. Feature_65_t1: importance=0.0014, rank=5

ðŸ”§ Applying universal feature engineering for ARR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ARR...

==================================================
Training Enhanced ARR (SVM)
==================================================
Training SVM model...

Enhanced ARR Performance:
MAE: 851592.2656
RMSE: 1186155.9904
MAPE: 15.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t2: importance=0.0081, rank=1
   2. Feature_5_t1: importance=0.0024, rank=2
   3. Feature_21_t3: importance=0.0022, rank=3
   4. Feature_5_t3: importance=0.0019, rank=4
   5. Feature_9_t3: importance=0.0018, rank=5

ðŸ“Š ARR Results:
  Baseline MAPE: 16.37%
  Enhanced MAPE: 15.42%
  MAPE Improvement: +0.95% (+5.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 43/464: ARWR
============================================================
ðŸ“Š Loading data for ARWR...
ðŸ“Š Loading data for ARWR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ARWR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ARWR...

==================================================
Training Baseline ARWR (SVM)
==================================================
Training SVM model...

Baseline ARWR Performance:
MAE: 553989.0860
RMSE: 670808.0336
MAPE: 5.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0003, rank=1
   2. Feature_67_t2: importance=0.0003, rank=2
   3. Feature_0_t2: importance=0.0002, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_0_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for ARWR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ARWR...

==================================================
Training Enhanced ARWR (SVM)
==================================================
Training SVM model...

Enhanced ARWR Performance:
MAE: 538148.3319
RMSE: 687704.9134
MAPE: 5.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0003, rank=1
   2. Feature_16_t2: importance=0.0002, rank=2
   3. Feature_12_t3: importance=0.0002, rank=3
   4. Feature_22_t1: importance=0.0002, rank=4
   5. Feature_15_t2: importance=0.0002, rank=5

ðŸ“Š ARWR Results:
  Baseline MAPE: 5.79%
  Enhanced MAPE: 5.67%
  MAPE Improvement: +0.12% (+2.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 44/464: ASIX
============================================================
ðŸ“Š Loading data for ASIX...
ðŸ“Š Loading data for ASIX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ASIX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ASIX...

==================================================
Training Baseline ASIX (SVM)
==================================================
Training SVM model...

Baseline ASIX Performance:
MAE: 28614.8631
RMSE: 34959.0416
MAPE: 9.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 154
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t1: importance=0.0018, rank=1
   2. Feature_65_t3: importance=0.0017, rank=2
   3. Feature_67_t3: importance=0.0015, rank=3
   4. Feature_63_t2: importance=0.0012, rank=4
   5. Feature_63_t1: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for ASIX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ASIX...

==================================================
Training Enhanced ASIX (SVM)
==================================================
Training SVM model...

Enhanced ASIX Performance:
MAE: 37337.0327
RMSE: 47674.2291
MAPE: 11.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0016, rank=1
   2. Feature_6_t3: importance=0.0011, rank=2
   3. Feature_13_t0: importance=0.0010, rank=3
   4. Feature_23_t0: importance=0.0010, rank=4
   5. Feature_19_t0: importance=0.0009, rank=5

ðŸ“Š ASIX Results:
  Baseline MAPE: 9.06%
  Enhanced MAPE: 11.04%
  MAPE Improvement: -1.98% (-21.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 45/464: ASTE
============================================================
ðŸ“Š Loading data for ASTE...
ðŸ“Š Loading data for ASTE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ASTE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ASTE...

==================================================
Training Baseline ASTE (SVM)
==================================================
Training SVM model...

Baseline ASTE Performance:
MAE: 50585.3850
RMSE: 63961.3900
MAPE: 12.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 148
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0007, rank=1
   2. Feature_1_t0: importance=0.0006, rank=2
   3. Feature_67_t0: importance=0.0006, rank=3
   4. Feature_67_t1: importance=0.0005, rank=4
   5. Feature_66_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for ASTE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ASTE...

==================================================
Training Enhanced ASTE (SVM)
==================================================
Training SVM model...

Enhanced ASTE Performance:
MAE: 51126.7850
RMSE: 67199.2957
MAPE: 12.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0015, rank=1
   2. Feature_13_t0: importance=0.0012, rank=2
   3. Feature_19_t3: importance=0.0012, rank=3
   4. Feature_19_t0: importance=0.0011, rank=4
   5. Feature_22_t3: importance=0.0010, rank=5

ðŸ“Š ASTE Results:
  Baseline MAPE: 12.04%
  Enhanced MAPE: 12.24%
  MAPE Improvement: -0.20% (-1.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 46/464: ATEN
============================================================
ðŸ“Š Loading data for ATEN...
ðŸ“Š Loading data for ATEN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ATEN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ATEN...

==================================================
Training Baseline ATEN (SVM)
==================================================
Training SVM model...

Baseline ATEN Performance:
MAE: 481203.2467
RMSE: 659454.9469
MAPE: 16.81%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 19

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0025, rank=1
   2. Feature_2_t3: importance=0.0011, rank=2
   3. Feature_63_t0: importance=0.0010, rank=3
   4. Feature_67_t1: importance=0.0009, rank=4
   5. Feature_64_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for ATEN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ATEN...

==================================================
Training Enhanced ATEN (SVM)
==================================================
Training SVM model...

Enhanced ATEN Performance:
MAE: 480650.5838
RMSE: 660458.0449
MAPE: 17.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t1: importance=0.0020, rank=1
   2. Feature_10_t3: importance=0.0020, rank=2
   3. Feature_7_t0: importance=0.0018, rank=3
   4. Feature_15_t2: importance=0.0018, rank=4
   5. Feature_11_t2: importance=0.0018, rank=5

ðŸ“Š ATEN Results:
  Baseline MAPE: 16.81%
  Enhanced MAPE: 17.93%
  MAPE Improvement: -1.11% (-6.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 47/464: ATGE
============================================================
ðŸ“Š Loading data for ATGE...
ðŸ“Š Loading data for ATGE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ATGE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ATGE...

==================================================
Training Baseline ATGE (SVM)
==================================================
Training SVM model...

Baseline ATGE Performance:
MAE: 190958.1598
RMSE: 255549.3163
MAPE: 14.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 203
   â€¢ Highly important features (top 5%): 119

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_67_t2: importance=0.0010, rank=2
   3. Feature_65_t1: importance=0.0008, rank=3
   4. Feature_0_t2: importance=0.0005, rank=4
   5. Feature_64_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ATGE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ATGE...

==================================================
Training Enhanced ATGE (SVM)
==================================================
Training SVM model...

Enhanced ATGE Performance:
MAE: 195145.7223
RMSE: 257757.2476
MAPE: 14.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0015, rank=1
   2. Feature_1_t2: importance=0.0015, rank=2
   3. Feature_5_t1: importance=0.0012, rank=3
   4. Feature_16_t3: importance=0.0011, rank=4
   5. Feature_13_t3: importance=0.0011, rank=5

ðŸ“Š ATGE Results:
  Baseline MAPE: 14.28%
  Enhanced MAPE: 14.84%
  MAPE Improvement: -0.56% (-4.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 48/464: AVA
============================================================
ðŸ“Š Loading data for AVA...
ðŸ“Š Loading data for AVA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AVA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AVA...

==================================================
Training Baseline AVA (SVM)
==================================================
Training SVM model...

Baseline AVA Performance:
MAE: 228800.7731
RMSE: 277813.9288
MAPE: 7.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 146
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0025, rank=1
   2. Feature_67_t3: importance=0.0018, rank=2
   3. Feature_2_t3: importance=0.0014, rank=3
   4. Feature_65_t3: importance=0.0011, rank=4
   5. Feature_67_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for AVA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AVA...

==================================================
Training Enhanced AVA (SVM)
==================================================
Training SVM model...

Enhanced AVA Performance:
MAE: 227586.7871
RMSE: 273376.6715
MAPE: 7.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0024, rank=1
   2. Feature_6_t3: importance=0.0017, rank=2
   3. Feature_0_t3: importance=0.0017, rank=3
   4. Feature_13_t3: importance=0.0017, rank=4
   5. Feature_17_t3: importance=0.0016, rank=5

ðŸ“Š AVA Results:
  Baseline MAPE: 7.74%
  Enhanced MAPE: 7.61%
  MAPE Improvement: +0.13% (+1.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 49/464: AWI
============================================================
ðŸ“Š Loading data for AWI...
ðŸ“Š Loading data for AWI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AWI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AWI...

==================================================
Training Baseline AWI (SVM)
==================================================
Training SVM model...

Baseline AWI Performance:
MAE: 125070.3389
RMSE: 169438.0870
MAPE: 18.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0028, rank=1
   2. Feature_65_t0: importance=0.0021, rank=2
   3. Feature_2_t3: importance=0.0012, rank=3
   4. Feature_65_t1: importance=0.0011, rank=4
   5. Feature_2_t0: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for AWI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AWI...

==================================================
Training Enhanced AWI (SVM)
==================================================
Training SVM model...

Enhanced AWI Performance:
MAE: 110805.5796
RMSE: 162360.1548
MAPE: 15.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t1: importance=0.0019, rank=1
   2. Feature_23_t3: importance=0.0012, rank=2
   3. Feature_11_t0: importance=0.0011, rank=3
   4. Feature_23_t0: importance=0.0010, rank=4
   5. Feature_18_t1: importance=0.0010, rank=5

ðŸ“Š AWI Results:
  Baseline MAPE: 18.48%
  Enhanced MAPE: 15.60%
  MAPE Improvement: +2.89% (+15.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 50/464: AWR
============================================================
ðŸ“Š Loading data for AWR...
ðŸ“Š Loading data for AWR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AWR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AWR...

==================================================
Training Baseline AWR (SVM)
==================================================
Training SVM model...

Baseline AWR Performance:
MAE: 55161.3454
RMSE: 73235.9684
MAPE: 11.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 182
   â€¢ Highly important features (top 5%): 104

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0030, rank=1
   2. Feature_65_t2: importance=0.0011, rank=2
   3. Feature_2_t3: importance=0.0008, rank=3
   4. Feature_65_t3: importance=0.0008, rank=4
   5. Feature_0_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for AWR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AWR...

==================================================
Training Enhanced AWR (SVM)
==================================================
Training SVM model...

Enhanced AWR Performance:
MAE: 61388.7368
RMSE: 79397.0322
MAPE: 12.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0022, rank=1
   2. Feature_9_t2: importance=0.0018, rank=2
   3. Feature_6_t3: importance=0.0011, rank=3
   4. Feature_13_t0: importance=0.0010, rank=4
   5. Feature_24_t3: importance=0.0009, rank=5

ðŸ“Š AWR Results:
  Baseline MAPE: 11.04%
  Enhanced MAPE: 12.12%
  MAPE Improvement: -1.08% (-9.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 51/464: AXL
============================================================
ðŸ“Š Loading data for AXL...
ðŸ“Š Loading data for AXL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing AXL: 'AXL'

============================================================
TESTING TICKER 52/464: AZZ
============================================================
ðŸ“Š Loading data for AZZ...
ðŸ“Š Loading data for AZZ from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for AZZ...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for AZZ...

==================================================
Training Baseline AZZ (SVM)
==================================================
Training SVM model...

Baseline AZZ Performance:
MAE: 79984.6138
RMSE: 123434.7310
MAPE: 12.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0033, rank=1
   2. Feature_67_t3: importance=0.0020, rank=2
   3. Feature_0_t3: importance=0.0015, rank=3
   4. Feature_2_t1: importance=0.0013, rank=4
   5. Feature_2_t2: importance=0.0013, rank=5

ðŸ”§ Applying universal feature engineering for AZZ...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for AZZ...

==================================================
Training Enhanced AZZ (SVM)
==================================================
Training SVM model...

Enhanced AZZ Performance:
MAE: 80487.8497
RMSE: 124345.7709
MAPE: 12.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0029, rank=1
   2. Feature_4_t2: importance=0.0025, rank=2
   3. Feature_15_t2: importance=0.0016, rank=3
   4. Feature_19_t3: importance=0.0016, rank=4
   5. Feature_11_t1: importance=0.0015, rank=5

ðŸ“Š AZZ Results:
  Baseline MAPE: 12.63%
  Enhanced MAPE: 12.45%
  MAPE Improvement: +0.18% (+1.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 53/464: BANC
============================================================
ðŸ“Š Loading data for BANC...
ðŸ“Š Loading data for BANC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BANC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BANC...

==================================================
Training Baseline BANC (SVM)
==================================================
Training SVM model...

Baseline BANC Performance:
MAE: 1559877.5134
RMSE: 1900571.6801
MAPE: 13.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 132
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_67_t2: importance=0.0007, rank=2
   3. Feature_2_t0: importance=0.0006, rank=3
   4. Feature_65_t2: importance=0.0005, rank=4
   5. Feature_1_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for BANC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BANC...

==================================================
Training Enhanced BANC (SVM)
==================================================
Training SVM model...

Enhanced BANC Performance:
MAE: 1019835.3047
RMSE: 1430205.5398
MAPE: 8.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0013, rank=1
   2. Feature_16_t1: importance=0.0013, rank=2
   3. Feature_9_t1: importance=0.0011, rank=3
   4. Feature_15_t3: importance=0.0010, rank=4
   5. Feature_6_t2: importance=0.0010, rank=5

ðŸ“Š BANC Results:
  Baseline MAPE: 13.01%
  Enhanced MAPE: 8.70%
  MAPE Improvement: +4.31% (+33.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 54/464: BANF
============================================================
ðŸ“Š Loading data for BANF...
ðŸ“Š Loading data for BANF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing BANF: 'BANF'

============================================================
TESTING TICKER 55/464: BANR
============================================================
ðŸ“Š Loading data for BANR...
ðŸ“Š Loading data for BANR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BANR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BANR...

==================================================
Training Baseline BANR (SVM)
==================================================
Training SVM model...

Baseline BANR Performance:
MAE: 79593.1518
RMSE: 102076.5125
MAPE: 9.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 102
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0008, rank=1
   2. Feature_63_t0: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0003, rank=3
   4. Feature_0_t2: importance=0.0003, rank=4
   5. Feature_2_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for BANR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BANR...

==================================================
Training Enhanced BANR (SVM)
==================================================
Training SVM model...

Enhanced BANR Performance:
MAE: 76291.3968
RMSE: 95967.9072
MAPE: 8.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0004, rank=1
   2. Feature_18_t3: importance=0.0003, rank=2
   3. Feature_17_t2: importance=0.0003, rank=3
   4. Feature_16_t0: importance=0.0003, rank=4
   5. Feature_12_t1: importance=0.0003, rank=5

ðŸ“Š BANR Results:
  Baseline MAPE: 9.19%
  Enhanced MAPE: 8.79%
  MAPE Improvement: +0.39% (+4.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 56/464: BCC
============================================================
ðŸ“Š Loading data for BCC...
ðŸ“Š Loading data for BCC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BCC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BCC...

==================================================
Training Baseline BCC (SVM)
==================================================
Training SVM model...

Baseline BCC Performance:
MAE: 136810.0738
RMSE: 190772.7333
MAPE: 12.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0017, rank=1
   2. Feature_65_t1: importance=0.0016, rank=2
   3. Feature_63_t2: importance=0.0013, rank=3
   4. Feature_65_t2: importance=0.0012, rank=4
   5. Feature_2_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for BCC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BCC...

==================================================
Training Enhanced BCC (SVM)
==================================================
Training SVM model...

Enhanced BCC Performance:
MAE: 142315.5363
RMSE: 196889.0687
MAPE: 13.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0010, rank=1
   2. Feature_24_t1: importance=0.0010, rank=2
   3. Feature_4_t1: importance=0.0008, rank=3
   4. Feature_24_t2: importance=0.0007, rank=4
   5. Feature_15_t3: importance=0.0006, rank=5

ðŸ“Š BCC Results:
  Baseline MAPE: 12.41%
  Enhanced MAPE: 13.57%
  MAPE Improvement: -1.15% (-9.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 57/464: BCPC
============================================================
ðŸ“Š Loading data for BCPC...
ðŸ“Š Loading data for BCPC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BCPC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BCPC...

==================================================
Training Baseline BCPC (SVM)
==================================================
Training SVM model...

Baseline BCPC Performance:
MAE: 37429.3616
RMSE: 46968.2397
MAPE: 8.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0008, rank=1
   2. Feature_1_t0: importance=0.0006, rank=2
   3. Feature_1_t1: importance=0.0005, rank=3
   4. Feature_2_t3: importance=0.0005, rank=4
   5. Feature_0_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for BCPC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BCPC...

==================================================
Training Enhanced BCPC (SVM)
==================================================
Training SVM model...

Enhanced BCPC Performance:
MAE: 38908.1044
RMSE: 49324.0306
MAPE: 9.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0006, rank=1
   2. Feature_24_t3: importance=0.0005, rank=2
   3. Feature_4_t1: importance=0.0004, rank=3
   4. Feature_11_t3: importance=0.0004, rank=4
   5. Feature_15_t0: importance=0.0004, rank=5

ðŸ“Š BCPC Results:
  Baseline MAPE: 8.86%
  Enhanced MAPE: 9.05%
  MAPE Improvement: -0.18% (-2.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 58/464: BDN
============================================================
ðŸ“Š Loading data for BDN...
ðŸ“Š Loading data for BDN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BDN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BDN...

==================================================
Training Baseline BDN (SVM)
==================================================
Training SVM model...

Baseline BDN Performance:
MAE: 727499.2793
RMSE: 1128778.2714
MAPE: 5.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 119
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0008, rank=1
   2. Feature_65_t0: importance=0.0008, rank=2
   3. Feature_67_t2: importance=0.0007, rank=3
   4. Feature_66_t2: importance=0.0006, rank=4
   5. Feature_0_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for BDN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BDN...

==================================================
Training Enhanced BDN (SVM)
==================================================
Training SVM model...

Enhanced BDN Performance:
MAE: 751431.5810
RMSE: 1090971.4752
MAPE: 6.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0012, rank=1
   2. Feature_20_t3: importance=0.0010, rank=2
   3. Feature_13_t3: importance=0.0010, rank=3
   4. Feature_6_t2: importance=0.0008, rank=4
   5. Feature_12_t3: importance=0.0008, rank=5

ðŸ“Š BDN Results:
  Baseline MAPE: 5.83%
  Enhanced MAPE: 6.01%
  MAPE Improvement: -0.18% (-3.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 59/464: BFS
============================================================
ðŸ“Š Loading data for BFS...
ðŸ“Š Loading data for BFS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing BFS: 'BFS'

============================================================
TESTING TICKER 60/464: BHE
============================================================
ðŸ“Š Loading data for BHE...
ðŸ“Š Loading data for BHE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BHE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BHE...

==================================================
Training Baseline BHE (SVM)
==================================================
Training SVM model...

Baseline BHE Performance:
MAE: 135798.6552
RMSE: 166790.5730
MAPE: 12.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 100
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0073, rank=1
   2. Feature_63_t2: importance=0.0033, rank=2
   3. Feature_2_t3: importance=0.0029, rank=3
   4. Feature_0_t0: importance=0.0023, rank=4
   5. Feature_0_t3: importance=0.0023, rank=5

ðŸ”§ Applying universal feature engineering for BHE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BHE...

==================================================
Training Enhanced BHE (SVM)
==================================================
Training SVM model...

Enhanced BHE Performance:
MAE: 109518.4344
RMSE: 142090.6992
MAPE: 10.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0030, rank=1
   2. Feature_19_t3: importance=0.0029, rank=2
   3. Feature_8_t3: importance=0.0025, rank=3
   4. Feature_23_t3: importance=0.0025, rank=4
   5. Feature_13_t3: importance=0.0025, rank=5

ðŸ“Š BHE Results:
  Baseline MAPE: 12.90%
  Enhanced MAPE: 10.84%
  MAPE Improvement: +2.06% (+15.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 61/464: BJRI
============================================================
ðŸ“Š Loading data for BJRI...
ðŸ“Š Loading data for BJRI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BJRI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BJRI...

==================================================
Training Baseline BJRI (SVM)
==================================================
Training SVM model...

Baseline BJRI Performance:
MAE: 103362.9228
RMSE: 137549.5956
MAPE: 5.97%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 164
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0008, rank=1
   2. Feature_64_t0: importance=0.0004, rank=2
   3. Feature_2_t3: importance=0.0004, rank=3
   4. Feature_1_t2: importance=0.0004, rank=4
   5. Feature_1_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for BJRI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BJRI...

==================================================
Training Enhanced BJRI (SVM)
==================================================
Training SVM model...

Enhanced BJRI Performance:
MAE: 128035.4248
RMSE: 153422.1076
MAPE: 7.34%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t3: importance=0.0007, rank=1
   2. Feature_22_t1: importance=0.0006, rank=2
   3. Feature_19_t2: importance=0.0005, rank=3
   4. Feature_13_t0: importance=0.0005, rank=4
   5. Feature_23_t2: importance=0.0004, rank=5

ðŸ“Š BJRI Results:
  Baseline MAPE: 5.97%
  Enhanced MAPE: 7.34%
  MAPE Improvement: -1.37% (-22.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 62/464: BKE
============================================================
ðŸ“Š Loading data for BKE...
ðŸ“Š Loading data for BKE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BKE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BKE...

==================================================
Training Baseline BKE (SVM)
==================================================
Training SVM model...

Baseline BKE Performance:
MAE: 235210.1823
RMSE: 303083.2851
MAPE: 8.10%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0002, rank=1
   2. Feature_67_t0: importance=0.0002, rank=2
   3. Feature_67_t3: importance=0.0001, rank=3
   4. Feature_67_t2: importance=0.0001, rank=4
   5. Feature_65_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for BKE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BKE...

==================================================
Training Enhanced BKE (SVM)
==================================================
Training SVM model...

Enhanced BKE Performance:
MAE: 202153.9128
RMSE: 275000.7325
MAPE: 7.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0002, rank=1
   2. Feature_4_t2: importance=0.0001, rank=2
   3. Feature_24_t3: importance=0.0001, rank=3
   4. Feature_18_t2: importance=0.0001, rank=4
   5. Feature_16_t3: importance=0.0001, rank=5

ðŸ“Š BKE Results:
  Baseline MAPE: 8.10%
  Enhanced MAPE: 7.00%
  MAPE Improvement: +1.10% (+13.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 63/464: BKU
============================================================
ðŸ“Š Loading data for BKU...
ðŸ“Š Loading data for BKU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BKU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BKU...

==================================================
Training Baseline BKU (SVM)
==================================================
Training SVM model...

Baseline BKU Performance:
MAE: 276531.2270
RMSE: 339897.3519
MAPE: 9.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 124
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0016, rank=1
   2. Feature_0_t3: importance=0.0014, rank=2
   3. Feature_67_t2: importance=0.0011, rank=3
   4. Feature_2_t0: importance=0.0007, rank=4
   5. Feature_1_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for BKU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BKU...

==================================================
Training Enhanced BKU (SVM)
==================================================
Training SVM model...

Enhanced BKU Performance:
MAE: 253247.3138
RMSE: 340292.4521
MAPE: 8.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0018, rank=1
   2. Feature_6_t2: importance=0.0012, rank=2
   3. Feature_23_t2: importance=0.0011, rank=3
   4. Feature_13_t2: importance=0.0011, rank=4
   5. Feature_22_t3: importance=0.0011, rank=5

ðŸ“Š BKU Results:
  Baseline MAPE: 9.31%
  Enhanced MAPE: 8.44%
  MAPE Improvement: +0.88% (+9.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 64/464: BL
============================================================
ðŸ“Š Loading data for BL...
ðŸ“Š Loading data for BL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BL...

==================================================
Training Baseline BL (SVM)
==================================================
Training SVM model...

Baseline BL Performance:
MAE: 384572.6671
RMSE: 542951.1155
MAPE: 9.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0006, rank=1
   2. Feature_2_t2: importance=0.0006, rank=2
   3. Feature_1_t2: importance=0.0003, rank=3
   4. Feature_1_t0: importance=0.0002, rank=4
   5. Feature_64_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for BL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BL...

==================================================
Training Enhanced BL (SVM)
==================================================
Training SVM model...

Enhanced BL Performance:
MAE: 394490.6983
RMSE: 559065.7079
MAPE: 9.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0004, rank=1
   2. Feature_16_t2: importance=0.0004, rank=2
   3. Feature_15_t2: importance=0.0003, rank=3
   4. Feature_13_t3: importance=0.0003, rank=4
   5. Feature_19_t3: importance=0.0003, rank=5

ðŸ“Š BL Results:
  Baseline MAPE: 9.11%
  Enhanced MAPE: 9.22%
  MAPE Improvement: -0.11% (-1.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 65/464: BLFS
============================================================
ðŸ“Š Loading data for BLFS...
ðŸ“Š Loading data for BLFS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing BLFS: 'BLFS'

============================================================
TESTING TICKER 66/464: BLMN
============================================================
ðŸ“Š Loading data for BLMN...
ðŸ“Š Loading data for BLMN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BLMN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BLMN...

==================================================
Training Baseline BLMN (SVM)
==================================================
Training SVM model...

Baseline BLMN Performance:
MAE: 920714.5399
RMSE: 1246764.7405
MAPE: 12.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 118
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0009, rank=1
   2. Feature_2_t2: importance=0.0005, rank=2
   3. Feature_2_t3: importance=0.0005, rank=3
   4. Feature_67_t0: importance=0.0003, rank=4
   5. Feature_63_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for BLMN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BLMN...

==================================================
Training Enhanced BLMN (SVM)
==================================================
Training SVM model...

Enhanced BLMN Performance:
MAE: 721031.2424
RMSE: 1084091.1870
MAPE: 9.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t3: importance=0.0009, rank=1
   2. Feature_20_t1: importance=0.0007, rank=2
   3. Feature_19_t3: importance=0.0006, rank=3
   4. Feature_15_t1: importance=0.0005, rank=4
   5. Feature_13_t3: importance=0.0004, rank=5

ðŸ“Š BLMN Results:
  Baseline MAPE: 12.36%
  Enhanced MAPE: 9.92%
  MAPE Improvement: +2.44% (+19.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 67/464: BMI
============================================================
ðŸ“Š Loading data for BMI...
ðŸ“Š Loading data for BMI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BMI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BMI...

==================================================
Training Baseline BMI (SVM)
==================================================
Training SVM model...

Baseline BMI Performance:
MAE: 83533.8600
RMSE: 120004.8036
MAPE: 5.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 23
   â€¢ Highly important features (top 5%): 11

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0002, rank=1
   2. Feature_0_t0: importance=0.0001, rank=2
   3. Feature_63_t3: importance=0.0001, rank=3
   4. Feature_64_t0: importance=0.0001, rank=4
   5. Feature_63_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for BMI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BMI...

==================================================
Training Enhanced BMI (SVM)
==================================================
Training SVM model...

Enhanced BMI Performance:
MAE: 68626.1916
RMSE: 108773.5023
MAPE: 4.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t1: importance=0.0001, rank=1
   2. Feature_22_t0: importance=0.0001, rank=2
   3. Feature_2_t0: importance=0.0001, rank=3
   4. Feature_9_t2: importance=0.0001, rank=4
   5. Feature_4_t3: importance=0.0001, rank=5

ðŸ“Š BMI Results:
  Baseline MAPE: 5.87%
  Enhanced MAPE: 4.80%
  MAPE Improvement: +1.07% (+18.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 68/464: BOH
============================================================
ðŸ“Š Loading data for BOH...
ðŸ“Š Loading data for BOH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BOH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BOH...

==================================================
Training Baseline BOH (SVM)
==================================================
Training SVM model...

Baseline BOH Performance:
MAE: 265372.3893
RMSE: 327215.8638
MAPE: 5.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 176
   â€¢ Highly important features (top 5%): 91

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0005, rank=1
   2. Feature_65_t3: importance=0.0002, rank=2
   3. Feature_64_t2: importance=0.0002, rank=3
   4. Feature_63_t1: importance=0.0002, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for BOH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BOH...

==================================================
Training Enhanced BOH (SVM)
==================================================
Training SVM model...

Enhanced BOH Performance:
MAE: 250112.2908
RMSE: 318714.7080
MAPE: 5.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0006, rank=1
   2. Feature_16_t1: importance=0.0005, rank=2
   3. Feature_24_t2: importance=0.0004, rank=3
   4. Feature_24_t1: importance=0.0003, rank=4
   5. Feature_7_t3: importance=0.0003, rank=5

ðŸ“Š BOH Results:
  Baseline MAPE: 5.63%
  Enhanced MAPE: 5.23%
  MAPE Improvement: +0.39% (+7.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 69/464: BOOT
============================================================
ðŸ“Š Loading data for BOOT...
ðŸ“Š Loading data for BOOT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BOOT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BOOT...

==================================================
Training Baseline BOOT (SVM)
==================================================
Training SVM model...

Baseline BOOT Performance:
MAE: 365025.6019
RMSE: 457537.8661
MAPE: 11.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 185
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0004, rank=1
   2. Feature_67_t2: importance=0.0003, rank=2
   3. Feature_2_t2: importance=0.0003, rank=3
   4. Feature_63_t1: importance=0.0003, rank=4
   5. Feature_2_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for BOOT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BOOT...

==================================================
Training Enhanced BOOT (SVM)
==================================================
Training SVM model...

Enhanced BOOT Performance:
MAE: 321126.2408
RMSE: 388016.0443
MAPE: 10.47%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t2: importance=0.0004, rank=1
   2. Feature_4_t3: importance=0.0004, rank=2
   3. Feature_20_t3: importance=0.0004, rank=3
   4. Feature_7_t0: importance=0.0004, rank=4
   5. Feature_16_t0: importance=0.0003, rank=5

ðŸ“Š BOOT Results:
  Baseline MAPE: 11.75%
  Enhanced MAPE: 10.47%
  MAPE Improvement: +1.28% (+10.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 70/464: BOX
============================================================
ðŸ“Š Loading data for BOX...
ðŸ“Š Loading data for BOX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing BOX: 'BOX'

============================================================
TESTING TICKER 71/464: BRC
============================================================
ðŸ“Š Loading data for BRC...
ðŸ“Š Loading data for BRC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BRC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BRC...

==================================================
Training Baseline BRC (SVM)
==================================================
Training SVM model...

Baseline BRC Performance:
MAE: 100337.9610
RMSE: 148000.3737
MAPE: 21.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0023, rank=1
   2. Feature_0_t3: importance=0.0020, rank=2
   3. Feature_1_t2: importance=0.0017, rank=3
   4. Feature_64_t2: importance=0.0012, rank=4
   5. Feature_63_t2: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for BRC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BRC...

==================================================
Training Enhanced BRC (SVM)
==================================================
Training SVM model...

Enhanced BRC Performance:
MAE: 88680.6380
RMSE: 143844.0479
MAPE: 19.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0045, rank=1
   2. Feature_13_t3: importance=0.0029, rank=2
   3. Feature_23_t3: importance=0.0028, rank=3
   4. Feature_15_t2: importance=0.0023, rank=4
   5. Feature_9_t3: importance=0.0018, rank=5

ðŸ“Š BRC Results:
  Baseline MAPE: 21.03%
  Enhanced MAPE: 19.75%
  MAPE Improvement: +1.29% (+6.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 72/464: BTU
============================================================
ðŸ“Š Loading data for BTU...
ðŸ“Š Loading data for BTU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing BTU: 'BTU'

============================================================
TESTING TICKER 73/464: BWA
============================================================
ðŸ“Š Loading data for BWA...
ðŸ“Š Loading data for BWA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BWA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BWA...

==================================================
Training Baseline BWA (SVM)
==================================================
Training SVM model...

Baseline BWA Performance:
MAE: 891296.2951
RMSE: 1122217.4840
MAPE: 9.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0009, rank=1
   2. Feature_67_t0: importance=0.0007, rank=2
   3. Feature_1_t1: importance=0.0005, rank=3
   4. Feature_64_t1: importance=0.0004, rank=4
   5. Feature_66_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for BWA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BWA...

==================================================
Training Enhanced BWA (SVM)
==================================================
Training SVM model...

Enhanced BWA Performance:
MAE: 1004413.2126
RMSE: 1176782.1775
MAPE: 10.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0007, rank=1
   2. Feature_22_t2: importance=0.0004, rank=2
   3. Feature_7_t0: importance=0.0004, rank=3
   4. Feature_12_t3: importance=0.0004, rank=4
   5. Feature_1_t1: importance=0.0004, rank=5

ðŸ“Š BWA Results:
  Baseline MAPE: 9.00%
  Enhanced MAPE: 10.16%
  MAPE Improvement: -1.15% (-12.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 74/464: BXMT
============================================================
ðŸ“Š Loading data for BXMT...
ðŸ“Š Loading data for BXMT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for BXMT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for BXMT...

==================================================
Training Baseline BXMT (SVM)
==================================================
Training SVM model...

Baseline BXMT Performance:
MAE: 1616812.8746
RMSE: 1949756.9311
MAPE: 8.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 158
   â€¢ Highly important features (top 5%): 101

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0006, rank=1
   2. Feature_65_t3: importance=0.0006, rank=2
   3. Feature_67_t3: importance=0.0005, rank=3
   4. Feature_64_t3: importance=0.0002, rank=4
   5. Feature_0_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for BXMT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for BXMT...

==================================================
Training Enhanced BXMT (SVM)
==================================================
Training SVM model...

Enhanced BXMT Performance:
MAE: 1703111.6346
RMSE: 1982626.3510
MAPE: 9.43%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t2: importance=0.0010, rank=1
   2. Feature_6_t3: importance=0.0006, rank=2
   3. Feature_22_t3: importance=0.0005, rank=3
   4. Feature_3_t1: importance=0.0005, rank=4
   5. Feature_9_t2: importance=0.0005, rank=5

ðŸ“Š BXMT Results:
  Baseline MAPE: 8.65%
  Enhanced MAPE: 9.43%
  MAPE Improvement: -0.78% (-9.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 75/464: CABO
============================================================
ðŸ“Š Loading data for CABO...
ðŸ“Š Loading data for CABO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CABO: 'CABO'

============================================================
TESTING TICKER 76/464: CAKE
============================================================
ðŸ“Š Loading data for CAKE...
ðŸ“Š Loading data for CAKE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CAKE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CAKE...

==================================================
Training Baseline CAKE (SVM)
==================================================
Training SVM model...

Baseline CAKE Performance:
MAE: 370173.9716
RMSE: 500429.0447
MAPE: 5.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0002, rank=1
   2. Feature_65_t3: importance=0.0002, rank=2
   3. Feature_63_t0: importance=0.0002, rank=3
   4. Feature_63_t1: importance=0.0001, rank=4
   5. Feature_65_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for CAKE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CAKE...

==================================================
Training Enhanced CAKE (SVM)
==================================================
Training SVM model...

Enhanced CAKE Performance:
MAE: 355141.8926
RMSE: 478573.8318
MAPE: 4.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0002, rank=1
   2. Feature_24_t3: importance=0.0002, rank=2
   3. Feature_11_t1: importance=0.0001, rank=3
   4. Feature_1_t0: importance=0.0001, rank=4
   5. Feature_8_t2: importance=0.0001, rank=5

ðŸ“Š CAKE Results:
  Baseline MAPE: 5.18%
  Enhanced MAPE: 4.98%
  MAPE Improvement: +0.20% (+3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 77/464: CAL
============================================================
ðŸ“Š Loading data for CAL...
ðŸ“Š Loading data for CAL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CAL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CAL...

==================================================
Training Baseline CAL (SVM)
==================================================
Training SVM model...

Baseline CAL Performance:
MAE: 343981.1847
RMSE: 406090.0659
MAPE: 8.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0004, rank=2
   3. Feature_66_t0: importance=0.0004, rank=3
   4. Feature_67_t2: importance=0.0003, rank=4
   5. Feature_67_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CAL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CAL...

==================================================
Training Enhanced CAL (SVM)
==================================================
Training SVM model...

Enhanced CAL Performance:
MAE: 328468.4340
RMSE: 386064.8665
MAPE: 8.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0006, rank=1
   2. Feature_15_t0: importance=0.0004, rank=2
   3. Feature_6_t2: importance=0.0003, rank=3
   4. Feature_18_t3: importance=0.0003, rank=4
   5. Feature_15_t2: importance=0.0003, rank=5

ðŸ“Š CAL Results:
  Baseline MAPE: 8.68%
  Enhanced MAPE: 8.61%
  MAPE Improvement: +0.07% (+0.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 78/464: CALM
============================================================
ðŸ“Š Loading data for CALM...
ðŸ“Š Loading data for CALM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CALM: 'CALM'

============================================================
TESTING TICKER 79/464: CALX
============================================================
ðŸ“Š Loading data for CALX...
ðŸ“Š Loading data for CALX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CALX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CALX...

==================================================
Training Baseline CALX (SVM)
==================================================
Training SVM model...

Baseline CALX Performance:
MAE: 240508.9273
RMSE: 313075.5798
MAPE: 9.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0012, rank=1
   2. Feature_2_t2: importance=0.0009, rank=2
   3. Feature_65_t2: importance=0.0009, rank=3
   4. Feature_67_t2: importance=0.0008, rank=4
   5. Feature_67_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for CALX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CALX...

==================================================
Training Enhanced CALX (SVM)
==================================================
Training SVM model...

Enhanced CALX Performance:
MAE: 232170.6251
RMSE: 309260.5353
MAPE: 8.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0007, rank=1
   2. Feature_16_t0: importance=0.0006, rank=2
   3. Feature_4_t3: importance=0.0005, rank=3
   4. Feature_8_t2: importance=0.0005, rank=4
   5. Feature_15_t2: importance=0.0005, rank=5

ðŸ“Š CALX Results:
  Baseline MAPE: 9.00%
  Enhanced MAPE: 8.51%
  MAPE Improvement: +0.48% (+5.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 80/464: CARG
============================================================
ðŸ“Š Loading data for CARG...
ðŸ“Š Loading data for CARG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CARG: 'CARG'

============================================================
TESTING TICKER 81/464: CARS
============================================================
ðŸ“Š Loading data for CARS...
ðŸ“Š Loading data for CARS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CARS: 'CARS'

============================================================
TESTING TICKER 82/464: CASH
============================================================
ðŸ“Š Loading data for CASH...
ðŸ“Š Loading data for CASH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CASH: 'CASH'

============================================================
TESTING TICKER 83/464: CATY
============================================================
ðŸ“Š Loading data for CATY...
ðŸ“Š Loading data for CATY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CATY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CATY...

==================================================
Training Baseline CATY (SVM)
==================================================
Training SVM model...

Baseline CATY Performance:
MAE: 174228.7412
RMSE: 205691.8567
MAPE: 8.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 100
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0026, rank=1
   2. Feature_1_t3: importance=0.0008, rank=2
   3. Feature_1_t0: importance=0.0006, rank=3
   4. Feature_63_t3: importance=0.0005, rank=4
   5. Feature_0_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CATY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CATY...

==================================================
Training Enhanced CATY (SVM)
==================================================
Training SVM model...

Enhanced CATY Performance:
MAE: 171807.0291
RMSE: 212063.7988
MAPE: 8.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0027, rank=1
   2. Feature_23_t3: importance=0.0024, rank=2
   3. Feature_6_t3: importance=0.0023, rank=3
   4. Feature_19_t3: importance=0.0022, rank=4
   5. Feature_1_t3: importance=0.0014, rank=5

ðŸ“Š CATY Results:
  Baseline MAPE: 8.78%
  Enhanced MAPE: 8.94%
  MAPE Improvement: -0.17% (-1.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 84/464: CBRL
============================================================
ðŸ“Š Loading data for CBRL...
ðŸ“Š Loading data for CBRL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CBRL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CBRL...

==================================================
Training Baseline CBRL (SVM)
==================================================
Training SVM model...

Baseline CBRL Performance:
MAE: 363756.5825
RMSE: 476452.3385
MAPE: 11.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t0: importance=0.0004, rank=1
   2. Feature_67_t2: importance=0.0003, rank=2
   3. Feature_0_t3: importance=0.0002, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_0_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for CBRL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CBRL...

==================================================
Training Enhanced CBRL (SVM)
==================================================
Training SVM model...

Enhanced CBRL Performance:
MAE: 352152.5976
RMSE: 464140.5285
MAPE: 11.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0002, rank=1
   2. Feature_22_t3: importance=0.0002, rank=2
   3. Feature_4_t3: importance=0.0002, rank=3
   4. Feature_24_t2: importance=0.0002, rank=4
   5. Feature_16_t2: importance=0.0002, rank=5

ðŸ“Š CBRL Results:
  Baseline MAPE: 11.69%
  Enhanced MAPE: 11.39%
  MAPE Improvement: +0.30% (+2.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 85/464: CBU
============================================================
ðŸ“Š Loading data for CBU...
ðŸ“Š Loading data for CBU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CBU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CBU...

==================================================
Training Baseline CBU (SVM)
==================================================
Training SVM model...

Baseline CBU Performance:
MAE: 87254.8109
RMSE: 107873.2843
MAPE: 5.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 109
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0002, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_63_t2: importance=0.0001, rank=3
   4. Feature_64_t0: importance=0.0001, rank=4
   5. Feature_65_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for CBU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CBU...

==================================================
Training Enhanced CBU (SVM)
==================================================
Training SVM model...

Enhanced CBU Performance:
MAE: 93080.3375
RMSE: 111579.0815
MAPE: 6.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0003, rank=1
   2. Feature_19_t3: importance=0.0003, rank=2
   3. Feature_13_t3: importance=0.0003, rank=3
   4. Feature_6_t3: importance=0.0002, rank=4
   5. Feature_1_t3: importance=0.0002, rank=5

ðŸ“Š CBU Results:
  Baseline MAPE: 5.84%
  Enhanced MAPE: 6.24%
  MAPE Improvement: -0.40% (-6.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 86/464: CC
============================================================
ðŸ“Š Loading data for CC...
ðŸ“Š Loading data for CC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CC...

==================================================
Training Baseline CC (SVM)
==================================================
Training SVM model...

Baseline CC Performance:
MAE: 625162.2586
RMSE: 967288.8626
MAPE: 6.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t2: importance=0.0003, rank=1
   2. Feature_2_t3: importance=0.0003, rank=2
   3. Feature_63_t3: importance=0.0002, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_64_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for CC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CC...

==================================================
Training Enhanced CC (SVM)
==================================================
Training SVM model...

Enhanced CC Performance:
MAE: 569243.5900
RMSE: 954615.7807
MAPE: 5.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t3: importance=0.0005, rank=1
   2. Feature_6_t1: importance=0.0005, rank=2
   3. Feature_15_t0: importance=0.0004, rank=3
   4. Feature_24_t1: importance=0.0003, rank=4
   5. Feature_11_t3: importance=0.0002, rank=5

ðŸ“Š CC Results:
  Baseline MAPE: 6.83%
  Enhanced MAPE: 5.82%
  MAPE Improvement: +1.01% (+14.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 87/464: CCOI
============================================================
ðŸ“Š Loading data for CCOI...
ðŸ“Š Loading data for CCOI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CCOI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CCOI...

==================================================
Training Baseline CCOI (SVM)
==================================================
Training SVM model...

Baseline CCOI Performance:
MAE: 391781.5672
RMSE: 507426.2869
MAPE: 9.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 142
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0005, rank=1
   2. Feature_67_t2: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0004, rank=3
   4. Feature_2_t3: importance=0.0003, rank=4
   5. Feature_63_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CCOI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CCOI...

==================================================
Training Enhanced CCOI (SVM)
==================================================
Training SVM model...

Enhanced CCOI Performance:
MAE: 304318.2363
RMSE: 403001.4244
MAPE: 8.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0006, rank=1
   2. Feature_19_t0: importance=0.0004, rank=2
   3. Feature_23_t0: importance=0.0004, rank=3
   4. Feature_23_t3: importance=0.0004, rank=4
   5. Feature_20_t1: importance=0.0004, rank=5

ðŸ“Š CCOI Results:
  Baseline MAPE: 9.99%
  Enhanced MAPE: 8.15%
  MAPE Improvement: +1.84% (+18.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 88/464: CCS
============================================================
ðŸ“Š Loading data for CCS...
ðŸ“Š Loading data for CCS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CCS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CCS...

==================================================
Training Baseline CCS (SVM)
==================================================
Training SVM model...

Baseline CCS Performance:
MAE: 154593.9031
RMSE: 214295.8195
MAPE: 10.10%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 61
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0006, rank=1
   2. Feature_67_t3: importance=0.0004, rank=2
   3. Feature_65_t1: importance=0.0002, rank=3
   4. Feature_63_t2: importance=0.0002, rank=4
   5. Feature_0_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for CCS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CCS...

==================================================
Training Enhanced CCS (SVM)
==================================================
Training SVM model...

Enhanced CCS Performance:
MAE: 171513.4886
RMSE: 231411.8393
MAPE: 11.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0004, rank=1
   2. Feature_23_t3: importance=0.0003, rank=2
   3. Feature_13_t3: importance=0.0003, rank=3
   4. Feature_20_t3: importance=0.0002, rank=4
   5. Feature_6_t3: importance=0.0002, rank=5

ðŸ“Š CCS Results:
  Baseline MAPE: 10.10%
  Enhanced MAPE: 11.26%
  MAPE Improvement: -1.17% (-11.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 89/464: CE
============================================================
ðŸ“Š Loading data for CE...
ðŸ“Š Loading data for CE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CE...

==================================================
Training Baseline CE (SVM)
==================================================
Training SVM model...

Baseline CE Performance:
MAE: 591512.7851
RMSE: 836288.8635
MAPE: 9.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 184
   â€¢ Highly important features (top 5%): 92

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0010, rank=1
   2. Feature_65_t1: importance=0.0009, rank=2
   3. Feature_2_t0: importance=0.0008, rank=3
   4. Feature_64_t0: importance=0.0008, rank=4
   5. Feature_0_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for CE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CE...

==================================================
Training Enhanced CE (SVM)
==================================================
Training SVM model...

Enhanced CE Performance:
MAE: 517916.6009
RMSE: 691158.9086
MAPE: 8.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0031, rank=1
   2. Feature_13_t0: importance=0.0015, rank=2
   3. Feature_13_t3: importance=0.0013, rank=3
   4. Feature_18_t2: importance=0.0012, rank=4
   5. Feature_1_t2: importance=0.0012, rank=5

ðŸ“Š CE Results:
  Baseline MAPE: 9.41%
  Enhanced MAPE: 8.36%
  MAPE Improvement: +1.05% (+11.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 90/464: CENT
============================================================
ðŸ“Š Loading data for CENT...
ðŸ“Š Loading data for CENT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CENT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CENT...

==================================================
Training Baseline CENT (SVM)
==================================================
Training SVM model...

Baseline CENT Performance:
MAE: 107534.9772
RMSE: 180874.1573
MAPE: 7.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 148
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0031, rank=1
   2. Feature_0_t3: importance=0.0011, rank=2
   3. Feature_1_t0: importance=0.0007, rank=3
   4. Feature_2_t1: importance=0.0006, rank=4
   5. Feature_0_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for CENT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CENT...

==================================================
Training Enhanced CENT (SVM)
==================================================
Training SVM model...

Enhanced CENT Performance:
MAE: 92537.3738
RMSE: 175306.4812
MAPE: 6.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0024, rank=1
   2. Feature_23_t3: importance=0.0023, rank=2
   3. Feature_13_t3: importance=0.0022, rank=3
   4. Feature_6_t3: importance=0.0014, rank=4
   5. Feature_15_t0: importance=0.0014, rank=5

ðŸ“Š CENT Results:
  Baseline MAPE: 7.92%
  Enhanced MAPE: 6.75%
  MAPE Improvement: +1.17% (+14.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 91/464: CENTA
============================================================
ðŸ“Š Loading data for CENTA...
ðŸ“Š Loading data for CENTA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CENTA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CENTA...

==================================================
Training Baseline CENTA (SVM)
==================================================
Training SVM model...

Baseline CENTA Performance:
MAE: 100412.2299
RMSE: 139070.5043
MAPE: 11.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 34
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0017, rank=1
   2. Feature_1_t0: importance=0.0014, rank=2
   3. Feature_2_t3: importance=0.0014, rank=3
   4. Feature_1_t1: importance=0.0007, rank=4
   5. Feature_0_t2: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for CENTA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CENTA...

==================================================
Training Enhanced CENTA (SVM)
==================================================
Training SVM model...

Enhanced CENTA Performance:
MAE: 95254.2692
RMSE: 126938.6059
MAPE: 11.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0007, rank=1
   2. Feature_12_t3: importance=0.0006, rank=2
   3. Feature_16_t1: importance=0.0006, rank=3
   4. Feature_22_t2: importance=0.0006, rank=4
   5. Feature_20_t0: importance=0.0006, rank=5

ðŸ“Š CENTA Results:
  Baseline MAPE: 11.94%
  Enhanced MAPE: 11.71%
  MAPE Improvement: +0.22% (+1.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 92/464: CENX
============================================================
ðŸ“Š Loading data for CENX...
ðŸ“Š Loading data for CENX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CENX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CENX...

==================================================
Training Baseline CENX (SVM)
==================================================
Training SVM model...

Baseline CENX Performance:
MAE: 548753.0397
RMSE: 697852.6370
MAPE: 9.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 200
   â€¢ Highly important features (top 5%): 85

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0008, rank=1
   2. Feature_65_t3: importance=0.0005, rank=2
   3. Feature_63_t2: importance=0.0004, rank=3
   4. Feature_1_t1: importance=0.0004, rank=4
   5. Feature_1_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CENX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CENX...

==================================================
Training Enhanced CENX (SVM)
==================================================
Training SVM model...

Enhanced CENX Performance:
MAE: 462605.4014
RMSE: 653324.7418
MAPE: 8.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0009, rank=1
   2. Feature_20_t3: importance=0.0008, rank=2
   3. Feature_11_t2: importance=0.0006, rank=3
   4. Feature_0_t0: importance=0.0005, rank=4
   5. Feature_22_t0: importance=0.0004, rank=5

ðŸ“Š CENX Results:
  Baseline MAPE: 9.73%
  Enhanced MAPE: 8.39%
  MAPE Improvement: +1.33% (+13.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 93/464: CEVA
============================================================
ðŸ“Š Loading data for CEVA...
ðŸ“Š Loading data for CEVA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CEVA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CEVA...

==================================================
Training Baseline CEVA (SVM)
==================================================
Training SVM model...

Baseline CEVA Performance:
MAE: 110003.8258
RMSE: 174010.1371
MAPE: 9.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0013, rank=1
   2. Feature_2_t3: importance=0.0008, rank=2
   3. Feature_0_t0: importance=0.0007, rank=3
   4. Feature_65_t1: importance=0.0007, rank=4
   5. Feature_66_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CEVA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CEVA...

==================================================
Training Enhanced CEVA (SVM)
==================================================
Training SVM model...

Enhanced CEVA Performance:
MAE: 107523.2428
RMSE: 157916.4282
MAPE: 10.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t0: importance=0.0009, rank=1
   2. Feature_13_t1: importance=0.0008, rank=2
   3. Feature_19_t1: importance=0.0008, rank=3
   4. Feature_4_t0: importance=0.0006, rank=4
   5. Feature_17_t0: importance=0.0006, rank=5

ðŸ“Š CEVA Results:
  Baseline MAPE: 9.61%
  Enhanced MAPE: 10.04%
  MAPE Improvement: -0.44% (-4.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 94/464: CFFN
============================================================
ðŸ“Š Loading data for CFFN...
ðŸ“Š Loading data for CFFN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CFFN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CFFN...

==================================================
Training Baseline CFFN (SVM)
==================================================
Training SVM model...

Baseline CFFN Performance:
MAE: 251391.3215
RMSE: 314873.1061
MAPE: 5.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0020, rank=1
   2. Feature_63_t2: importance=0.0009, rank=2
   3. Feature_64_t2: importance=0.0006, rank=3
   4. Feature_65_t2: importance=0.0005, rank=4
   5. Feature_0_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for CFFN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CFFN...

==================================================
Training Enhanced CFFN (SVM)
==================================================
Training SVM model...

Enhanced CFFN Performance:
MAE: 296759.6058
RMSE: 375185.1339
MAPE: 6.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0030, rank=1
   2. Feature_11_t2: importance=0.0011, rank=2
   3. Feature_4_t2: importance=0.0009, rank=3
   4. Feature_2_t3: importance=0.0008, rank=4
   5. Feature_0_t3: importance=0.0008, rank=5

ðŸ“Š CFFN Results:
  Baseline MAPE: 5.60%
  Enhanced MAPE: 6.66%
  MAPE Improvement: -1.07% (-19.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 95/464: CHCO
============================================================
ðŸ“Š Loading data for CHCO...
ðŸ“Š Loading data for CHCO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CHCO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CHCO...

==================================================
Training Baseline CHCO (SVM)
==================================================
Training SVM model...

Baseline CHCO Performance:
MAE: 25199.2963
RMSE: 37261.8579
MAPE: 2.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 30

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0004, rank=1
   2. Feature_65_t3: importance=0.0001, rank=2
   3. Feature_1_t2: importance=0.0001, rank=3
   4. Feature_64_t0: importance=0.0001, rank=4
   5. Feature_63_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for CHCO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CHCO...

==================================================
Training Enhanced CHCO (SVM)
==================================================
Training SVM model...

Enhanced CHCO Performance:
MAE: 42847.0717
RMSE: 54296.9517
MAPE: 4.53%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t3: importance=0.0006, rank=1
   2. Feature_9_t1: importance=0.0002, rank=2
   3. Feature_4_t3: importance=0.0001, rank=3
   4. Feature_15_t1: importance=0.0001, rank=4
   5. Feature_20_t2: importance=0.0001, rank=5

ðŸ“Š CHCO Results:
  Baseline MAPE: 2.66%
  Enhanced MAPE: 4.53%
  MAPE Improvement: -1.87% (-70.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 96/464: CHEF
============================================================
ðŸ“Š Loading data for CHEF...
ðŸ“Š Loading data for CHEF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CHEF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CHEF...

==================================================
Training Baseline CHEF (SVM)
==================================================
Training SVM model...

Baseline CHEF Performance:
MAE: 212584.8396
RMSE: 258964.9212
MAPE: 6.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 129
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0002, rank=1
   2. Feature_63_t1: importance=0.0001, rank=2
   3. Feature_67_t2: importance=0.0001, rank=3
   4. Feature_65_t3: importance=0.0001, rank=4
   5. Feature_0_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for CHEF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CHEF...

==================================================
Training Enhanced CHEF (SVM)
==================================================
Training SVM model...

Enhanced CHEF Performance:
MAE: 198978.8128
RMSE: 254038.1722
MAPE: 6.34%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0006, rank=1
   2. Feature_18_t1: importance=0.0004, rank=2
   3. Feature_14_t1: importance=0.0004, rank=3
   4. Feature_23_t1: importance=0.0003, rank=4
   5. Feature_13_t1: importance=0.0003, rank=5

ðŸ“Š CHEF Results:
  Baseline MAPE: 6.68%
  Enhanced MAPE: 6.34%
  MAPE Improvement: +0.34% (+5.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 97/464: CLB
============================================================
ðŸ“Š Loading data for CLB...
ðŸ“Š Loading data for CLB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CLB: 'CLB'

============================================================
TESTING TICKER 98/464: CNK
============================================================
ðŸ“Š Loading data for CNK...
ðŸ“Š Loading data for CNK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CNK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CNK...

==================================================
Training Baseline CNK (SVM)
==================================================
Training SVM model...

Baseline CNK Performance:
MAE: 1839944.8905
RMSE: 2163319.3075
MAPE: 7.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0006, rank=1
   2. Feature_65_t3: importance=0.0004, rank=2
   3. Feature_2_t1: importance=0.0003, rank=3
   4. Feature_1_t1: importance=0.0003, rank=4
   5. Feature_63_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CNK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CNK...

==================================================
Training Enhanced CNK (SVM)
==================================================
Training SVM model...

Enhanced CNK Performance:
MAE: 1197306.0658
RMSE: 1486938.2901
MAPE: 4.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t0: importance=0.0004, rank=1
   2. Feature_4_t3: importance=0.0004, rank=2
   3. Feature_16_t0: importance=0.0003, rank=3
   4. Feature_13_t0: importance=0.0003, rank=4
   5. Feature_8_t1: importance=0.0003, rank=5

ðŸ“Š CNK Results:
  Baseline MAPE: 7.50%
  Enhanced MAPE: 4.94%
  MAPE Improvement: +2.56% (+34.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 99/464: CNMD
============================================================
ðŸ“Š Loading data for CNMD...
ðŸ“Š Loading data for CNMD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CNMD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CNMD...

==================================================
Training Baseline CNMD (SVM)
==================================================
Training SVM model...

Baseline CNMD Performance:
MAE: 225382.7366
RMSE: 298332.3074
MAPE: 7.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 41
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0006, rank=1
   2. Feature_64_t0: importance=0.0005, rank=2
   3. Feature_1_t0: importance=0.0004, rank=3
   4. Feature_64_t3: importance=0.0004, rank=4
   5. Feature_2_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CNMD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CNMD...

==================================================
Training Enhanced CNMD (SVM)
==================================================
Training SVM model...

Enhanced CNMD Performance:
MAE: 223998.4767
RMSE: 286613.8214
MAPE: 7.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0006, rank=1
   2. Feature_11_t1: importance=0.0006, rank=2
   3. Feature_20_t3: importance=0.0006, rank=3
   4. Feature_16_t3: importance=0.0006, rank=4
   5. Feature_9_t1: importance=0.0006, rank=5

ðŸ“Š CNMD Results:
  Baseline MAPE: 7.19%
  Enhanced MAPE: 7.50%
  MAPE Improvement: -0.31% (-4.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 100/464: CNS
============================================================
ðŸ“Š Loading data for CNS...
ðŸ“Š Loading data for CNS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CNS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CNS...

==================================================
Training Baseline CNS (SVM)
==================================================
Training SVM model...

Baseline CNS Performance:
MAE: 165906.2398
RMSE: 238298.1259
MAPE: 8.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0007, rank=1
   2. Feature_65_t0: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0006, rank=3
   4. Feature_67_t0: importance=0.0004, rank=4
   5. Feature_64_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CNS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CNS...

==================================================
Training Enhanced CNS (SVM)
==================================================
Training SVM model...

Enhanced CNS Performance:
MAE: 160250.7749
RMSE: 228477.0640
MAPE: 8.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0018, rank=1
   2. Feature_15_t2: importance=0.0011, rank=2
   3. Feature_13_t0: importance=0.0006, rank=3
   4. Feature_23_t0: importance=0.0006, rank=4
   5. Feature_19_t0: importance=0.0006, rank=5

ðŸ“Š CNS Results:
  Baseline MAPE: 8.95%
  Enhanced MAPE: 8.56%
  MAPE Improvement: +0.39% (+4.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 101/464: CNXN
============================================================
ðŸ“Š Loading data for CNXN...
ðŸ“Š Loading data for CNXN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CNXN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CNXN...

==================================================
Training Baseline CNXN (SVM)
==================================================
Training SVM model...

Baseline CNXN Performance:
MAE: 36263.4399
RMSE: 51340.3863
MAPE: 13.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 139
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0005, rank=1
   2. Feature_2_t1: importance=0.0005, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_67_t2: importance=0.0004, rank=4
   5. Feature_67_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CNXN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CNXN...

==================================================
Training Enhanced CNXN (SVM)
==================================================
Training SVM model...

Enhanced CNXN Performance:
MAE: 41223.5803
RMSE: 56398.9404
MAPE: 16.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0012, rank=1
   2. Feature_4_t0: importance=0.0009, rank=2
   3. Feature_22_t0: importance=0.0008, rank=3
   4. Feature_15_t2: importance=0.0008, rank=4
   5. Feature_16_t2: importance=0.0007, rank=5

ðŸ“Š CNXN Results:
  Baseline MAPE: 13.68%
  Enhanced MAPE: 16.51%
  MAPE Improvement: -2.83% (-20.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 102/464: COHU
============================================================
ðŸ“Š Loading data for COHU...
ðŸ“Š Loading data for COHU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for COHU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for COHU...

==================================================
Training Baseline COHU (SVM)
==================================================
Training SVM model...

Baseline COHU Performance:
MAE: 151079.0375
RMSE: 198764.3280
MAPE: 11.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 151
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0006, rank=2
   3. Feature_65_t0: importance=0.0005, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_1_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for COHU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for COHU...

==================================================
Training Enhanced COHU (SVM)
==================================================
Training SVM model...

Enhanced COHU Performance:
MAE: 150243.5912
RMSE: 195747.8113
MAPE: 11.43%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0007, rank=1
   2. Feature_21_t2: importance=0.0007, rank=2
   3. Feature_1_t3: importance=0.0006, rank=3
   4. Feature_24_t1: importance=0.0006, rank=4
   5. Feature_6_t1: importance=0.0006, rank=5

ðŸ“Š COHU Results:
  Baseline MAPE: 11.44%
  Enhanced MAPE: 11.43%
  MAPE Improvement: +0.01% (+0.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 103/464: COLL
============================================================
ðŸ“Š Loading data for COLL...
ðŸ“Š Loading data for COLL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for COLL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for COLL...

==================================================
Training Baseline COLL (SVM)
==================================================
Training SVM model...

Baseline COLL Performance:
MAE: 222864.9336
RMSE: 300422.8141
MAPE: 4.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 46
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0006, rank=1
   2. Feature_2_t1: importance=0.0005, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_0_t2: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for COLL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for COLL...

==================================================
Training Enhanced COLL (SVM)
==================================================
Training SVM model...

Enhanced COLL Performance:
MAE: 248219.1503
RMSE: 320012.2334
MAPE: 4.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0004, rank=1
   2. Feature_22_t2: importance=0.0004, rank=2
   3. Feature_17_t0: importance=0.0004, rank=3
   4. Feature_9_t1: importance=0.0003, rank=4
   5. Feature_23_t1: importance=0.0003, rank=5

ðŸ“Š COLL Results:
  Baseline MAPE: 4.03%
  Enhanced MAPE: 4.51%
  MAPE Improvement: -0.48% (-11.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 104/464: CORT
============================================================
ðŸ“Š Loading data for CORT...
ðŸ“Š Loading data for CORT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CORT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CORT...

==================================================
Training Baseline CORT (SVM)
==================================================
Training SVM model...

Baseline CORT Performance:
MAE: 811153.8988
RMSE: 1201587.2280
MAPE: 4.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 29
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0003, rank=1
   2. Feature_2_t2: importance=0.0002, rank=2
   3. Feature_65_t0: importance=0.0002, rank=3
   4. Feature_2_t3: importance=0.0001, rank=4
   5. Feature_0_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for CORT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CORT...

==================================================
Training Enhanced CORT (SVM)
==================================================
Training SVM model...

Enhanced CORT Performance:
MAE: 891234.5285
RMSE: 1271137.6672
MAPE: 5.47%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 47
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0004, rank=1
   2. Feature_21_t1: importance=0.0003, rank=2
   3. Feature_19_t2: importance=0.0002, rank=3
   4. Feature_17_t1: importance=0.0002, rank=4
   5. Feature_0_t0: importance=0.0002, rank=5

ðŸ“Š CORT Results:
  Baseline MAPE: 4.84%
  Enhanced MAPE: 5.47%
  MAPE Improvement: -0.64% (-13.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 105/464: CPF
============================================================
ðŸ“Š Loading data for CPF...
ðŸ“Š Loading data for CPF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CPF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CPF...

==================================================
Training Baseline CPF (SVM)
==================================================
Training SVM model...

Baseline CPF Performance:
MAE: 102820.5068
RMSE: 126878.9287
MAPE: 17.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 122
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0018, rank=1
   2. Feature_67_t3: importance=0.0014, rank=2
   3. Feature_65_t1: importance=0.0014, rank=3
   4. Feature_0_t3: importance=0.0014, rank=4
   5. Feature_67_t1: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for CPF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CPF...

==================================================
Training Enhanced CPF (SVM)
==================================================
Training SVM model...

Enhanced CPF Performance:
MAE: 104245.0661
RMSE: 130302.5179
MAPE: 18.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0014, rank=1
   2. Feature_19_t3: importance=0.0013, rank=2
   3. Feature_15_t2: importance=0.0013, rank=3
   4. Feature_9_t3: importance=0.0012, rank=4
   5. Feature_12_t1: importance=0.0012, rank=5

ðŸ“Š CPF Results:
  Baseline MAPE: 17.67%
  Enhanced MAPE: 18.49%
  MAPE Improvement: -0.81% (-4.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 106/464: CPK
============================================================
ðŸ“Š Loading data for CPK...
ðŸ“Š Loading data for CPK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CPK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CPK...

==================================================
Training Baseline CPK (SVM)
==================================================
Training SVM model...

Baseline CPK Performance:
MAE: 51595.2352
RMSE: 73623.8943
MAPE: 12.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 106
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0050, rank=1
   2. Feature_0_t3: importance=0.0046, rank=2
   3. Feature_67_t3: importance=0.0045, rank=3
   4. Feature_2_t3: importance=0.0039, rank=4
   5. Feature_1_t3: importance=0.0026, rank=5

ðŸ”§ Applying universal feature engineering for CPK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CPK...

==================================================
Training Enhanced CPK (SVM)
==================================================
Training SVM model...

Enhanced CPK Performance:
MAE: 47004.0292
RMSE: 68598.6250
MAPE: 11.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0058, rank=1
   2. Feature_16_t2: importance=0.0034, rank=2
   3. Feature_12_t2: importance=0.0033, rank=3
   4. Feature_0_t3: importance=0.0030, rank=4
   5. Feature_17_t3: importance=0.0028, rank=5

ðŸ“Š CPK Results:
  Baseline MAPE: 12.96%
  Enhanced MAPE: 11.45%
  MAPE Improvement: +1.51% (+11.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 107/464: CPRX
============================================================
ðŸ“Š Loading data for CPRX...
ðŸ“Š Loading data for CPRX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing CPRX: 'CPRX'

============================================================
TESTING TICKER 108/464: CRI
============================================================
ðŸ“Š Loading data for CRI...
ðŸ“Š Loading data for CRI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CRI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CRI...

==================================================
Training Baseline CRI (SVM)
==================================================
Training SVM model...

Baseline CRI Performance:
MAE: 459691.6726
RMSE: 551922.1969
MAPE: 10.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 134
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0007, rank=1
   2. Feature_1_t3: importance=0.0005, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_67_t3: importance=0.0004, rank=4
   5. Feature_63_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CRI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CRI...

==================================================
Training Enhanced CRI (SVM)
==================================================
Training SVM model...

Enhanced CRI Performance:
MAE: 477584.5059
RMSE: 563777.6737
MAPE: 10.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0011, rank=1
   2. Feature_23_t3: importance=0.0010, rank=2
   3. Feature_13_t2: importance=0.0009, rank=3
   4. Feature_23_t2: importance=0.0008, rank=4
   5. Feature_19_t2: importance=0.0008, rank=5

ðŸ“Š CRI Results:
  Baseline MAPE: 10.19%
  Enhanced MAPE: 10.58%
  MAPE Improvement: -0.38% (-3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 109/464: CRK
============================================================
ðŸ“Š Loading data for CRK...
ðŸ“Š Loading data for CRK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CRK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CRK...

==================================================
Training Baseline CRK (SVM)
==================================================
Training SVM model...

Baseline CRK Performance:
MAE: 1718094.9762
RMSE: 2282951.6459
MAPE: 6.97%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t0: importance=0.0011, rank=1
   2. Feature_63_t3: importance=0.0007, rank=2
   3. Feature_0_t1: importance=0.0007, rank=3
   4. Feature_65_t0: importance=0.0007, rank=4
   5. Feature_67_t1: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for CRK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CRK...

==================================================
Training Enhanced CRK (SVM)
==================================================
Training SVM model...

Enhanced CRK Performance:
MAE: 2073346.9278
RMSE: 2424532.1560
MAPE: 8.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0008, rank=1
   2. Feature_19_t1: importance=0.0008, rank=2
   3. Feature_23_t1: importance=0.0008, rank=3
   4. Feature_20_t1: importance=0.0007, rank=4
   5. Feature_7_t0: importance=0.0007, rank=5

ðŸ“Š CRK Results:
  Baseline MAPE: 6.97%
  Enhanced MAPE: 8.79%
  MAPE Improvement: -1.82% (-26.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 110/464: CRVL
============================================================
ðŸ“Š Loading data for CRVL...
ðŸ“Š Loading data for CRVL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CRVL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CRVL...

==================================================
Training Baseline CRVL (SVM)
==================================================
Training SVM model...

Baseline CRVL Performance:
MAE: 82425.9377
RMSE: 141733.7499
MAPE: 13.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0015, rank=1
   2. Feature_67_t3: importance=0.0011, rank=2
   3. Feature_67_t1: importance=0.0010, rank=3
   4. Feature_0_t3: importance=0.0009, rank=4
   5. Feature_65_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for CRVL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CRVL...

==================================================
Training Enhanced CRVL (SVM)
==================================================
Training SVM model...

Enhanced CRVL Performance:
MAE: 61908.1319
RMSE: 133992.0236
MAPE: 8.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0022, rank=1
   2. Feature_13_t1: importance=0.0013, rank=2
   3. Feature_19_t1: importance=0.0013, rank=3
   4. Feature_23_t1: importance=0.0011, rank=4
   5. Feature_10_t3: importance=0.0011, rank=5

ðŸ“Š CRVL Results:
  Baseline MAPE: 13.15%
  Enhanced MAPE: 8.42%
  MAPE Improvement: +4.72% (+35.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 111/464: CSGS
============================================================
ðŸ“Š Loading data for CSGS...
ðŸ“Š Loading data for CSGS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CSGS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CSGS...

==================================================
Training Baseline CSGS (SVM)
==================================================
Training SVM model...

Baseline CSGS Performance:
MAE: 192668.5032
RMSE: 248510.4283
MAPE: 7.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 34
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0018, rank=1
   2. Feature_48_t3: importance=0.0009, rank=2
   3. Feature_65_t0: importance=0.0008, rank=3
   4. Feature_48_t2: importance=0.0007, rank=4
   5. Feature_2_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for CSGS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CSGS...

==================================================
Training Enhanced CSGS (SVM)
==================================================
Training SVM model...

Enhanced CSGS Performance:
MAE: 275256.8382
RMSE: 401768.5786
MAPE: 11.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0100, rank=1
   2. Feature_23_t2: importance=0.0095, rank=2
   3. Feature_19_t2: importance=0.0092, rank=3
   4. Feature_20_t2: importance=0.0035, rank=4
   5. Feature_6_t3: importance=0.0016, rank=5

ðŸ“Š CSGS Results:
  Baseline MAPE: 7.84%
  Enhanced MAPE: 11.05%
  MAPE Improvement: -3.22% (-41.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 112/464: CTRE
============================================================
ðŸ“Š Loading data for CTRE...
ðŸ“Š Loading data for CTRE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CTRE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CTRE...

==================================================
Training Baseline CTRE (SVM)
==================================================
Training SVM model...

Baseline CTRE Performance:
MAE: 774185.5113
RMSE: 1064644.2986
MAPE: 17.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 120
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0014, rank=1
   2. Feature_67_t1: importance=0.0009, rank=2
   3. Feature_63_t0: importance=0.0008, rank=3
   4. Feature_0_t0: importance=0.0008, rank=4
   5. Feature_65_t1: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for CTRE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CTRE...

==================================================
Training Enhanced CTRE (SVM)
==================================================
Training SVM model...

Enhanced CTRE Performance:
MAE: 746052.4034
RMSE: 1036644.4959
MAPE: 16.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t2: importance=0.0018, rank=1
   2. Feature_4_t1: importance=0.0008, rank=2
   3. Feature_12_t1: importance=0.0007, rank=3
   4. Feature_3_t0: importance=0.0007, rank=4
   5. Feature_5_t1: importance=0.0007, rank=5

ðŸ“Š CTRE Results:
  Baseline MAPE: 17.40%
  Enhanced MAPE: 16.56%
  MAPE Improvement: +0.83% (+4.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 113/464: CTS
============================================================
ðŸ“Š Loading data for CTS...
ðŸ“Š Loading data for CTS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CTS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CTS...

==================================================
Training Baseline CTS (SVM)
==================================================
Training SVM model...

Baseline CTS Performance:
MAE: 77442.4674
RMSE: 99324.1191
MAPE: 11.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0050, rank=1
   2. Feature_2_t3: importance=0.0024, rank=2
   3. Feature_65_t0: importance=0.0021, rank=3
   4. Feature_0_t0: importance=0.0020, rank=4
   5. Feature_1_t3: importance=0.0018, rank=5

ðŸ”§ Applying universal feature engineering for CTS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CTS...

==================================================
Training Enhanced CTS (SVM)
==================================================
Training SVM model...

Enhanced CTS Performance:
MAE: 60883.1129
RMSE: 77978.4708
MAPE: 9.10%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0035, rank=1
   2. Feature_7_t0: importance=0.0018, rank=2
   3. Feature_17_t3: importance=0.0017, rank=3
   4. Feature_15_t3: importance=0.0016, rank=4
   5. Feature_16_t0: importance=0.0014, rank=5

ðŸ“Š CTS Results:
  Baseline MAPE: 11.99%
  Enhanced MAPE: 9.10%
  MAPE Improvement: +2.89% (+24.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 114/464: CUBI
============================================================
ðŸ“Š Loading data for CUBI...
ðŸ“Š Loading data for CUBI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CUBI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CUBI...

==================================================
Training Baseline CUBI (SVM)
==================================================
Training SVM model...

Baseline CUBI Performance:
MAE: 268366.9098
RMSE: 322270.7887
MAPE: 13.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 166
   â€¢ Highly important features (top 5%): 90

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0005, rank=1
   2. Feature_2_t2: importance=0.0005, rank=2
   3. Feature_65_t2: importance=0.0004, rank=3
   4. Feature_65_t1: importance=0.0004, rank=4
   5. Feature_0_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CUBI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CUBI...

==================================================
Training Enhanced CUBI (SVM)
==================================================
Training SVM model...

Enhanced CUBI Performance:
MAE: 159309.9932
RMSE: 201712.0554
MAPE: 7.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0012, rank=1
   2. Feature_23_t0: importance=0.0011, rank=2
   3. Feature_10_t3: importance=0.0009, rank=3
   4. Feature_7_t1: importance=0.0009, rank=4
   5. Feature_20_t0: importance=0.0006, rank=5

ðŸ“Š CUBI Results:
  Baseline MAPE: 13.35%
  Enhanced MAPE: 7.61%
  MAPE Improvement: +5.75% (+43.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 115/464: CVBF
============================================================
ðŸ“Š Loading data for CVBF...
ðŸ“Š Loading data for CVBF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CVBF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CVBF...

==================================================
Training Baseline CVBF (SVM)
==================================================
Training SVM model...

Baseline CVBF Performance:
MAE: 560956.8621
RMSE: 799967.1075
MAPE: 11.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0009, rank=1
   2. Feature_64_t3: importance=0.0006, rank=2
   3. Feature_1_t0: importance=0.0004, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_65_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for CVBF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CVBF...

==================================================
Training Enhanced CVBF (SVM)
==================================================
Training SVM model...

Enhanced CVBF Performance:
MAE: 477589.2200
RMSE: 711681.4731
MAPE: 10.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0030, rank=1
   2. Feature_9_t3: importance=0.0028, rank=2
   3. Feature_19_t3: importance=0.0028, rank=3
   4. Feature_22_t3: importance=0.0009, rank=4
   5. Feature_19_t0: importance=0.0008, rank=5

ðŸ“Š CVBF Results:
  Baseline MAPE: 11.82%
  Enhanced MAPE: 10.59%
  MAPE Improvement: +1.22% (+10.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 116/464: CVCO
============================================================
ðŸ“Š Loading data for CVCO...
ðŸ“Š Loading data for CVCO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CVCO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CVCO...

==================================================
Training Baseline CVCO (SVM)
==================================================
Training SVM model...

Baseline CVCO Performance:
MAE: 23438.2784
RMSE: 30800.9182
MAPE: 12.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 115
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0007, rank=1
   2. Feature_1_t1: importance=0.0006, rank=2
   3. Feature_2_t2: importance=0.0006, rank=3
   4. Feature_0_t2: importance=0.0004, rank=4
   5. Feature_67_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CVCO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CVCO...

==================================================
Training Enhanced CVCO (SVM)
==================================================
Training SVM model...

Enhanced CVCO Performance:
MAE: 20701.4573
RMSE: 28089.4485
MAPE: 11.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0007, rank=1
   2. Feature_11_t1: importance=0.0007, rank=2
   3. Feature_7_t1: importance=0.0007, rank=3
   4. Feature_16_t1: importance=0.0006, rank=4
   5. Feature_13_t2: importance=0.0006, rank=5

ðŸ“Š CVCO Results:
  Baseline MAPE: 12.86%
  Enhanced MAPE: 11.45%
  MAPE Improvement: +1.41% (+10.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 117/464: CVI
============================================================
ðŸ“Š Loading data for CVI...
ðŸ“Š Loading data for CVI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CVI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CVI...

==================================================
Training Baseline CVI (SVM)
==================================================
Training SVM model...

Baseline CVI Performance:
MAE: 478796.1582
RMSE: 618578.3406
MAPE: 8.97%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0010, rank=1
   2. Feature_1_t2: importance=0.0007, rank=2
   3. Feature_0_t3: importance=0.0006, rank=3
   4. Feature_1_t3: importance=0.0004, rank=4
   5. Feature_65_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CVI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CVI...

==================================================
Training Enhanced CVI (SVM)
==================================================
Training SVM model...

Enhanced CVI Performance:
MAE: 490217.6405
RMSE: 632296.8246
MAPE: 9.54%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0012, rank=1
   2. Feature_4_t1: importance=0.0007, rank=2
   3. Feature_16_t0: importance=0.0006, rank=3
   4. Feature_23_t2: importance=0.0006, rank=4
   5. Feature_10_t3: importance=0.0005, rank=5

ðŸ“Š CVI Results:
  Baseline MAPE: 8.97%
  Enhanced MAPE: 9.54%
  MAPE Improvement: -0.57% (-6.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 118/464: CWT
============================================================
ðŸ“Š Loading data for CWT...
ðŸ“Š Loading data for CWT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CWT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CWT...

==================================================
Training Baseline CWT (SVM)
==================================================
Training SVM model...

Baseline CWT Performance:
MAE: 119191.9080
RMSE: 147011.8962
MAPE: 15.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0015, rank=1
   2. Feature_1_t0: importance=0.0007, rank=2
   3. Feature_1_t3: importance=0.0007, rank=3
   4. Feature_63_t0: importance=0.0005, rank=4
   5. Feature_2_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for CWT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CWT...

==================================================
Training Enhanced CWT (SVM)
==================================================
Training SVM model...

Enhanced CWT Performance:
MAE: 111249.0429
RMSE: 135144.9420
MAPE: 15.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t3: importance=0.0014, rank=1
   2. Feature_22_t3: importance=0.0011, rank=2
   3. Feature_16_t0: importance=0.0009, rank=3
   4. Feature_23_t0: importance=0.0009, rank=4
   5. Feature_23_t3: importance=0.0008, rank=5

ðŸ“Š CWT Results:
  Baseline MAPE: 15.95%
  Enhanced MAPE: 15.08%
  MAPE Improvement: +0.86% (+5.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 119/464: CXW
============================================================
ðŸ“Š Loading data for CXW...
ðŸ“Š Loading data for CXW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CXW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CXW...

==================================================
Training Baseline CXW (SVM)
==================================================
Training SVM model...

Baseline CXW Performance:
MAE: 456183.4439
RMSE: 597173.2394
MAPE: 12.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 30

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0050, rank=1
   2. Feature_65_t1: importance=0.0019, rank=2
   3. Feature_63_t2: importance=0.0010, rank=3
   4. Feature_1_t1: importance=0.0007, rank=4
   5. Feature_65_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for CXW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CXW...

==================================================
Training Enhanced CXW (SVM)
==================================================
Training SVM model...

Enhanced CXW Performance:
MAE: 384915.0600
RMSE: 488326.7744
MAPE: 10.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0025, rank=1
   2. Feature_23_t3: importance=0.0025, rank=2
   3. Feature_5_t3: importance=0.0021, rank=3
   4. Feature_5_t1: importance=0.0020, rank=4
   5. Feature_19_t3: importance=0.0020, rank=5

ðŸ“Š CXW Results:
  Baseline MAPE: 12.04%
  Enhanced MAPE: 10.52%
  MAPE Improvement: +1.52% (+12.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 120/464: CZR
============================================================
ðŸ“Š Loading data for CZR...
ðŸ“Š Loading data for CZR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for CZR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for CZR...

==================================================
Training Baseline CZR (SVM)
==================================================
Training SVM model...

Baseline CZR Performance:
MAE: 1843416.9707
RMSE: 2437385.3241
MAPE: 10.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0021, rank=1
   2. Feature_67_t3: importance=0.0007, rank=2
   3. Feature_64_t3: importance=0.0005, rank=3
   4. Feature_67_t1: importance=0.0004, rank=4
   5. Feature_0_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for CZR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for CZR...

==================================================
Training Enhanced CZR (SVM)
==================================================
Training SVM model...

Enhanced CZR Performance:
MAE: 1917050.6425
RMSE: 2640876.9452
MAPE: 10.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 61
   â€¢ Highly important features (top 5%): 25

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t0: importance=0.0014, rank=1
   2. Feature_7_t3: importance=0.0007, rank=2
   3. Feature_11_t1: importance=0.0006, rank=3
   4. Feature_6_t3: importance=0.0006, rank=4
   5. Feature_20_t0: importance=0.0006, rank=5

ðŸ“Š CZR Results:
  Baseline MAPE: 10.33%
  Enhanced MAPE: 10.44%
  MAPE Improvement: -0.11% (-1.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 121/464: DAN
============================================================
ðŸ“Š Loading data for DAN...
ðŸ“Š Loading data for DAN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DAN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DAN...

==================================================
Training Baseline DAN (SVM)
==================================================
Training SVM model...

Baseline DAN Performance:
MAE: 465499.5909
RMSE: 597310.9153
MAPE: 8.97%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 168
   â€¢ Highly important features (top 5%): 113

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0020, rank=1
   2. Feature_0_t3: importance=0.0017, rank=2
   3. Feature_67_t1: importance=0.0016, rank=3
   4. Feature_2_t3: importance=0.0012, rank=4
   5. Feature_65_t1: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for DAN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DAN...

==================================================
Training Enhanced DAN (SVM)
==================================================
Training SVM model...

Enhanced DAN Performance:
MAE: 505665.3260
RMSE: 659138.0637
MAPE: 10.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t0: importance=0.0010, rank=1
   2. Feature_21_t2: importance=0.0009, rank=2
   3. Feature_4_t3: importance=0.0008, rank=3
   4. Feature_12_t2: importance=0.0008, rank=4
   5. Feature_22_t2: importance=0.0007, rank=5

ðŸ“Š DAN Results:
  Baseline MAPE: 8.97%
  Enhanced MAPE: 10.16%
  MAPE Improvement: -1.19% (-13.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 122/464: DCOM
============================================================
ðŸ“Š Loading data for DCOM...
ðŸ“Š Loading data for DCOM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DCOM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DCOM...

==================================================
Training Baseline DCOM (SVM)
==================================================
Training SVM model...

Baseline DCOM Performance:
MAE: 134959.2597
RMSE: 198430.3204
MAPE: 6.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0016, rank=1
   2. Feature_1_t1: importance=0.0011, rank=2
   3. Feature_63_t1: importance=0.0008, rank=3
   4. Feature_64_t1: importance=0.0005, rank=4
   5. Feature_0_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for DCOM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DCOM...

==================================================
Training Enhanced DCOM (SVM)
==================================================
Training SVM model...

Enhanced DCOM Performance:
MAE: 164396.7014
RMSE: 232048.5148
MAPE: 7.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0015, rank=1
   2. Feature_16_t1: importance=0.0014, rank=2
   3. Feature_16_t3: importance=0.0014, rank=3
   4. Feature_22_t2: importance=0.0011, rank=4
   5. Feature_7_t0: importance=0.0009, rank=5

ðŸ“Š DCOM Results:
  Baseline MAPE: 6.17%
  Enhanced MAPE: 7.42%
  MAPE Improvement: -1.25% (-20.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 123/464: DEA
============================================================
ðŸ“Š Loading data for DEA...
ðŸ“Š Loading data for DEA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DEA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DEA...

==================================================
Training Baseline DEA (SVM)
==================================================
Training SVM model...

Baseline DEA Performance:
MAE: 541308.1530
RMSE: 860407.6368
MAPE: 17.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 141
   â€¢ Highly important features (top 5%): 91

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0012, rank=1
   2. Feature_67_t1: importance=0.0011, rank=2
   3. Feature_67_t2: importance=0.0010, rank=3
   4. Feature_63_t1: importance=0.0007, rank=4
   5. Feature_65_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for DEA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DEA...

==================================================
Training Enhanced DEA (SVM)
==================================================
Training SVM model...

Enhanced DEA Performance:
MAE: 587453.0838
RMSE: 858348.2775
MAPE: 18.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t1: importance=0.0027, rank=1
   2. Feature_20_t3: importance=0.0020, rank=2
   3. Feature_21_t3: importance=0.0016, rank=3
   4. Feature_18_t3: importance=0.0016, rank=4
   5. Feature_22_t3: importance=0.0014, rank=5

ðŸ“Š DEA Results:
  Baseline MAPE: 17.28%
  Enhanced MAPE: 18.44%
  MAPE Improvement: -1.17% (-6.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 124/464: DEI
============================================================
ðŸ“Š Loading data for DEI...
ðŸ“Š Loading data for DEI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DEI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DEI...

==================================================
Training Baseline DEI (SVM)
==================================================
Training SVM model...

Baseline DEI Performance:
MAE: 1838297.9288
RMSE: 2199201.8868
MAPE: 9.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_64_t2: importance=0.0011, rank=2
   3. Feature_1_t1: importance=0.0010, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_2_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for DEI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DEI...

==================================================
Training Enhanced DEI (SVM)
==================================================
Training SVM model...

Enhanced DEI Performance:
MAE: 1493966.6779
RMSE: 1902106.7934
MAPE: 7.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t0: importance=0.0008, rank=1
   2. Feature_15_t2: importance=0.0006, rank=2
   3. Feature_6_t3: importance=0.0006, rank=3
   4. Feature_11_t0: importance=0.0006, rank=4
   5. Feature_23_t1: importance=0.0006, rank=5

ðŸ“Š DEI Results:
  Baseline MAPE: 9.06%
  Enhanced MAPE: 7.28%
  MAPE Improvement: +1.78% (+19.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 125/464: DFIN
============================================================
ðŸ“Š Loading data for DFIN...
ðŸ“Š Loading data for DFIN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DFIN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DFIN...

==================================================
Training Baseline DFIN (SVM)
==================================================
Training SVM model...

Baseline DFIN Performance:
MAE: 137345.0914
RMSE: 167329.4527
MAPE: 12.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 167
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0008, rank=1
   2. Feature_64_t0: importance=0.0008, rank=2
   3. Feature_67_t2: importance=0.0006, rank=3
   4. Feature_65_t2: importance=0.0005, rank=4
   5. Feature_67_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for DFIN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DFIN...

==================================================
Training Enhanced DFIN (SVM)
==================================================
Training SVM model...

Enhanced DFIN Performance:
MAE: 97674.9043
RMSE: 140087.2513
MAPE: 9.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0020, rank=1
   2. Feature_19_t0: importance=0.0014, rank=2
   3. Feature_13_t0: importance=0.0012, rank=3
   4. Feature_4_t0: importance=0.0007, rank=4
   5. Feature_13_t1: importance=0.0006, rank=5

ðŸ“Š DFIN Results:
  Baseline MAPE: 12.52%
  Enhanced MAPE: 9.23%
  MAPE Improvement: +3.29% (+26.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 126/464: DGII
============================================================
ðŸ“Š Loading data for DGII...
ðŸ“Š Loading data for DGII from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DGII...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DGII...

==================================================
Training Baseline DGII (SVM)
==================================================
Training SVM model...

Baseline DGII Performance:
MAE: 156085.2570
RMSE: 190738.8359
MAPE: 6.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0018, rank=1
   2. Feature_64_t1: importance=0.0010, rank=2
   3. Feature_2_t3: importance=0.0010, rank=3
   4. Feature_1_t0: importance=0.0008, rank=4
   5. Feature_0_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for DGII...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DGII...

==================================================
Training Enhanced DGII (SVM)
==================================================
Training SVM model...

Enhanced DGII Performance:
MAE: 143573.9546
RMSE: 176331.6711
MAPE: 6.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t0: importance=0.0022, rank=1
   2. Feature_6_t3: importance=0.0020, rank=2
   3. Feature_19_t2: importance=0.0016, rank=3
   4. Feature_22_t0: importance=0.0014, rank=4
   5. Feature_12_t1: importance=0.0012, rank=5

ðŸ“Š DGII Results:
  Baseline MAPE: 6.87%
  Enhanced MAPE: 6.35%
  MAPE Improvement: +0.51% (+7.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 127/464: DIOD
============================================================
ðŸ“Š Loading data for DIOD...
ðŸ“Š Loading data for DIOD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DIOD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DIOD...

==================================================
Training Baseline DIOD (SVM)
==================================================
Training SVM model...

Baseline DIOD Performance:
MAE: 174689.0716
RMSE: 213079.1050
MAPE: 8.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 32

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0010, rank=1
   2. Feature_64_t2: importance=0.0007, rank=2
   3. Feature_65_t1: importance=0.0006, rank=3
   4. Feature_1_t0: importance=0.0006, rank=4
   5. Feature_1_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for DIOD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DIOD...

==================================================
Training Enhanced DIOD (SVM)
==================================================
Training SVM model...

Enhanced DIOD Performance:
MAE: 145007.1498
RMSE: 177573.4895
MAPE: 7.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t2: importance=0.0007, rank=1
   2. Feature_20_t2: importance=0.0006, rank=2
   3. Feature_16_t1: importance=0.0005, rank=3
   4. Feature_4_t2: importance=0.0005, rank=4
   5. Feature_9_t1: importance=0.0005, rank=5

ðŸ“Š DIOD Results:
  Baseline MAPE: 8.45%
  Enhanced MAPE: 7.12%
  MAPE Improvement: +1.33% (+15.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 128/464: DLX
============================================================
ðŸ“Š Loading data for DLX...
ðŸ“Š Loading data for DLX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DLX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DLX...

==================================================
Training Baseline DLX (SVM)
==================================================
Training SVM model...

Baseline DLX Performance:
MAE: 99223.9520
RMSE: 127692.1339
MAPE: 3.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 38
   â€¢ Highly important features (top 5%): 10

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0003, rank=1
   2. Feature_1_t3: importance=0.0003, rank=2
   3. Feature_63_t3: importance=0.0002, rank=3
   4. Feature_1_t0: importance=0.0002, rank=4
   5. Feature_64_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for DLX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DLX...

==================================================
Training Enhanced DLX (SVM)
==================================================
Training SVM model...

Enhanced DLX Performance:
MAE: 105587.6712
RMSE: 142066.5100
MAPE: 3.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t1: importance=0.0002, rank=1
   2. Feature_10_t2: importance=0.0002, rank=2
   3. Feature_12_t3: importance=0.0001, rank=3
   4. Feature_2_t3: importance=0.0001, rank=4
   5. Feature_16_t0: importance=0.0001, rank=5

ðŸ“Š DLX Results:
  Baseline MAPE: 3.77%
  Enhanced MAPE: 3.96%
  MAPE Improvement: -0.19% (-5.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 129/464: DNOW
============================================================
ðŸ“Š Loading data for DNOW...
ðŸ“Š Loading data for DNOW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DNOW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DNOW...

==================================================
Training Baseline DNOW (SVM)
==================================================
Training SVM model...

Baseline DNOW Performance:
MAE: 487552.0723
RMSE: 611344.1863
MAPE: 15.89%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 214
   â€¢ Highly important features (top 5%): 118

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0033, rank=1
   2. Feature_67_t3: importance=0.0023, rank=2
   3. Feature_1_t3: importance=0.0012, rank=3
   4. Feature_2_t1: importance=0.0007, rank=4
   5. Feature_67_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for DNOW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DNOW...

==================================================
Training Enhanced DNOW (SVM)
==================================================
Training SVM model...

Enhanced DNOW Performance:
MAE: 640525.7913
RMSE: 851410.1483
MAPE: 21.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0027, rank=1
   2. Feature_1_t3: importance=0.0015, rank=2
   3. Feature_5_t1: importance=0.0013, rank=3
   4. Feature_23_t3: importance=0.0012, rank=4
   5. Feature_10_t3: importance=0.0012, rank=5

ðŸ“Š DNOW Results:
  Baseline MAPE: 15.89%
  Enhanced MAPE: 21.68%
  MAPE Improvement: -5.78% (-36.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 130/464: DORM
============================================================
ðŸ“Š Loading data for DORM...
ðŸ“Š Loading data for DORM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DORM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DORM...

==================================================
Training Baseline DORM (SVM)
==================================================
Training SVM model...

Baseline DORM Performance:
MAE: 60400.5159
RMSE: 87524.1418
MAPE: 11.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 180
   â€¢ Highly important features (top 5%): 120

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0004, rank=1
   2. Feature_65_t2: importance=0.0002, rank=2
   3. Feature_65_t1: importance=0.0002, rank=3
   4. Feature_67_t3: importance=0.0001, rank=4
   5. Feature_20_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for DORM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DORM...

==================================================
Training Enhanced DORM (SVM)
==================================================
Training SVM model...

Enhanced DORM Performance:
MAE: 57161.6345
RMSE: 84362.7495
MAPE: 11.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_24_t3: importance=0.0004, rank=1
   2. Feature_18_t2: importance=0.0003, rank=2
   3. Feature_21_t2: importance=0.0003, rank=3
   4. Feature_19_t3: importance=0.0002, rank=4
   5. Feature_15_t2: importance=0.0002, rank=5

ðŸ“Š DORM Results:
  Baseline MAPE: 11.71%
  Enhanced MAPE: 11.23%
  MAPE Improvement: +0.48% (+4.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 131/464: DRH
============================================================
ðŸ“Š Loading data for DRH...
ðŸ“Š Loading data for DRH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DRH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DRH...

==================================================
Training Baseline DRH (SVM)
==================================================
Training SVM model...

Baseline DRH Performance:
MAE: 1661758.7085
RMSE: 1940643.0202
MAPE: 9.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 150
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0011, rank=1
   2. Feature_0_t3: importance=0.0011, rank=2
   3. Feature_67_t2: importance=0.0009, rank=3
   4. Feature_64_t2: importance=0.0008, rank=4
   5. Feature_1_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for DRH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DRH...

==================================================
Training Enhanced DRH (SVM)
==================================================
Training SVM model...

Enhanced DRH Performance:
MAE: 1215120.9519
RMSE: 1494294.2448
MAPE: 7.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0027, rank=1
   2. Feature_21_t3: importance=0.0018, rank=2
   3. Feature_14_t3: importance=0.0015, rank=3
   4. Feature_20_t3: importance=0.0014, rank=4
   5. Feature_19_t3: importance=0.0014, rank=5

ðŸ“Š DRH Results:
  Baseline MAPE: 9.96%
  Enhanced MAPE: 7.33%
  MAPE Improvement: +2.62% (+26.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 132/464: DVAX
============================================================
ðŸ“Š Loading data for DVAX...
ðŸ“Š Loading data for DVAX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing DVAX: 'DVAX'

============================================================
TESTING TICKER 133/464: DXC
============================================================
ðŸ“Š Loading data for DXC...
ðŸ“Š Loading data for DXC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing DXC: 'DXC'

============================================================
TESTING TICKER 134/464: DXPE
============================================================
ðŸ“Š Loading data for DXPE...
ðŸ“Š Loading data for DXPE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DXPE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DXPE...

==================================================
Training Baseline DXPE (SVM)
==================================================
Training SVM model...

Baseline DXPE Performance:
MAE: 60596.1040
RMSE: 81873.0981
MAPE: 10.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t0: importance=0.0004, rank=1
   2. Feature_0_t2: importance=0.0003, rank=2
   3. Feature_0_t3: importance=0.0003, rank=3
   4. Feature_66_t0: importance=0.0002, rank=4
   5. Feature_65_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for DXPE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DXPE...

==================================================
Training Enhanced DXPE (SVM)
==================================================
Training SVM model...

Enhanced DXPE Performance:
MAE: 58987.4930
RMSE: 79635.7960
MAPE: 9.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0009, rank=1
   2. Feature_5_t0: importance=0.0006, rank=2
   3. Feature_15_t2: importance=0.0003, rank=3
   4. Feature_1_t2: importance=0.0002, rank=4
   5. Feature_15_t0: importance=0.0002, rank=5

ðŸ“Š DXPE Results:
  Baseline MAPE: 10.40%
  Enhanced MAPE: 9.92%
  MAPE Improvement: +0.48% (+4.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 135/464: DY
============================================================
ðŸ“Š Loading data for DY...
ðŸ“Š Loading data for DY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for DY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for DY...

==================================================
Training Baseline DY (SVM)
==================================================
Training SVM model...

Baseline DY Performance:
MAE: 128419.4153
RMSE: 173281.7530
MAPE: 10.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 118
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0005, rank=1
   2. Feature_1_t1: importance=0.0004, rank=2
   3. Feature_0_t2: importance=0.0004, rank=3
   4. Feature_0_t3: importance=0.0003, rank=4
   5. Feature_63_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for DY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for DY...

==================================================
Training Enhanced DY (SVM)
==================================================
Training SVM model...

Enhanced DY Performance:
MAE: 141426.0163
RMSE: 188621.3245
MAPE: 12.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0007, rank=1
   2. Feature_9_t3: importance=0.0007, rank=2
   3. Feature_15_t2: importance=0.0006, rank=3
   4. Feature_12_t3: importance=0.0005, rank=4
   5. Feature_1_t2: importance=0.0005, rank=5

ðŸ“Š DY Results:
  Baseline MAPE: 10.87%
  Enhanced MAPE: 12.03%
  MAPE Improvement: -1.16% (-10.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 136/464: EAT
============================================================
ðŸ“Š Loading data for EAT...
ðŸ“Š Loading data for EAT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EAT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EAT...

==================================================
Training Baseline EAT (SVM)
==================================================
Training SVM model...

Baseline EAT Performance:
MAE: 456394.1869
RMSE: 627828.1034
MAPE: 7.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 101
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0004, rank=1
   2. Feature_67_t0: importance=0.0003, rank=2
   3. Feature_1_t3: importance=0.0003, rank=3
   4. Feature_64_t1: importance=0.0003, rank=4
   5. Feature_67_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for EAT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EAT...

==================================================
Training Enhanced EAT (SVM)
==================================================
Training SVM model...

Enhanced EAT Performance:
MAE: 400453.8966
RMSE: 583349.9664
MAPE: 6.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0010, rank=1
   2. Feature_9_t3: importance=0.0006, rank=2
   3. Feature_13_t1: importance=0.0006, rank=3
   4. Feature_16_t1: importance=0.0004, rank=4
   5. Feature_4_t3: importance=0.0004, rank=5

ðŸ“Š EAT Results:
  Baseline MAPE: 7.29%
  Enhanced MAPE: 6.25%
  MAPE Improvement: +1.04% (+14.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 137/464: ECPG
============================================================
ðŸ“Š Loading data for ECPG...
ðŸ“Š Loading data for ECPG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ECPG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ECPG...

==================================================
Training Baseline ECPG (SVM)
==================================================
Training SVM model...

Baseline ECPG Performance:
MAE: 104202.5940
RMSE: 154620.2478
MAPE: 6.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0001, rank=1
   2. Feature_63_t2: importance=0.0001, rank=2
   3. Feature_65_t2: importance=0.0001, rank=3
   4. Feature_1_t2: importance=0.0000, rank=4
   5. Feature_1_t3: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for ECPG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ECPG...

==================================================
Training Enhanced ECPG (SVM)
==================================================
Training SVM model...

Enhanced ECPG Performance:
MAE: 80238.4360
RMSE: 122558.8527
MAPE: 4.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0001, rank=1
   2. Feature_22_t3: importance=0.0001, rank=2
   3. Feature_22_t2: importance=0.0001, rank=3
   4. Feature_6_t2: importance=0.0001, rank=4
   5. Feature_12_t0: importance=0.0001, rank=5

ðŸ“Š ECPG Results:
  Baseline MAPE: 6.26%
  Enhanced MAPE: 4.73%
  MAPE Improvement: +1.53% (+24.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 138/464: EFC
============================================================
ðŸ“Š Loading data for EFC...
ðŸ“Š Loading data for EFC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing EFC: 'EFC'

============================================================
TESTING TICKER 139/464: EGBN
============================================================
ðŸ“Š Loading data for EGBN...
ðŸ“Š Loading data for EGBN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EGBN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EGBN...

==================================================
Training Baseline EGBN (SVM)
==================================================
Training SVM model...

Baseline EGBN Performance:
MAE: 192205.3087
RMSE: 239503.9651
MAPE: 9.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0022, rank=1
   2. Feature_67_t3: importance=0.0014, rank=2
   3. Feature_2_t3: importance=0.0011, rank=3
   4. Feature_2_t2: importance=0.0010, rank=4
   5. Feature_1_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for EGBN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EGBN...

==================================================
Training Enhanced EGBN (SVM)
==================================================
Training SVM model...

Enhanced EGBN Performance:
MAE: 219978.5646
RMSE: 264613.0577
MAPE: 11.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0087, rank=1
   2. Feature_13_t3: importance=0.0086, rank=2
   3. Feature_19_t3: importance=0.0086, rank=3
   4. Feature_11_t2: importance=0.0055, rank=4
   5. Feature_12_t0: importance=0.0017, rank=5

ðŸ“Š EGBN Results:
  Baseline MAPE: 9.73%
  Enhanced MAPE: 11.05%
  MAPE Improvement: -1.33% (-13.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 140/464: EIG
============================================================
ðŸ“Š Loading data for EIG...
ðŸ“Š Loading data for EIG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EIG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EIG...

==================================================
Training Baseline EIG (SVM)
==================================================
Training SVM model...

Baseline EIG Performance:
MAE: 56904.6234
RMSE: 72986.9420
MAPE: 15.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 166
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_67_t0: importance=0.0009, rank=2
   3. Feature_67_t2: importance=0.0008, rank=3
   4. Feature_64_t2: importance=0.0007, rank=4
   5. Feature_65_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for EIG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EIG...

==================================================
Training Enhanced EIG (SVM)
==================================================
Training SVM model...

Enhanced EIG Performance:
MAE: 54788.7718
RMSE: 71070.5440
MAPE: 14.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t1: importance=0.0008, rank=1
   2. Feature_22_t0: importance=0.0008, rank=2
   3. Feature_6_t3: importance=0.0007, rank=3
   4. Feature_9_t2: importance=0.0007, rank=4
   5. Feature_11_t2: importance=0.0007, rank=5

ðŸ“Š EIG Results:
  Baseline MAPE: 15.11%
  Enhanced MAPE: 14.65%
  MAPE Improvement: +0.45% (+3.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 141/464: ENPH
============================================================
ðŸ“Š Loading data for ENPH...
ðŸ“Š Loading data for ENPH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ENPH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ENPH...

==================================================
Training Baseline ENPH (SVM)
==================================================
Training SVM model...

Baseline ENPH Performance:
MAE: 1446090.0452
RMSE: 1978402.6412
MAPE: 8.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 35
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t0: importance=0.0008, rank=1
   2. Feature_64_t3: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0004, rank=3
   4. Feature_65_t0: importance=0.0003, rank=4
   5. Feature_0_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ENPH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ENPH...

==================================================
Training Enhanced ENPH (SVM)
==================================================
Training SVM model...

Enhanced ENPH Performance:
MAE: 1340247.4954
RMSE: 1767612.9250
MAPE: 7.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0004, rank=1
   2. Feature_6_t0: importance=0.0004, rank=2
   3. Feature_20_t1: importance=0.0004, rank=3
   4. Feature_15_t2: importance=0.0003, rank=4
   5. Feature_15_t1: importance=0.0003, rank=5

ðŸ“Š ENPH Results:
  Baseline MAPE: 8.13%
  Enhanced MAPE: 7.87%
  MAPE Improvement: +0.26% (+3.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 142/464: ENR
============================================================
ðŸ“Š Loading data for ENR...
ðŸ“Š Loading data for ENR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ENR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ENR...

==================================================
Training Baseline ENR (SVM)
==================================================
Training SVM model...

Baseline ENR Performance:
MAE: 213057.9984
RMSE: 288286.7237
MAPE: 6.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0005, rank=2
   3. Feature_65_t0: importance=0.0003, rank=3
   4. Feature_1_t0: importance=0.0003, rank=4
   5. Feature_65_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ENR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ENR...

==================================================
Training Enhanced ENR (SVM)
==================================================
Training SVM model...

Enhanced ENR Performance:
MAE: 221392.3909
RMSE: 259471.3782
MAPE: 7.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0007, rank=1
   2. Feature_15_t0: importance=0.0006, rank=2
   3. Feature_16_t3: importance=0.0005, rank=3
   4. Feature_15_t3: importance=0.0005, rank=4
   5. Feature_13_t3: importance=0.0005, rank=5

ðŸ“Š ENR Results:
  Baseline MAPE: 6.23%
  Enhanced MAPE: 7.29%
  MAPE Improvement: -1.06% (-17.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 143/464: ENVA
============================================================
ðŸ“Š Loading data for ENVA...
ðŸ“Š Loading data for ENVA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ENVA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ENVA...

==================================================
Training Baseline ENVA (SVM)
==================================================
Training SVM model...

Baseline ENVA Performance:
MAE: 83229.1331
RMSE: 114171.5295
MAPE: 5.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 101
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0008, rank=1
   2. Feature_67_t2: importance=0.0007, rank=2
   3. Feature_0_t3: importance=0.0007, rank=3
   4. Feature_65_t3: importance=0.0005, rank=4
   5. Feature_67_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ENVA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ENVA...

==================================================
Training Enhanced ENVA (SVM)
==================================================
Training SVM model...

Enhanced ENVA Performance:
MAE: 117431.1323
RMSE: 144395.9979
MAPE: 7.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0007, rank=1
   2. Feature_19_t2: importance=0.0006, rank=2
   3. Feature_23_t3: importance=0.0006, rank=3
   4. Feature_1_t1: importance=0.0006, rank=4
   5. Feature_24_t1: importance=0.0006, rank=5

ðŸ“Š ENVA Results:
  Baseline MAPE: 5.05%
  Enhanced MAPE: 7.07%
  MAPE Improvement: -2.02% (-40.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 144/464: EPC
============================================================
ðŸ“Š Loading data for EPC...
ðŸ“Š Loading data for EPC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EPC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EPC...

==================================================
Training Baseline EPC (SVM)
==================================================
Training SVM model...

Baseline EPC Performance:
MAE: 144917.3696
RMSE: 188963.1735
MAPE: 6.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0004, rank=1
   2. Feature_67_t3: importance=0.0003, rank=2
   3. Feature_1_t1: importance=0.0002, rank=3
   4. Feature_67_t2: importance=0.0002, rank=4
   5. Feature_67_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for EPC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EPC...

==================================================
Training Enhanced EPC (SVM)
==================================================
Training SVM model...

Enhanced EPC Performance:
MAE: 151788.5183
RMSE: 209723.9947
MAPE: 6.62%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0004, rank=1
   2. Feature_6_t3: importance=0.0003, rank=2
   3. Feature_20_t1: importance=0.0003, rank=3
   4. Feature_21_t2: importance=0.0002, rank=4
   5. Feature_16_t0: importance=0.0002, rank=5

ðŸ“Š EPC Results:
  Baseline MAPE: 6.26%
  Enhanced MAPE: 6.62%
  MAPE Improvement: -0.36% (-5.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 145/464: ESE
============================================================
ðŸ“Š Loading data for ESE...
ðŸ“Š Loading data for ESE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ESE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ESE...

==================================================
Training Baseline ESE (SVM)
==================================================
Training SVM model...

Baseline ESE Performance:
MAE: 44714.0182
RMSE: 57850.7239
MAPE: 17.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 104
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0021, rank=1
   2. Feature_0_t3: importance=0.0016, rank=2
   3. Feature_67_t2: importance=0.0009, rank=3
   4. Feature_67_t3: importance=0.0006, rank=4
   5. Feature_63_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for ESE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ESE...

==================================================
Training Enhanced ESE (SVM)
==================================================
Training SVM model...

Enhanced ESE Performance:
MAE: 37149.9891
RMSE: 46809.3031
MAPE: 13.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t2: importance=0.0008, rank=1
   2. Feature_19_t2: importance=0.0008, rank=2
   3. Feature_5_t0: importance=0.0007, rank=3
   4. Feature_16_t3: importance=0.0007, rank=4
   5. Feature_16_t2: importance=0.0006, rank=5

ðŸ“Š ESE Results:
  Baseline MAPE: 17.33%
  Enhanced MAPE: 13.92%
  MAPE Improvement: +3.41% (+19.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 146/464: ETSY
============================================================
ðŸ“Š Loading data for ETSY...
ðŸ“Š Loading data for ETSY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ETSY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ETSY...

==================================================
Training Baseline ETSY (SVM)
==================================================
Training SVM model...

Baseline ETSY Performance:
MAE: 1371703.7555
RMSE: 1668940.3206
MAPE: 7.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0016, rank=1
   2. Feature_63_t0: importance=0.0008, rank=2
   3. Feature_0_t3: importance=0.0007, rank=3
   4. Feature_2_t2: importance=0.0007, rank=4
   5. Feature_65_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ETSY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ETSY...

==================================================
Training Enhanced ETSY (SVM)
==================================================
Training SVM model...

Enhanced ETSY Performance:
MAE: 1031170.5209
RMSE: 1334958.5218
MAPE: 5.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0011, rank=1
   2. Feature_14_t3: importance=0.0011, rank=2
   3. Feature_13_t3: importance=0.0010, rank=3
   4. Feature_23_t3: importance=0.0010, rank=4
   5. Feature_18_t3: importance=0.0005, rank=5

ðŸ“Š ETSY Results:
  Baseline MAPE: 7.42%
  Enhanced MAPE: 5.79%
  MAPE Improvement: +1.63% (+21.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 147/464: EVTC
============================================================
ðŸ“Š Loading data for EVTC...
ðŸ“Š Loading data for EVTC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EVTC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EVTC...

==================================================
Training Baseline EVTC (SVM)
==================================================
Training SVM model...

Baseline EVTC Performance:
MAE: 274136.2205
RMSE: 321267.3033
MAPE: 13.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 159
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0016, rank=1
   2. Feature_65_t3: importance=0.0010, rank=2
   3. Feature_67_t2: importance=0.0009, rank=3
   4. Feature_0_t1: importance=0.0008, rank=4
   5. Feature_1_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for EVTC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EVTC...

==================================================
Training Enhanced EVTC (SVM)
==================================================
Training SVM model...

Enhanced EVTC Performance:
MAE: 213956.2090
RMSE: 268785.9961
MAPE: 11.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t1: importance=0.0019, rank=1
   2. Feature_15_t1: importance=0.0018, rank=2
   3. Feature_1_t3: importance=0.0017, rank=3
   4. Feature_13_t3: importance=0.0013, rank=4
   5. Feature_15_t3: importance=0.0012, rank=5

ðŸ“Š EVTC Results:
  Baseline MAPE: 13.09%
  Enhanced MAPE: 11.13%
  MAPE Improvement: +1.96% (+15.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 148/464: EXPI
============================================================
ðŸ“Š Loading data for EXPI...
ðŸ“Š Loading data for EXPI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing EXPI: 'EXPI'

============================================================
TESTING TICKER 149/464: EXTR
============================================================
ðŸ“Š Loading data for EXTR...
ðŸ“Š Loading data for EXTR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EXTR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EXTR...

==================================================
Training Baseline EXTR (SVM)
==================================================
Training SVM model...

Baseline EXTR Performance:
MAE: 829719.9631
RMSE: 1117329.8231
MAPE: 9.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0021, rank=1
   2. Feature_65_t2: importance=0.0021, rank=2
   3. Feature_0_t2: importance=0.0013, rank=3
   4. Feature_2_t3: importance=0.0010, rank=4
   5. Feature_67_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for EXTR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EXTR...

==================================================
Training Enhanced EXTR (SVM)
==================================================
Training SVM model...

Enhanced EXTR Performance:
MAE: 865838.8348
RMSE: 1156772.3307
MAPE: 10.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0019, rank=1
   2. Feature_20_t3: importance=0.0019, rank=2
   3. Feature_16_t1: importance=0.0016, rank=3
   4. Feature_4_t2: importance=0.0011, rank=4
   5. Feature_23_t3: importance=0.0011, rank=5

ðŸ“Š EXTR Results:
  Baseline MAPE: 9.46%
  Enhanced MAPE: 10.57%
  MAPE Improvement: -1.10% (-11.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 150/464: EYE
============================================================
ðŸ“Š Loading data for EYE...
ðŸ“Š Loading data for EYE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing EYE: 'EYE'

============================================================
TESTING TICKER 151/464: EZPW
============================================================
ðŸ“Š Loading data for EZPW...
ðŸ“Š Loading data for EZPW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for EZPW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for EZPW...

==================================================
Training Baseline EZPW (SVM)
==================================================
Training SVM model...

Baseline EZPW Performance:
MAE: 567904.6339
RMSE: 717839.1673
MAPE: 7.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0004, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_67_t3: importance=0.0001, rank=3
   4. Feature_64_t0: importance=0.0001, rank=4
   5. Feature_67_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for EZPW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for EZPW...

==================================================
Training Enhanced EZPW (SVM)
==================================================
Training SVM model...

Enhanced EZPW Performance:
MAE: 577682.7823
RMSE: 735596.8592
MAPE: 7.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0001, rank=1
   2. Feature_1_t1: importance=0.0001, rank=2
   3. Feature_22_t2: importance=0.0001, rank=3
   4. Feature_16_t3: importance=0.0001, rank=4
   5. Feature_1_t3: importance=0.0001, rank=5

ðŸ“Š EZPW Results:
  Baseline MAPE: 7.22%
  Enhanced MAPE: 7.31%
  MAPE Improvement: -0.09% (-1.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 152/464: FBK
============================================================
ðŸ“Š Loading data for FBK...
ðŸ“Š Loading data for FBK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing FBK: 'FBK'

============================================================
TESTING TICKER 153/464: FBNC
============================================================
ðŸ“Š Loading data for FBNC...
ðŸ“Š Loading data for FBNC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FBNC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FBNC...

==================================================
Training Baseline FBNC (SVM)
==================================================
Training SVM model...

Baseline FBNC Performance:
MAE: 86625.1484
RMSE: 107449.7263
MAPE: 8.62%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 175
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0011, rank=1
   2. Feature_63_t3: importance=0.0008, rank=2
   3. Feature_67_t2: importance=0.0007, rank=3
   4. Feature_63_t0: importance=0.0005, rank=4
   5. Feature_64_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for FBNC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FBNC...

==================================================
Training Enhanced FBNC (SVM)
==================================================
Training SVM model...

Enhanced FBNC Performance:
MAE: 76745.2172
RMSE: 93074.8994
MAPE: 7.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0032, rank=1
   2. Feature_13_t3: importance=0.0032, rank=2
   3. Feature_23_t3: importance=0.0032, rank=3
   4. Feature_1_t3: importance=0.0019, rank=4
   5. Feature_6_t3: importance=0.0009, rank=5

ðŸ“Š FBNC Results:
  Baseline MAPE: 8.62%
  Enhanced MAPE: 7.67%
  MAPE Improvement: +0.95% (+11.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 154/464: FBP
============================================================
ðŸ“Š Loading data for FBP...
ðŸ“Š Loading data for FBP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FBP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FBP...

==================================================
Training Baseline FBP (SVM)
==================================================
Training SVM model...

Baseline FBP Performance:
MAE: 639975.7641
RMSE: 776722.8777
MAPE: 20.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 113
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0021, rank=1
   2. Feature_65_t3: importance=0.0020, rank=2
   3. Feature_67_t1: importance=0.0014, rank=3
   4. Feature_2_t2: importance=0.0013, rank=4
   5. Feature_65_t0: importance=0.0012, rank=5

ðŸ”§ Applying universal feature engineering for FBP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FBP...

==================================================
Training Enhanced FBP (SVM)
==================================================
Training SVM model...

Enhanced FBP Performance:
MAE: 532025.2921
RMSE: 683125.9557
MAPE: 17.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0023, rank=1
   2. Feature_18_t2: importance=0.0011, rank=2
   3. Feature_4_t2: importance=0.0011, rank=3
   4. Feature_6_t1: importance=0.0010, rank=4
   5. Feature_6_t2: importance=0.0010, rank=5

ðŸ“Š FBP Results:
  Baseline MAPE: 20.27%
  Enhanced MAPE: 17.17%
  MAPE Improvement: +3.10% (+15.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 155/464: FCF
============================================================
ðŸ“Š Loading data for FCF...
ðŸ“Š Loading data for FCF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FCF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FCF...

==================================================
Training Baseline FCF (SVM)
==================================================
Training SVM model...

Baseline FCF Performance:
MAE: 167311.8883
RMSE: 230699.5170
MAPE: 10.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0004, rank=1
   2. Feature_65_t3: importance=0.0003, rank=2
   3. Feature_2_t1: importance=0.0003, rank=3
   4. Feature_1_t1: importance=0.0002, rank=4
   5. Feature_63_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for FCF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FCF...

==================================================
Training Enhanced FCF (SVM)
==================================================
Training SVM model...

Enhanced FCF Performance:
MAE: 184560.7306
RMSE: 253493.6954
MAPE: 11.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t2: importance=0.0003, rank=1
   2. Feature_11_t3: importance=0.0003, rank=2
   3. Feature_19_t3: importance=0.0002, rank=3
   4. Feature_21_t3: importance=0.0002, rank=4
   5. Feature_5_t3: importance=0.0002, rank=5

ðŸ“Š FCF Results:
  Baseline MAPE: 10.66%
  Enhanced MAPE: 11.56%
  MAPE Improvement: -0.90% (-8.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 156/464: FCPT
============================================================
ðŸ“Š Loading data for FCPT...
ðŸ“Š Loading data for FCPT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FCPT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FCPT...

==================================================
Training Baseline FCPT (SVM)
==================================================
Training SVM model...

Baseline FCPT Performance:
MAE: 400879.7786
RMSE: 529059.8438
MAPE: 10.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 155
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0010, rank=1
   2. Feature_1_t3: importance=0.0005, rank=2
   3. Feature_65_t2: importance=0.0005, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_67_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for FCPT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FCPT...

==================================================
Training Enhanced FCPT (SVM)
==================================================
Training SVM model...

Enhanced FCPT Performance:
MAE: 412154.5712
RMSE: 569673.7527
MAPE: 11.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t3: importance=0.0016, rank=1
   2. Feature_9_t1: importance=0.0012, rank=2
   3. Feature_6_t3: importance=0.0011, rank=3
   4. Feature_6_t0: importance=0.0010, rank=4
   5. Feature_13_t3: importance=0.0010, rank=5

ðŸ“Š FCPT Results:
  Baseline MAPE: 10.51%
  Enhanced MAPE: 11.02%
  MAPE Improvement: -0.51% (-4.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 157/464: FDP
============================================================
ðŸ“Š Loading data for FDP...
ðŸ“Š Loading data for FDP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing FDP: 'FDP'

============================================================
TESTING TICKER 158/464: FELE
============================================================
ðŸ“Š Loading data for FELE...
ðŸ“Š Loading data for FELE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FELE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FELE...

==================================================
Training Baseline FELE (SVM)
==================================================
Training SVM model...

Baseline FELE Performance:
MAE: 75082.8100
RMSE: 97582.3201
MAPE: 11.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0055, rank=1
   2. Feature_67_t0: importance=0.0021, rank=2
   3. Feature_0_t1: importance=0.0011, rank=3
   4. Feature_65_t0: importance=0.0008, rank=4
   5. Feature_1_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for FELE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FELE...

==================================================
Training Enhanced FELE (SVM)
==================================================
Training SVM model...

Enhanced FELE Performance:
MAE: 71714.6631
RMSE: 86822.5327
MAPE: 11.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 85

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0013, rank=1
   2. Feature_12_t3: importance=0.0010, rank=2
   3. Feature_9_t3: importance=0.0008, rank=3
   4. Feature_23_t3: importance=0.0008, rank=4
   5. Feature_0_t3: importance=0.0008, rank=5

ðŸ“Š FELE Results:
  Baseline MAPE: 11.61%
  Enhanced MAPE: 11.48%
  MAPE Improvement: +0.13% (+1.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 159/464: FFBC
============================================================
ðŸ“Š Loading data for FFBC...
ðŸ“Š Loading data for FFBC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FFBC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FFBC...

==================================================
Training Baseline FFBC (SVM)
==================================================
Training SVM model...

Baseline FFBC Performance:
MAE: 127437.4895
RMSE: 165785.6180
MAPE: 9.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 164
   â€¢ Highly important features (top 5%): 91

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0005, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_1_t2: importance=0.0002, rank=3
   4. Feature_65_t2: importance=0.0002, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for FFBC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FFBC...

==================================================
Training Enhanced FFBC (SVM)
==================================================
Training SVM model...

Enhanced FFBC Performance:
MAE: 123416.8292
RMSE: 168974.1574
MAPE: 9.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0005, rank=1
   2. Feature_6_t3: importance=0.0004, rank=2
   3. Feature_19_t3: importance=0.0004, rank=3
   4. Feature_13_t3: importance=0.0004, rank=4
   5. Feature_16_t0: importance=0.0003, rank=5

ðŸ“Š FFBC Results:
  Baseline MAPE: 9.41%
  Enhanced MAPE: 9.52%
  MAPE Improvement: -0.11% (-1.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 160/464: FHB
============================================================
ðŸ“Š Loading data for FHB...
ðŸ“Š Loading data for FHB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FHB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FHB...

==================================================
Training Baseline FHB (SVM)
==================================================
Training SVM model...

Baseline FHB Performance:
MAE: 316466.2191
RMSE: 364207.1535
MAPE: 7.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 141
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0012, rank=1
   2. Feature_65_t3: importance=0.0010, rank=2
   3. Feature_65_t1: importance=0.0010, rank=3
   4. Feature_0_t2: importance=0.0007, rank=4
   5. Feature_0_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for FHB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FHB...

==================================================
Training Enhanced FHB (SVM)
==================================================
Training SVM model...

Enhanced FHB Performance:
MAE: 362432.4474
RMSE: 426329.9681
MAPE: 9.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t0: importance=0.0024, rank=1
   2. Feature_19_t3: importance=0.0013, rank=2
   3. Feature_3_t0: importance=0.0010, rank=3
   4. Feature_20_t3: importance=0.0009, rank=4
   5. Feature_15_t0: importance=0.0009, rank=5

ðŸ“Š FHB Results:
  Baseline MAPE: 7.77%
  Enhanced MAPE: 9.02%
  MAPE Improvement: -1.25% (-16.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 161/464: FIZZ
============================================================
ðŸ“Š Loading data for FIZZ...
ðŸ“Š Loading data for FIZZ from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FIZZ...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FIZZ...

==================================================
Training Baseline FIZZ (SVM)
==================================================
Training SVM model...

Baseline FIZZ Performance:
MAE: 145332.7480
RMSE: 176195.9689
MAPE: 5.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 229
   â€¢ Highly important features (top 5%): 125

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0003, rank=1
   2. Feature_67_t2: importance=0.0002, rank=2
   3. Feature_63_t0: importance=0.0002, rank=3
   4. Feature_64_t3: importance=0.0002, rank=4
   5. Feature_2_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for FIZZ...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FIZZ...

==================================================
Training Enhanced FIZZ (SVM)
==================================================
Training SVM model...

Enhanced FIZZ Performance:
MAE: 116274.7559
RMSE: 139491.3808
MAPE: 4.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t2: importance=0.0023, rank=1
   2. Feature_19_t2: importance=0.0021, rank=2
   3. Feature_13_t2: importance=0.0021, rank=3
   4. Feature_20_t3: importance=0.0013, rank=4
   5. Feature_9_t3: importance=0.0013, rank=5

ðŸ“Š FIZZ Results:
  Baseline MAPE: 5.74%
  Enhanced MAPE: 4.76%
  MAPE Improvement: +0.97% (+17.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 162/464: FMC
============================================================
ðŸ“Š Loading data for FMC...
ðŸ“Š Loading data for FMC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FMC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FMC...

==================================================
Training Baseline FMC (SVM)
==================================================
Training SVM model...

Baseline FMC Performance:
MAE: 478394.0227
RMSE: 603043.6493
MAPE: 6.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 155
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_1_t3: importance=0.0007, rank=2
   3. Feature_65_t3: importance=0.0005, rank=3
   4. Feature_1_t1: importance=0.0005, rank=4
   5. Feature_63_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for FMC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FMC...

==================================================
Training Enhanced FMC (SVM)
==================================================
Training SVM model...

Enhanced FMC Performance:
MAE: 477413.8696
RMSE: 605288.5786
MAPE: 6.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 81

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0011, rank=1
   2. Feature_9_t3: importance=0.0011, rank=2
   3. Feature_7_t0: importance=0.0008, rank=3
   4. Feature_13_t3: importance=0.0008, rank=4
   5. Feature_23_t3: importance=0.0007, rank=5

ðŸ“Š FMC Results:
  Baseline MAPE: 6.76%
  Enhanced MAPE: 6.76%
  MAPE Improvement: +0.01% (+0.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 163/464: FORM
============================================================
ðŸ“Š Loading data for FORM...
ðŸ“Š Loading data for FORM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FORM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FORM...

==================================================
Training Baseline FORM (SVM)
==================================================
Training SVM model...

Baseline FORM Performance:
MAE: 261709.1427
RMSE: 322307.3084
MAPE: 9.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 42
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0012, rank=1
   2. Feature_1_t0: importance=0.0008, rank=2
   3. Feature_67_t0: importance=0.0005, rank=3
   4. Feature_1_t3: importance=0.0005, rank=4
   5. Feature_67_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for FORM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FORM...

==================================================
Training Enhanced FORM (SVM)
==================================================
Training SVM model...

Enhanced FORM Performance:
MAE: 217266.1363
RMSE: 273636.9296
MAPE: 8.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0024, rank=1
   2. Feature_19_t1: importance=0.0023, rank=2
   3. Feature_23_t1: importance=0.0022, rank=3
   4. Feature_18_t2: importance=0.0006, rank=4
   5. Feature_13_t2: importance=0.0005, rank=5

ðŸ“Š FORM Results:
  Baseline MAPE: 9.78%
  Enhanced MAPE: 8.37%
  MAPE Improvement: +1.41% (+14.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 164/464: FOXF
============================================================
ðŸ“Š Loading data for FOXF...
ðŸ“Š Loading data for FOXF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FOXF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FOXF...

==================================================
Training Baseline FOXF (SVM)
==================================================
Training SVM model...

Baseline FOXF Performance:
MAE: 240080.5605
RMSE: 295491.8160
MAPE: 10.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0003, rank=1
   2. Feature_65_t3: importance=0.0003, rank=2
   3. Feature_67_t2: importance=0.0003, rank=3
   4. Feature_0_t0: importance=0.0002, rank=4
   5. Feature_65_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for FOXF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FOXF...

==================================================
Training Enhanced FOXF (SVM)
==================================================
Training SVM model...

Enhanced FOXF Performance:
MAE: 220177.1972
RMSE: 283433.4234
MAPE: 9.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 35
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t1: importance=0.0020, rank=1
   2. Feature_20_t1: importance=0.0005, rank=2
   3. Feature_7_t3: importance=0.0004, rank=3
   4. Feature_7_t0: importance=0.0004, rank=4
   5. Feature_16_t2: importance=0.0003, rank=5

ðŸ“Š FOXF Results:
  Baseline MAPE: 10.15%
  Enhanced MAPE: 9.61%
  MAPE Improvement: +0.54% (+5.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 165/464: FRPT
============================================================
ðŸ“Š Loading data for FRPT...
ðŸ“Š Loading data for FRPT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FRPT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FRPT...

==================================================
Training Baseline FRPT (SVM)
==================================================
Training SVM model...

Baseline FRPT Performance:
MAE: 304510.7326
RMSE: 363639.2226
MAPE: 5.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 213
   â€¢ Highly important features (top 5%): 138

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0003, rank=1
   2. Feature_1_t2: importance=0.0003, rank=2
   3. Feature_63_t1: importance=0.0002, rank=3
   4. Feature_64_t3: importance=0.0002, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for FRPT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FRPT...

==================================================
Training Enhanced FRPT (SVM)
==================================================
Training SVM model...

Enhanced FRPT Performance:
MAE: 264669.5787
RMSE: 332207.7085
MAPE: 4.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t3: importance=0.0004, rank=1
   2. Feature_13_t1: importance=0.0003, rank=2
   3. Feature_12_t1: importance=0.0003, rank=3
   4. Feature_9_t1: importance=0.0003, rank=4
   5. Feature_10_t2: importance=0.0003, rank=5

ðŸ“Š FRPT Results:
  Baseline MAPE: 5.66%
  Enhanced MAPE: 4.88%
  MAPE Improvement: +0.78% (+13.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 166/464: FSS
============================================================
ðŸ“Š Loading data for FSS...
ðŸ“Š Loading data for FSS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FSS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FSS...

==================================================
Training Baseline FSS (SVM)
==================================================
Training SVM model...

Baseline FSS Performance:
MAE: 241702.7153
RMSE: 335490.0291
MAPE: 13.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 103
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0053, rank=1
   2. Feature_65_t1: importance=0.0018, rank=2
   3. Feature_66_t3: importance=0.0014, rank=3
   4. Feature_2_t3: importance=0.0011, rank=4
   5. Feature_1_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for FSS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FSS...

==================================================
Training Enhanced FSS (SVM)
==================================================
Training SVM model...

Enhanced FSS Performance:
MAE: 240054.2120
RMSE: 337483.2032
MAPE: 13.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 87

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0016, rank=1
   2. Feature_12_t2: importance=0.0016, rank=2
   3. Feature_1_t2: importance=0.0013, rank=3
   4. Feature_4_t1: importance=0.0013, rank=4
   5. Feature_10_t3: importance=0.0012, rank=5

ðŸ“Š FSS Results:
  Baseline MAPE: 13.59%
  Enhanced MAPE: 13.42%
  MAPE Improvement: +0.17% (+1.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 167/464: FUL
============================================================
ðŸ“Š Loading data for FUL...
ðŸ“Š Loading data for FUL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FUL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FUL...

==================================================
Training Baseline FUL (SVM)
==================================================
Training SVM model...

Baseline FUL Performance:
MAE: 96203.5210
RMSE: 123368.5024
MAPE: 8.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 115
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0009, rank=1
   2. Feature_65_t1: importance=0.0008, rank=2
   3. Feature_65_t2: importance=0.0006, rank=3
   4. Feature_1_t1: importance=0.0005, rank=4
   5. Feature_63_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for FUL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FUL...

==================================================
Training Enhanced FUL (SVM)
==================================================
Training SVM model...

Enhanced FUL Performance:
MAE: 96712.2450
RMSE: 128879.7712
MAPE: 8.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0004, rank=1
   2. Feature_5_t0: importance=0.0004, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_24_t3: importance=0.0003, rank=4
   5. Feature_24_t2: importance=0.0003, rank=5

ðŸ“Š FUL Results:
  Baseline MAPE: 8.85%
  Enhanced MAPE: 8.59%
  MAPE Improvement: +0.25% (+2.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 168/464: FULT
============================================================
ðŸ“Š Loading data for FULT...
ðŸ“Š Loading data for FULT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FULT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FULT...

==================================================
Training Baseline FULT (SVM)
==================================================
Training SVM model...

Baseline FULT Performance:
MAE: 567678.6854
RMSE: 687241.5202
MAPE: 9.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0008, rank=1
   2. Feature_65_t3: importance=0.0004, rank=2
   3. Feature_2_t1: importance=0.0003, rank=3
   4. Feature_65_t1: importance=0.0003, rank=4
   5. Feature_64_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for FULT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FULT...

==================================================
Training Enhanced FULT (SVM)
==================================================
Training SVM model...

Enhanced FULT Performance:
MAE: 457902.0776
RMSE: 595009.4467
MAPE: 7.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0004, rank=1
   2. Feature_15_t1: importance=0.0004, rank=2
   3. Feature_9_t3: importance=0.0003, rank=3
   4. Feature_20_t1: importance=0.0003, rank=4
   5. Feature_24_t1: importance=0.0003, rank=5

ðŸ“Š FULT Results:
  Baseline MAPE: 9.13%
  Enhanced MAPE: 7.40%
  MAPE Improvement: +1.74% (+19.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 169/464: FUN
============================================================
ðŸ“Š Loading data for FUN...
ðŸ“Š Loading data for FUN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing FUN: 'FUN'

============================================================
TESTING TICKER 170/464: FWRD
============================================================
ðŸ“Š Loading data for FWRD...
ðŸ“Š Loading data for FWRD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for FWRD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for FWRD...

==================================================
Training Baseline FWRD (SVM)
==================================================
Training SVM model...

Baseline FWRD Performance:
MAE: 419224.2965
RMSE: 561711.1074
MAPE: 10.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 180
   â€¢ Highly important features (top 5%): 95

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0027, rank=1
   2. Feature_2_t3: importance=0.0011, rank=2
   3. Feature_2_t1: importance=0.0009, rank=3
   4. Feature_64_t3: importance=0.0008, rank=4
   5. Feature_0_t1: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for FWRD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for FWRD...

==================================================
Training Enhanced FWRD (SVM)
==================================================
Training SVM model...

Enhanced FWRD Performance:
MAE: 417986.2680
RMSE: 555121.9531
MAPE: 10.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0023, rank=1
   2. Feature_13_t3: importance=0.0023, rank=2
   3. Feature_22_t3: importance=0.0017, rank=3
   4. Feature_20_t3: importance=0.0015, rank=4
   5. Feature_11_t3: importance=0.0014, rank=5

ðŸ“Š FWRD Results:
  Baseline MAPE: 10.57%
  Enhanced MAPE: 10.44%
  MAPE Improvement: +0.13% (+1.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 171/464: GBX
============================================================
ðŸ“Š Loading data for GBX...
ðŸ“Š Loading data for GBX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GBX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GBX...

==================================================
Training Baseline GBX (SVM)
==================================================
Training SVM model...

Baseline GBX Performance:
MAE: 96614.4355
RMSE: 118759.3993
MAPE: 5.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0004, rank=1
   2. Feature_67_t3: importance=0.0004, rank=2
   3. Feature_67_t1: importance=0.0003, rank=3
   4. Feature_64_t0: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for GBX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GBX...

==================================================
Training Enhanced GBX (SVM)
==================================================
Training SVM model...

Enhanced GBX Performance:
MAE: 108831.8163
RMSE: 130419.6740
MAPE: 5.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t1: importance=0.0002, rank=1
   2. Feature_12_t0: importance=0.0002, rank=2
   3. Feature_5_t0: importance=0.0002, rank=3
   4. Feature_21_t1: importance=0.0002, rank=4
   5. Feature_20_t1: importance=0.0002, rank=5

ðŸ“Š GBX Results:
  Baseline MAPE: 5.09%
  Enhanced MAPE: 5.77%
  MAPE Improvement: -0.69% (-13.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 172/464: GDEN
============================================================
ðŸ“Š Loading data for GDEN...
ðŸ“Š Loading data for GDEN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GDEN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GDEN...

==================================================
Training Baseline GDEN (SVM)
==================================================
Training SVM model...

Baseline GDEN Performance:
MAE: 59147.9170
RMSE: 73238.5800
MAPE: 9.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 17

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0018, rank=1
   2. Feature_67_t3: importance=0.0013, rank=2
   3. Feature_0_t2: importance=0.0011, rank=3
   4. Feature_0_t3: importance=0.0009, rank=4
   5. Feature_2_t3: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for GDEN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GDEN...

==================================================
Training Enhanced GDEN (SVM)
==================================================
Training SVM model...

Enhanced GDEN Performance:
MAE: 91209.2674
RMSE: 109698.6649
MAPE: 12.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 41
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0070, rank=1
   2. Feature_13_t3: importance=0.0049, rank=2
   3. Feature_22_t0: importance=0.0020, rank=3
   4. Feature_0_t3: importance=0.0016, rank=4
   5. Feature_9_t1: importance=0.0016, rank=5

ðŸ“Š GDEN Results:
  Baseline MAPE: 9.22%
  Enhanced MAPE: 12.58%
  MAPE Improvement: -3.35% (-36.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 173/464: GEO
============================================================
ðŸ“Š Loading data for GEO...
ðŸ“Š Loading data for GEO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GEO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GEO...

==================================================
Training Baseline GEO (SVM)
==================================================
Training SVM model...

Baseline GEO Performance:
MAE: 1417508.8851
RMSE: 1936359.9695
MAPE: 13.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0007, rank=2
   3. Feature_2_t0: importance=0.0005, rank=3
   4. Feature_65_t2: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for GEO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GEO...

==================================================
Training Enhanced GEO (SVM)
==================================================
Training SVM model...

Enhanced GEO Performance:
MAE: 1295830.0760
RMSE: 2038836.6281
MAPE: 12.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0015, rank=1
   2. Feature_15_t1: importance=0.0008, rank=2
   3. Feature_20_t3: importance=0.0008, rank=3
   4. Feature_13_t2: importance=0.0008, rank=4
   5. Feature_10_t3: importance=0.0007, rank=5

ðŸ“Š GEO Results:
  Baseline MAPE: 13.98%
  Enhanced MAPE: 12.56%
  MAPE Improvement: +1.42% (+10.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 174/464: GES
============================================================
ðŸ“Š Loading data for GES...
ðŸ“Š Loading data for GES from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GES...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GES...

==================================================
Training Baseline GES (SVM)
==================================================
Training SVM model...

Baseline GES Performance:
MAE: 655255.7125
RMSE: 851494.1054
MAPE: 7.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0006, rank=1
   2. Feature_0_t2: importance=0.0004, rank=2
   3. Feature_0_t0: importance=0.0003, rank=3
   4. Feature_0_t1: importance=0.0003, rank=4
   5. Feature_67_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for GES...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GES...

==================================================
Training Enhanced GES (SVM)
==================================================
Training SVM model...

Enhanced GES Performance:
MAE: 577439.4142
RMSE: 781015.3136
MAPE: 6.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0007, rank=1
   2. Feature_16_t1: importance=0.0006, rank=2
   3. Feature_21_t3: importance=0.0005, rank=3
   4. Feature_18_t3: importance=0.0005, rank=4
   5. Feature_0_t0: importance=0.0005, rank=5

ðŸ“Š GES Results:
  Baseline MAPE: 7.88%
  Enhanced MAPE: 6.91%
  MAPE Improvement: +0.97% (+12.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 175/464: GFF
============================================================
ðŸ“Š Loading data for GFF...
ðŸ“Š Loading data for GFF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GFF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GFF...

==================================================
Training Baseline GFF (SVM)
==================================================
Training SVM model...

Baseline GFF Performance:
MAE: 195774.3607
RMSE: 260791.0642
MAPE: 9.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0020, rank=1
   2. Feature_65_t2: importance=0.0007, rank=2
   3. Feature_2_t1: importance=0.0007, rank=3
   4. Feature_1_t0: importance=0.0005, rank=4
   5. Feature_0_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for GFF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GFF...

==================================================
Training Enhanced GFF (SVM)
==================================================
Training SVM model...

Enhanced GFF Performance:
MAE: 278058.7306
RMSE: 341076.0282
MAPE: 13.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t2: importance=0.0017, rank=1
   2. Feature_6_t3: importance=0.0013, rank=2
   3. Feature_12_t0: importance=0.0012, rank=3
   4. Feature_4_t0: importance=0.0010, rank=4
   5. Feature_21_t2: importance=0.0008, rank=5

ðŸ“Š GFF Results:
  Baseline MAPE: 9.48%
  Enhanced MAPE: 13.85%
  MAPE Improvement: -4.37% (-46.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 176/464: GIII
============================================================
ðŸ“Š Loading data for GIII...
ðŸ“Š Loading data for GIII from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GIII...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GIII...

==================================================
Training Baseline GIII (SVM)
==================================================
Training SVM model...

Baseline GIII Performance:
MAE: 301982.4521
RMSE: 347132.3376
MAPE: 6.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 116
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0002, rank=1
   2. Feature_1_t1: importance=0.0001, rank=2
   3. Feature_3_t3: importance=0.0001, rank=3
   4. Feature_67_t3: importance=0.0001, rank=4
   5. Feature_66_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for GIII...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GIII...

==================================================
Training Enhanced GIII (SVM)
==================================================
Training SVM model...

Enhanced GIII Performance:
MAE: 261956.6165
RMSE: 307204.0714
MAPE: 5.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t2: importance=0.0010, rank=1
   2. Feature_6_t2: importance=0.0003, rank=2
   3. Feature_16_t3: importance=0.0003, rank=3
   4. Feature_22_t0: importance=0.0002, rank=4
   5. Feature_3_t2: importance=0.0002, rank=5

ðŸ“Š GIII Results:
  Baseline MAPE: 6.63%
  Enhanced MAPE: 5.86%
  MAPE Improvement: +0.77% (+11.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 177/464: GKOS
============================================================
ðŸ“Š Loading data for GKOS...
ðŸ“Š Loading data for GKOS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GKOS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GKOS...

==================================================
Training Baseline GKOS (SVM)
==================================================
Training SVM model...

Baseline GKOS Performance:
MAE: 442481.6805
RMSE: 693609.9098
MAPE: 12.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 37
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t0: importance=0.0008, rank=1
   2. Feature_65_t2: importance=0.0007, rank=2
   3. Feature_65_t3: importance=0.0007, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_64_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for GKOS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GKOS...

==================================================
Training Enhanced GKOS (SVM)
==================================================
Training SVM model...

Enhanced GKOS Performance:
MAE: 409894.2748
RMSE: 721037.1018
MAPE: 12.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0008, rank=1
   2. Feature_19_t2: importance=0.0008, rank=2
   3. Feature_4_t3: importance=0.0007, rank=3
   4. Feature_16_t1: importance=0.0006, rank=4
   5. Feature_6_t3: importance=0.0006, rank=5

ðŸ“Š GKOS Results:
  Baseline MAPE: 12.83%
  Enhanced MAPE: 12.29%
  MAPE Improvement: +0.54% (+4.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 178/464: GNL
============================================================
ðŸ“Š Loading data for GNL...
ðŸ“Š Loading data for GNL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GNL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GNL...

==================================================
Training Baseline GNL (SVM)
==================================================
Training SVM model...

Baseline GNL Performance:
MAE: 1128695.0528
RMSE: 1664995.7482
MAPE: 14.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0026, rank=1
   2. Feature_1_t1: importance=0.0016, rank=2
   3. Feature_64_t0: importance=0.0011, rank=3
   4. Feature_67_t1: importance=0.0007, rank=4
   5. Feature_1_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for GNL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GNL...

==================================================
Training Enhanced GNL (SVM)
==================================================
Training SVM model...

Enhanced GNL Performance:
MAE: 1139078.8487
RMSE: 1675008.8865
MAPE: 14.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0015, rank=1
   2. Feature_13_t1: importance=0.0013, rank=2
   3. Feature_4_t1: importance=0.0012, rank=3
   4. Feature_1_t1: importance=0.0012, rank=4
   5. Feature_16_t3: importance=0.0012, rank=5

ðŸ“Š GNL Results:
  Baseline MAPE: 14.08%
  Enhanced MAPE: 14.15%
  MAPE Improvement: -0.07% (-0.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 179/464: GNW
============================================================
ðŸ“Š Loading data for GNW...
ðŸ“Š Loading data for GNW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GNW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GNW...

==================================================
Training Baseline GNW (SVM)
==================================================
Training SVM model...

Baseline GNW Performance:
MAE: 715071.6386
RMSE: 916695.2371
MAPE: 9.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0010, rank=1
   2. Feature_65_t0: importance=0.0008, rank=2
   3. Feature_65_t1: importance=0.0005, rank=3
   4. Feature_63_t1: importance=0.0003, rank=4
   5. Feature_63_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for GNW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GNW...

==================================================
Training Enhanced GNW (SVM)
==================================================
Training SVM model...

Enhanced GNW Performance:
MAE: 671243.0937
RMSE: 884619.8814
MAPE: 9.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_8_t2: importance=0.0008, rank=1
   2. Feature_16_t3: importance=0.0007, rank=2
   3. Feature_2_t2: importance=0.0007, rank=3
   4. Feature_16_t2: importance=0.0006, rank=4
   5. Feature_19_t3: importance=0.0005, rank=5

ðŸ“Š GNW Results:
  Baseline MAPE: 9.92%
  Enhanced MAPE: 9.25%
  MAPE Improvement: +0.67% (+6.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 180/464: GOGO
============================================================
ðŸ“Š Loading data for GOGO...
ðŸ“Š Loading data for GOGO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing GOGO: 'GOGO'

============================================================
TESTING TICKER 181/464: GOLF
============================================================
ðŸ“Š Loading data for GOLF...
ðŸ“Š Loading data for GOLF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GOLF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GOLF...

==================================================
Training Baseline GOLF (SVM)
==================================================
Training SVM model...

Baseline GOLF Performance:
MAE: 300480.6944
RMSE: 424075.2980
MAPE: 6.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0007, rank=1
   2. Feature_64_t3: importance=0.0006, rank=2
   3. Feature_67_t1: importance=0.0005, rank=3
   4. Feature_64_t1: importance=0.0004, rank=4
   5. Feature_1_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for GOLF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GOLF...

==================================================
Training Enhanced GOLF (SVM)
==================================================
Training SVM model...

Enhanced GOLF Performance:
MAE: 448370.5091
RMSE: 548525.7217
MAPE: 9.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0016, rank=1
   2. Feature_19_t1: importance=0.0016, rank=2
   3. Feature_23_t1: importance=0.0015, rank=3
   4. Feature_15_t1: importance=0.0006, rank=4
   5. Feature_4_t0: importance=0.0004, rank=5

ðŸ“Š GOLF Results:
  Baseline MAPE: 6.00%
  Enhanced MAPE: 9.07%
  MAPE Improvement: -3.07% (-51.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 182/464: GPI
============================================================
ðŸ“Š Loading data for GPI...
ðŸ“Š Loading data for GPI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GPI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GPI...

==================================================
Training Baseline GPI (SVM)
==================================================
Training SVM model...

Baseline GPI Performance:
MAE: 56159.4902
RMSE: 77487.7333
MAPE: 3.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 30
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0001, rank=1
   2. Feature_0_t3: importance=0.0000, rank=2
   3. Feature_65_t3: importance=0.0000, rank=3
   4. Feature_64_t3: importance=0.0000, rank=4
   5. Feature_0_t1: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for GPI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GPI...

==================================================
Training Enhanced GPI (SVM)
==================================================
Training SVM model...

Enhanced GPI Performance:
MAE: 54691.7246
RMSE: 71984.4855
MAPE: 3.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0001, rank=1
   2. Feature_18_t2: importance=0.0000, rank=2
   3. Feature_11_t3: importance=0.0000, rank=3
   4. Feature_6_t2: importance=0.0000, rank=4
   5. Feature_12_t3: importance=0.0000, rank=5

ðŸ“Š GPI Results:
  Baseline MAPE: 3.83%
  Enhanced MAPE: 3.73%
  MAPE Improvement: +0.10% (+2.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 183/464: GRBK
============================================================
ðŸ“Š Loading data for GRBK...
ðŸ“Š Loading data for GRBK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GRBK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GRBK...

==================================================
Training Baseline GRBK (SVM)
==================================================
Training SVM model...

Baseline GRBK Performance:
MAE: 151229.0471
RMSE: 209802.2449
MAPE: 10.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0020, rank=1
   2. Feature_2_t3: importance=0.0015, rank=2
   3. Feature_67_t0: importance=0.0013, rank=3
   4. Feature_67_t1: importance=0.0012, rank=4
   5. Feature_0_t3: importance=0.0012, rank=5

ðŸ”§ Applying universal feature engineering for GRBK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GRBK...

==================================================
Training Enhanced GRBK (SVM)
==================================================
Training SVM model...

Enhanced GRBK Performance:
MAE: 146176.6787
RMSE: 205405.1291
MAPE: 10.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0036, rank=1
   2. Feature_1_t1: importance=0.0021, rank=2
   3. Feature_1_t3: importance=0.0020, rank=3
   4. Feature_23_t1: importance=0.0019, rank=4
   5. Feature_13_t2: importance=0.0018, rank=5

ðŸ“Š GRBK Results:
  Baseline MAPE: 10.91%
  Enhanced MAPE: 10.27%
  MAPE Improvement: +0.64% (+5.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 184/464: GTY
============================================================
ðŸ“Š Loading data for GTY...
ðŸ“Š Loading data for GTY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GTY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GTY...

==================================================
Training Baseline GTY (SVM)
==================================================
Training SVM model...

Baseline GTY Performance:
MAE: 377041.3588
RMSE: 547506.6324
MAPE: 11.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 142
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0017, rank=1
   2. Feature_63_t3: importance=0.0016, rank=2
   3. Feature_65_t1: importance=0.0015, rank=3
   4. Feature_67_t3: importance=0.0012, rank=4
   5. Feature_2_t3: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for GTY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GTY...

==================================================
Training Enhanced GTY (SVM)
==================================================
Training SVM model...

Enhanced GTY Performance:
MAE: 380800.6501
RMSE: 566676.5336
MAPE: 11.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0039, rank=1
   2. Feature_12_t3: importance=0.0024, rank=2
   3. Feature_22_t3: importance=0.0022, rank=3
   4. Feature_11_t0: importance=0.0019, rank=4
   5. Feature_11_t1: importance=0.0018, rank=5

ðŸ“Š GTY Results:
  Baseline MAPE: 11.50%
  Enhanced MAPE: 11.69%
  MAPE Improvement: -0.19% (-1.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 185/464: GVA
============================================================
ðŸ“Š Loading data for GVA...
ðŸ“Š Loading data for GVA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for GVA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for GVA...

==================================================
Training Baseline GVA (SVM)
==================================================
Training SVM model...

Baseline GVA Performance:
MAE: 325807.2988
RMSE: 408620.2463
MAPE: 7.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0011, rank=1
   2. Feature_65_t0: importance=0.0008, rank=2
   3. Feature_0_t3: importance=0.0008, rank=3
   4. Feature_63_t3: importance=0.0006, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for GVA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for GVA...

==================================================
Training Enhanced GVA (SVM)
==================================================
Training SVM model...

Enhanced GVA Performance:
MAE: 269724.1126
RMSE: 345370.1730
MAPE: 5.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 30

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0010, rank=1
   2. Feature_21_t1: importance=0.0009, rank=2
   3. Feature_19_t3: importance=0.0007, rank=3
   4. Feature_6_t0: importance=0.0007, rank=4
   5. Feature_23_t3: importance=0.0007, rank=5

ðŸ“Š GVA Results:
  Baseline MAPE: 7.13%
  Enhanced MAPE: 5.69%
  MAPE Improvement: +1.44% (+20.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 186/464: HAFC
============================================================
ðŸ“Š Loading data for HAFC...
ðŸ“Š Loading data for HAFC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HAFC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HAFC...

==================================================
Training Baseline HAFC (SVM)
==================================================
Training SVM model...

Baseline HAFC Performance:
MAE: 86547.7905
RMSE: 122284.7529
MAPE: 14.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0016, rank=1
   2. Feature_0_t3: importance=0.0011, rank=2
   3. Feature_64_t0: importance=0.0009, rank=3
   4. Feature_0_t2: importance=0.0007, rank=4
   5. Feature_65_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for HAFC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HAFC...

==================================================
Training Enhanced HAFC (SVM)
==================================================
Training SVM model...

Enhanced HAFC Performance:
MAE: 97201.4229
RMSE: 128473.1592
MAPE: 17.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0030, rank=1
   2. Feature_13_t3: importance=0.0025, rank=2
   3. Feature_23_t3: importance=0.0024, rank=3
   4. Feature_4_t3: importance=0.0013, rank=4
   5. Feature_22_t3: importance=0.0008, rank=5

ðŸ“Š HAFC Results:
  Baseline MAPE: 14.63%
  Enhanced MAPE: 17.07%
  MAPE Improvement: -2.44% (-16.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 187/464: HASI
============================================================
ðŸ“Š Loading data for HASI...
ðŸ“Š Loading data for HASI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HASI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HASI...

==================================================
Training Baseline HASI (SVM)
==================================================
Training SVM model...

Baseline HASI Performance:
MAE: 803557.8722
RMSE: 914221.0500
MAPE: 6.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0004, rank=1
   2. Feature_64_t3: importance=0.0003, rank=2
   3. Feature_63_t0: importance=0.0003, rank=3
   4. Feature_2_t1: importance=0.0002, rank=4
   5. Feature_63_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for HASI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HASI...

==================================================
Training Enhanced HASI (SVM)
==================================================
Training SVM model...

Enhanced HASI Performance:
MAE: 457046.2434
RMSE: 560311.8862
MAPE: 3.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0006, rank=1
   2. Feature_13_t3: importance=0.0004, rank=2
   3. Feature_23_t3: importance=0.0004, rank=3
   4. Feature_6_t3: importance=0.0004, rank=4
   5. Feature_18_t1: importance=0.0003, rank=5

ðŸ“Š HASI Results:
  Baseline MAPE: 6.07%
  Enhanced MAPE: 3.46%
  MAPE Improvement: +2.61% (+43.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 188/464: HBI
============================================================
ðŸ“Š Loading data for HBI...
ðŸ“Š Loading data for HBI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HBI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HBI...

==================================================
Training Baseline HBI (SVM)
==================================================
Training SVM model...

Baseline HBI Performance:
MAE: 2686166.2573
RMSE: 3615759.1562
MAPE: 6.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 112
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0003, rank=1
   2. Feature_65_t1: importance=0.0002, rank=2
   3. Feature_2_t0: importance=0.0002, rank=3
   4. Feature_64_t1: importance=0.0002, rank=4
   5. Feature_63_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for HBI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HBI...

==================================================
Training Enhanced HBI (SVM)
==================================================
Training SVM model...

Enhanced HBI Performance:
MAE: 2522512.9544
RMSE: 3390789.0027
MAPE: 6.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0002, rank=1
   2. Feature_22_t2: importance=0.0002, rank=2
   3. Feature_22_t0: importance=0.0002, rank=3
   4. Feature_24_t3: importance=0.0002, rank=4
   5. Feature_24_t1: importance=0.0002, rank=5

ðŸ“Š HBI Results:
  Baseline MAPE: 6.52%
  Enhanced MAPE: 6.04%
  MAPE Improvement: +0.48% (+7.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 189/464: HCC
============================================================
ðŸ“Š Loading data for HCC...
ðŸ“Š Loading data for HCC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing HCC: 'HCC'

============================================================
TESTING TICKER 190/464: HCI
============================================================
ðŸ“Š Loading data for HCI...
ðŸ“Š Loading data for HCI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing HCI: 'HCI'

============================================================
TESTING TICKER 191/464: HCSG
============================================================
ðŸ“Š Loading data for HCSG...
ðŸ“Š Loading data for HCSG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HCSG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HCSG...

==================================================
Training Baseline HCSG (SVM)
==================================================
Training SVM model...

Baseline HCSG Performance:
MAE: 193547.5204
RMSE: 247204.7580
MAPE: 7.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0007, rank=1
   2. Feature_67_t3: importance=0.0002, rank=2
   3. Feature_65_t3: importance=0.0002, rank=3
   4. Feature_1_t3: importance=0.0001, rank=4
   5. Feature_63_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for HCSG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HCSG...

==================================================
Training Enhanced HCSG (SVM)
==================================================
Training SVM model...

Enhanced HCSG Performance:
MAE: 207435.4908
RMSE: 262634.8295
MAPE: 8.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 46
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t0: importance=0.0003, rank=1
   2. Feature_10_t2: importance=0.0002, rank=2
   3. Feature_16_t1: importance=0.0002, rank=3
   4. Feature_6_t3: importance=0.0002, rank=4
   5. Feature_13_t0: importance=0.0002, rank=5

ðŸ“Š HCSG Results:
  Baseline MAPE: 7.61%
  Enhanced MAPE: 8.14%
  MAPE Improvement: -0.53% (-6.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 192/464: HELE
============================================================
ðŸ“Š Loading data for HELE...
ðŸ“Š Loading data for HELE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HELE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HELE...

==================================================
Training Baseline HELE (SVM)
==================================================
Training SVM model...

Baseline HELE Performance:
MAE: 391568.4423
RMSE: 453602.6475
MAPE: 15.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0013, rank=1
   2. Feature_63_t1: importance=0.0012, rank=2
   3. Feature_64_t3: importance=0.0011, rank=3
   4. Feature_66_t1: importance=0.0005, rank=4
   5. Feature_63_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for HELE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HELE...

==================================================
Training Enhanced HELE (SVM)
==================================================
Training SVM model...

Enhanced HELE Performance:
MAE: 207009.5001
RMSE: 280442.6913
MAPE: 8.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0012, rank=1
   2. Feature_19_t2: importance=0.0009, rank=2
   3. Feature_24_t1: importance=0.0008, rank=3
   4. Feature_20_t3: importance=0.0008, rank=4
   5. Feature_7_t1: importance=0.0008, rank=5

ðŸ“Š HELE Results:
  Baseline MAPE: 15.74%
  Enhanced MAPE: 8.33%
  MAPE Improvement: +7.40% (+47.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 193/464: HFWA
============================================================
ðŸ“Š Loading data for HFWA...
ðŸ“Š Loading data for HFWA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HFWA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HFWA...

==================================================
Training Baseline HFWA (SVM)
==================================================
Training SVM model...

Baseline HFWA Performance:
MAE: 66148.6605
RMSE: 82760.7880
MAPE: 10.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 159
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0003, rank=1
   2. Feature_65_t1: importance=0.0002, rank=2
   3. Feature_67_t3: importance=0.0002, rank=3
   4. Feature_64_t0: importance=0.0002, rank=4
   5. Feature_67_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for HFWA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HFWA...

==================================================
Training Enhanced HFWA (SVM)
==================================================
Training SVM model...

Enhanced HFWA Performance:
MAE: 65655.6080
RMSE: 82079.8292
MAPE: 10.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0005, rank=1
   2. Feature_23_t3: importance=0.0005, rank=2
   3. Feature_19_t3: importance=0.0004, rank=3
   4. Feature_9_t1: importance=0.0004, rank=4
   5. Feature_6_t3: importance=0.0004, rank=5

ðŸ“Š HFWA Results:
  Baseline MAPE: 10.41%
  Enhanced MAPE: 10.19%
  MAPE Improvement: +0.23% (+2.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 194/464: HI
============================================================
ðŸ“Š Loading data for HI...
ðŸ“Š Loading data for HI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HI...

==================================================
Training Baseline HI (SVM)
==================================================
Training SVM model...

Baseline HI Performance:
MAE: 200131.3502
RMSE: 240212.3924
MAPE: 10.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0026, rank=1
   2. Feature_67_t3: importance=0.0015, rank=2
   3. Feature_1_t2: importance=0.0015, rank=3
   4. Feature_67_t2: importance=0.0014, rank=4
   5. Feature_2_t0: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for HI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HI...

==================================================
Training Enhanced HI (SVM)
==================================================
Training SVM model...

Enhanced HI Performance:
MAE: 202844.8420
RMSE: 248867.8120
MAPE: 10.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0054, rank=1
   2. Feature_21_t3: importance=0.0041, rank=2
   3. Feature_23_t3: importance=0.0035, rank=3
   4. Feature_13_t3: importance=0.0033, rank=4
   5. Feature_8_t2: importance=0.0017, rank=5

ðŸ“Š HI Results:
  Baseline MAPE: 10.49%
  Enhanced MAPE: 10.68%
  MAPE Improvement: -0.19% (-1.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 195/464: HIW
============================================================
ðŸ“Š Loading data for HIW...
ðŸ“Š Loading data for HIW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HIW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HIW...

==================================================
Training Baseline HIW (SVM)
==================================================
Training SVM model...

Baseline HIW Performance:
MAE: 824242.3434
RMSE: 1051227.6813
MAPE: 16.20%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 130
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0026, rank=1
   2. Feature_0_t3: importance=0.0012, rank=2
   3. Feature_64_t2: importance=0.0011, rank=3
   4. Feature_65_t2: importance=0.0011, rank=4
   5. Feature_65_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for HIW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HIW...

==================================================
Training Enhanced HIW (SVM)
==================================================
Training SVM model...

Enhanced HIW Performance:
MAE: 820643.0875
RMSE: 1039947.0338
MAPE: 16.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0019, rank=1
   2. Feature_15_t3: importance=0.0017, rank=2
   3. Feature_7_t1: importance=0.0014, rank=3
   4. Feature_2_t3: importance=0.0013, rank=4
   5. Feature_22_t3: importance=0.0012, rank=5

ðŸ“Š HIW Results:
  Baseline MAPE: 16.20%
  Enhanced MAPE: 16.25%
  MAPE Improvement: -0.05% (-0.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 196/464: HL
============================================================
ðŸ“Š Loading data for HL...
ðŸ“Š Loading data for HL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HL...

==================================================
Training Baseline HL (SVM)
==================================================
Training SVM model...

Baseline HL Performance:
MAE: 2604679.7924
RMSE: 3000180.3175
MAPE: 12.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0013, rank=1
   2. Feature_65_t2: importance=0.0009, rank=2
   3. Feature_67_t3: importance=0.0008, rank=3
   4. Feature_65_t3: importance=0.0008, rank=4
   5. Feature_1_t1: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for HL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HL...

==================================================
Training Enhanced HL (SVM)
==================================================
Training SVM model...

Enhanced HL Performance:
MAE: 2423394.0172
RMSE: 2840700.4223
MAPE: 12.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0009, rank=1
   2. Feature_23_t3: importance=0.0008, rank=2
   3. Feature_19_t3: importance=0.0008, rank=3
   4. Feature_22_t1: importance=0.0005, rank=4
   5. Feature_6_t1: importance=0.0005, rank=5

ðŸ“Š HL Results:
  Baseline MAPE: 12.93%
  Enhanced MAPE: 12.35%
  MAPE Improvement: +0.58% (+4.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 197/464: HLIT
============================================================
ðŸ“Š Loading data for HLIT...
ðŸ“Š Loading data for HLIT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HLIT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HLIT...

==================================================
Training Baseline HLIT (SVM)
==================================================
Training SVM model...

Baseline HLIT Performance:
MAE: 622925.6795
RMSE: 790274.0211
MAPE: 14.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 99
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0002, rank=1
   2. Feature_0_t2: importance=0.0002, rank=2
   3. Feature_65_t1: importance=0.0001, rank=3
   4. Feature_65_t2: importance=0.0001, rank=4
   5. Feature_64_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for HLIT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HLIT...

==================================================
Training Enhanced HLIT (SVM)
==================================================
Training SVM model...

Enhanced HLIT Performance:
MAE: 424936.6103
RMSE: 600865.3564
MAPE: 9.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0002, rank=1
   2. Feature_22_t3: importance=0.0002, rank=2
   3. Feature_4_t1: importance=0.0002, rank=3
   4. Feature_18_t2: importance=0.0001, rank=4
   5. Feature_21_t2: importance=0.0001, rank=5

ðŸ“Š HLIT Results:
  Baseline MAPE: 14.87%
  Enhanced MAPE: 9.49%
  MAPE Improvement: +5.39% (+36.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 198/464: HLX
============================================================
ðŸ“Š Loading data for HLX...
ðŸ“Š Loading data for HLX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HLX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HLX...

==================================================
Training Baseline HLX (SVM)
==================================================
Training SVM model...

Baseline HLX Performance:
MAE: 669660.9870
RMSE: 816386.1117
MAPE: 12.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0007, rank=1
   2. Feature_67_t2: importance=0.0004, rank=2
   3. Feature_2_t3: importance=0.0004, rank=3
   4. Feature_0_t0: importance=0.0003, rank=4
   5. Feature_67_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for HLX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HLX...

==================================================
Training Enhanced HLX (SVM)
==================================================
Training SVM model...

Enhanced HLX Performance:
MAE: 578218.6442
RMSE: 764476.9369
MAPE: 10.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0010, rank=1
   2. Feature_0_t3: importance=0.0004, rank=2
   3. Feature_21_t1: importance=0.0004, rank=3
   4. Feature_0_t0: importance=0.0003, rank=4
   5. Feature_7_t1: importance=0.0002, rank=5

ðŸ“Š HLX Results:
  Baseline MAPE: 12.77%
  Enhanced MAPE: 10.50%
  MAPE Improvement: +2.26% (+17.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 199/464: HMN
============================================================
ðŸ“Š Loading data for HMN...
ðŸ“Š Loading data for HMN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HMN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HMN...

==================================================
Training Baseline HMN (SVM)
==================================================
Training SVM model...

Baseline HMN Performance:
MAE: 97280.6030
RMSE: 119208.6797
MAPE: 15.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 189
   â€¢ Highly important features (top 5%): 102

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0013, rank=1
   2. Feature_63_t2: importance=0.0011, rank=2
   3. Feature_63_t3: importance=0.0009, rank=3
   4. Feature_1_t1: importance=0.0008, rank=4
   5. Feature_67_t1: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for HMN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HMN...

==================================================
Training Enhanced HMN (SVM)
==================================================
Training SVM model...

Enhanced HMN Performance:
MAE: 93168.2243
RMSE: 117114.7739
MAPE: 15.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 81

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0011, rank=1
   2. Feature_15_t2: importance=0.0009, rank=2
   3. Feature_9_t3: importance=0.0009, rank=3
   4. Feature_4_t1: importance=0.0008, rank=4
   5. Feature_22_t3: importance=0.0008, rank=5

ðŸ“Š HMN Results:
  Baseline MAPE: 15.30%
  Enhanced MAPE: 15.04%
  MAPE Improvement: +0.26% (+1.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 200/464: HNI
============================================================
ðŸ“Š Loading data for HNI...
ðŸ“Š Loading data for HNI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HNI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HNI...

==================================================
Training Baseline HNI (SVM)
==================================================
Training SVM model...

Baseline HNI Performance:
MAE: 96146.7508
RMSE: 143902.1506
MAPE: 11.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 150
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0009, rank=1
   2. Feature_63_t2: importance=0.0006, rank=2
   3. Feature_67_t0: importance=0.0005, rank=3
   4. Feature_2_t3: importance=0.0004, rank=4
   5. Feature_63_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for HNI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HNI...

==================================================
Training Enhanced HNI (SVM)
==================================================
Training SVM model...

Enhanced HNI Performance:
MAE: 87379.8333
RMSE: 144973.3766
MAPE: 10.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0008, rank=1
   2. Feature_19_t3: importance=0.0008, rank=2
   3. Feature_16_t0: importance=0.0006, rank=3
   4. Feature_21_t3: importance=0.0004, rank=4
   5. Feature_14_t3: importance=0.0004, rank=5

ðŸ“Š HNI Results:
  Baseline MAPE: 11.55%
  Enhanced MAPE: 10.60%
  MAPE Improvement: +0.95% (+8.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 201/464: HOPE
============================================================
ðŸ“Š Loading data for HOPE...
ðŸ“Š Loading data for HOPE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HOPE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HOPE...

==================================================
Training Baseline HOPE (SVM)
==================================================
Training SVM model...

Baseline HOPE Performance:
MAE: 303546.0644
RMSE: 390468.4289
MAPE: 13.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 131
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0009, rank=1
   2. Feature_65_t1: importance=0.0008, rank=2
   3. Feature_67_t3: importance=0.0006, rank=3
   4. Feature_65_t0: importance=0.0006, rank=4
   5. Feature_65_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for HOPE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HOPE...

==================================================
Training Enhanced HOPE (SVM)
==================================================
Training SVM model...

Enhanced HOPE Performance:
MAE: 302690.8327
RMSE: 376570.8531
MAPE: 13.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0018, rank=1
   2. Feature_22_t0: importance=0.0007, rank=2
   3. Feature_17_t3: importance=0.0007, rank=3
   4. Feature_5_t1: importance=0.0005, rank=4
   5. Feature_4_t1: importance=0.0005, rank=5

ðŸ“Š HOPE Results:
  Baseline MAPE: 13.05%
  Enhanced MAPE: 13.16%
  MAPE Improvement: -0.10% (-0.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 202/464: HP
============================================================
ðŸ“Š Loading data for HP...
ðŸ“Š Loading data for HP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HP...

==================================================
Training Baseline HP (SVM)
==================================================
Training SVM model...

Baseline HP Performance:
MAE: 869349.7964
RMSE: 1067754.0670
MAPE: 8.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 19

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0012, rank=1
   2. Feature_67_t2: importance=0.0005, rank=2
   3. Feature_1_t2: importance=0.0005, rank=3
   4. Feature_65_t1: importance=0.0004, rank=4
   5. Feature_1_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for HP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HP...

==================================================
Training Enhanced HP (SVM)
==================================================
Training SVM model...

Enhanced HP Performance:
MAE: 748354.1837
RMSE: 867977.1278
MAPE: 7.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0007, rank=1
   2. Feature_13_t1: importance=0.0006, rank=2
   3. Feature_22_t0: importance=0.0006, rank=3
   4. Feature_6_t1: importance=0.0005, rank=4
   5. Feature_23_t3: importance=0.0005, rank=5

ðŸ“Š HP Results:
  Baseline MAPE: 8.30%
  Enhanced MAPE: 7.24%
  MAPE Improvement: +1.06% (+12.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 203/464: HSII
============================================================
ðŸ“Š Loading data for HSII...
ðŸ“Š Loading data for HSII from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HSII...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HSII...

==================================================
Training Baseline HSII (SVM)
==================================================
Training SVM model...

Baseline HSII Performance:
MAE: 66371.3135
RMSE: 79242.6931
MAPE: 16.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 141
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0054, rank=1
   2. Feature_2_t3: importance=0.0049, rank=2
   3. Feature_63_t3: importance=0.0034, rank=3
   4. Feature_0_t2: importance=0.0027, rank=4
   5. Feature_0_t1: importance=0.0021, rank=5

ðŸ”§ Applying universal feature engineering for HSII...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HSII...

==================================================
Training Enhanced HSII (SVM)
==================================================
Training SVM model...

Enhanced HSII Performance:
MAE: 57445.8365
RMSE: 70744.5098
MAPE: 14.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 87

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0036, rank=1
   2. Feature_17_t3: importance=0.0025, rank=2
   3. Feature_11_t0: importance=0.0022, rank=3
   4. Feature_15_t0: importance=0.0021, rank=4
   5. Feature_13_t3: importance=0.0018, rank=5

ðŸ“Š HSII Results:
  Baseline MAPE: 16.24%
  Enhanced MAPE: 14.39%
  MAPE Improvement: +1.84% (+11.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 204/464: HSTM
============================================================
ðŸ“Š Loading data for HSTM...
ðŸ“Š Loading data for HSTM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HSTM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HSTM...

==================================================
Training Baseline HSTM (SVM)
==================================================
Training SVM model...

Baseline HSTM Performance:
MAE: 62041.5000
RMSE: 76383.6866
MAPE: 13.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0027, rank=1
   2. Feature_1_t3: importance=0.0012, rank=2
   3. Feature_1_t2: importance=0.0006, rank=3
   4. Feature_67_t0: importance=0.0005, rank=4
   5. Feature_0_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for HSTM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HSTM...

==================================================
Training Enhanced HSTM (SVM)
==================================================
Training SVM model...

Enhanced HSTM Performance:
MAE: 68185.5594
RMSE: 83797.7107
MAPE: 14.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0023, rank=1
   2. Feature_19_t0: importance=0.0016, rank=2
   3. Feature_7_t0: importance=0.0016, rank=3
   4. Feature_19_t3: importance=0.0016, rank=4
   5. Feature_9_t1: importance=0.0014, rank=5

ðŸ“Š HSTM Results:
  Baseline MAPE: 13.70%
  Enhanced MAPE: 14.15%
  MAPE Improvement: -0.45% (-3.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 205/464: HTH
============================================================
ðŸ“Š Loading data for HTH...
ðŸ“Š Loading data for HTH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HTH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HTH...

==================================================
Training Baseline HTH (SVM)
==================================================
Training SVM model...

Baseline HTH Performance:
MAE: 116891.9033
RMSE: 164763.0187
MAPE: 10.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 98
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0034, rank=1
   2. Feature_63_t2: importance=0.0018, rank=2
   3. Feature_64_t1: importance=0.0010, rank=3
   4. Feature_67_t2: importance=0.0009, rank=4
   5. Feature_1_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for HTH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HTH...

==================================================
Training Enhanced HTH (SVM)
==================================================
Training SVM model...

Enhanced HTH Performance:
MAE: 154161.4635
RMSE: 197945.5387
MAPE: 14.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0054, rank=1
   2. Feature_16_t1: importance=0.0029, rank=2
   3. Feature_21_t3: importance=0.0025, rank=3
   4. Feature_16_t0: importance=0.0023, rank=4
   5. Feature_18_t3: importance=0.0021, rank=5

ðŸ“Š HTH Results:
  Baseline MAPE: 10.21%
  Enhanced MAPE: 14.06%
  MAPE Improvement: -3.86% (-37.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 206/464: HTLD
============================================================
ðŸ“Š Loading data for HTLD...
ðŸ“Š Loading data for HTLD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HTLD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HTLD...

==================================================
Training Baseline HTLD (SVM)
==================================================
Training SVM model...

Baseline HTLD Performance:
MAE: 149577.7434
RMSE: 200716.8213
MAPE: 10.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0023, rank=1
   2. Feature_2_t3: importance=0.0019, rank=2
   3. Feature_1_t3: importance=0.0013, rank=3
   4. Feature_0_t3: importance=0.0010, rank=4
   5. Feature_65_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for HTLD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HTLD...

==================================================
Training Enhanced HTLD (SVM)
==================================================
Training SVM model...

Enhanced HTLD Performance:
MAE: 142571.6331
RMSE: 189347.9251
MAPE: 10.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0033, rank=1
   2. Feature_19_t1: importance=0.0018, rank=2
   3. Feature_11_t1: importance=0.0018, rank=3
   4. Feature_13_t3: importance=0.0016, rank=4
   5. Feature_23_t3: importance=0.0015, rank=5

ðŸ“Š HTLD Results:
  Baseline MAPE: 10.55%
  Enhanced MAPE: 10.08%
  MAPE Improvement: +0.47% (+4.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 207/464: HUBG
============================================================
ðŸ“Š Loading data for HUBG...
ðŸ“Š Loading data for HUBG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HUBG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HUBG...

==================================================
Training Baseline HUBG (SVM)
==================================================
Training SVM model...

Baseline HUBG Performance:
MAE: 218495.8160
RMSE: 271829.0763
MAPE: 24.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0018, rank=1
   2. Feature_65_t3: importance=0.0015, rank=2
   3. Feature_63_t3: importance=0.0014, rank=3
   4. Feature_0_t2: importance=0.0013, rank=4
   5. Feature_1_t3: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for HUBG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HUBG...

==================================================
Training Enhanced HUBG (SVM)
==================================================
Training SVM model...

Enhanced HUBG Performance:
MAE: 208955.1589
RMSE: 265673.7691
MAPE: 23.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0017, rank=1
   2. Feature_16_t3: importance=0.0015, rank=2
   3. Feature_1_t2: importance=0.0014, rank=3
   4. Feature_1_t1: importance=0.0014, rank=4
   5. Feature_13_t2: importance=0.0011, rank=5

ðŸ“Š HUBG Results:
  Baseline MAPE: 24.36%
  Enhanced MAPE: 23.79%
  MAPE Improvement: +0.57% (+2.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 208/464: HWKN
============================================================
ðŸ“Š Loading data for HWKN...
ðŸ“Š Loading data for HWKN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HWKN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HWKN...

==================================================
Training Baseline HWKN (SVM)
==================================================
Training SVM model...

Baseline HWKN Performance:
MAE: 103020.5204
RMSE: 145317.3702
MAPE: 18.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 174
   â€¢ Highly important features (top 5%): 110

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0035, rank=1
   2. Feature_63_t3: importance=0.0027, rank=2
   3. Feature_64_t2: importance=0.0017, rank=3
   4. Feature_63_t2: importance=0.0014, rank=4
   5. Feature_67_t3: importance=0.0013, rank=5

ðŸ”§ Applying universal feature engineering for HWKN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HWKN...

==================================================
Training Enhanced HWKN (SVM)
==================================================
Training SVM model...

Enhanced HWKN Performance:
MAE: 103258.8858
RMSE: 145995.8957
MAPE: 18.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0054, rank=1
   2. Feature_16_t2: importance=0.0037, rank=2
   3. Feature_20_t0: importance=0.0028, rank=3
   4. Feature_18_t3: importance=0.0028, rank=4
   5. Feature_13_t2: importance=0.0027, rank=5

ðŸ“Š HWKN Results:
  Baseline MAPE: 18.48%
  Enhanced MAPE: 18.56%
  MAPE Improvement: -0.08% (-0.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 209/464: HZO
============================================================
ðŸ“Š Loading data for HZO...
ðŸ“Š Loading data for HZO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for HZO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for HZO...

==================================================
Training Baseline HZO (SVM)
==================================================
Training SVM model...

Baseline HZO Performance:
MAE: 164760.1345
RMSE: 204361.0045
MAPE: 7.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 48
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0007, rank=1
   2. Feature_65_t1: importance=0.0003, rank=2
   3. Feature_2_t2: importance=0.0002, rank=3
   4. Feature_0_t3: importance=0.0002, rank=4
   5. Feature_67_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for HZO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for HZO...

==================================================
Training Enhanced HZO (SVM)
==================================================
Training SVM model...

Enhanced HZO Performance:
MAE: 161527.3230
RMSE: 200288.1488
MAPE: 6.89%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t2: importance=0.0002, rank=1
   2. Feature_19_t2: importance=0.0002, rank=2
   3. Feature_19_t1: importance=0.0002, rank=3
   4. Feature_11_t3: importance=0.0002, rank=4
   5. Feature_12_t3: importance=0.0002, rank=5

ðŸ“Š HZO Results:
  Baseline MAPE: 7.01%
  Enhanced MAPE: 6.89%
  MAPE Improvement: +0.12% (+1.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 210/464: IAC
============================================================
ðŸ“Š Loading data for IAC...
ðŸ“Š Loading data for IAC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing IAC: 'IAC'

============================================================
TESTING TICKER 211/464: IART
============================================================
ðŸ“Š Loading data for IART...
ðŸ“Š Loading data for IART from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IART...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IART...

==================================================
Training Baseline IART (SVM)
==================================================
Training SVM model...

Baseline IART Performance:
MAE: 431841.2019
RMSE: 572325.1398
MAPE: 10.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0006, rank=1
   2. Feature_65_t3: importance=0.0005, rank=2
   3. Feature_67_t1: importance=0.0005, rank=3
   4. Feature_1_t2: importance=0.0005, rank=4
   5. Feature_63_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for IART...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IART...

==================================================
Training Enhanced IART (SVM)
==================================================
Training SVM model...

Enhanced IART Performance:
MAE: 380506.2518
RMSE: 516844.6598
MAPE: 9.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t3: importance=0.0007, rank=1
   2. Feature_4_t1: importance=0.0007, rank=2
   3. Feature_23_t0: importance=0.0005, rank=3
   4. Feature_19_t0: importance=0.0005, rank=4
   5. Feature_18_t3: importance=0.0005, rank=5

ðŸ“Š IART Results:
  Baseline MAPE: 10.73%
  Enhanced MAPE: 9.74%
  MAPE Improvement: +0.99% (+9.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 212/464: IBP
============================================================
ðŸ“Š Loading data for IBP...
ðŸ“Š Loading data for IBP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IBP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IBP...

==================================================
Training Baseline IBP (SVM)
==================================================
Training SVM model...

Baseline IBP Performance:
MAE: 157625.4957
RMSE: 208793.7748
MAPE: 9.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 153
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0020, rank=1
   2. Feature_64_t1: importance=0.0007, rank=2
   3. Feature_64_t3: importance=0.0005, rank=3
   4. Feature_64_t2: importance=0.0005, rank=4
   5. Feature_1_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for IBP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IBP...

==================================================
Training Enhanced IBP (SVM)
==================================================
Training SVM model...

Enhanced IBP Performance:
MAE: 129116.9376
RMSE: 177823.5778
MAPE: 7.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0013, rank=1
   2. Feature_22_t2: importance=0.0008, rank=2
   3. Feature_11_t1: importance=0.0006, rank=3
   4. Feature_11_t2: importance=0.0006, rank=4
   5. Feature_11_t0: importance=0.0006, rank=5

ðŸ“Š IBP Results:
  Baseline MAPE: 9.71%
  Enhanced MAPE: 7.82%
  MAPE Improvement: +1.89% (+19.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 213/464: ICHR
============================================================
ðŸ“Š Loading data for ICHR...
ðŸ“Š Loading data for ICHR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ICHR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ICHR...

==================================================
Training Baseline ICHR (SVM)
==================================================
Training SVM model...

Baseline ICHR Performance:
MAE: 97398.6632
RMSE: 136972.8186
MAPE: 11.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0006, rank=1
   2. Feature_64_t3: importance=0.0005, rank=2
   3. Feature_65_t2: importance=0.0004, rank=3
   4. Feature_67_t0: importance=0.0003, rank=4
   5. Feature_65_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ICHR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ICHR...

==================================================
Training Enhanced ICHR (SVM)
==================================================
Training SVM model...

Enhanced ICHR Performance:
MAE: 104630.3810
RMSE: 134490.2450
MAPE: 11.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0005, rank=1
   2. Feature_18_t3: importance=0.0004, rank=2
   3. Feature_15_t1: importance=0.0004, rank=3
   4. Feature_12_t1: importance=0.0002, rank=4
   5. Feature_20_t1: importance=0.0002, rank=5

ðŸ“Š ICHR Results:
  Baseline MAPE: 11.01%
  Enhanced MAPE: 11.91%
  MAPE Improvement: -0.91% (-8.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 214/464: ICUI
============================================================
ðŸ“Š Loading data for ICUI...
ðŸ“Š Loading data for ICUI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ICUI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ICUI...

==================================================
Training Baseline ICUI (SVM)
==================================================
Training SVM model...

Baseline ICUI Performance:
MAE: 89604.3957
RMSE: 122961.0208
MAPE: 8.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0009, rank=1
   2. Feature_65_t2: importance=0.0008, rank=2
   3. Feature_67_t2: importance=0.0005, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_1_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for ICUI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ICUI...

==================================================
Training Enhanced ICUI (SVM)
==================================================
Training SVM model...

Enhanced ICUI Performance:
MAE: 94720.9499
RMSE: 125514.3020
MAPE: 9.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t2: importance=0.0011, rank=1
   2. Feature_19_t3: importance=0.0010, rank=2
   3. Feature_13_t3: importance=0.0010, rank=3
   4. Feature_23_t3: importance=0.0010, rank=4
   5. Feature_20_t0: importance=0.0006, rank=5

ðŸ“Š ICUI Results:
  Baseline MAPE: 8.85%
  Enhanced MAPE: 9.55%
  MAPE Improvement: -0.71% (-8.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 215/464: IDCC
============================================================
ðŸ“Š Loading data for IDCC...
ðŸ“Š Loading data for IDCC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IDCC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IDCC...

==================================================
Training Baseline IDCC (SVM)
==================================================
Training SVM model...

Baseline IDCC Performance:
MAE: 235161.0792
RMSE: 323143.7058
MAPE: 7.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0007, rank=1
   2. Feature_63_t3: importance=0.0007, rank=2
   3. Feature_67_t1: importance=0.0005, rank=3
   4. Feature_67_t0: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for IDCC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IDCC...

==================================================
Training Enhanced IDCC (SVM)
==================================================
Training SVM model...

Enhanced IDCC Performance:
MAE: 209935.7264
RMSE: 291378.8247
MAPE: 5.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0009, rank=1
   2. Feature_4_t3: importance=0.0007, rank=2
   3. Feature_7_t1: importance=0.0006, rank=3
   4. Feature_7_t3: importance=0.0006, rank=4
   5. Feature_13_t0: importance=0.0005, rank=5

ðŸ“Š IDCC Results:
  Baseline MAPE: 7.22%
  Enhanced MAPE: 5.80%
  MAPE Improvement: +1.42% (+19.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 216/464: IIIN
============================================================
ðŸ“Š Loading data for IIIN...
ðŸ“Š Loading data for IIIN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IIIN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IIIN...

==================================================
Training Baseline IIIN (SVM)
==================================================
Training SVM model...

Baseline IIIN Performance:
MAE: 68541.1673
RMSE: 92782.0156
MAPE: 12.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 105
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0019, rank=1
   2. Feature_67_t2: importance=0.0009, rank=2
   3. Feature_0_t2: importance=0.0008, rank=3
   4. Feature_65_t2: importance=0.0008, rank=4
   5. Feature_1_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for IIIN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IIIN...

==================================================
Training Enhanced IIIN (SVM)
==================================================
Training SVM model...

Enhanced IIIN Performance:
MAE: 67663.7909
RMSE: 86646.2995
MAPE: 13.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0011, rank=1
   2. Feature_20_t1: importance=0.0009, rank=2
   3. Feature_4_t1: importance=0.0009, rank=3
   4. Feature_7_t0: importance=0.0009, rank=4
   5. Feature_0_t3: importance=0.0008, rank=5

ðŸ“Š IIIN Results:
  Baseline MAPE: 12.93%
  Enhanced MAPE: 13.38%
  MAPE Improvement: -0.45% (-3.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 217/464: IIPR
============================================================
ðŸ“Š Loading data for IIPR...
ðŸ“Š Loading data for IIPR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing IIPR: 'IIPR'

============================================================
TESTING TICKER 218/464: INDB
============================================================
ðŸ“Š Loading data for INDB...
ðŸ“Š Loading data for INDB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for INDB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for INDB...

==================================================
Training Baseline INDB (SVM)
==================================================
Training SVM model...

Baseline INDB Performance:
MAE: 136896.2948
RMSE: 169260.5043
MAPE: 13.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 144
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0008, rank=1
   2. Feature_0_t3: importance=0.0008, rank=2
   3. Feature_67_t3: importance=0.0004, rank=3
   4. Feature_0_t2: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for INDB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for INDB...

==================================================
Training Enhanced INDB (SVM)
==================================================
Training SVM model...

Enhanced INDB Performance:
MAE: 142719.6548
RMSE: 175624.0250
MAPE: 14.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0004, rank=1
   2. Feature_12_t1: importance=0.0003, rank=2
   3. Feature_18_t1: importance=0.0003, rank=3
   4. Feature_22_t2: importance=0.0003, rank=4
   5. Feature_7_t0: importance=0.0003, rank=5

ðŸ“Š INDB Results:
  Baseline MAPE: 13.93%
  Enhanced MAPE: 14.79%
  MAPE Improvement: -0.86% (-6.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 219/464: INN
============================================================
ðŸ“Š Loading data for INN...
ðŸ“Š Loading data for INN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for INN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for INN...

==================================================
Training Baseline INN (SVM)
==================================================
Training SVM model...

Baseline INN Performance:
MAE: 700385.7831
RMSE: 916744.9282
MAPE: 15.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0011, rank=1
   2. Feature_64_t0: importance=0.0006, rank=2
   3. Feature_64_t2: importance=0.0006, rank=3
   4. Feature_65_t3: importance=0.0004, rank=4
   5. Feature_1_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for INN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for INN...

==================================================
Training Enhanced INN (SVM)
==================================================
Training SVM model...

Enhanced INN Performance:
MAE: 661337.5279
RMSE: 894814.8852
MAPE: 13.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 20
   â€¢ Highly important features (top 5%): 15

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0039, rank=1
   2. Feature_13_t0: importance=0.0039, rank=2
   3. Feature_19_t0: importance=0.0036, rank=3
   4. Feature_22_t0: importance=0.0014, rank=4
   5. Feature_1_t1: importance=0.0010, rank=5

ðŸ“Š INN Results:
  Baseline MAPE: 15.27%
  Enhanced MAPE: 13.86%
  MAPE Improvement: +1.41% (+9.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 220/464: INSW
============================================================
ðŸ“Š Loading data for INSW...
ðŸ“Š Loading data for INSW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing INSW: 'INSW'

============================================================
TESTING TICKER 221/464: INVA
============================================================
ðŸ“Š Loading data for INVA...
ðŸ“Š Loading data for INVA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for INVA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for INVA...

==================================================
Training Baseline INVA (SVM)
==================================================
Training SVM model...

Baseline INVA Performance:
MAE: 392718.0305
RMSE: 544337.0469
MAPE: 3.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 31
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0001, rank=1
   2. Feature_66_t1: importance=0.0000, rank=2
   3. Feature_65_t2: importance=0.0000, rank=3
   4. Feature_64_t3: importance=0.0000, rank=4
   5. Feature_0_t2: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for INVA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for INVA...

==================================================
Training Enhanced INVA (SVM)
==================================================
Training SVM model...

Enhanced INVA Performance:
MAE: 392583.2766
RMSE: 545630.3387
MAPE: 3.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t0: importance=0.0001, rank=1
   2. Feature_11_t3: importance=0.0000, rank=2
   3. Feature_20_t0: importance=0.0000, rank=3
   4. Feature_18_t1: importance=0.0000, rank=4
   5. Feature_19_t2: importance=0.0000, rank=5

ðŸ“Š INVA Results:
  Baseline MAPE: 3.72%
  Enhanced MAPE: 3.71%
  MAPE Improvement: +0.02% (+0.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 222/464: IOSP
============================================================
ðŸ“Š Loading data for IOSP...
ðŸ“Š Loading data for IOSP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IOSP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IOSP...

==================================================
Training Baseline IOSP (SVM)
==================================================
Training SVM model...

Baseline IOSP Performance:
MAE: 43409.2690
RMSE: 59553.7133
MAPE: 15.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 175
   â€¢ Highly important features (top 5%): 94

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0027, rank=1
   2. Feature_65_t3: importance=0.0025, rank=2
   3. Feature_63_t3: importance=0.0020, rank=3
   4. Feature_2_t3: importance=0.0018, rank=4
   5. Feature_65_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for IOSP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IOSP...

==================================================
Training Enhanced IOSP (SVM)
==================================================
Training SVM model...

Enhanced IOSP Performance:
MAE: 47965.1082
RMSE: 65363.1286
MAPE: 17.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0018, rank=1
   2. Feature_15_t3: importance=0.0016, rank=2
   3. Feature_0_t3: importance=0.0014, rank=3
   4. Feature_15_t2: importance=0.0013, rank=4
   5. Feature_16_t0: importance=0.0012, rank=5

ðŸ“Š IOSP Results:
  Baseline MAPE: 15.63%
  Enhanced MAPE: 17.00%
  MAPE Improvement: -1.37% (-8.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 223/464: IPAR
============================================================
ðŸ“Š Loading data for IPAR...
ðŸ“Š Loading data for IPAR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for IPAR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for IPAR...

==================================================
Training Baseline IPAR (SVM)
==================================================
Training SVM model...

Baseline IPAR Performance:
MAE: 98835.0654
RMSE: 131104.1050
MAPE: 13.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 18

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0035, rank=1
   2. Feature_64_t1: importance=0.0032, rank=2
   3. Feature_2_t3: importance=0.0020, rank=3
   4. Feature_65_t3: importance=0.0016, rank=4
   5. Feature_64_t3: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for IPAR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for IPAR...

==================================================
Training Enhanced IPAR (SVM)
==================================================
Training SVM model...

Enhanced IPAR Performance:
MAE: 77702.3572
RMSE: 97204.7004
MAPE: 11.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t0: importance=0.0014, rank=1
   2. Feature_16_t1: importance=0.0013, rank=2
   3. Feature_14_t2: importance=0.0012, rank=3
   4. Feature_20_t2: importance=0.0012, rank=4
   5. Feature_2_t3: importance=0.0011, rank=5

ðŸ“Š IPAR Results:
  Baseline MAPE: 13.56%
  Enhanced MAPE: 11.02%
  MAPE Improvement: +2.54% (+18.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 224/464: ITGR
============================================================
ðŸ“Š Loading data for ITGR...
ðŸ“Š Loading data for ITGR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ITGR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ITGR...

==================================================
Training Baseline ITGR (SVM)
==================================================
Training SVM model...

Baseline ITGR Performance:
MAE: 192028.3048
RMSE: 240041.7831
MAPE: 7.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 163
   â€¢ Highly important features (top 5%): 89

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0047, rank=1
   2. Feature_63_t3: importance=0.0005, rank=2
   3. Feature_63_t1: importance=0.0005, rank=3
   4. Feature_64_t2: importance=0.0005, rank=4
   5. Feature_66_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ITGR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ITGR...

==================================================
Training Enhanced ITGR (SVM)
==================================================
Training SVM model...

Enhanced ITGR Performance:
MAE: 272305.4111
RMSE: 347659.9485
MAPE: 11.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 26
   â€¢ Highly important features (top 5%): 11

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0040, rank=1
   2. Feature_13_t3: importance=0.0039, rank=2
   3. Feature_19_t3: importance=0.0036, rank=3
   4. Feature_6_t0: importance=0.0034, rank=4
   5. Feature_6_t3: importance=0.0028, rank=5

ðŸ“Š ITGR Results:
  Baseline MAPE: 7.71%
  Enhanced MAPE: 11.09%
  MAPE Improvement: -3.39% (-43.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 225/464: ITRI
============================================================
ðŸ“Š Loading data for ITRI...
ðŸ“Š Loading data for ITRI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ITRI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ITRI...

==================================================
Training Baseline ITRI (SVM)
==================================================
Training SVM model...

Baseline ITRI Performance:
MAE: 208334.8800
RMSE: 270870.8341
MAPE: 8.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 102
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0012, rank=1
   2. Feature_65_t3: importance=0.0007, rank=2
   3. Feature_65_t2: importance=0.0007, rank=3
   4. Feature_67_t1: importance=0.0004, rank=4
   5. Feature_2_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for ITRI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ITRI...

==================================================
Training Enhanced ITRI (SVM)
==================================================
Training SVM model...

Enhanced ITRI Performance:
MAE: 213705.1886
RMSE: 268596.9972
MAPE: 7.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t0: importance=0.0009, rank=1
   2. Feature_22_t3: importance=0.0006, rank=2
   3. Feature_16_t0: importance=0.0005, rank=3
   4. Feature_16_t3: importance=0.0005, rank=4
   5. Feature_13_t2: importance=0.0004, rank=5

ðŸ“Š ITRI Results:
  Baseline MAPE: 8.00%
  Enhanced MAPE: 7.85%
  MAPE Improvement: +0.16% (+2.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 226/464: JBGS
============================================================
ðŸ“Š Loading data for JBGS...
ðŸ“Š Loading data for JBGS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing JBGS: 'JBGS'

============================================================
TESTING TICKER 227/464: JBLU
============================================================
ðŸ“Š Loading data for JBLU...
ðŸ“Š Loading data for JBLU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for JBLU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for JBLU...

==================================================
Training Baseline JBLU (SVM)
==================================================
Training SVM model...

Baseline JBLU Performance:
MAE: 6337395.6954
RMSE: 7843039.8282
MAPE: 12.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 122
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0012, rank=1
   2. Feature_63_t1: importance=0.0009, rank=2
   3. Feature_2_t2: importance=0.0004, rank=3
   4. Feature_2_t1: importance=0.0004, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for JBLU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for JBLU...

==================================================
Training Enhanced JBLU (SVM)
==================================================
Training SVM model...

Enhanced JBLU Performance:
MAE: 4999775.7692
RMSE: 6593622.6621
MAPE: 9.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t0: importance=0.0012, rank=1
   2. Feature_16_t0: importance=0.0011, rank=2
   3. Feature_12_t0: importance=0.0009, rank=3
   4. Feature_9_t3: importance=0.0008, rank=4
   5. Feature_20_t2: importance=0.0008, rank=5

ðŸ“Š JBLU Results:
  Baseline MAPE: 12.18%
  Enhanced MAPE: 9.42%
  MAPE Improvement: +2.76% (+22.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 228/464: JBSS
============================================================
ðŸ“Š Loading data for JBSS...
ðŸ“Š Loading data for JBSS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for JBSS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for JBSS...

==================================================
Training Baseline JBSS (SVM)
==================================================
Training SVM model...

Baseline JBSS Performance:
MAE: 27988.5279
RMSE: 43297.3414
MAPE: 16.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_63_t0: importance=0.0009, rank=2
   3. Feature_64_t3: importance=0.0007, rank=3
   4. Feature_63_t1: importance=0.0006, rank=4
   5. Feature_1_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for JBSS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for JBSS...

==================================================
Training Enhanced JBSS (SVM)
==================================================
Training SVM model...

Enhanced JBSS Performance:
MAE: 26782.0742
RMSE: 42380.4119
MAPE: 15.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0014, rank=1
   2. Feature_1_t2: importance=0.0013, rank=2
   3. Feature_13_t3: importance=0.0013, rank=3
   4. Feature_23_t1: importance=0.0013, rank=4
   5. Feature_19_t3: importance=0.0013, rank=5

ðŸ“Š JBSS Results:
  Baseline MAPE: 16.44%
  Enhanced MAPE: 15.24%
  MAPE Improvement: +1.20% (+7.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 229/464: JJSF
============================================================
ðŸ“Š Loading data for JJSF...
ðŸ“Š Loading data for JJSF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing JJSF: 'JJSF'

============================================================
TESTING TICKER 230/464: JOE
============================================================
ðŸ“Š Loading data for JOE...
ðŸ“Š Loading data for JOE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for JOE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for JOE...

==================================================
Training Baseline JOE (SVM)
==================================================
Training SVM model...

Baseline JOE Performance:
MAE: 62485.4563
RMSE: 80557.3127
MAPE: 9.43%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 115
   â€¢ Highly important features (top 5%): 64

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0006, rank=1
   2. Feature_67_t3: importance=0.0004, rank=2
   3. Feature_63_t0: importance=0.0003, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_65_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for JOE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for JOE...

==================================================
Training Enhanced JOE (SVM)
==================================================
Training SVM model...

Enhanced JOE Performance:
MAE: 65794.4436
RMSE: 82048.1158
MAPE: 9.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0008, rank=1
   2. Feature_22_t3: importance=0.0004, rank=2
   3. Feature_4_t2: importance=0.0004, rank=3
   4. Feature_4_t0: importance=0.0004, rank=4
   5. Feature_13_t3: importance=0.0004, rank=5

ðŸ“Š JOE Results:
  Baseline MAPE: 9.43%
  Enhanced MAPE: 9.84%
  MAPE Improvement: -0.40% (-4.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 231/464: KAI
============================================================
ðŸ“Š Loading data for KAI...
ðŸ“Š Loading data for KAI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KAI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KAI...

==================================================
Training Baseline KAI (SVM)
==================================================
Training SVM model...

Baseline KAI Performance:
MAE: 122218.5818
RMSE: 151344.2393
MAPE: 8.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0008, rank=1
   2. Feature_1_t3: importance=0.0007, rank=2
   3. Feature_63_t1: importance=0.0007, rank=3
   4. Feature_63_t2: importance=0.0005, rank=4
   5. Feature_65_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for KAI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KAI...

==================================================
Training Enhanced KAI (SVM)
==================================================
Training SVM model...

Enhanced KAI Performance:
MAE: 139697.1104
RMSE: 169516.0940
MAPE: 10.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0012, rank=1
   2. Feature_19_t3: importance=0.0012, rank=2
   3. Feature_13_t3: importance=0.0009, rank=3
   4. Feature_21_t3: importance=0.0007, rank=4
   5. Feature_12_t2: importance=0.0006, rank=5

ðŸ“Š KAI Results:
  Baseline MAPE: 8.88%
  Enhanced MAPE: 10.02%
  MAPE Improvement: -1.14% (-12.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 232/464: KALU
============================================================
ðŸ“Š Loading data for KALU...
ðŸ“Š Loading data for KALU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KALU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KALU...

==================================================
Training Baseline KALU (SVM)
==================================================
Training SVM model...

Baseline KALU Performance:
MAE: 58205.5558
RMSE: 82594.1368
MAPE: 13.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 166
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0008, rank=1
   2. Feature_2_t2: importance=0.0006, rank=2
   3. Feature_0_t3: importance=0.0006, rank=3
   4. Feature_63_t0: importance=0.0006, rank=4
   5. Feature_2_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for KALU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KALU...

==================================================
Training Enhanced KALU (SVM)
==================================================
Training SVM model...

Enhanced KALU Performance:
MAE: 61811.3662
RMSE: 80350.9822
MAPE: 13.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t2: importance=0.0010, rank=1
   2. Feature_16_t3: importance=0.0009, rank=2
   3. Feature_9_t3: importance=0.0008, rank=3
   4. Feature_8_t2: importance=0.0006, rank=4
   5. Feature_6_t0: importance=0.0006, rank=5

ðŸ“Š KALU Results:
  Baseline MAPE: 13.48%
  Enhanced MAPE: 13.99%
  MAPE Improvement: -0.50% (-3.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 233/464: KAR
============================================================
ðŸ“Š Loading data for KAR...
ðŸ“Š Loading data for KAR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing KAR: 'KAR'

============================================================
TESTING TICKER 234/464: KFY
============================================================
ðŸ“Š Loading data for KFY...
ðŸ“Š Loading data for KFY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KFY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KFY...

==================================================
Training Baseline KFY (SVM)
==================================================
Training SVM model...

Baseline KFY Performance:
MAE: 145718.8317
RMSE: 185403.0052
MAPE: 15.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0012, rank=1
   2. Feature_1_t1: importance=0.0009, rank=2
   3. Feature_64_t0: importance=0.0007, rank=3
   4. Feature_63_t0: importance=0.0006, rank=4
   5. Feature_1_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for KFY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KFY...

==================================================
Training Enhanced KFY (SVM)
==================================================
Training SVM model...

Enhanced KFY Performance:
MAE: 153489.7418
RMSE: 195782.2301
MAPE: 16.47%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0013, rank=1
   2. Feature_23_t2: importance=0.0010, rank=2
   3. Feature_21_t3: importance=0.0010, rank=3
   4. Feature_19_t2: importance=0.0010, rank=4
   5. Feature_19_t1: importance=0.0009, rank=5

ðŸ“Š KFY Results:
  Baseline MAPE: 15.99%
  Enhanced MAPE: 16.47%
  MAPE Improvement: -0.48% (-3.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 235/464: KLIC
============================================================
ðŸ“Š Loading data for KLIC...
ðŸ“Š Loading data for KLIC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KLIC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KLIC...

==================================================
Training Baseline KLIC (SVM)
==================================================
Training SVM model...

Baseline KLIC Performance:
MAE: 228814.2561
RMSE: 294271.8662
MAPE: 7.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 87

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0016, rank=1
   2. Feature_1_t2: importance=0.0011, rank=2
   3. Feature_1_t3: importance=0.0010, rank=3
   4. Feature_65_t3: importance=0.0008, rank=4
   5. Feature_64_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for KLIC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KLIC...

==================================================
Training Enhanced KLIC (SVM)
==================================================
Training SVM model...

Enhanced KLIC Performance:
MAE: 232368.0179
RMSE: 284271.0748
MAPE: 7.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0017, rank=1
   2. Feature_11_t3: importance=0.0012, rank=2
   3. Feature_22_t2: importance=0.0011, rank=3
   4. Feature_1_t1: importance=0.0010, rank=4
   5. Feature_15_t0: importance=0.0009, rank=5

ðŸ“Š KLIC Results:
  Baseline MAPE: 7.42%
  Enhanced MAPE: 7.41%
  MAPE Improvement: +0.00% (+0.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 236/464: KMT
============================================================
ðŸ“Š Loading data for KMT...
ðŸ“Š Loading data for KMT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KMT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KMT...

==================================================
Training Baseline KMT (SVM)
==================================================
Training SVM model...

Baseline KMT Performance:
MAE: 461573.9695
RMSE: 537720.7620
MAPE: 9.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 126
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0006, rank=1
   2. Feature_67_t2: importance=0.0006, rank=2
   3. Feature_64_t3: importance=0.0004, rank=3
   4. Feature_2_t2: importance=0.0003, rank=4
   5. Feature_0_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for KMT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KMT...

==================================================
Training Enhanced KMT (SVM)
==================================================
Training SVM model...

Enhanced KMT Performance:
MAE: 395800.1091
RMSE: 507202.5354
MAPE: 8.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0004, rank=1
   2. Feature_23_t2: importance=0.0004, rank=2
   3. Feature_13_t2: importance=0.0004, rank=3
   4. Feature_1_t2: importance=0.0004, rank=4
   5. Feature_12_t3: importance=0.0004, rank=5

ðŸ“Š KMT Results:
  Baseline MAPE: 9.63%
  Enhanced MAPE: 8.67%
  MAPE Improvement: +0.95% (+9.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 237/464: KN
============================================================
ðŸ“Š Loading data for KN...
ðŸ“Š Loading data for KN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KN...

==================================================
Training Baseline KN (SVM)
==================================================
Training SVM model...

Baseline KN Performance:
MAE: 260519.7178
RMSE: 333338.3006
MAPE: 11.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 135
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0005, rank=1
   2. Feature_67_t2: importance=0.0004, rank=2
   3. Feature_65_t2: importance=0.0004, rank=3
   4. Feature_65_t1: importance=0.0002, rank=4
   5. Feature_67_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for KN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KN...

==================================================
Training Enhanced KN (SVM)
==================================================
Training SVM model...

Enhanced KN Performance:
MAE: 239692.9523
RMSE: 305326.5091
MAPE: 11.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0005, rank=1
   2. Feature_11_t0: importance=0.0005, rank=2
   3. Feature_9_t2: importance=0.0004, rank=3
   4. Feature_12_t3: importance=0.0003, rank=4
   5. Feature_10_t2: importance=0.0003, rank=5

ðŸ“Š KN Results:
  Baseline MAPE: 11.80%
  Enhanced MAPE: 11.12%
  MAPE Improvement: +0.68% (+5.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 238/464: KOP
============================================================
ðŸ“Š Loading data for KOP...
ðŸ“Š Loading data for KOP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KOP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KOP...

==================================================
Training Baseline KOP (SVM)
==================================================
Training SVM model...

Baseline KOP Performance:
MAE: 56148.6449
RMSE: 73340.0956
MAPE: 13.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 180
   â€¢ Highly important features (top 5%): 120

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0021, rank=1
   2. Feature_67_t2: importance=0.0018, rank=2
   3. Feature_2_t3: importance=0.0011, rank=3
   4. Feature_67_t3: importance=0.0010, rank=4
   5. Feature_63_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for KOP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KOP...

==================================================
Training Enhanced KOP (SVM)
==================================================
Training SVM model...

Enhanced KOP Performance:
MAE: 61587.9584
RMSE: 76492.4209
MAPE: 15.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t1: importance=0.0021, rank=1
   2. Feature_23_t0: importance=0.0015, rank=2
   3. Feature_7_t1: importance=0.0014, rank=3
   4. Feature_6_t2: importance=0.0011, rank=4
   5. Feature_19_t0: importance=0.0011, rank=5

ðŸ“Š KOP Results:
  Baseline MAPE: 13.59%
  Enhanced MAPE: 15.30%
  MAPE Improvement: -1.71% (-12.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 239/464: KREF
============================================================
ðŸ“Š Loading data for KREF...
ðŸ“Š Loading data for KREF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing KREF: 'KREF'

============================================================
TESTING TICKER 240/464: KRYS
============================================================
ðŸ“Š Loading data for KRYS...
ðŸ“Š Loading data for KRYS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing KRYS: 'KRYS'

============================================================
TESTING TICKER 241/464: KSS
============================================================
ðŸ“Š Loading data for KSS...
ðŸ“Š Loading data for KSS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KSS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KSS...

==================================================
Training Baseline KSS (SVM)
==================================================
Training SVM model...

Baseline KSS Performance:
MAE: 2954698.6127
RMSE: 3801119.9554
MAPE: 7.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_67_t1: importance=0.0008, rank=2
   3. Feature_1_t0: importance=0.0007, rank=3
   4. Feature_1_t3: importance=0.0005, rank=4
   5. Feature_0_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for KSS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KSS...

==================================================
Training Enhanced KSS (SVM)
==================================================
Training SVM model...

Enhanced KSS Performance:
MAE: 3035444.3379
RMSE: 3812050.7230
MAPE: 7.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 40
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0016, rank=1
   2. Feature_1_t0: importance=0.0005, rank=2
   3. Feature_15_t3: importance=0.0005, rank=3
   4. Feature_24_t1: importance=0.0005, rank=4
   5. Feature_21_t2: importance=0.0004, rank=5

ðŸ“Š KSS Results:
  Baseline MAPE: 7.00%
  Enhanced MAPE: 7.15%
  MAPE Improvement: -0.15% (-2.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 242/464: KW
============================================================
ðŸ“Š Loading data for KW...
ðŸ“Š Loading data for KW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KW...

==================================================
Training Baseline KW (SVM)
==================================================
Training SVM model...

Baseline KW Performance:
MAE: 340938.0271
RMSE: 433776.6322
MAPE: 6.97%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 114
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0019, rank=1
   2. Feature_67_t0: importance=0.0008, rank=2
   3. Feature_65_t1: importance=0.0007, rank=3
   4. Feature_1_t3: importance=0.0006, rank=4
   5. Feature_63_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for KW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KW...

==================================================
Training Enhanced KW (SVM)
==================================================
Training SVM model...

Enhanced KW Performance:
MAE: 529953.9680
RMSE: 688665.9611
MAPE: 9.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0041, rank=1
   2. Feature_19_t3: importance=0.0039, rank=2
   3. Feature_13_t3: importance=0.0038, rank=3
   4. Feature_23_t3: importance=0.0038, rank=4
   5. Feature_5_t1: importance=0.0019, rank=5

ðŸ“Š KW Results:
  Baseline MAPE: 6.97%
  Enhanced MAPE: 9.94%
  MAPE Improvement: -2.97% (-42.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 243/464: KWR
============================================================
ðŸ“Š Loading data for KWR...
ðŸ“Š Loading data for KWR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for KWR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for KWR...

==================================================
Training Baseline KWR (SVM)
==================================================
Training SVM model...

Baseline KWR Performance:
MAE: 79463.3854
RMSE: 122628.2866
MAPE: 5.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 17

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0003, rank=1
   2. Feature_67_t2: importance=0.0003, rank=2
   3. Feature_65_t1: importance=0.0002, rank=3
   4. Feature_1_t1: importance=0.0002, rank=4
   5. Feature_67_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for KWR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for KWR...

==================================================
Training Enhanced KWR (SVM)
==================================================
Training SVM model...

Enhanced KWR Performance:
MAE: 69898.0708
RMSE: 114282.5511
MAPE: 4.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0002, rank=1
   2. Feature_20_t0: importance=0.0002, rank=2
   3. Feature_24_t3: importance=0.0002, rank=3
   4. Feature_15_t0: importance=0.0001, rank=4
   5. Feature_11_t3: importance=0.0001, rank=5

ðŸ“Š KWR Results:
  Baseline MAPE: 5.40%
  Enhanced MAPE: 4.86%
  MAPE Improvement: +0.55% (+10.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 244/464: LCII
============================================================
ðŸ“Š Loading data for LCII...
ðŸ“Š Loading data for LCII from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LCII...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LCII...

==================================================
Training Baseline LCII (SVM)
==================================================
Training SVM model...

Baseline LCII Performance:
MAE: 157297.4401
RMSE: 208428.2644
MAPE: 8.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_66_t1: importance=0.0002, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_65_t1: importance=0.0001, rank=3
   4. Feature_67_t1: importance=0.0001, rank=4
   5. Feature_67_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for LCII...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LCII...

==================================================
Training Enhanced LCII (SVM)
==================================================
Training SVM model...

Enhanced LCII Performance:
MAE: 134099.6292
RMSE: 184493.8402
MAPE: 6.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0004, rank=1
   2. Feature_19_t3: importance=0.0003, rank=2
   3. Feature_13_t3: importance=0.0003, rank=3
   4. Feature_22_t3: importance=0.0003, rank=4
   5. Feature_2_t1: importance=0.0002, rank=5

ðŸ“Š LCII Results:
  Baseline MAPE: 8.02%
  Enhanced MAPE: 6.77%
  MAPE Improvement: +1.25% (+15.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 245/464: LEG
============================================================
ðŸ“Š Loading data for LEG...
ðŸ“Š Loading data for LEG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LEG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LEG...

==================================================
Training Baseline LEG (SVM)
==================================================
Training SVM model...

Baseline LEG Performance:
MAE: 1109224.2352
RMSE: 1489917.9912
MAPE: 14.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0007, rank=1
   2. Feature_64_t0: importance=0.0006, rank=2
   3. Feature_2_t1: importance=0.0005, rank=3
   4. Feature_0_t0: importance=0.0005, rank=4
   5. Feature_1_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for LEG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LEG...

==================================================
Training Enhanced LEG (SVM)
==================================================
Training SVM model...

Enhanced LEG Performance:
MAE: 1077260.0542
RMSE: 1469581.8217
MAPE: 14.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0016, rank=1
   2. Feature_19_t2: importance=0.0013, rank=2
   3. Feature_9_t1: importance=0.0009, rank=3
   4. Feature_9_t3: importance=0.0008, rank=4
   5. Feature_10_t2: importance=0.0007, rank=5

ðŸ“Š LEG Results:
  Baseline MAPE: 14.52%
  Enhanced MAPE: 14.04%
  MAPE Improvement: +0.49% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 246/464: LGIH
============================================================
ðŸ“Š Loading data for LGIH...
ðŸ“Š Loading data for LGIH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LGIH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LGIH...

==================================================
Training Baseline LGIH (SVM)
==================================================
Training SVM model...

Baseline LGIH Performance:
MAE: 71636.2080
RMSE: 100219.7435
MAPE: 2.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 31
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0003, rank=1
   2. Feature_2_t1: importance=0.0002, rank=2
   3. Feature_0_t3: importance=0.0002, rank=3
   4. Feature_0_t2: importance=0.0001, rank=4
   5. Feature_65_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for LGIH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LGIH...

==================================================
Training Enhanced LGIH (SVM)
==================================================
Training SVM model...

Enhanced LGIH Performance:
MAE: 80561.5799
RMSE: 112293.9162
MAPE: 3.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t1: importance=0.0002, rank=1
   2. Feature_19_t3: importance=0.0002, rank=2
   3. Feature_7_t0: importance=0.0001, rank=3
   4. Feature_20_t1: importance=0.0001, rank=4
   5. Feature_12_t3: importance=0.0001, rank=5

ðŸ“Š LGIH Results:
  Baseline MAPE: 2.85%
  Enhanced MAPE: 3.29%
  MAPE Improvement: -0.44% (-15.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 247/464: LGND
============================================================
ðŸ“Š Loading data for LGND...
ðŸ“Š Loading data for LGND from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LGND...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LGND...

==================================================
Training Baseline LGND (SVM)
==================================================
Training SVM model...

Baseline LGND Performance:
MAE: 95370.4206
RMSE: 113896.0038
MAPE: 10.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 163
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0005, rank=1
   2. Feature_2_t3: importance=0.0003, rank=2
   3. Feature_64_t3: importance=0.0003, rank=3
   4. Feature_8_t3: importance=0.0002, rank=4
   5. Feature_6_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for LGND...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LGND...

==================================================
Training Enhanced LGND (SVM)
==================================================
Training SVM model...

Enhanced LGND Performance:
MAE: 95646.6835
RMSE: 115847.7171
MAPE: 10.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0007, rank=1
   2. Feature_1_t3: importance=0.0005, rank=2
   3. Feature_22_t3: importance=0.0005, rank=3
   4. Feature_9_t3: importance=0.0005, rank=4
   5. Feature_6_t3: importance=0.0004, rank=5

ðŸ“Š LGND Results:
  Baseline MAPE: 10.16%
  Enhanced MAPE: 10.14%
  MAPE Improvement: +0.02% (+0.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 248/464: LKFN
============================================================
ðŸ“Š Loading data for LKFN...
ðŸ“Š Loading data for LKFN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LKFN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LKFN...

==================================================
Training Baseline LKFN (SVM)
==================================================
Training SVM model...

Baseline LKFN Performance:
MAE: 84640.7828
RMSE: 112595.3624
MAPE: 3.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0011, rank=1
   2. Feature_1_t3: importance=0.0008, rank=2
   3. Feature_0_t3: importance=0.0004, rank=3
   4. Feature_0_t0: importance=0.0003, rank=4
   5. Feature_65_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for LKFN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LKFN...

==================================================
Training Enhanced LKFN (SVM)
==================================================
Training SVM model...

Enhanced LKFN Performance:
MAE: 102141.2345
RMSE: 125546.7125
MAPE: 4.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0041, rank=1
   2. Feature_13_t3: importance=0.0040, rank=2
   3. Feature_23_t3: importance=0.0040, rank=3
   4. Feature_6_t3: importance=0.0005, rank=4
   5. Feature_1_t3: importance=0.0004, rank=5

ðŸ“Š LKFN Results:
  Baseline MAPE: 3.87%
  Enhanced MAPE: 4.70%
  MAPE Improvement: -0.83% (-21.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 249/464: LMAT
============================================================
ðŸ“Š Loading data for LMAT...
ðŸ“Š Loading data for LMAT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LMAT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LMAT...

==================================================
Training Baseline LMAT (SVM)
==================================================
Training SVM model...

Baseline LMAT Performance:
MAE: 82320.5905
RMSE: 119470.9465
MAPE: 8.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 117
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0005, rank=1
   2. Feature_67_t2: importance=0.0005, rank=2
   3. Feature_2_t0: importance=0.0003, rank=3
   4. Feature_2_t3: importance=0.0003, rank=4
   5. Feature_1_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for LMAT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LMAT...

==================================================
Training Enhanced LMAT (SVM)
==================================================
Training SVM model...

Enhanced LMAT Performance:
MAE: 74530.6290
RMSE: 109988.4080
MAPE: 7.43%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0007, rank=1
   2. Feature_21_t3: importance=0.0006, rank=2
   3. Feature_20_t0: importance=0.0004, rank=3
   4. Feature_22_t3: importance=0.0004, rank=4
   5. Feature_15_t1: importance=0.0004, rank=5

ðŸ“Š LMAT Results:
  Baseline MAPE: 8.42%
  Enhanced MAPE: 7.43%
  MAPE Improvement: +0.99% (+11.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 250/464: LNC
============================================================
ðŸ“Š Loading data for LNC...
ðŸ“Š Loading data for LNC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LNC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LNC...

==================================================
Training Baseline LNC (SVM)
==================================================
Training SVM model...

Baseline LNC Performance:
MAE: 337161.2831
RMSE: 418957.3928
MAPE: 7.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 36
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0021, rank=1
   2. Feature_2_t3: importance=0.0015, rank=2
   3. Feature_63_t2: importance=0.0014, rank=3
   4. Feature_0_t1: importance=0.0011, rank=4
   5. Feature_2_t0: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for LNC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LNC...

==================================================
Training Enhanced LNC (SVM)
==================================================
Training SVM model...

Enhanced LNC Performance:
MAE: 298881.8645
RMSE: 430889.8700
MAPE: 6.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0022, rank=1
   2. Feature_16_t1: importance=0.0011, rank=2
   3. Feature_9_t3: importance=0.0011, rank=3
   4. Feature_7_t3: importance=0.0009, rank=4
   5. Feature_24_t3: importance=0.0008, rank=5

ðŸ“Š LNC Results:
  Baseline MAPE: 7.88%
  Enhanced MAPE: 6.93%
  MAPE Improvement: +0.95% (+12.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 251/464: LNN
============================================================
ðŸ“Š Loading data for LNN...
ðŸ“Š Loading data for LNN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LNN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LNN...

==================================================
Training Baseline LNN (SVM)
==================================================
Training SVM model...

Baseline LNN Performance:
MAE: 27536.4174
RMSE: 38783.1531
MAPE: 7.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0009, rank=1
   2. Feature_1_t1: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0004, rank=3
   4. Feature_66_t1: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for LNN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LNN...

==================================================
Training Enhanced LNN (SVM)
==================================================
Training SVM model...

Enhanced LNN Performance:
MAE: 34729.2904
RMSE: 42794.8050
MAPE: 9.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0011, rank=1
   2. Feature_19_t1: importance=0.0011, rank=2
   3. Feature_19_t2: importance=0.0007, rank=3
   4. Feature_23_t1: importance=0.0006, rank=4
   5. Feature_15_t2: importance=0.0005, rank=5

ðŸ“Š LNN Results:
  Baseline MAPE: 7.80%
  Enhanced MAPE: 9.76%
  MAPE Improvement: -1.97% (-25.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 252/464: LPG
============================================================
ðŸ“Š Loading data for LPG...
ðŸ“Š Loading data for LPG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing LPG: 'LPG'

============================================================
TESTING TICKER 253/464: LQDT
============================================================
ðŸ“Š Loading data for LQDT...
ðŸ“Š Loading data for LQDT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LQDT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LQDT...

==================================================
Training Baseline LQDT (SVM)
==================================================
Training SVM model...

Baseline LQDT Performance:
MAE: 105209.0078
RMSE: 120382.8370
MAPE: 10.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t2: importance=0.0010, rank=1
   2. Feature_1_t3: importance=0.0008, rank=2
   3. Feature_65_t2: importance=0.0006, rank=3
   4. Feature_0_t3: importance=0.0005, rank=4
   5. Feature_2_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for LQDT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LQDT...

==================================================
Training Enhanced LQDT (SVM)
==================================================
Training SVM model...

Enhanced LQDT Performance:
MAE: 96817.1237
RMSE: 115712.7057
MAPE: 9.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t2: importance=0.0012, rank=1
   2. Feature_22_t3: importance=0.0012, rank=2
   3. Feature_1_t3: importance=0.0005, rank=3
   4. Feature_20_t0: importance=0.0005, rank=4
   5. Feature_9_t3: importance=0.0005, rank=5

ðŸ“Š LQDT Results:
  Baseline MAPE: 10.26%
  Enhanced MAPE: 9.33%
  MAPE Improvement: +0.94% (+9.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 254/464: LRN
============================================================
ðŸ“Š Loading data for LRN...
ðŸ“Š Loading data for LRN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LRN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LRN...

==================================================
Training Baseline LRN (SVM)
==================================================
Training SVM model...

Baseline LRN Performance:
MAE: 410189.3594
RMSE: 616707.3078
MAPE: 8.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 155
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0012, rank=1
   2. Feature_63_t1: importance=0.0005, rank=2
   3. Feature_67_t3: importance=0.0005, rank=3
   4. Feature_64_t0: importance=0.0004, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for LRN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LRN...

==================================================
Training Enhanced LRN (SVM)
==================================================
Training SVM model...

Enhanced LRN Performance:
MAE: 510782.3465
RMSE: 704186.7576
MAPE: 10.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0018, rank=1
   2. Feature_13_t3: importance=0.0015, rank=2
   3. Feature_19_t3: importance=0.0015, rank=3
   4. Feature_23_t3: importance=0.0013, rank=4
   5. Feature_23_t2: importance=0.0011, rank=5

ðŸ“Š LRN Results:
  Baseline MAPE: 8.14%
  Enhanced MAPE: 10.33%
  MAPE Improvement: -2.19% (-26.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 255/464: LTC
============================================================
ðŸ“Š Loading data for LTC...
ðŸ“Š Loading data for LTC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LTC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LTC...

==================================================
Training Baseline LTC (SVM)
==================================================
Training SVM model...

Baseline LTC Performance:
MAE: 105276.1933
RMSE: 142121.7460
MAPE: 6.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0011, rank=1
   2. Feature_67_t0: importance=0.0010, rank=2
   3. Feature_2_t3: importance=0.0009, rank=3
   4. Feature_2_t0: importance=0.0006, rank=4
   5. Feature_63_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for LTC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LTC...

==================================================
Training Enhanced LTC (SVM)
==================================================
Training SVM model...

Enhanced LTC Performance:
MAE: 106332.7304
RMSE: 135065.3837
MAPE: 6.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t1: importance=0.0011, rank=1
   2. Feature_11_t2: importance=0.0010, rank=2
   3. Feature_20_t1: importance=0.0008, rank=3
   4. Feature_6_t3: importance=0.0008, rank=4
   5. Feature_24_t3: importance=0.0007, rank=5

ðŸ“Š LTC Results:
  Baseline MAPE: 6.57%
  Enhanced MAPE: 6.71%
  MAPE Improvement: -0.14% (-2.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 256/464: LXP
============================================================
ðŸ“Š Loading data for LXP...
ðŸ“Š Loading data for LXP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LXP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LXP...

==================================================
Training Baseline LXP (SVM)
==================================================
Training SVM model...

Baseline LXP Performance:
MAE: 1004738.3622
RMSE: 1296525.9452
MAPE: 19.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 29
   â€¢ Highly important features (top 5%): 15

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0032, rank=1
   2. Feature_63_t2: importance=0.0023, rank=2
   3. Feature_64_t1: importance=0.0018, rank=3
   4. Feature_67_t1: importance=0.0011, rank=4
   5. Feature_64_t0: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for LXP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LXP...

==================================================
Training Enhanced LXP (SVM)
==================================================
Training SVM model...

Enhanced LXP Performance:
MAE: 966829.9765
RMSE: 1250517.8619
MAPE: 19.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t2: importance=0.0025, rank=1
   2. Feature_6_t2: importance=0.0021, rank=2
   3. Feature_6_t0: importance=0.0015, rank=3
   4. Feature_22_t0: importance=0.0015, rank=4
   5. Feature_6_t1: importance=0.0012, rank=5

ðŸ“Š LXP Results:
  Baseline MAPE: 19.46%
  Enhanced MAPE: 19.57%
  MAPE Improvement: -0.11% (-0.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 257/464: LZB
============================================================
ðŸ“Š Loading data for LZB...
ðŸ“Š Loading data for LZB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for LZB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for LZB...

==================================================
Training Baseline LZB (SVM)
==================================================
Training SVM model...

Baseline LZB Performance:
MAE: 230581.4253
RMSE: 287049.2222
MAPE: 9.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 110
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0004, rank=1
   2. Feature_0_t3: importance=0.0002, rank=2
   3. Feature_63_t3: importance=0.0002, rank=3
   4. Feature_65_t2: importance=0.0002, rank=4
   5. Feature_66_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for LZB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for LZB...

==================================================
Training Enhanced LZB (SVM)
==================================================
Training SVM model...

Enhanced LZB Performance:
MAE: 178700.9957
RMSE: 224011.0852
MAPE: 7.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0006, rank=1
   2. Feature_4_t2: importance=0.0003, rank=2
   3. Feature_1_t3: importance=0.0002, rank=3
   4. Feature_20_t3: importance=0.0002, rank=4
   5. Feature_24_t3: importance=0.0002, rank=5

ðŸ“Š LZB Results:
  Baseline MAPE: 9.44%
  Enhanced MAPE: 7.50%
  MAPE Improvement: +1.94% (+20.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 258/464: MAC
============================================================
ðŸ“Š Loading data for MAC...
ðŸ“Š Loading data for MAC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MAC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MAC...

==================================================
Training Baseline MAC (SVM)
==================================================
Training SVM model...

Baseline MAC Performance:
MAE: 805756.0023
RMSE: 1029896.1936
MAPE: 5.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0006, rank=1
   2. Feature_65_t2: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_1_t2: importance=0.0003, rank=4
   5. Feature_67_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for MAC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MAC...

==================================================
Training Enhanced MAC (SVM)
==================================================
Training SVM model...

Enhanced MAC Performance:
MAE: 847377.5696
RMSE: 1046040.1534
MAPE: 6.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 61
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t0: importance=0.0005, rank=1
   2. Feature_15_t3: importance=0.0004, rank=2
   3. Feature_2_t1: importance=0.0004, rank=3
   4. Feature_9_t3: importance=0.0003, rank=4
   5. Feature_8_t1: importance=0.0003, rank=5

ðŸ“Š MAC Results:
  Baseline MAPE: 5.75%
  Enhanced MAPE: 6.15%
  MAPE Improvement: -0.40% (-6.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 259/464: MAN
============================================================
ðŸ“Š Loading data for MAN...
ðŸ“Š Loading data for MAN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MAN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MAN...

==================================================
Training Baseline MAN (SVM)
==================================================
Training SVM model...

Baseline MAN Performance:
MAE: 201871.2782
RMSE: 277556.1684
MAPE: 8.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 168
   â€¢ Highly important features (top 5%): 114

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0025, rank=1
   2. Feature_2_t2: importance=0.0008, rank=2
   3. Feature_65_t2: importance=0.0007, rank=3
   4. Feature_2_t3: importance=0.0006, rank=4
   5. Feature_67_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for MAN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MAN...

==================================================
Training Enhanced MAN (SVM)
==================================================
Training SVM model...

Enhanced MAN Performance:
MAE: 168453.3495
RMSE: 213963.4429
MAPE: 7.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0015, rank=1
   2. Feature_23_t0: importance=0.0013, rank=2
   3. Feature_13_t0: importance=0.0012, rank=3
   4. Feature_19_t3: importance=0.0008, rank=4
   5. Feature_16_t0: importance=0.0007, rank=5

ðŸ“Š MAN Results:
  Baseline MAPE: 8.77%
  Enhanced MAPE: 7.51%
  MAPE Improvement: +1.26% (+14.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 260/464: MARA
============================================================
ðŸ“Š Loading data for MARA...
ðŸ“Š Loading data for MARA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MARA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MARA...

==================================================
Training Baseline MARA (SVM)
==================================================
Training SVM model...

Baseline MARA Performance:
MAE: 7562952.3296
RMSE: 8899709.9333
MAPE: 10.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 24
   â€¢ Highly important features (top 5%): 10

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0028, rank=1
   2. Feature_2_t2: importance=0.0022, rank=2
   3. Feature_2_t3: importance=0.0018, rank=3
   4. Feature_1_t1: importance=0.0008, rank=4
   5. Feature_64_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for MARA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MARA...

==================================================
Training Enhanced MARA (SVM)
==================================================
Training SVM model...

Enhanced MARA Performance:
MAE: 5058261.1331
RMSE: 5975798.8020
MAPE: 7.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t0: importance=0.0022, rank=1
   2. Feature_2_t1: importance=0.0021, rank=2
   3. Feature_13_t3: importance=0.0019, rank=3
   4. Feature_21_t1: importance=0.0018, rank=4
   5. Feature_4_t3: importance=0.0015, rank=5

ðŸ“Š MARA Results:
  Baseline MAPE: 10.74%
  Enhanced MAPE: 7.30%
  MAPE Improvement: +3.44% (+32.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 261/464: MATW
============================================================
ðŸ“Š Loading data for MATW...
ðŸ“Š Loading data for MATW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MATW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MATW...

==================================================
Training Baseline MATW (SVM)
==================================================
Training SVM model...

Baseline MATW Performance:
MAE: 87709.9612
RMSE: 107538.6780
MAPE: 9.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0008, rank=1
   2. Feature_65_t3: importance=0.0008, rank=2
   3. Feature_67_t3: importance=0.0004, rank=3
   4. Feature_67_t0: importance=0.0003, rank=4
   5. Feature_1_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for MATW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MATW...

==================================================
Training Enhanced MATW (SVM)
==================================================
Training SVM model...

Enhanced MATW Performance:
MAE: 67054.2932
RMSE: 88387.4197
MAPE: 7.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t3: importance=0.0005, rank=1
   2. Feature_10_t2: importance=0.0005, rank=2
   3. Feature_9_t2: importance=0.0005, rank=3
   4. Feature_22_t1: importance=0.0004, rank=4
   5. Feature_20_t3: importance=0.0004, rank=5

ðŸ“Š MATW Results:
  Baseline MAPE: 9.35%
  Enhanced MAPE: 7.05%
  MAPE Improvement: +2.30% (+24.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 262/464: MATX
============================================================
ðŸ“Š Loading data for MATX...
ðŸ“Š Loading data for MATX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MATX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MATX...

==================================================
Training Baseline MATX (SVM)
==================================================
Training SVM model...

Baseline MATX Performance:
MAE: 152941.5814
RMSE: 233839.8045
MAPE: 14.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0020, rank=1
   2. Feature_0_t1: importance=0.0010, rank=2
   3. Feature_0_t2: importance=0.0008, rank=3
   4. Feature_1_t1: importance=0.0008, rank=4
   5. Feature_2_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for MATX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MATX...

==================================================
Training Enhanced MATX (SVM)
==================================================
Training SVM model...

Enhanced MATX Performance:
MAE: 150863.2670
RMSE: 231867.9571
MAPE: 14.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0014, rank=1
   2. Feature_23_t3: importance=0.0014, rank=2
   3. Feature_13_t3: importance=0.0014, rank=3
   4. Feature_6_t3: importance=0.0010, rank=4
   5. Feature_9_t1: importance=0.0006, rank=5

ðŸ“Š MATX Results:
  Baseline MAPE: 14.73%
  Enhanced MAPE: 14.71%
  MAPE Improvement: +0.02% (+0.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 263/464: MC
============================================================
ðŸ“Š Loading data for MC...
ðŸ“Š Loading data for MC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing MC: 'MC'

============================================================
TESTING TICKER 264/464: MCRI
============================================================
ðŸ“Š Loading data for MCRI...
ðŸ“Š Loading data for MCRI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MCRI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MCRI...

==================================================
Training Baseline MCRI (SVM)
==================================================
Training SVM model...

Baseline MCRI Performance:
MAE: 55883.9574
RMSE: 82199.8097
MAPE: 19.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 121
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0020, rank=1
   2. Feature_67_t2: importance=0.0014, rank=2
   3. Feature_63_t3: importance=0.0012, rank=3
   4. Feature_2_t3: importance=0.0011, rank=4
   5. Feature_0_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for MCRI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MCRI...

==================================================
Training Enhanced MCRI (SVM)
==================================================
Training SVM model...

Enhanced MCRI Performance:
MAE: 56099.9362
RMSE: 79141.6633
MAPE: 20.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0017, rank=1
   2. Feature_6_t3: importance=0.0017, rank=2
   3. Feature_9_t1: importance=0.0016, rank=3
   4. Feature_21_t3: importance=0.0016, rank=4
   5. Feature_9_t2: importance=0.0013, rank=5

ðŸ“Š MCRI Results:
  Baseline MAPE: 19.94%
  Enhanced MAPE: 20.69%
  MAPE Improvement: -0.75% (-3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 265/464: MCY
============================================================
ðŸ“Š Loading data for MCY...
ðŸ“Š Loading data for MCY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MCY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MCY...

==================================================
Training Baseline MCY (SVM)
==================================================
Training SVM model...

Baseline MCY Performance:
MAE: 144096.9077
RMSE: 225867.1914
MAPE: 13.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 133
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0010, rank=1
   2. Feature_65_t0: importance=0.0009, rank=2
   3. Feature_0_t3: importance=0.0009, rank=3
   4. Feature_63_t0: importance=0.0009, rank=4
   5. Feature_63_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for MCY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MCY...

==================================================
Training Enhanced MCY (SVM)
==================================================
Training SVM model...

Enhanced MCY Performance:
MAE: 151743.0791
RMSE: 214113.3586
MAPE: 14.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0012, rank=1
   2. Feature_22_t0: importance=0.0011, rank=2
   3. Feature_12_t3: importance=0.0011, rank=3
   4. Feature_16_t2: importance=0.0011, rank=4
   5. Feature_10_t3: importance=0.0010, rank=5

ðŸ“Š MCY Results:
  Baseline MAPE: 13.25%
  Enhanced MAPE: 14.24%
  MAPE Improvement: -0.99% (-7.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 266/464: MD
============================================================
ðŸ“Š Loading data for MD...
ðŸ“Š Loading data for MD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MD...

==================================================
Training Baseline MD (SVM)
==================================================
Training SVM model...

Baseline MD Performance:
MAE: 481801.8424
RMSE: 676134.7213
MAPE: 13.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 106
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0010, rank=1
   2. Feature_2_t3: importance=0.0007, rank=2
   3. Feature_63_t2: importance=0.0006, rank=3
   4. Feature_63_t3: importance=0.0005, rank=4
   5. Feature_0_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for MD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MD...

==================================================
Training Enhanced MD (SVM)
==================================================
Training SVM model...

Enhanced MD Performance:
MAE: 454820.0795
RMSE: 680068.9810
MAPE: 13.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t0: importance=0.0008, rank=1
   2. Feature_7_t0: importance=0.0007, rank=2
   3. Feature_7_t1: importance=0.0006, rank=3
   4. Feature_23_t0: importance=0.0006, rank=4
   5. Feature_10_t3: importance=0.0006, rank=5

ðŸ“Š MD Results:
  Baseline MAPE: 13.88%
  Enhanced MAPE: 13.68%
  MAPE Improvement: +0.20% (+1.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 267/464: MDU
============================================================
ðŸ“Š Loading data for MDU...
ðŸ“Š Loading data for MDU from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MDU...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MDU...

==================================================
Training Baseline MDU (SVM)
==================================================
Training SVM model...

Baseline MDU Performance:
MAE: 426758.9971
RMSE: 591357.9193
MAPE: 10.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 98
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0040, rank=1
   2. Feature_0_t2: importance=0.0021, rank=2
   3. Feature_67_t2: importance=0.0016, rank=3
   4. Feature_65_t3: importance=0.0016, rank=4
   5. Feature_65_t0: importance=0.0015, rank=5

ðŸ”§ Applying universal feature engineering for MDU...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MDU...

==================================================
Training Enhanced MDU (SVM)
==================================================
Training SVM model...

Enhanced MDU Performance:
MAE: 427075.4664
RMSE: 588030.2389
MAPE: 10.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 88

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t1: importance=0.0014, rank=1
   2. Feature_7_t2: importance=0.0013, rank=2
   3. Feature_4_t0: importance=0.0013, rank=3
   4. Feature_2_t2: importance=0.0013, rank=4
   5. Feature_0_t3: importance=0.0011, rank=5

ðŸ“Š MDU Results:
  Baseline MAPE: 10.03%
  Enhanced MAPE: 10.30%
  MAPE Improvement: -0.27% (-2.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 268/464: MGEE
============================================================
ðŸ“Š Loading data for MGEE...
ðŸ“Š Loading data for MGEE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MGEE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MGEE...

==================================================
Training Baseline MGEE (SVM)
==================================================
Training SVM model...

Baseline MGEE Performance:
MAE: 168866.0730
RMSE: 268327.5532
MAPE: 13.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0044, rank=1
   2. Feature_67_t0: importance=0.0027, rank=2
   3. Feature_0_t3: importance=0.0026, rank=3
   4. Feature_67_t2: importance=0.0024, rank=4
   5. Feature_1_t0: importance=0.0017, rank=5

ðŸ”§ Applying universal feature engineering for MGEE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MGEE...

==================================================
Training Enhanced MGEE (SVM)
==================================================
Training SVM model...

Enhanced MGEE Performance:
MAE: 136523.4912
RMSE: 255240.1764
MAPE: 10.81%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0024, rank=1
   2. Feature_15_t3: importance=0.0023, rank=2
   3. Feature_23_t0: importance=0.0022, rank=3
   4. Feature_18_t3: importance=0.0019, rank=4
   5. Feature_19_t0: importance=0.0018, rank=5

ðŸ“Š MGEE Results:
  Baseline MAPE: 13.37%
  Enhanced MAPE: 10.81%
  MAPE Improvement: +2.56% (+19.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 269/464: MGPI
============================================================
ðŸ“Š Loading data for MGPI...
ðŸ“Š Loading data for MGPI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MGPI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MGPI...

==================================================
Training Baseline MGPI (SVM)
==================================================
Training SVM model...

Baseline MGPI Performance:
MAE: 194841.3816
RMSE: 285392.2797
MAPE: 7.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 26
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0005, rank=1
   2. Feature_65_t1: importance=0.0002, rank=2
   3. Feature_64_t3: importance=0.0002, rank=3
   4. Feature_63_t3: importance=0.0001, rank=4
   5. Feature_0_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for MGPI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MGPI...

==================================================
Training Enhanced MGPI (SVM)
==================================================
Training SVM model...

Enhanced MGPI Performance:
MAE: 202280.5565
RMSE: 284506.0345
MAPE: 7.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0002, rank=1
   2. Feature_7_t1: importance=0.0002, rank=2
   3. Feature_5_t2: importance=0.0001, rank=3
   4. Feature_21_t1: importance=0.0001, rank=4
   5. Feature_9_t2: importance=0.0001, rank=5

ðŸ“Š MGPI Results:
  Baseline MAPE: 7.17%
  Enhanced MAPE: 7.76%
  MAPE Improvement: -0.59% (-8.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 270/464: MHO
============================================================
ðŸ“Š Loading data for MHO...
ðŸ“Š Loading data for MHO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MHO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MHO...

==================================================
Training Baseline MHO (SVM)
==================================================
Training SVM model...

Baseline MHO Performance:
MAE: 78447.2625
RMSE: 99353.0524
MAPE: 9.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 40
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0015, rank=1
   2. Feature_1_t3: importance=0.0012, rank=2
   3. Feature_0_t0: importance=0.0008, rank=3
   4. Feature_0_t3: importance=0.0007, rank=4
   5. Feature_1_t1: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for MHO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MHO...

==================================================
Training Enhanced MHO (SVM)
==================================================
Training SVM model...

Enhanced MHO Performance:
MAE: 76587.3510
RMSE: 97045.6695
MAPE: 9.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0009, rank=1
   2. Feature_23_t0: importance=0.0008, rank=2
   3. Feature_15_t0: importance=0.0007, rank=3
   4. Feature_21_t2: importance=0.0007, rank=4
   5. Feature_13_t0: importance=0.0007, rank=5

ðŸ“Š MHO Results:
  Baseline MAPE: 9.09%
  Enhanced MAPE: 9.13%
  MAPE Improvement: -0.04% (-0.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 271/464: MKTX
============================================================
ðŸ“Š Loading data for MKTX...
ðŸ“Š Loading data for MKTX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing MKTX: 'MKTX'

============================================================
TESTING TICKER 272/464: MMI
============================================================
ðŸ“Š Loading data for MMI...
ðŸ“Š Loading data for MMI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MMI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MMI...

==================================================
Training Baseline MMI (SVM)
==================================================
Training SVM model...

Baseline MMI Performance:
MAE: 50836.0042
RMSE: 81287.9650
MAPE: 8.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 140
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0010, rank=1
   2. Feature_65_t2: importance=0.0010, rank=2
   3. Feature_67_t1: importance=0.0009, rank=3
   4. Feature_67_t2: importance=0.0006, rank=4
   5. Feature_65_t1: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for MMI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MMI...

==================================================
Training Enhanced MMI (SVM)
==================================================
Training SVM model...

Enhanced MMI Performance:
MAE: 55825.4567
RMSE: 81716.4105
MAPE: 10.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0020, rank=1
   2. Feature_7_t2: importance=0.0013, rank=2
   3. Feature_8_t0: importance=0.0011, rank=3
   4. Feature_19_t1: importance=0.0010, rank=4
   5. Feature_23_t1: importance=0.0010, rank=5

ðŸ“Š MMI Results:
  Baseline MAPE: 8.56%
  Enhanced MAPE: 10.37%
  MAPE Improvement: -1.81% (-21.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 273/464: MMSI
============================================================
ðŸ“Š Loading data for MMSI...
ðŸ“Š Loading data for MMSI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MMSI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MMSI...

==================================================
Training Baseline MMSI (SVM)
==================================================
Training SVM model...

Baseline MMSI Performance:
MAE: 182878.8460
RMSE: 230395.6972
MAPE: 6.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0037, rank=1
   2. Feature_63_t2: importance=0.0020, rank=2
   3. Feature_65_t3: importance=0.0009, rank=3
   4. Feature_2_t1: importance=0.0006, rank=4
   5. Feature_1_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for MMSI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MMSI...

==================================================
Training Enhanced MMSI (SVM)
==================================================
Training SVM model...

Enhanced MMSI Performance:
MAE: 243380.2576
RMSE: 295431.3494
MAPE: 8.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0010, rank=1
   2. Feature_22_t2: importance=0.0009, rank=2
   3. Feature_6_t1: importance=0.0008, rank=3
   4. Feature_15_t1: importance=0.0007, rank=4
   5. Feature_12_t2: importance=0.0005, rank=5

ðŸ“Š MMSI Results:
  Baseline MAPE: 6.75%
  Enhanced MAPE: 8.74%
  MAPE Improvement: -1.99% (-29.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 274/464: MNRO
============================================================
ðŸ“Š Loading data for MNRO...
ðŸ“Š Loading data for MNRO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MNRO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MNRO...

==================================================
Training Baseline MNRO (SVM)
==================================================
Training SVM model...

Baseline MNRO Performance:
MAE: 284569.6530
RMSE: 354739.8571
MAPE: 7.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 107
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0001, rank=1
   2. Feature_63_t2: importance=0.0001, rank=2
   3. Feature_63_t1: importance=0.0000, rank=3
   4. Feature_65_t3: importance=0.0000, rank=4
   5. Feature_1_t0: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for MNRO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MNRO...

==================================================
Training Enhanced MNRO (SVM)
==================================================
Training SVM model...

Enhanced MNRO Performance:
MAE: 266899.8038
RMSE: 337539.3589
MAPE: 6.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0001, rank=1
   2. Feature_10_t3: importance=0.0001, rank=2
   3. Feature_4_t2: importance=0.0001, rank=3
   4. Feature_12_t3: importance=0.0000, rank=4
   5. Feature_4_t3: importance=0.0000, rank=5

ðŸ“Š MNRO Results:
  Baseline MAPE: 7.21%
  Enhanced MAPE: 6.82%
  MAPE Improvement: +0.39% (+5.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 275/464: MPW
============================================================
ðŸ“Š Loading data for MPW...
ðŸ“Š Loading data for MPW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MPW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MPW...

==================================================
Training Baseline MPW (SVM)
==================================================
Training SVM model...

Baseline MPW Performance:
MAE: 7883773.1324
RMSE: 11125628.1880
MAPE: 3.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 41
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0010, rank=1
   2. Feature_63_t2: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_2_t3: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for MPW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MPW...

==================================================
Training Enhanced MPW (SVM)
==================================================
Training SVM model...

Enhanced MPW Performance:
MAE: 7801256.4973
RMSE: 11054393.7046
MAPE: 3.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0005, rank=1
   2. Feature_11_t2: importance=0.0004, rank=2
   3. Feature_19_t3: importance=0.0004, rank=3
   4. Feature_13_t1: importance=0.0003, rank=4
   5. Feature_4_t1: importance=0.0003, rank=5

ðŸ“Š MPW Results:
  Baseline MAPE: 3.88%
  Enhanced MAPE: 3.84%
  MAPE Improvement: +0.04% (+1.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 276/464: MRCY
============================================================
ðŸ“Š Loading data for MRCY...
ðŸ“Š Loading data for MRCY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MRCY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MRCY...

==================================================
Training Baseline MRCY (SVM)
==================================================
Training SVM model...

Baseline MRCY Performance:
MAE: 239714.1683
RMSE: 319483.9843
MAPE: 5.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0011, rank=1
   2. Feature_65_t0: importance=0.0003, rank=2
   3. Feature_67_t3: importance=0.0003, rank=3
   4. Feature_0_t0: importance=0.0003, rank=4
   5. Feature_66_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for MRCY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MRCY...

==================================================
Training Enhanced MRCY (SVM)
==================================================
Training SVM model...

Enhanced MRCY Performance:
MAE: 267671.7034
RMSE: 322993.5804
MAPE: 6.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0005, rank=1
   2. Feature_7_t2: importance=0.0004, rank=2
   3. Feature_13_t3: importance=0.0003, rank=3
   4. Feature_19_t0: importance=0.0003, rank=4
   5. Feature_6_t3: importance=0.0003, rank=5

ðŸ“Š MRCY Results:
  Baseline MAPE: 5.29%
  Enhanced MAPE: 6.00%
  MAPE Improvement: -0.71% (-13.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 277/464: MRTN
============================================================
ðŸ“Š Loading data for MRTN...
ðŸ“Š Loading data for MRTN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing MRTN: 'MRTN'

============================================================
TESTING TICKER 278/464: MSEX
============================================================
ðŸ“Š Loading data for MSEX...
ðŸ“Š Loading data for MSEX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MSEX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MSEX...

==================================================
Training Baseline MSEX (SVM)
==================================================
Training SVM model...

Baseline MSEX Performance:
MAE: 59462.0297
RMSE: 89774.3743
MAPE: 9.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0017, rank=1
   2. Feature_1_t2: importance=0.0014, rank=2
   3. Feature_1_t3: importance=0.0009, rank=3
   4. Feature_0_t2: importance=0.0008, rank=4
   5. Feature_2_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for MSEX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MSEX...

==================================================
Training Enhanced MSEX (SVM)
==================================================
Training SVM model...

Enhanced MSEX Performance:
MAE: 64582.3122
RMSE: 90704.9938
MAPE: 11.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0014, rank=1
   2. Feature_9_t2: importance=0.0012, rank=2
   3. Feature_10_t3: importance=0.0010, rank=3
   4. Feature_12_t3: importance=0.0009, rank=4
   5. Feature_1_t2: importance=0.0009, rank=5

ðŸ“Š MSEX Results:
  Baseline MAPE: 9.82%
  Enhanced MAPE: 11.35%
  MAPE Improvement: -1.53% (-15.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 279/464: MTH
============================================================
ðŸ“Š Loading data for MTH...
ðŸ“Š Loading data for MTH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MTH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MTH...

==================================================
Training Baseline MTH (SVM)
==================================================
Training SVM model...

Baseline MTH Performance:
MAE: 217473.1431
RMSE: 348204.1745
MAPE: 12.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 162
   â€¢ Highly important features (top 5%): 93

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0014, rank=1
   2. Feature_0_t3: importance=0.0011, rank=2
   3. Feature_63_t2: importance=0.0009, rank=3
   4. Feature_0_t2: importance=0.0009, rank=4
   5. Feature_2_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for MTH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MTH...

==================================================
Training Enhanced MTH (SVM)
==================================================
Training SVM model...

Enhanced MTH Performance:
MAE: 230520.2631
RMSE: 349299.1140
MAPE: 13.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t1: importance=0.0012, rank=1
   2. Feature_19_t3: importance=0.0010, rank=2
   3. Feature_16_t1: importance=0.0010, rank=3
   4. Feature_13_t3: importance=0.0009, rank=4
   5. Feature_15_t0: importance=0.0009, rank=5

ðŸ“Š MTH Results:
  Baseline MAPE: 12.83%
  Enhanced MAPE: 13.65%
  MAPE Improvement: -0.82% (-6.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 280/464: MTRN
============================================================
ðŸ“Š Loading data for MTRN...
ðŸ“Š Loading data for MTRN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MTRN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MTRN...

==================================================
Training Baseline MTRN (SVM)
==================================================
Training SVM model...

Baseline MTRN Performance:
MAE: 30518.6532
RMSE: 44362.9860
MAPE: 8.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0040, rank=1
   2. Feature_2_t3: importance=0.0027, rank=2
   3. Feature_0_t2: importance=0.0021, rank=3
   4. Feature_64_t3: importance=0.0013, rank=4
   5. Feature_1_t3: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for MTRN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MTRN...

==================================================
Training Enhanced MTRN (SVM)
==================================================
Training SVM model...

Enhanced MTRN Performance:
MAE: 32049.5682
RMSE: 41170.2038
MAPE: 9.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 88

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0017, rank=1
   2. Feature_16_t3: importance=0.0014, rank=2
   3. Feature_11_t0: importance=0.0012, rank=3
   4. Feature_8_t3: importance=0.0012, rank=4
   5. Feature_9_t2: importance=0.0012, rank=5

ðŸ“Š MTRN Results:
  Baseline MAPE: 8.67%
  Enhanced MAPE: 9.16%
  MAPE Improvement: -0.48% (-5.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 281/464: MTX
============================================================
ðŸ“Š Loading data for MTX...
ðŸ“Š Loading data for MTX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MTX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MTX...

==================================================
Training Baseline MTX (SVM)
==================================================
Training SVM model...

Baseline MTX Performance:
MAE: 38011.6570
RMSE: 49261.5746
MAPE: 9.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 126
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0019, rank=1
   2. Feature_64_t2: importance=0.0011, rank=2
   3. Feature_67_t3: importance=0.0010, rank=3
   4. Feature_67_t2: importance=0.0010, rank=4
   5. Feature_2_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for MTX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MTX...

==================================================
Training Enhanced MTX (SVM)
==================================================
Training SVM model...

Enhanced MTX Performance:
MAE: 46844.9932
RMSE: 56615.9545
MAPE: 11.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0019, rank=1
   2. Feature_23_t1: importance=0.0019, rank=2
   3. Feature_13_t0: importance=0.0016, rank=3
   4. Feature_18_t1: importance=0.0014, rank=4
   5. Feature_19_t0: importance=0.0014, rank=5

ðŸ“Š MTX Results:
  Baseline MAPE: 9.79%
  Enhanced MAPE: 11.76%
  MAPE Improvement: -1.98% (-20.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 282/464: MWA
============================================================
ðŸ“Š Loading data for MWA...
ðŸ“Š Loading data for MWA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MWA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MWA...

==================================================
Training Baseline MWA (SVM)
==================================================
Training SVM model...

Baseline MWA Performance:
MAE: 509203.9712
RMSE: 776293.2162
MAPE: 13.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 101
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t2: importance=0.0012, rank=1
   2. Feature_65_t0: importance=0.0009, rank=2
   3. Feature_0_t3: importance=0.0007, rank=3
   4. Feature_66_t1: importance=0.0007, rank=4
   5. Feature_0_t1: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for MWA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MWA...

==================================================
Training Enhanced MWA (SVM)
==================================================
Training SVM model...

Enhanced MWA Performance:
MAE: 507935.4361
RMSE: 781724.7108
MAPE: 13.62%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0006, rank=1
   2. Feature_22_t0: importance=0.0005, rank=2
   3. Feature_11_t0: importance=0.0005, rank=3
   4. Feature_17_t1: importance=0.0005, rank=4
   5. Feature_15_t1: importance=0.0005, rank=5

ðŸ“Š MWA Results:
  Baseline MAPE: 13.56%
  Enhanced MAPE: 13.62%
  MAPE Improvement: -0.06% (-0.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 283/464: MXL
============================================================
ðŸ“Š Loading data for MXL...
ðŸ“Š Loading data for MXL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing MXL: 'MXL'

============================================================
TESTING TICKER 284/464: MYGN
============================================================
ðŸ“Š Loading data for MYGN...
ðŸ“Š Loading data for MYGN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MYGN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MYGN...

==================================================
Training Baseline MYGN (SVM)
==================================================
Training SVM model...

Baseline MYGN Performance:
MAE: 557127.0360
RMSE: 767684.1768
MAPE: 10.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0006, rank=1
   2. Feature_1_t1: importance=0.0003, rank=2
   3. Feature_2_t0: importance=0.0002, rank=3
   4. Feature_65_t1: importance=0.0002, rank=4
   5. Feature_37_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for MYGN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MYGN...

==================================================
Training Enhanced MYGN (SVM)
==================================================
Training SVM model...

Enhanced MYGN Performance:
MAE: 526667.2269
RMSE: 735341.7626
MAPE: 9.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0003, rank=1
   2. Feature_4_t0: importance=0.0003, rank=2
   3. Feature_4_t2: importance=0.0002, rank=3
   4. Feature_6_t1: importance=0.0002, rank=4
   5. Feature_23_t1: importance=0.0002, rank=5

ðŸ“Š MYGN Results:
  Baseline MAPE: 10.59%
  Enhanced MAPE: 9.95%
  MAPE Improvement: +0.64% (+6.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 285/464: MYRG
============================================================
ðŸ“Š Loading data for MYRG...
ðŸ“Š Loading data for MYRG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for MYRG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for MYRG...

==================================================
Training Baseline MYRG (SVM)
==================================================
Training SVM model...

Baseline MYRG Performance:
MAE: 71836.3829
RMSE: 97950.2905
MAPE: 10.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 41
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0017, rank=1
   2. Feature_65_t0: importance=0.0015, rank=2
   3. Feature_67_t0: importance=0.0012, rank=3
   4. Feature_65_t2: importance=0.0010, rank=4
   5. Feature_1_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for MYRG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for MYRG...

==================================================
Training Enhanced MYRG (SVM)
==================================================
Training SVM model...

Enhanced MYRG Performance:
MAE: 75929.5184
RMSE: 101549.3337
MAPE: 11.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0014, rank=1
   2. Feature_15_t2: importance=0.0010, rank=2
   3. Feature_5_t0: importance=0.0008, rank=3
   4. Feature_20_t0: importance=0.0008, rank=4
   5. Feature_6_t0: importance=0.0008, rank=5

ðŸ“Š MYRG Results:
  Baseline MAPE: 10.69%
  Enhanced MAPE: 11.46%
  MAPE Improvement: -0.78% (-7.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 286/464: NAVI
============================================================
ðŸ“Š Loading data for NAVI...
ðŸ“Š Loading data for NAVI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NAVI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NAVI...

==================================================
Training Baseline NAVI (SVM)
==================================================
Training SVM model...

Baseline NAVI Performance:
MAE: 623299.4824
RMSE: 746530.9411
MAPE: 7.62%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0004, rank=2
   3. Feature_0_t2: importance=0.0003, rank=3
   4. Feature_67_t2: importance=0.0003, rank=4
   5. Feature_65_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for NAVI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NAVI...

==================================================
Training Enhanced NAVI (SVM)
==================================================
Training SVM model...

Enhanced NAVI Performance:
MAE: 419331.2557
RMSE: 546377.6341
MAPE: 5.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0005, rank=1
   2. Feature_13_t2: importance=0.0005, rank=2
   3. Feature_23_t2: importance=0.0003, rank=3
   4. Feature_19_t2: importance=0.0003, rank=4
   5. Feature_1_t3: importance=0.0003, rank=5

ðŸ“Š NAVI Results:
  Baseline MAPE: 7.62%
  Enhanced MAPE: 5.14%
  MAPE Improvement: +2.47% (+32.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 287/464: NBHC
============================================================
ðŸ“Š Loading data for NBHC...
ðŸ“Š Loading data for NBHC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NBHC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NBHC...

==================================================
Training Baseline NBHC (SVM)
==================================================
Training SVM model...

Baseline NBHC Performance:
MAE: 77092.7970
RMSE: 103044.8179
MAPE: 12.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 184
   â€¢ Highly important features (top 5%): 108

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0009, rank=1
   2. Feature_65_t0: importance=0.0007, rank=2
   3. Feature_65_t1: importance=0.0006, rank=3
   4. Feature_65_t3: importance=0.0006, rank=4
   5. Feature_63_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for NBHC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NBHC...

==================================================
Training Enhanced NBHC (SVM)
==================================================
Training SVM model...

Enhanced NBHC Performance:
MAE: 70824.7666
RMSE: 91495.2971
MAPE: 10.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t3: importance=0.0016, rank=1
   2. Feature_18_t3: importance=0.0013, rank=2
   3. Feature_15_t1: importance=0.0012, rank=3
   4. Feature_7_t0: importance=0.0010, rank=4
   5. Feature_9_t3: importance=0.0010, rank=5

ðŸ“Š NBHC Results:
  Baseline MAPE: 12.59%
  Enhanced MAPE: 10.80%
  MAPE Improvement: +1.79% (+14.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 288/464: NBTB
============================================================
ðŸ“Š Loading data for NBTB...
ðŸ“Š Loading data for NBTB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NBTB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NBTB...

==================================================
Training Baseline NBTB (SVM)
==================================================
Training SVM model...

Baseline NBTB Performance:
MAE: 122556.5500
RMSE: 166346.7693
MAPE: 9.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 125
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0003, rank=1
   2. Feature_67_t0: importance=0.0002, rank=2
   3. Feature_23_t0: importance=0.0002, rank=3
   4. Feature_67_t3: importance=0.0002, rank=4
   5. Feature_1_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for NBTB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NBTB...

==================================================
Training Enhanced NBTB (SVM)
==================================================
Training SVM model...

Enhanced NBTB Performance:
MAE: 119828.6028
RMSE: 167604.4969
MAPE: 9.54%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t1: importance=0.0005, rank=1
   2. Feature_19_t1: importance=0.0004, rank=2
   3. Feature_12_t3: importance=0.0003, rank=3
   4. Feature_10_t3: importance=0.0003, rank=4
   5. Feature_1_t3: importance=0.0003, rank=5

ðŸ“Š NBTB Results:
  Baseline MAPE: 9.93%
  Enhanced MAPE: 9.54%
  MAPE Improvement: +0.39% (+3.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 289/464: NEO
============================================================
ðŸ“Š Loading data for NEO...
ðŸ“Š Loading data for NEO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NEO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NEO...

==================================================
Training Baseline NEO (SVM)
==================================================
Training SVM model...

Baseline NEO Performance:
MAE: 590974.4690
RMSE: 728882.8422
MAPE: 12.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 28
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0014, rank=1
   2. Feature_0_t2: importance=0.0007, rank=2
   3. Feature_63_t2: importance=0.0007, rank=3
   4. Feature_1_t0: importance=0.0007, rank=4
   5. Feature_0_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for NEO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NEO...

==================================================
Training Enhanced NEO (SVM)
==================================================
Training SVM model...

Enhanced NEO Performance:
MAE: 486800.1833
RMSE: 644475.7559
MAPE: 9.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 25

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0031, rank=1
   2. Feature_18_t3: importance=0.0015, rank=2
   3. Feature_19_t1: importance=0.0011, rank=3
   4. Feature_21_t3: importance=0.0011, rank=4
   5. Feature_19_t0: importance=0.0007, rank=5

ðŸ“Š NEO Results:
  Baseline MAPE: 12.28%
  Enhanced MAPE: 9.57%
  MAPE Improvement: +2.71% (+22.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 290/464: NEOG
============================================================
ðŸ“Š Loading data for NEOG...
ðŸ“Š Loading data for NEOG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NEOG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NEOG...

==================================================
Training Baseline NEOG (SVM)
==================================================
Training SVM model...

Baseline NEOG Performance:
MAE: 2277114.5618
RMSE: 2913958.1058
MAPE: 10.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 161
   â€¢ Highly important features (top 5%): 89

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0013, rank=1
   2. Feature_1_t3: importance=0.0011, rank=2
   3. Feature_0_t2: importance=0.0009, rank=3
   4. Feature_67_t2: importance=0.0007, rank=4
   5. Feature_63_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for NEOG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NEOG...

==================================================
Training Enhanced NEOG (SVM)
==================================================
Training SVM model...

Enhanced NEOG Performance:
MAE: 1589628.2949
RMSE: 1929676.8496
MAPE: 7.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t2: importance=0.0015, rank=1
   2. Feature_21_t3: importance=0.0014, rank=2
   3. Feature_15_t1: importance=0.0014, rank=3
   4. Feature_9_t3: importance=0.0012, rank=4
   5. Feature_6_t3: importance=0.0012, rank=5

ðŸ“Š NEOG Results:
  Baseline MAPE: 10.26%
  Enhanced MAPE: 7.70%
  MAPE Improvement: +2.57% (+25.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 291/464: NGVT
============================================================
ðŸ“Š Loading data for NGVT...
ðŸ“Š Loading data for NGVT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NGVT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NGVT...

==================================================
Training Baseline NGVT (SVM)
==================================================
Training SVM model...

Baseline NGVT Performance:
MAE: 64096.9243
RMSE: 84632.0600
MAPE: 6.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0010, rank=1
   2. Feature_2_t3: importance=0.0007, rank=2
   3. Feature_67_t3: importance=0.0005, rank=3
   4. Feature_63_t2: importance=0.0004, rank=4
   5. Feature_0_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for NGVT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NGVT...

==================================================
Training Enhanced NGVT (SVM)
==================================================
Training SVM model...

Enhanced NGVT Performance:
MAE: 63245.6733
RMSE: 87219.2883
MAPE: 6.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0009, rank=1
   2. Feature_20_t2: importance=0.0007, rank=2
   3. Feature_17_t3: importance=0.0006, rank=3
   4. Feature_2_t2: importance=0.0005, rank=4
   5. Feature_22_t2: importance=0.0005, rank=5

ðŸ“Š NGVT Results:
  Baseline MAPE: 6.52%
  Enhanced MAPE: 6.25%
  MAPE Improvement: +0.26% (+4.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 292/464: NHC
============================================================
ðŸ“Š Loading data for NHC...
ðŸ“Š Loading data for NHC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NHC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NHC...

==================================================
Training Baseline NHC (SVM)
==================================================
Training SVM model...

Baseline NHC Performance:
MAE: 49465.6306
RMSE: 60025.7299
MAPE: 13.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 150
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0020, rank=1
   2. Feature_67_t3: importance=0.0018, rank=2
   3. Feature_65_t3: importance=0.0012, rank=3
   4. Feature_1_t3: importance=0.0011, rank=4
   5. Feature_67_t1: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for NHC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NHC...

==================================================
Training Enhanced NHC (SVM)
==================================================
Training SVM model...

Enhanced NHC Performance:
MAE: 46437.8900
RMSE: 58610.1580
MAPE: 12.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0029, rank=1
   2. Feature_13_t3: importance=0.0028, rank=2
   3. Feature_19_t3: importance=0.0027, rank=3
   4. Feature_14_t3: importance=0.0027, rank=4
   5. Feature_6_t3: importance=0.0027, rank=5

ðŸ“Š NHC Results:
  Baseline MAPE: 13.68%
  Enhanced MAPE: 12.61%
  MAPE Improvement: +1.07% (+7.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 293/464: NMIH
============================================================
ðŸ“Š Loading data for NMIH...
ðŸ“Š Loading data for NMIH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NMIH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NMIH...

==================================================
Training Baseline NMIH (SVM)
==================================================
Training SVM model...

Baseline NMIH Performance:
MAE: 169300.6527
RMSE: 208900.9219
MAPE: 11.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 210
   â€¢ Highly important features (top 5%): 128

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0016, rank=1
   2. Feature_1_t2: importance=0.0008, rank=2
   3. Feature_0_t2: importance=0.0007, rank=3
   4. Feature_2_t2: importance=0.0007, rank=4
   5. Feature_1_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for NMIH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NMIH...

==================================================
Training Enhanced NMIH (SVM)
==================================================
Training SVM model...

Enhanced NMIH Performance:
MAE: 206780.9820
RMSE: 250119.1547
MAPE: 13.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t1: importance=0.0017, rank=1
   2. Feature_15_t2: importance=0.0013, rank=2
   3. Feature_9_t2: importance=0.0011, rank=3
   4. Feature_24_t2: importance=0.0009, rank=4
   5. Feature_15_t0: importance=0.0008, rank=5

ðŸ“Š NMIH Results:
  Baseline MAPE: 11.17%
  Enhanced MAPE: 13.99%
  MAPE Improvement: -2.83% (-25.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 294/464: NOG
============================================================
ðŸ“Š Loading data for NOG...
ðŸ“Š Loading data for NOG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NOG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NOG...

==================================================
Training Baseline NOG (SVM)
==================================================
Training SVM model...

Baseline NOG Performance:
MAE: 964395.9033
RMSE: 1309844.0107
MAPE: 6.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 49
   â€¢ Highly important features (top 5%): 18

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0053, rank=1
   2. Feature_65_t1: importance=0.0019, rank=2
   3. Feature_2_t0: importance=0.0017, rank=3
   4. Feature_0_t2: importance=0.0013, rank=4
   5. Feature_2_t2: importance=0.0013, rank=5

ðŸ”§ Applying universal feature engineering for NOG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NOG...

==================================================
Training Enhanced NOG (SVM)
==================================================
Training SVM model...

Enhanced NOG Performance:
MAE: 928286.7565
RMSE: 1328330.5182
MAPE: 6.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0048, rank=1
   2. Feature_6_t0: importance=0.0042, rank=2
   3. Feature_23_t3: importance=0.0032, rank=3
   4. Feature_8_t0: importance=0.0027, rank=4
   5. Feature_2_t0: importance=0.0025, rank=5

ðŸ“Š NOG Results:
  Baseline MAPE: 6.45%
  Enhanced MAPE: 6.23%
  MAPE Improvement: +0.22% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 295/464: NPK
============================================================
ðŸ“Š Loading data for NPK...
ðŸ“Š Loading data for NPK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing NPK: 'NPK'

============================================================
TESTING TICKER 296/464: NPO
============================================================
ðŸ“Š Loading data for NPO...
ðŸ“Š Loading data for NPO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NPO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NPO...

==================================================
Training Baseline NPO (SVM)
==================================================
Training SVM model...

Baseline NPO Performance:
MAE: 54130.1118
RMSE: 83100.0950
MAPE: 11.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 131
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t2: importance=0.0009, rank=1
   2. Feature_65_t2: importance=0.0006, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_1_t3: importance=0.0005, rank=4
   5. Feature_1_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for NPO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NPO...

==================================================
Training Enhanced NPO (SVM)
==================================================
Training SVM model...

Enhanced NPO Performance:
MAE: 51498.4693
RMSE: 80761.0433
MAPE: 10.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0017, rank=1
   2. Feature_23_t3: importance=0.0010, rank=2
   3. Feature_4_t1: importance=0.0010, rank=3
   4. Feature_7_t3: importance=0.0008, rank=4
   5. Feature_15_t2: importance=0.0008, rank=5

ðŸ“Š NPO Results:
  Baseline MAPE: 11.72%
  Enhanced MAPE: 10.99%
  MAPE Improvement: +0.73% (+6.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 297/464: NSIT
============================================================
ðŸ“Š Loading data for NSIT...
ðŸ“Š Loading data for NSIT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NSIT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NSIT...

==================================================
Training Baseline NSIT (SVM)
==================================================
Training SVM model...

Baseline NSIT Performance:
MAE: 192011.6298
RMSE: 309203.9486
MAPE: 10.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0013, rank=1
   2. Feature_1_t3: importance=0.0008, rank=2
   3. Feature_2_t3: importance=0.0004, rank=3
   4. Feature_1_t1: importance=0.0003, rank=4
   5. Feature_64_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for NSIT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NSIT...

==================================================
Training Enhanced NSIT (SVM)
==================================================
Training SVM model...

Enhanced NSIT Performance:
MAE: 222272.6588
RMSE: 334149.4821
MAPE: 11.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0010, rank=1
   2. Feature_4_t0: importance=0.0010, rank=2
   3. Feature_23_t3: importance=0.0010, rank=3
   4. Feature_13_t3: importance=0.0009, rank=4
   5. Feature_23_t1: importance=0.0005, rank=5

ðŸ“Š NSIT Results:
  Baseline MAPE: 10.14%
  Enhanced MAPE: 11.82%
  MAPE Improvement: -1.68% (-16.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 298/464: NTCT
============================================================
ðŸ“Š Loading data for NTCT...
ðŸ“Š Loading data for NTCT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NTCT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NTCT...

==================================================
Training Baseline NTCT (SVM)
==================================================
Training SVM model...

Baseline NTCT Performance:
MAE: 202266.7074
RMSE: 261747.5258
MAPE: 12.42%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 128
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0004, rank=1
   2. Feature_64_t0: importance=0.0004, rank=2
   3. Feature_65_t2: importance=0.0003, rank=3
   4. Feature_2_t2: importance=0.0002, rank=4
   5. Feature_67_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for NTCT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NTCT...

==================================================
Training Enhanced NTCT (SVM)
==================================================
Training SVM model...

Enhanced NTCT Performance:
MAE: 210640.0483
RMSE: 272541.1297
MAPE: 13.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t1: importance=0.0009, rank=1
   2. Feature_16_t0: importance=0.0006, rank=2
   3. Feature_13_t3: importance=0.0005, rank=3
   4. Feature_13_t0: importance=0.0004, rank=4
   5. Feature_11_t3: importance=0.0003, rank=5

ðŸ“Š NTCT Results:
  Baseline MAPE: 12.42%
  Enhanced MAPE: 13.07%
  MAPE Improvement: -0.65% (-5.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 299/464: NWBI
============================================================
ðŸ“Š Loading data for NWBI...
ðŸ“Š Loading data for NWBI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NWBI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NWBI...

==================================================
Training Baseline NWBI (SVM)
==================================================
Training SVM model...

Baseline NWBI Performance:
MAE: 262745.9714
RMSE: 351454.0808
MAPE: 7.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 117
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0002, rank=1
   2. Feature_1_t2: importance=0.0001, rank=2
   3. Feature_1_t3: importance=0.0001, rank=3
   4. Feature_1_t0: importance=0.0001, rank=4
   5. Feature_66_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for NWBI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NWBI...

==================================================
Training Enhanced NWBI (SVM)
==================================================
Training SVM model...

Enhanced NWBI Performance:
MAE: 241453.6111
RMSE: 304096.5377
MAPE: 6.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t2: importance=0.0006, rank=1
   2. Feature_22_t1: importance=0.0002, rank=2
   3. Feature_20_t1: importance=0.0002, rank=3
   4. Feature_6_t2: importance=0.0002, rank=4
   5. Feature_18_t2: importance=0.0002, rank=5

ðŸ“Š NWBI Results:
  Baseline MAPE: 7.73%
  Enhanced MAPE: 6.96%
  MAPE Improvement: +0.77% (+9.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 300/464: NWL
============================================================
ðŸ“Š Loading data for NWL...
ðŸ“Š Loading data for NWL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing NWL: 'NWL'

============================================================
TESTING TICKER 301/464: NWN
============================================================
ðŸ“Š Loading data for NWN...
ðŸ“Š Loading data for NWN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NWN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NWN...

==================================================
Training Baseline NWN (SVM)
==================================================
Training SVM model...

Baseline NWN Performance:
MAE: 148996.7433
RMSE: 216263.0433
MAPE: 17.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0051, rank=1
   2. Feature_65_t1: importance=0.0037, rank=2
   3. Feature_1_t2: importance=0.0034, rank=3
   4. Feature_0_t3: importance=0.0026, rank=4
   5. Feature_2_t3: importance=0.0024, rank=5

ðŸ”§ Applying universal feature engineering for NWN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NWN...

==================================================
Training Enhanced NWN (SVM)
==================================================
Training SVM model...

Enhanced NWN Performance:
MAE: 154877.0415
RMSE: 217940.2377
MAPE: 18.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t1: importance=0.0034, rank=1
   2. Feature_9_t3: importance=0.0033, rank=2
   3. Feature_6_t3: importance=0.0029, rank=3
   4. Feature_24_t2: importance=0.0023, rank=4
   5. Feature_21_t3: importance=0.0022, rank=5

ðŸ“Š NWN Results:
  Baseline MAPE: 17.72%
  Enhanced MAPE: 18.90%
  MAPE Improvement: -1.18% (-6.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 302/464: NX
============================================================
ðŸ“Š Loading data for NX...
ðŸ“Š Loading data for NX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NX...

==================================================
Training Baseline NX (SVM)
==================================================
Training SVM model...

Baseline NX Performance:
MAE: 239098.0647
RMSE: 373867.3668
MAPE: 13.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 109
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0007, rank=1
   2. Feature_67_t3: importance=0.0005, rank=2
   3. Feature_65_t0: importance=0.0003, rank=3
   4. Feature_1_t3: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for NX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NX...

==================================================
Training Enhanced NX (SVM)
==================================================
Training SVM model...

Enhanced NX Performance:
MAE: 230921.3926
RMSE: 367616.7932
MAPE: 13.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0006, rank=1
   2. Feature_23_t3: importance=0.0004, rank=2
   3. Feature_18_t3: importance=0.0004, rank=3
   4. Feature_6_t1: importance=0.0003, rank=4
   5. Feature_8_t3: importance=0.0002, rank=5

ðŸ“Š NX Results:
  Baseline MAPE: 13.05%
  Enhanced MAPE: 13.02%
  MAPE Improvement: +0.03% (+0.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 303/464: NXRT
============================================================
ðŸ“Š Loading data for NXRT...
ðŸ“Š Loading data for NXRT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for NXRT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for NXRT...

==================================================
Training Baseline NXRT (SVM)
==================================================
Training SVM model...

Baseline NXRT Performance:
MAE: 49684.8145
RMSE: 69299.9838
MAPE: 13.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0037, rank=1
   2. Feature_0_t3: importance=0.0032, rank=2
   3. Feature_65_t3: importance=0.0012, rank=3
   4. Feature_2_t1: importance=0.0009, rank=4
   5. Feature_2_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for NXRT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for NXRT...

==================================================
Training Enhanced NXRT (SVM)
==================================================
Training SVM model...

Enhanced NXRT Performance:
MAE: 61294.9729
RMSE: 80593.1042
MAPE: 16.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0033, rank=1
   2. Feature_6_t3: importance=0.0032, rank=2
   3. Feature_13_t3: importance=0.0022, rank=3
   4. Feature_22_t0: importance=0.0020, rank=4
   5. Feature_12_t3: importance=0.0013, rank=5

ðŸ“Š NXRT Results:
  Baseline MAPE: 13.13%
  Enhanced MAPE: 16.67%
  MAPE Improvement: -3.54% (-26.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 304/464: OFG
============================================================
ðŸ“Š Loading data for OFG...
ðŸ“Š Loading data for OFG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OFG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OFG...

==================================================
Training Baseline OFG (SVM)
==================================================
Training SVM model...

Baseline OFG Performance:
MAE: 120766.0885
RMSE: 173534.5833
MAPE: 24.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 119
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t1: importance=0.0010, rank=1
   2. Feature_64_t3: importance=0.0010, rank=2
   3. Feature_67_t3: importance=0.0008, rank=3
   4. Feature_65_t1: importance=0.0007, rank=4
   5. Feature_67_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for OFG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OFG...

==================================================
Training Enhanced OFG (SVM)
==================================================
Training SVM model...

Enhanced OFG Performance:
MAE: 129296.1720
RMSE: 170755.4867
MAPE: 24.32%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0035, rank=1
   2. Feature_23_t3: importance=0.0034, rank=2
   3. Feature_13_t3: importance=0.0032, rank=3
   4. Feature_19_t3: importance=0.0030, rank=4
   5. Feature_19_t0: importance=0.0025, rank=5

ðŸ“Š OFG Results:
  Baseline MAPE: 24.23%
  Enhanced MAPE: 24.32%
  MAPE Improvement: -0.09% (-0.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 305/464: OI
============================================================
ðŸ“Š Loading data for OI...
ðŸ“Š Loading data for OI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OI...

==================================================
Training Baseline OI (SVM)
==================================================
Training SVM model...

Baseline OI Performance:
MAE: 514934.2113
RMSE: 648288.0678
MAPE: 9.53%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 96
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0009, rank=1
   2. Feature_64_t3: importance=0.0009, rank=2
   3. Feature_63_t1: importance=0.0008, rank=3
   4. Feature_67_t0: importance=0.0006, rank=4
   5. Feature_64_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for OI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OI...

==================================================
Training Enhanced OI (SVM)
==================================================
Training SVM model...

Enhanced OI Performance:
MAE: 498067.5392
RMSE: 695883.6110
MAPE: 9.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0007, rank=1
   2. Feature_23_t3: importance=0.0006, rank=2
   3. Feature_19_t2: importance=0.0006, rank=3
   4. Feature_19_t3: importance=0.0005, rank=4
   5. Feature_23_t2: importance=0.0005, rank=5

ðŸ“Š OI Results:
  Baseline MAPE: 9.53%
  Enhanced MAPE: 9.23%
  MAPE Improvement: +0.31% (+3.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 306/464: OII
============================================================
ðŸ“Š Loading data for OII...
ðŸ“Š Loading data for OII from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OII...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OII...

==================================================
Training Baseline OII (SVM)
==================================================
Training SVM model...

Baseline OII Performance:
MAE: 298102.5077
RMSE: 373386.3448
MAPE: 6.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0011, rank=1
   2. Feature_2_t3: importance=0.0006, rank=2
   3. Feature_1_t0: importance=0.0006, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_65_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for OII...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OII...

==================================================
Training Enhanced OII (SVM)
==================================================
Training SVM model...

Enhanced OII Performance:
MAE: 282581.7909
RMSE: 328040.0018
MAPE: 6.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0021, rank=1
   2. Feature_21_t3: importance=0.0014, rank=2
   3. Feature_14_t3: importance=0.0009, rank=3
   4. Feature_11_t2: importance=0.0008, rank=4
   5. Feature_4_t1: importance=0.0007, rank=5

ðŸ“Š OII Results:
  Baseline MAPE: 6.79%
  Enhanced MAPE: 6.45%
  MAPE Improvement: +0.34% (+5.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 307/464: OMCL
============================================================
ðŸ“Š Loading data for OMCL...
ðŸ“Š Loading data for OMCL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OMCL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OMCL...

==================================================
Training Baseline OMCL (SVM)
==================================================
Training SVM model...

Baseline OMCL Performance:
MAE: 217680.1597
RMSE: 270082.3063
MAPE: 10.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 21
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t2: importance=0.0005, rank=1
   2. Feature_0_t0: importance=0.0004, rank=2
   3. Feature_2_t0: importance=0.0003, rank=3
   4. Feature_0_t1: importance=0.0003, rank=4
   5. Feature_67_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for OMCL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OMCL...

==================================================
Training Enhanced OMCL (SVM)
==================================================
Training SVM model...

Enhanced OMCL Performance:
MAE: 173183.0953
RMSE: 241189.7796
MAPE: 8.53%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0002, rank=1
   2. Feature_23_t3: importance=0.0002, rank=2
   3. Feature_19_t3: importance=0.0002, rank=3
   4. Feature_13_t3: importance=0.0002, rank=4
   5. Feature_16_t2: importance=0.0002, rank=5

ðŸ“Š OMCL Results:
  Baseline MAPE: 10.98%
  Enhanced MAPE: 8.53%
  MAPE Improvement: +2.45% (+22.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 308/464: OSIS
============================================================
ðŸ“Š Loading data for OSIS...
ðŸ“Š Loading data for OSIS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OSIS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OSIS...

==================================================
Training Baseline OSIS (SVM)
==================================================
Training SVM model...

Baseline OSIS Performance:
MAE: 103114.1306
RMSE: 159842.3559
MAPE: 8.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0006, rank=1
   2. Feature_64_t3: importance=0.0003, rank=2
   3. Feature_65_t1: importance=0.0002, rank=3
   4. Feature_15_t0: importance=0.0001, rank=4
   5. Feature_2_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for OSIS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OSIS...

==================================================
Training Enhanced OSIS (SVM)
==================================================
Training SVM model...

Enhanced OSIS Performance:
MAE: 102102.0299
RMSE: 165493.4856
MAPE: 9.01%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0010, rank=1
   2. Feature_13_t1: importance=0.0010, rank=2
   3. Feature_19_t1: importance=0.0010, rank=3
   4. Feature_13_t2: importance=0.0005, rank=4
   5. Feature_23_t2: importance=0.0004, rank=5

ðŸ“Š OSIS Results:
  Baseline MAPE: 8.40%
  Enhanced MAPE: 9.01%
  MAPE Improvement: -0.61% (-7.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 309/464: OTTR
============================================================
ðŸ“Š Loading data for OTTR...
ðŸ“Š Loading data for OTTR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OTTR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OTTR...

==================================================
Training Baseline OTTR (SVM)
==================================================
Training SVM model...

Baseline OTTR Performance:
MAE: 174598.5673
RMSE: 220821.1433
MAPE: 4.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0041, rank=1
   2. Feature_63_t0: importance=0.0040, rank=2
   3. Feature_64_t3: importance=0.0021, rank=3
   4. Feature_65_t1: importance=0.0013, rank=4
   5. Feature_65_t2: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for OTTR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OTTR...

==================================================
Training Enhanced OTTR (SVM)
==================================================
Training SVM model...

Enhanced OTTR Performance:
MAE: 203300.3030
RMSE: 264577.4304
MAPE: 5.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0020, rank=1
   2. Feature_16_t2: importance=0.0020, rank=2
   3. Feature_21_t3: importance=0.0020, rank=3
   4. Feature_4_t1: importance=0.0016, rank=4
   5. Feature_10_t2: importance=0.0015, rank=5

ðŸ“Š OTTR Results:
  Baseline MAPE: 4.94%
  Enhanced MAPE: 5.80%
  MAPE Improvement: -0.86% (-17.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 310/464: OUT
============================================================
ðŸ“Š Loading data for OUT...
ðŸ“Š Loading data for OUT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OUT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OUT...

==================================================
Training Baseline OUT (SVM)
==================================================
Training SVM model...

Baseline OUT Performance:
MAE: 1434686.2897
RMSE: 1848711.9650
MAPE: 12.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0025, rank=1
   2. Feature_2_t1: importance=0.0015, rank=2
   3. Feature_2_t3: importance=0.0013, rank=3
   4. Feature_2_t0: importance=0.0012, rank=4
   5. Feature_0_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for OUT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OUT...

==================================================
Training Enhanced OUT (SVM)
==================================================
Training SVM model...

Enhanced OUT Performance:
MAE: 1006020.6852
RMSE: 1433710.7174
MAPE: 10.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0015, rank=1
   2. Feature_6_t3: importance=0.0015, rank=2
   3. Feature_8_t1: importance=0.0014, rank=3
   4. Feature_18_t3: importance=0.0013, rank=4
   5. Feature_19_t3: importance=0.0012, rank=5

ðŸ“Š OUT Results:
  Baseline MAPE: 12.35%
  Enhanced MAPE: 10.29%
  MAPE Improvement: +2.06% (+16.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 311/464: OXM
============================================================
ðŸ“Š Loading data for OXM...
ðŸ“Š Loading data for OXM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for OXM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for OXM...

==================================================
Training Baseline OXM (SVM)
==================================================
Training SVM model...

Baseline OXM Performance:
MAE: 174147.6023
RMSE: 227115.1451
MAPE: 7.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 109
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0006, rank=1
   2. Feature_65_t1: importance=0.0004, rank=2
   3. Feature_64_t1: importance=0.0003, rank=3
   4. Feature_64_t3: importance=0.0002, rank=4
   5. Feature_64_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for OXM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for OXM...

==================================================
Training Enhanced OXM (SVM)
==================================================
Training SVM model...

Enhanced OXM Performance:
MAE: 144872.2416
RMSE: 191975.5752
MAPE: 6.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0005, rank=1
   2. Feature_24_t3: importance=0.0005, rank=2
   3. Feature_16_t3: importance=0.0005, rank=3
   4. Feature_9_t1: importance=0.0004, rank=4
   5. Feature_16_t1: importance=0.0004, rank=5

ðŸ“Š OXM Results:
  Baseline MAPE: 7.85%
  Enhanced MAPE: 6.50%
  MAPE Improvement: +1.35% (+17.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 312/464: PAHC
============================================================
ðŸ“Š Loading data for PAHC...
ðŸ“Š Loading data for PAHC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing PAHC: 'PAHC'

============================================================
TESTING TICKER 313/464: PARR
============================================================
ðŸ“Š Loading data for PARR...
ðŸ“Š Loading data for PARR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PARR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PARR...

==================================================
Training Baseline PARR (SVM)
==================================================
Training SVM model...

Baseline PARR Performance:
MAE: 439160.6544
RMSE: 606339.7148
MAPE: 13.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 145
   â€¢ Highly important features (top 5%): 81

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0010, rank=1
   2. Feature_0_t3: importance=0.0009, rank=2
   3. Feature_0_t2: importance=0.0008, rank=3
   4. Feature_65_t1: importance=0.0007, rank=4
   5. Feature_65_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for PARR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PARR...

==================================================
Training Enhanced PARR (SVM)
==================================================
Training SVM model...

Enhanced PARR Performance:
MAE: 372749.7781
RMSE: 551646.2241
MAPE: 10.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0017, rank=1
   2. Feature_19_t1: importance=0.0016, rank=2
   3. Feature_23_t1: importance=0.0016, rank=3
   4. Feature_19_t2: importance=0.0010, rank=4
   5. Feature_9_t2: importance=0.0009, rank=5

ðŸ“Š PARR Results:
  Baseline MAPE: 13.04%
  Enhanced MAPE: 10.86%
  MAPE Improvement: +2.17% (+16.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 314/464: PATK
============================================================
ðŸ“Š Loading data for PATK...
ðŸ“Š Loading data for PATK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PATK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PATK...

==================================================
Training Baseline PATK (SVM)
==================================================
Training SVM model...

Baseline PATK Performance:
MAE: 136302.0616
RMSE: 164991.6845
MAPE: 7.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 132
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0003, rank=1
   2. Feature_2_t2: importance=0.0003, rank=2
   3. Feature_67_t0: importance=0.0002, rank=3
   4. Feature_64_t3: importance=0.0001, rank=4
   5. Feature_65_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for PATK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PATK...

==================================================
Training Enhanced PATK (SVM)
==================================================
Training SVM model...

Enhanced PATK Performance:
MAE: 141514.7266
RMSE: 177669.8472
MAPE: 7.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 32

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0010, rank=1
   2. Feature_2_t1: importance=0.0005, rank=2
   3. Feature_15_t0: importance=0.0004, rank=3
   4. Feature_16_t1: importance=0.0004, rank=4
   5. Feature_16_t0: importance=0.0003, rank=5

ðŸ“Š PATK Results:
  Baseline MAPE: 7.08%
  Enhanced MAPE: 7.35%
  MAPE Improvement: -0.27% (-3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 315/464: PBH
============================================================
ðŸ“Š Loading data for PBH...
ðŸ“Š Loading data for PBH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PBH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PBH...

==================================================
Training Baseline PBH (SVM)
==================================================
Training SVM model...

Baseline PBH Performance:
MAE: 165653.3715
RMSE: 213865.3973
MAPE: 9.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 23
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0004, rank=1
   2. Feature_1_t1: importance=0.0003, rank=2
   3. Feature_67_t2: importance=0.0002, rank=3
   4. Feature_65_t0: importance=0.0002, rank=4
   5. Feature_2_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for PBH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PBH...

==================================================
Training Enhanced PBH (SVM)
==================================================
Training SVM model...

Enhanced PBH Performance:
MAE: 157498.8878
RMSE: 194666.7924
MAPE: 8.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0006, rank=1
   2. Feature_16_t3: importance=0.0003, rank=2
   3. Feature_21_t1: importance=0.0002, rank=3
   4. Feature_8_t2: importance=0.0002, rank=4
   5. Feature_8_t0: importance=0.0002, rank=5

ðŸ“Š PBH Results:
  Baseline MAPE: 9.09%
  Enhanced MAPE: 8.27%
  MAPE Improvement: +0.83% (+9.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 316/464: PBI
============================================================
ðŸ“Š Loading data for PBI...
ðŸ“Š Loading data for PBI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PBI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PBI...

==================================================
Training Baseline PBI (SVM)
==================================================
Training SVM model...

Baseline PBI Performance:
MAE: 1084931.1047
RMSE: 1383451.5770
MAPE: 9.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 174
   â€¢ Highly important features (top 5%): 95

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0008, rank=1
   2. Feature_65_t3: importance=0.0008, rank=2
   3. Feature_67_t2: importance=0.0005, rank=3
   4. Feature_65_t2: importance=0.0004, rank=4
   5. Feature_63_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for PBI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PBI...

==================================================
Training Enhanced PBI (SVM)
==================================================
Training SVM model...

Enhanced PBI Performance:
MAE: 1047904.1375
RMSE: 1266239.8631
MAPE: 9.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t2: importance=0.0013, rank=1
   2. Feature_16_t3: importance=0.0012, rank=2
   3. Feature_11_t1: importance=0.0010, rank=3
   4. Feature_5_t3: importance=0.0006, rank=4
   5. Feature_0_t2: importance=0.0005, rank=5

ðŸ“Š PBI Results:
  Baseline MAPE: 9.55%
  Enhanced MAPE: 9.07%
  MAPE Improvement: +0.48% (+5.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 317/464: PCRX
============================================================
ðŸ“Š Loading data for PCRX...
ðŸ“Š Loading data for PCRX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PCRX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PCRX...

==================================================
Training Baseline PCRX (SVM)
==================================================
Training SVM model...

Baseline PCRX Performance:
MAE: 451485.0293
RMSE: 643011.0599
MAPE: 8.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 57
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0006, rank=1
   2. Feature_67_t3: importance=0.0003, rank=2
   3. Feature_65_t1: importance=0.0002, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_66_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for PCRX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PCRX...

==================================================
Training Enhanced PCRX (SVM)
==================================================
Training SVM model...

Enhanced PCRX Performance:
MAE: 391191.8103
RMSE: 586836.2097
MAPE: 7.27%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t1: importance=0.0002, rank=1
   2. Feature_4_t0: importance=0.0002, rank=2
   3. Feature_23_t0: importance=0.0001, rank=3
   4. Feature_19_t3: importance=0.0001, rank=4
   5. Feature_5_t3: importance=0.0001, rank=5

ðŸ“Š PCRX Results:
  Baseline MAPE: 8.18%
  Enhanced MAPE: 7.27%
  MAPE Improvement: +0.91% (+11.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 318/464: PDFS
============================================================
ðŸ“Š Loading data for PDFS...
ðŸ“Š Loading data for PDFS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PDFS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PDFS...

==================================================
Training Baseline PDFS (SVM)
==================================================
Training SVM model...

Baseline PDFS Performance:
MAE: 67197.6346
RMSE: 85921.0322
MAPE: 9.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 130
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t0: importance=0.0008, rank=1
   2. Feature_63_t3: importance=0.0006, rank=2
   3. Feature_67_t2: importance=0.0006, rank=3
   4. Feature_2_t2: importance=0.0005, rank=4
   5. Feature_65_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for PDFS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PDFS...

==================================================
Training Enhanced PDFS (SVM)
==================================================
Training SVM model...

Enhanced PDFS Performance:
MAE: 71206.5426
RMSE: 96263.6877
MAPE: 9.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0007, rank=1
   2. Feature_23_t1: importance=0.0007, rank=2
   3. Feature_10_t3: importance=0.0006, rank=3
   4. Feature_13_t1: importance=0.0006, rank=4
   5. Feature_9_t3: importance=0.0005, rank=5

ðŸ“Š PDFS Results:
  Baseline MAPE: 9.71%
  Enhanced MAPE: 9.94%
  MAPE Improvement: -0.23% (-2.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 319/464: PEB
============================================================
ðŸ“Š Loading data for PEB...
ðŸ“Š Loading data for PEB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PEB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PEB...

==================================================
Training Baseline PEB (SVM)
==================================================
Training SVM model...

Baseline PEB Performance:
MAE: 1056749.7769
RMSE: 1472492.1124
MAPE: 7.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 34

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0054, rank=1
   2. Feature_63_t0: importance=0.0010, rank=2
   3. Feature_1_t3: importance=0.0007, rank=3
   4. Feature_0_t3: importance=0.0006, rank=4
   5. Feature_2_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for PEB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PEB...

==================================================
Training Enhanced PEB (SVM)
==================================================
Training SVM model...

Enhanced PEB Performance:
MAE: 1132463.9538
RMSE: 1358894.0721
MAPE: 7.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0037, rank=1
   2. Feature_20_t1: importance=0.0007, rank=2
   3. Feature_13_t0: importance=0.0007, rank=3
   4. Feature_18_t2: importance=0.0006, rank=4
   5. Feature_23_t0: importance=0.0006, rank=5

ðŸ“Š PEB Results:
  Baseline MAPE: 7.03%
  Enhanced MAPE: 7.14%
  MAPE Improvement: -0.11% (-1.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 320/464: PENN
============================================================
ðŸ“Š Loading data for PENN...
ðŸ“Š Loading data for PENN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing PENN: 'PENN'

============================================================
TESTING TICKER 321/464: PFBC
============================================================
ðŸ“Š Loading data for PFBC...
ðŸ“Š Loading data for PFBC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing PFBC: 'PFBC'

============================================================
TESTING TICKER 322/464: PFS
============================================================
ðŸ“Š Loading data for PFS...
ðŸ“Š Loading data for PFS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PFS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PFS...

==================================================
Training Baseline PFS (SVM)
==================================================
Training SVM model...

Baseline PFS Performance:
MAE: 276764.7104
RMSE: 407458.3434
MAPE: 7.22%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 150
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_66_t0: importance=0.0006, rank=2
   3. Feature_63_t2: importance=0.0004, rank=3
   4. Feature_65_t0: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for PFS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PFS...

==================================================
Training Enhanced PFS (SVM)
==================================================
Training SVM model...

Enhanced PFS Performance:
MAE: 355355.3571
RMSE: 457497.3669
MAPE: 9.77%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0008, rank=1
   2. Feature_11_t0: importance=0.0007, rank=2
   3. Feature_4_t0: importance=0.0007, rank=3
   4. Feature_12_t3: importance=0.0006, rank=4
   5. Feature_19_t0: importance=0.0006, rank=5

ðŸ“Š PFS Results:
  Baseline MAPE: 7.22%
  Enhanced MAPE: 9.77%
  MAPE Improvement: -2.54% (-35.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 323/464: PI
============================================================
ðŸ“Š Loading data for PI...
ðŸ“Š Loading data for PI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PI...

==================================================
Training Baseline PI (SVM)
==================================================
Training SVM model...

Baseline PI Performance:
MAE: 248556.4719
RMSE: 348181.8495
MAPE: 7.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0005, rank=1
   2. Feature_2_t3: importance=0.0004, rank=2
   3. Feature_64_t0: importance=0.0003, rank=3
   4. Feature_2_t2: importance=0.0003, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for PI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PI...

==================================================
Training Enhanced PI (SVM)
==================================================
Training SVM model...

Enhanced PI Performance:
MAE: 247341.3345
RMSE: 326501.4023
MAPE: 7.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 38
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0013, rank=1
   2. Feature_15_t1: importance=0.0004, rank=2
   3. Feature_4_t2: importance=0.0004, rank=3
   4. Feature_23_t2: importance=0.0004, rank=4
   5. Feature_19_t2: importance=0.0004, rank=5

ðŸ“Š PI Results:
  Baseline MAPE: 7.45%
  Enhanced MAPE: 7.39%
  MAPE Improvement: +0.06% (+0.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 324/464: PINC
============================================================
ðŸ“Š Loading data for PINC...
ðŸ“Š Loading data for PINC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PINC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PINC...

==================================================
Training Baseline PINC (SVM)
==================================================
Training SVM model...

Baseline PINC Performance:
MAE: 916470.4537
RMSE: 1756134.8557
MAPE: 9.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 151
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0013, rank=1
   2. Feature_63_t1: importance=0.0006, rank=2
   3. Feature_64_t1: importance=0.0005, rank=3
   4. Feature_67_t3: importance=0.0004, rank=4
   5. Feature_0_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for PINC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PINC...

==================================================
Training Enhanced PINC (SVM)
==================================================
Training SVM model...

Enhanced PINC Performance:
MAE: 999706.9346
RMSE: 1845942.4468
MAPE: 10.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 48
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0022, rank=1
   2. Feature_19_t3: importance=0.0011, rank=2
   3. Feature_14_t3: importance=0.0010, rank=3
   4. Feature_6_t3: importance=0.0009, rank=4
   5. Feature_4_t3: importance=0.0009, rank=5

ðŸ“Š PINC Results:
  Baseline MAPE: 9.76%
  Enhanced MAPE: 10.60%
  MAPE Improvement: -0.84% (-8.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 325/464: PJT
============================================================
ðŸ“Š Loading data for PJT...
ðŸ“Š Loading data for PJT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PJT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PJT...

==================================================
Training Baseline PJT (SVM)
==================================================
Training SVM model...

Baseline PJT Performance:
MAE: 136507.4809
RMSE: 169667.9705
MAPE: 8.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 111
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0027, rank=1
   2. Feature_65_t1: importance=0.0023, rank=2
   3. Feature_63_t3: importance=0.0022, rank=3
   4. Feature_0_t3: importance=0.0017, rank=4
   5. Feature_0_t2: importance=0.0016, rank=5

ðŸ”§ Applying universal feature engineering for PJT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PJT...

==================================================
Training Enhanced PJT (SVM)
==================================================
Training SVM model...

Enhanced PJT Performance:
MAE: 121385.2046
RMSE: 150696.6346
MAPE: 7.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0024, rank=1
   2. Feature_12_t0: importance=0.0020, rank=2
   3. Feature_4_t1: importance=0.0019, rank=3
   4. Feature_9_t2: importance=0.0018, rank=4
   5. Feature_18_t2: importance=0.0014, rank=5

ðŸ“Š PJT Results:
  Baseline MAPE: 8.74%
  Enhanced MAPE: 7.90%
  MAPE Improvement: +0.84% (+9.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 326/464: PLAB
============================================================
ðŸ“Š Loading data for PLAB...
ðŸ“Š Loading data for PLAB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PLAB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PLAB...

==================================================
Training Baseline PLAB (SVM)
==================================================
Training SVM model...

Baseline PLAB Performance:
MAE: 202592.5713
RMSE: 256145.1422
MAPE: 8.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 64
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t2: importance=0.0024, rank=1
   2. Feature_0_t3: importance=0.0021, rank=2
   3. Feature_65_t1: importance=0.0011, rank=3
   4. Feature_2_t2: importance=0.0007, rank=4
   5. Feature_1_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for PLAB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PLAB...

==================================================
Training Enhanced PLAB (SVM)
==================================================
Training SVM model...

Enhanced PLAB Performance:
MAE: 204617.6928
RMSE: 279987.4214
MAPE: 8.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0011, rank=1
   2. Feature_9_t1: importance=0.0010, rank=2
   3. Feature_20_t3: importance=0.0009, rank=3
   4. Feature_17_t2: importance=0.0009, rank=4
   5. Feature_13_t2: importance=0.0008, rank=5

ðŸ“Š PLAB Results:
  Baseline MAPE: 8.58%
  Enhanced MAPE: 8.39%
  MAPE Improvement: +0.19% (+2.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 327/464: PLAY
============================================================
ðŸ“Š Loading data for PLAY...
ðŸ“Š Loading data for PLAY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PLAY...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PLAY...

==================================================
Training Baseline PLAY (SVM)
==================================================
Training SVM model...

Baseline PLAY Performance:
MAE: 786215.3501
RMSE: 1013694.4581
MAPE: 12.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 154
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0005, rank=1
   2. Feature_0_t3: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0003, rank=3
   4. Feature_0_t2: importance=0.0003, rank=4
   5. Feature_67_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for PLAY...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PLAY...

==================================================
Training Enhanced PLAY (SVM)
==================================================
Training SVM model...

Enhanced PLAY Performance:
MAE: 680366.7588
RMSE: 880781.5089
MAPE: 11.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t3: importance=0.0011, rank=1
   2. Feature_19_t3: importance=0.0011, rank=2
   3. Feature_13_t3: importance=0.0010, rank=3
   4. Feature_23_t3: importance=0.0009, rank=4
   5. Feature_1_t3: importance=0.0005, rank=5

ðŸ“Š PLAY Results:
  Baseline MAPE: 12.65%
  Enhanced MAPE: 11.11%
  MAPE Improvement: +1.53% (+12.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 328/464: PLUS
============================================================
ðŸ“Š Loading data for PLUS...
ðŸ“Š Loading data for PLUS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing PLUS: 'PLUS'

============================================================
TESTING TICKER 329/464: PLXS
============================================================
ðŸ“Š Loading data for PLXS...
ðŸ“Š Loading data for PLXS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PLXS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PLXS...

==================================================
Training Baseline PLXS (SVM)
==================================================
Training SVM model...

Baseline PLXS Performance:
MAE: 51286.6080
RMSE: 70907.5375
MAPE: 10.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0008, rank=1
   2. Feature_64_t3: importance=0.0006, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_64_t1: importance=0.0005, rank=4
   5. Feature_65_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for PLXS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PLXS...

==================================================
Training Enhanced PLXS (SVM)
==================================================
Training SVM model...

Enhanced PLXS Performance:
MAE: 61681.3249
RMSE: 77298.7848
MAPE: 12.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t2: importance=0.0007, rank=1
   2. Feature_22_t3: importance=0.0006, rank=2
   3. Feature_12_t3: importance=0.0005, rank=3
   4. Feature_16_t2: importance=0.0004, rank=4
   5. Feature_18_t3: importance=0.0004, rank=5

ðŸ“Š PLXS Results:
  Baseline MAPE: 10.49%
  Enhanced MAPE: 12.79%
  MAPE Improvement: -2.31% (-22.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 330/464: PMT
============================================================
ðŸ“Š Loading data for PMT...
ðŸ“Š Loading data for PMT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PMT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PMT...

==================================================
Training Baseline PMT (SVM)
==================================================
Training SVM model...

Baseline PMT Performance:
MAE: 291197.4902
RMSE: 442370.2562
MAPE: 7.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 94
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0015, rank=1
   2. Feature_67_t3: importance=0.0013, rank=2
   3. Feature_0_t2: importance=0.0012, rank=3
   4. Feature_0_t3: importance=0.0008, rank=4
   5. Feature_65_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for PMT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PMT...

==================================================
Training Enhanced PMT (SVM)
==================================================
Training SVM model...

Enhanced PMT Performance:
MAE: 297974.7818
RMSE: 458247.1240
MAPE: 8.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0047, rank=1
   2. Feature_13_t3: importance=0.0046, rank=2
   3. Feature_23_t3: importance=0.0042, rank=3
   4. Feature_4_t0: importance=0.0029, rank=4
   5. Feature_7_t2: importance=0.0017, rank=5

ðŸ“Š PMT Results:
  Baseline MAPE: 7.90%
  Enhanced MAPE: 8.19%
  MAPE Improvement: -0.29% (-3.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 331/464: POWL
============================================================
ðŸ“Š Loading data for POWL...
ðŸ“Š Loading data for POWL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing POWL: 'POWL'

============================================================
TESTING TICKER 332/464: PRA
============================================================
ðŸ“Š Loading data for PRA...
ðŸ“Š Loading data for PRA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PRA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PRA...

==================================================
Training Baseline PRA (SVM)
==================================================
Training SVM model...

Baseline PRA Performance:
MAE: 138272.5380
RMSE: 219085.5171
MAPE: 13.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 119
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0029, rank=1
   2. Feature_67_t2: importance=0.0017, rank=2
   3. Feature_2_t3: importance=0.0011, rank=3
   4. Feature_2_t1: importance=0.0009, rank=4
   5. Feature_0_t3: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for PRA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PRA...

==================================================
Training Enhanced PRA (SVM)
==================================================
Training SVM model...

Enhanced PRA Performance:
MAE: 131867.2233
RMSE: 215350.1540
MAPE: 12.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 86

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t3: importance=0.0009, rank=1
   2. Feature_6_t2: importance=0.0009, rank=2
   3. Feature_15_t3: importance=0.0007, rank=3
   4. Feature_23_t0: importance=0.0007, rank=4
   5. Feature_22_t2: importance=0.0007, rank=5

ðŸ“Š PRA Results:
  Baseline MAPE: 13.17%
  Enhanced MAPE: 12.73%
  MAPE Improvement: +0.44% (+3.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 333/464: PRAA
============================================================
ðŸ“Š Loading data for PRAA...
ðŸ“Š Loading data for PRAA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PRAA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PRAA...

==================================================
Training Baseline PRAA (SVM)
==================================================
Training SVM model...

Baseline PRAA Performance:
MAE: 99201.2345
RMSE: 138747.5881
MAPE: 12.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0002, rank=1
   2. Feature_67_t2: importance=0.0001, rank=2
   3. Feature_65_t3: importance=0.0001, rank=3
   4. Feature_63_t2: importance=0.0001, rank=4
   5. Feature_65_t1: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for PRAA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PRAA...

==================================================
Training Enhanced PRAA (SVM)
==================================================
Training SVM model...

Enhanced PRAA Performance:
MAE: 97500.3782
RMSE: 135959.9652
MAPE: 12.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0001, rank=1
   2. Feature_4_t1: importance=0.0001, rank=2
   3. Feature_6_t1: importance=0.0001, rank=3
   4. Feature_22_t1: importance=0.0001, rank=4
   5. Feature_4_t3: importance=0.0001, rank=5

ðŸ“Š PRAA Results:
  Baseline MAPE: 12.51%
  Enhanced MAPE: 12.51%
  MAPE Improvement: +0.00% (+0.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 334/464: PRGS
============================================================
ðŸ“Š Loading data for PRGS...
ðŸ“Š Loading data for PRGS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PRGS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PRGS...

==================================================
Training Baseline PRGS (SVM)
==================================================
Training SVM model...

Baseline PRGS Performance:
MAE: 269507.4387
RMSE: 342926.9313
MAPE: 5.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0033, rank=1
   2. Feature_65_t3: importance=0.0015, rank=2
   3. Feature_67_t2: importance=0.0014, rank=3
   4. Feature_64_t3: importance=0.0012, rank=4
   5. Feature_67_t3: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for PRGS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PRGS...

==================================================
Training Enhanced PRGS (SVM)
==================================================
Training SVM model...

Enhanced PRGS Performance:
MAE: 371169.1705
RMSE: 436487.9168
MAPE: 7.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0023, rank=1
   2. Feature_6_t0: importance=0.0022, rank=2
   3. Feature_13_t0: importance=0.0020, rank=3
   4. Feature_13_t3: importance=0.0018, rank=4
   5. Feature_23_t2: importance=0.0018, rank=5

ðŸ“Š PRGS Results:
  Baseline MAPE: 5.73%
  Enhanced MAPE: 7.72%
  MAPE Improvement: -1.99% (-34.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 335/464: PRK
============================================================
ðŸ“Š Loading data for PRK...
ðŸ“Š Loading data for PRK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PRK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PRK...

==================================================
Training Baseline PRK (SVM)
==================================================
Training SVM model...

Baseline PRK Performance:
MAE: 48227.1336
RMSE: 58996.9133
MAPE: 8.52%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 195
   â€¢ Highly important features (top 5%): 154

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0078, rank=1
   2. Feature_1_t3: importance=0.0032, rank=2
   3. Feature_64_t1: importance=0.0007, rank=3
   4. Feature_65_t2: importance=0.0006, rank=4
   5. Feature_64_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for PRK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PRK...

==================================================
Training Enhanced PRK (SVM)
==================================================
Training SVM model...

Enhanced PRK Performance:
MAE: 55132.9255
RMSE: 65929.3668
MAPE: 10.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0077, rank=1
   2. Feature_13_t3: importance=0.0075, rank=2
   3. Feature_19_t3: importance=0.0075, rank=3
   4. Feature_6_t3: importance=0.0074, rank=4
   5. Feature_1_t3: importance=0.0017, rank=5

ðŸ“Š PRK Results:
  Baseline MAPE: 8.52%
  Enhanced MAPE: 10.06%
  MAPE Improvement: -1.54% (-18.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 336/464: PRLB
============================================================
ðŸ“Š Loading data for PRLB...
ðŸ“Š Loading data for PRLB from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PRLB...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PRLB...

==================================================
Training Baseline PRLB (SVM)
==================================================
Training SVM model...

Baseline PRLB Performance:
MAE: 81009.6420
RMSE: 108723.4774
MAPE: 11.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0007, rank=1
   2. Feature_63_t0: importance=0.0006, rank=2
   3. Feature_1_t1: importance=0.0005, rank=3
   4. Feature_67_t1: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for PRLB...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PRLB...

==================================================
Training Enhanced PRLB (SVM)
==================================================
Training SVM model...

Enhanced PRLB Performance:
MAE: 81024.4088
RMSE: 107153.7090
MAPE: 11.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 23

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0013, rank=1
   2. Feature_6_t1: importance=0.0006, rank=2
   3. Feature_21_t1: importance=0.0005, rank=3
   4. Feature_12_t1: importance=0.0004, rank=4
   5. Feature_18_t3: importance=0.0003, rank=5

ðŸ“Š PRLB Results:
  Baseline MAPE: 11.40%
  Enhanced MAPE: 11.41%
  MAPE Improvement: -0.00% (-0.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 337/464: PSMT
============================================================
ðŸ“Š Loading data for PSMT...
ðŸ“Š Loading data for PSMT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PSMT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PSMT...

==================================================
Training Baseline PSMT (SVM)
==================================================
Training SVM model...

Baseline PSMT Performance:
MAE: 93421.9602
RMSE: 116127.9945
MAPE: 9.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 133
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0013, rank=1
   2. Feature_65_t1: importance=0.0010, rank=2
   3. Feature_1_t2: importance=0.0004, rank=3
   4. Feature_67_t3: importance=0.0004, rank=4
   5. Feature_65_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for PSMT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PSMT...

==================================================
Training Enhanced PSMT (SVM)
==================================================
Training SVM model...

Enhanced PSMT Performance:
MAE: 93772.2320
RMSE: 118344.0321
MAPE: 9.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0012, rank=1
   2. Feature_9_t3: importance=0.0007, rank=2
   3. Feature_15_t1: importance=0.0007, rank=3
   4. Feature_20_t2: importance=0.0007, rank=4
   5. Feature_10_t2: importance=0.0007, rank=5

ðŸ“Š PSMT Results:
  Baseline MAPE: 9.38%
  Enhanced MAPE: 9.68%
  MAPE Improvement: -0.30% (-3.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 338/464: PTEN
============================================================
ðŸ“Š Loading data for PTEN...
ðŸ“Š Loading data for PTEN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PTEN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PTEN...

==================================================
Training Baseline PTEN (SVM)
==================================================
Training SVM model...

Baseline PTEN Performance:
MAE: 3270173.5433
RMSE: 4085367.5515
MAPE: 12.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 18

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0011, rank=1
   2. Feature_67_t0: importance=0.0005, rank=2
   3. Feature_67_t2: importance=0.0005, rank=3
   4. Feature_65_t1: importance=0.0004, rank=4
   5. Feature_1_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for PTEN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PTEN...

==================================================
Training Enhanced PTEN (SVM)
==================================================
Training SVM model...

Enhanced PTEN Performance:
MAE: 2926506.8737
RMSE: 3707884.8282
MAPE: 11.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0006, rank=1
   2. Feature_12_t3: importance=0.0005, rank=2
   3. Feature_6_t0: importance=0.0005, rank=3
   4. Feature_7_t0: importance=0.0004, rank=4
   5. Feature_2_t2: importance=0.0004, rank=5

ðŸ“Š PTEN Results:
  Baseline MAPE: 12.65%
  Enhanced MAPE: 11.38%
  MAPE Improvement: +1.27% (+10.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 339/464: PTGX
============================================================
ðŸ“Š Loading data for PTGX...
ðŸ“Š Loading data for PTGX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing PTGX: 'PTGX'

============================================================
TESTING TICKER 340/464: PZZA
============================================================
ðŸ“Š Loading data for PZZA...
ðŸ“Š Loading data for PZZA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for PZZA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for PZZA...

==================================================
Training Baseline PZZA (SVM)
==================================================
Training SVM model...

Baseline PZZA Performance:
MAE: 650131.6697
RMSE: 928037.0091
MAPE: 17.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_63_t0: importance=0.0009, rank=2
   3. Feature_0_t3: importance=0.0007, rank=3
   4. Feature_65_t3: importance=0.0006, rank=4
   5. Feature_63_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for PZZA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for PZZA...

==================================================
Training Enhanced PZZA (SVM)
==================================================
Training SVM model...

Enhanced PZZA Performance:
MAE: 561780.2849
RMSE: 809232.8266
MAPE: 14.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0012, rank=1
   2. Feature_23_t2: importance=0.0011, rank=2
   3. Feature_13_t3: importance=0.0010, rank=3
   4. Feature_23_t3: importance=0.0009, rank=4
   5. Feature_18_t2: importance=0.0009, rank=5

ðŸ“Š PZZA Results:
  Baseline MAPE: 17.48%
  Enhanced MAPE: 14.85%
  MAPE Improvement: +2.62% (+15.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 341/464: QDEL
============================================================
ðŸ“Š Loading data for QDEL...
ðŸ“Š Loading data for QDEL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing QDEL: 'QDEL'

============================================================
TESTING TICKER 342/464: QNST
============================================================
ðŸ“Š Loading data for QNST...
ðŸ“Š Loading data for QNST from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for QNST...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for QNST...

==================================================
Training Baseline QNST (SVM)
==================================================
Training SVM model...

Baseline QNST Performance:
MAE: 231321.8181
RMSE: 375288.1743
MAPE: 13.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 145
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0023, rank=1
   2. Feature_67_t0: importance=0.0014, rank=2
   3. Feature_2_t3: importance=0.0013, rank=3
   4. Feature_63_t0: importance=0.0009, rank=4
   5. Feature_67_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for QNST...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for QNST...

==================================================
Training Enhanced QNST (SVM)
==================================================
Training SVM model...

Enhanced QNST Performance:
MAE: 208757.4468
RMSE: 397075.4791
MAPE: 11.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t2: importance=0.0027, rank=1
   2. Feature_19_t2: importance=0.0023, rank=2
   3. Feature_13_t2: importance=0.0021, rank=3
   4. Feature_23_t2: importance=0.0020, rank=4
   5. Feature_6_t0: importance=0.0020, rank=5

ðŸ“Š QNST Results:
  Baseline MAPE: 13.19%
  Enhanced MAPE: 11.87%
  MAPE Improvement: +1.32% (+10.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 343/464: QRVO
============================================================
ðŸ“Š Loading data for QRVO...
ðŸ“Š Loading data for QRVO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for QRVO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for QRVO...

==================================================
Training Baseline QRVO (SVM)
==================================================
Training SVM model...

Baseline QRVO Performance:
MAE: 565226.6536
RMSE: 909065.3983
MAPE: 12.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0017, rank=1
   2. Feature_65_t3: importance=0.0012, rank=2
   3. Feature_67_t0: importance=0.0011, rank=3
   4. Feature_0_t3: importance=0.0007, rank=4
   5. Feature_63_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for QRVO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for QRVO...

==================================================
Training Enhanced QRVO (SVM)
==================================================
Training SVM model...

Enhanced QRVO Performance:
MAE: 578801.2168
RMSE: 953258.2720
MAPE: 12.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t0: importance=0.0008, rank=1
   2. Feature_4_t2: importance=0.0007, rank=2
   3. Feature_24_t3: importance=0.0006, rank=3
   4. Feature_15_t2: importance=0.0006, rank=4
   5. Feature_23_t3: importance=0.0006, rank=5

ðŸ“Š QRVO Results:
  Baseline MAPE: 12.07%
  Enhanced MAPE: 12.35%
  MAPE Improvement: -0.29% (-2.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 344/464: QTWO
============================================================
ðŸ“Š Loading data for QTWO...
ðŸ“Š Loading data for QTWO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing QTWO: 'QTWO'

============================================================
TESTING TICKER 345/464: RDN
============================================================
ðŸ“Š Loading data for RDN...
ðŸ“Š Loading data for RDN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RDN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RDN...

==================================================
Training Baseline RDN (SVM)
==================================================
Training SVM model...

Baseline RDN Performance:
MAE: 890557.3013
RMSE: 1187258.3306
MAPE: 11.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 133
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0037, rank=1
   2. Feature_2_t3: importance=0.0018, rank=2
   3. Feature_0_t1: importance=0.0010, rank=3
   4. Feature_67_t3: importance=0.0010, rank=4
   5. Feature_2_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for RDN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RDN...

==================================================
Training Enhanced RDN (SVM)
==================================================
Training SVM model...

Enhanced RDN Performance:
MAE: 817322.4777
RMSE: 1190623.1204
MAPE: 9.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0015, rank=1
   2. Feature_0_t3: importance=0.0014, rank=2
   3. Feature_11_t0: importance=0.0014, rank=3
   4. Feature_16_t2: importance=0.0013, rank=4
   5. Feature_7_t3: importance=0.0012, rank=5

ðŸ“Š RDN Results:
  Baseline MAPE: 11.63%
  Enhanced MAPE: 9.92%
  MAPE Improvement: +1.71% (+14.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 346/464: RDNT
============================================================
ðŸ“Š Loading data for RDNT...
ðŸ“Š Loading data for RDNT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RDNT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RDNT...

==================================================
Training Baseline RDNT (SVM)
==================================================
Training SVM model...

Baseline RDNT Performance:
MAE: 352504.6231
RMSE: 470400.4135
MAPE: 8.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0036, rank=1
   2. Feature_1_t0: importance=0.0012, rank=2
   3. Feature_2_t3: importance=0.0010, rank=3
   4. Feature_0_t2: importance=0.0007, rank=4
   5. Feature_65_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for RDNT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RDNT...

==================================================
Training Enhanced RDNT (SVM)
==================================================
Training SVM model...

Enhanced RDNT Performance:
MAE: 260026.3805
RMSE: 337657.3559
MAPE: 6.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0025, rank=1
   2. Feature_22_t3: importance=0.0016, rank=2
   3. Feature_20_t0: importance=0.0015, rank=3
   4. Feature_23_t0: importance=0.0015, rank=4
   5. Feature_13_t3: importance=0.0015, rank=5

ðŸ“Š RDNT Results:
  Baseline MAPE: 8.04%
  Enhanced MAPE: 6.26%
  MAPE Improvement: +1.78% (+22.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 347/464: RES
============================================================
ðŸ“Š Loading data for RES...
ðŸ“Š Loading data for RES from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RES...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RES...

==================================================
Training Baseline RES (SVM)
==================================================
Training SVM model...

Baseline RES Performance:
MAE: 1685815.4060
RMSE: 2139366.2164
MAPE: 10.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 129
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0010, rank=1
   2. Feature_67_t1: importance=0.0009, rank=2
   3. Feature_67_t3: importance=0.0008, rank=3
   4. Feature_0_t3: importance=0.0006, rank=4
   5. Feature_65_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for RES...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RES...

==================================================
Training Enhanced RES (SVM)
==================================================
Training SVM model...

Enhanced RES Performance:
MAE: 1587373.1407
RMSE: 1851589.7341
MAPE: 9.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t1: importance=0.0009, rank=1
   2. Feature_13_t3: importance=0.0007, rank=2
   3. Feature_23_t3: importance=0.0007, rank=3
   4. Feature_9_t3: importance=0.0007, rank=4
   5. Feature_19_t3: importance=0.0007, rank=5

ðŸ“Š RES Results:
  Baseline MAPE: 10.63%
  Enhanced MAPE: 9.75%
  MAPE Improvement: +0.89% (+8.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 348/464: REX
============================================================
ðŸ“Š Loading data for REX...
ðŸ“Š Loading data for REX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing REX: 'REX'

============================================================
TESTING TICKER 349/464: RGR
============================================================
ðŸ“Š Loading data for RGR...
ðŸ“Š Loading data for RGR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RGR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RGR...

==================================================
Training Baseline RGR (SVM)
==================================================
Training SVM model...

Baseline RGR Performance:
MAE: 58726.2742
RMSE: 75760.4619
MAPE: 10.10%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 127
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0007, rank=1
   2. Feature_2_t2: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0005, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_65_t1: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for RGR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RGR...

==================================================
Training Enhanced RGR (SVM)
==================================================
Training SVM model...

Enhanced RGR Performance:
MAE: 53263.2115
RMSE: 70152.1030
MAPE: 9.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 49

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0013, rank=1
   2. Feature_23_t3: importance=0.0012, rank=2
   3. Feature_13_t3: importance=0.0011, rank=3
   4. Feature_16_t0: importance=0.0011, rank=4
   5. Feature_18_t3: importance=0.0007, rank=5

ðŸ“Š RGR Results:
  Baseline MAPE: 10.10%
  Enhanced MAPE: 9.37%
  MAPE Improvement: +0.73% (+7.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 350/464: RHI
============================================================
ðŸ“Š Loading data for RHI...
ðŸ“Š Loading data for RHI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RHI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RHI...

==================================================
Training Baseline RHI (SVM)
==================================================
Training SVM model...

Baseline RHI Performance:
MAE: 567277.1093
RMSE: 787809.0269
MAPE: 7.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t0: importance=0.0004, rank=1
   2. Feature_0_t1: importance=0.0004, rank=2
   3. Feature_65_t3: importance=0.0003, rank=3
   4. Feature_67_t0: importance=0.0002, rank=4
   5. Feature_0_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for RHI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RHI...

==================================================
Training Enhanced RHI (SVM)
==================================================
Training SVM model...

Enhanced RHI Performance:
MAE: 621596.4237
RMSE: 775837.5666
MAPE: 8.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t0: importance=0.0007, rank=1
   2. Feature_23_t2: importance=0.0004, rank=2
   3. Feature_19_t0: importance=0.0004, rank=3
   4. Feature_10_t3: importance=0.0003, rank=4
   5. Feature_9_t3: importance=0.0003, rank=5

ðŸ“Š RHI Results:
  Baseline MAPE: 7.66%
  Enhanced MAPE: 8.60%
  MAPE Improvement: -0.94% (-12.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 351/464: RHP
============================================================
ðŸ“Š Loading data for RHP...
ðŸ“Š Loading data for RHP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RHP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RHP...

==================================================
Training Baseline RHP (SVM)
==================================================
Training SVM model...

Baseline RHP Performance:
MAE: 320025.5514
RMSE: 502774.6610
MAPE: 11.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0017, rank=1
   2. Feature_2_t2: importance=0.0013, rank=2
   3. Feature_2_t3: importance=0.0007, rank=3
   4. Feature_2_t1: importance=0.0006, rank=4
   5. Feature_65_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for RHP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RHP...

==================================================
Training Enhanced RHP (SVM)
==================================================
Training SVM model...

Enhanced RHP Performance:
MAE: 358603.4426
RMSE: 547571.5999
MAPE: 12.56%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t3: importance=0.0022, rank=1
   2. Feature_9_t3: importance=0.0012, rank=2
   3. Feature_13_t3: importance=0.0011, rank=3
   4. Feature_19_t3: importance=0.0010, rank=4
   5. Feature_6_t3: importance=0.0009, rank=5

ðŸ“Š RHP Results:
  Baseline MAPE: 11.85%
  Enhanced MAPE: 12.56%
  MAPE Improvement: -0.71% (-6.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 352/464: RNST
============================================================
ðŸ“Š Loading data for RNST...
ðŸ“Š Loading data for RNST from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RNST...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RNST...

==================================================
Training Baseline RNST (SVM)
==================================================
Training SVM model...

Baseline RNST Performance:
MAE: 348784.9029
RMSE: 575057.9849
MAPE: 11.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 149
   â€¢ Highly important features (top 5%): 96

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0010, rank=1
   2. Feature_65_t2: importance=0.0006, rank=2
   3. Feature_65_t3: importance=0.0004, rank=3
   4. Feature_64_t0: importance=0.0004, rank=4
   5. Feature_1_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for RNST...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RNST...

==================================================
Training Enhanced RNST (SVM)
==================================================
Training SVM model...

Enhanced RNST Performance:
MAE: 350858.2583
RMSE: 576076.3778
MAPE: 11.81%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0045, rank=1
   2. Feature_16_t1: importance=0.0012, rank=2
   3. Feature_4_t0: importance=0.0012, rank=3
   4. Feature_22_t0: importance=0.0011, rank=4
   5. Feature_9_t3: importance=0.0010, rank=5

ðŸ“Š RNST Results:
  Baseline MAPE: 11.60%
  Enhanced MAPE: 11.81%
  MAPE Improvement: -0.21% (-1.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 353/464: ROCK
============================================================
ðŸ“Š Loading data for ROCK...
ðŸ“Š Loading data for ROCK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ROCK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ROCK...

==================================================
Training Baseline ROCK (SVM)
==================================================
Training SVM model...

Baseline ROCK Performance:
MAE: 56968.0187
RMSE: 71755.2347
MAPE: 14.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 92
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0014, rank=1
   2. Feature_63_t3: importance=0.0013, rank=2
   3. Feature_67_t1: importance=0.0011, rank=3
   4. Feature_1_t1: importance=0.0011, rank=4
   5. Feature_63_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for ROCK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ROCK...

==================================================
Training Enhanced ROCK (SVM)
==================================================
Training SVM model...

Enhanced ROCK Performance:
MAE: 54407.7841
RMSE: 66269.8295
MAPE: 12.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0030, rank=1
   2. Feature_13_t1: importance=0.0020, rank=2
   3. Feature_23_t1: importance=0.0020, rank=3
   4. Feature_15_t1: importance=0.0011, rank=4
   5. Feature_6_t1: importance=0.0009, rank=5

ðŸ“Š ROCK Results:
  Baseline MAPE: 14.11%
  Enhanced MAPE: 12.84%
  MAPE Improvement: +1.28% (+9.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 354/464: ROG
============================================================
ðŸ“Š Loading data for ROG...
ðŸ“Š Loading data for ROG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for ROG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for ROG...

==================================================
Training Baseline ROG (SVM)
==================================================
Training SVM model...

Baseline ROG Performance:
MAE: 57369.7499
RMSE: 76820.8182
MAPE: 11.28%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 49
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0020, rank=1
   2. Feature_67_t0: importance=0.0019, rank=2
   3. Feature_65_t3: importance=0.0018, rank=3
   4. Feature_65_t1: importance=0.0014, rank=4
   5. Feature_1_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for ROG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for ROG...

==================================================
Training Enhanced ROG (SVM)
==================================================
Training SVM model...

Enhanced ROG Performance:
MAE: 56644.3068
RMSE: 72985.2640
MAPE: 10.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0020, rank=1
   2. Feature_19_t2: importance=0.0016, rank=2
   3. Feature_6_t3: importance=0.0014, rank=3
   4. Feature_4_t3: importance=0.0013, rank=4
   5. Feature_23_t3: importance=0.0013, rank=5

ðŸ“Š ROG Results:
  Baseline MAPE: 11.28%
  Enhanced MAPE: 10.66%
  MAPE Improvement: +0.62% (+5.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 355/464: RUN
============================================================
ðŸ“Š Loading data for RUN...
ðŸ“Š Loading data for RUN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RUN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RUN...

==================================================
Training Baseline RUN (SVM)
==================================================
Training SVM model...

Baseline RUN Performance:
MAE: 3135957.3451
RMSE: 4037234.4447
MAPE: 6.20%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 45
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0004, rank=1
   2. Feature_63_t3: importance=0.0003, rank=2
   3. Feature_67_t2: importance=0.0003, rank=3
   4. Feature_0_t0: importance=0.0002, rank=4
   5. Feature_63_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for RUN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RUN...

==================================================
Training Enhanced RUN (SVM)
==================================================
Training SVM model...

Enhanced RUN Performance:
MAE: 2587787.3381
RMSE: 3320438.0350
MAPE: 5.11%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0001, rank=1
   2. Feature_9_t2: importance=0.0001, rank=2
   3. Feature_10_t3: importance=0.0001, rank=3
   4. Feature_6_t3: importance=0.0001, rank=4
   5. Feature_15_t0: importance=0.0001, rank=5

ðŸ“Š RUN Results:
  Baseline MAPE: 6.20%
  Enhanced MAPE: 5.11%
  MAPE Improvement: +1.09% (+17.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 356/464: RUSHA
============================================================
ðŸ“Š Loading data for RUSHA...
ðŸ“Š Loading data for RUSHA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RUSHA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RUSHA...

==================================================
Training Baseline RUSHA (SVM)
==================================================
Training SVM model...

Baseline RUSHA Performance:
MAE: 267611.3618
RMSE: 333189.4711
MAPE: 8.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 97
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0024, rank=1
   2. Feature_67_t2: importance=0.0016, rank=2
   3. Feature_63_t0: importance=0.0013, rank=3
   4. Feature_63_t1: importance=0.0011, rank=4
   5. Feature_65_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for RUSHA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RUSHA...

==================================================
Training Enhanced RUSHA (SVM)
==================================================
Training SVM model...

Enhanced RUSHA Performance:
MAE: 255339.8337
RMSE: 337390.4784
MAPE: 7.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0017, rank=1
   2. Feature_6_t0: importance=0.0015, rank=2
   3. Feature_19_t2: importance=0.0012, rank=3
   4. Feature_5_t3: importance=0.0012, rank=4
   5. Feature_13_t0: importance=0.0011, rank=5

ðŸ“Š RUSHA Results:
  Baseline MAPE: 8.24%
  Enhanced MAPE: 7.76%
  MAPE Improvement: +0.48% (+5.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 357/464: RWT
============================================================
ðŸ“Š Loading data for RWT...
ðŸ“Š Loading data for RWT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for RWT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for RWT...

==================================================
Training Baseline RWT (SVM)
==================================================
Training SVM model...

Baseline RWT Performance:
MAE: 503322.0893
RMSE: 708108.9609
MAPE: 10.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 32
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0009, rank=1
   2. Feature_67_t2: importance=0.0007, rank=2
   3. Feature_65_t0: importance=0.0006, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_0_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for RWT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for RWT...

==================================================
Training Enhanced RWT (SVM)
==================================================
Training SVM model...

Enhanced RWT Performance:
MAE: 496354.0303
RMSE: 721007.3225
MAPE: 10.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 41
   â€¢ Highly important features (top 5%): 32

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t1: importance=0.0035, rank=1
   2. Feature_11_t2: importance=0.0010, rank=2
   3. Feature_19_t1: importance=0.0010, rank=3
   4. Feature_23_t1: importance=0.0008, rank=4
   5. Feature_11_t1: importance=0.0006, rank=5

ðŸ“Š RWT Results:
  Baseline MAPE: 10.25%
  Enhanced MAPE: 10.40%
  MAPE Improvement: -0.15% (-1.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 358/464: SAFE
============================================================
ðŸ“Š Loading data for SAFE...
ðŸ“Š Loading data for SAFE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing SAFE: 'SAFE'

============================================================
TESTING TICKER 359/464: SABR
============================================================
ðŸ“Š Loading data for SABR...
ðŸ“Š Loading data for SABR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SABR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SABR...

==================================================
Training Baseline SABR (SVM)
==================================================
Training SVM model...

Baseline SABR Performance:
MAE: 1782157.5862
RMSE: 2121990.0621
MAPE: 7.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 215
   â€¢ Highly important features (top 5%): 122

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0005, rank=1
   2. Feature_65_t3: importance=0.0004, rank=2
   3. Feature_2_t1: importance=0.0003, rank=3
   4. Feature_64_t2: importance=0.0003, rank=4
   5. Feature_65_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for SABR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SABR...

==================================================
Training Enhanced SABR (SVM)
==================================================
Training SVM model...

Enhanced SABR Performance:
MAE: 1817230.1439
RMSE: 2214051.4502
MAPE: 7.45%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_14_t3: importance=0.0030, rank=1
   2. Feature_19_t3: importance=0.0029, rank=2
   3. Feature_23_t3: importance=0.0024, rank=3
   4. Feature_11_t3: importance=0.0020, rank=4
   5. Feature_13_t3: importance=0.0018, rank=5

ðŸ“Š SABR Results:
  Baseline MAPE: 7.19%
  Enhanced MAPE: 7.45%
  MAPE Improvement: -0.26% (-3.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 360/464: SAFT
============================================================
ðŸ“Š Loading data for SAFT...
ðŸ“Š Loading data for SAFT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SAFT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SAFT...

==================================================
Training Baseline SAFT (SVM)
==================================================
Training SVM model...

Baseline SAFT Performance:
MAE: 12723.9310
RMSE: 16643.5529
MAPE: 10.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 166
   â€¢ Highly important features (top 5%): 75

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0040, rank=1
   2. Feature_63_t3: importance=0.0012, rank=2
   3. Feature_65_t1: importance=0.0009, rank=3
   4. Feature_0_t2: importance=0.0009, rank=4
   5. Feature_65_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for SAFT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SAFT...

==================================================
Training Enhanced SAFT (SVM)
==================================================
Training SVM model...

Enhanced SAFT Performance:
MAE: 12770.8924
RMSE: 17551.6268
MAPE: 10.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t0: importance=0.0011, rank=1
   2. Feature_6_t3: importance=0.0011, rank=2
   3. Feature_15_t1: importance=0.0011, rank=3
   4. Feature_19_t0: importance=0.0010, rank=4
   5. Feature_23_t1: importance=0.0009, rank=5

ðŸ“Š SAFT Results:
  Baseline MAPE: 10.73%
  Enhanced MAPE: 10.44%
  MAPE Improvement: +0.29% (+2.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 361/464: SAH
============================================================
ðŸ“Š Loading data for SAH...
ðŸ“Š Loading data for SAH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SAH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SAH...

==================================================
Training Baseline SAH (SVM)
==================================================
Training SVM model...

Baseline SAH Performance:
MAE: 70466.5973
RMSE: 85603.9971
MAPE: 4.35%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 30
   â€¢ Highly important features (top 5%): 12

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0001, rank=1
   2. Feature_66_t2: importance=0.0001, rank=2
   3. Feature_67_t3: importance=0.0001, rank=3
   4. Feature_66_t1: importance=0.0001, rank=4
   5. Feature_1_t0: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for SAH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SAH...

==================================================
Training Enhanced SAH (SVM)
==================================================
Training SVM model...

Enhanced SAH Performance:
MAE: 75532.7070
RMSE: 92732.8748
MAPE: 4.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t1: importance=0.0001, rank=1
   2. Feature_5_t2: importance=0.0001, rank=2
   3. Feature_16_t0: importance=0.0000, rank=3
   4. Feature_5_t3: importance=0.0000, rank=4
   5. Feature_1_t3: importance=0.0000, rank=5

ðŸ“Š SAH Results:
  Baseline MAPE: 4.35%
  Enhanced MAPE: 4.73%
  MAPE Improvement: -0.37% (-8.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 362/464: SANM
============================================================
ðŸ“Š Loading data for SANM...
ðŸ“Š Loading data for SANM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SANM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SANM...

==================================================
Training Baseline SANM (SVM)
==================================================
Training SVM model...

Baseline SANM Performance:
MAE: 153139.0900
RMSE: 201447.9043
MAPE: 10.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0010, rank=1
   2. Feature_0_t3: importance=0.0008, rank=2
   3. Feature_0_t0: importance=0.0008, rank=3
   4. Feature_67_t1: importance=0.0007, rank=4
   5. Feature_64_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for SANM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SANM...

==================================================
Training Enhanced SANM (SVM)
==================================================
Training SVM model...

Enhanced SANM Performance:
MAE: 153816.0359
RMSE: 201283.6870
MAPE: 10.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0014, rank=1
   2. Feature_13_t2: importance=0.0011, rank=2
   3. Feature_23_t2: importance=0.0011, rank=3
   4. Feature_16_t2: importance=0.0007, rank=4
   5. Feature_1_t2: importance=0.0007, rank=5

ðŸ“Š SANM Results:
  Baseline MAPE: 10.21%
  Enhanced MAPE: 10.14%
  MAPE Improvement: +0.07% (+0.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 363/464: SBCF
============================================================
ðŸ“Š Loading data for SBCF...
ðŸ“Š Loading data for SBCF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SBCF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SBCF...

==================================================
Training Baseline SBCF (SVM)
==================================================
Training SVM model...

Baseline SBCF Performance:
MAE: 170498.1697
RMSE: 233644.6272
MAPE: 9.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0017, rank=1
   2. Feature_0_t1: importance=0.0016, rank=2
   3. Feature_67_t3: importance=0.0014, rank=3
   4. Feature_2_t3: importance=0.0013, rank=4
   5. Feature_1_t3: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for SBCF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SBCF...

==================================================
Training Enhanced SBCF (SVM)
==================================================
Training SVM model...

Enhanced SBCF Performance:
MAE: 172678.6705
RMSE: 219109.4476
MAPE: 9.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0108, rank=1
   2. Feature_19_t3: importance=0.0108, rank=2
   3. Feature_13_t3: importance=0.0107, rank=3
   4. Feature_6_t3: importance=0.0026, rank=4
   5. Feature_1_t2: importance=0.0020, rank=5

ðŸ“Š SBCF Results:
  Baseline MAPE: 9.49%
  Enhanced MAPE: 9.67%
  MAPE Improvement: -0.18% (-1.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 364/464: SBH
============================================================
ðŸ“Š Loading data for SBH...
ðŸ“Š Loading data for SBH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SBH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SBH...

==================================================
Training Baseline SBH (SVM)
==================================================
Training SVM model...

Baseline SBH Performance:
MAE: 1222813.0976
RMSE: 1698998.7579
MAPE: 9.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0005, rank=1
   2. Feature_65_t0: importance=0.0002, rank=2
   3. Feature_65_t2: importance=0.0002, rank=3
   4. Feature_67_t3: importance=0.0001, rank=4
   5. Feature_63_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for SBH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SBH...

==================================================
Training Enhanced SBH (SVM)
==================================================
Training SVM model...

Enhanced SBH Performance:
MAE: 1175457.6460
RMSE: 1612749.5093
MAPE: 9.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0003, rank=1
   2. Feature_18_t3: importance=0.0002, rank=2
   3. Feature_10_t2: importance=0.0002, rank=3
   4. Feature_10_t3: importance=0.0002, rank=4
   5. Feature_19_t1: importance=0.0002, rank=5

ðŸ“Š SBH Results:
  Baseline MAPE: 9.63%
  Enhanced MAPE: 9.25%
  MAPE Improvement: +0.38% (+4.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 365/464: SBSI
============================================================
ðŸ“Š Loading data for SBSI...
ðŸ“Š Loading data for SBSI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SBSI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SBSI...

==================================================
Training Baseline SBSI (SVM)
==================================================
Training SVM model...

Baseline SBSI Performance:
MAE: 61446.1311
RMSE: 131941.2735
MAPE: 5.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0004, rank=1
   2. Feature_1_t1: importance=0.0002, rank=2
   3. Feature_66_t3: importance=0.0001, rank=3
   4. Feature_65_t2: importance=0.0001, rank=4
   5. Feature_64_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for SBSI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SBSI...

==================================================
Training Enhanced SBSI (SVM)
==================================================
Training SVM model...

Enhanced SBSI Performance:
MAE: 73069.4618
RMSE: 138822.0453
MAPE: 7.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 46

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0005, rank=1
   2. Feature_11_t0: importance=0.0004, rank=2
   3. Feature_22_t0: importance=0.0004, rank=3
   4. Feature_5_t3: importance=0.0003, rank=4
   5. Feature_3_t0: importance=0.0003, rank=5

ðŸ“Š SBSI Results:
  Baseline MAPE: 5.99%
  Enhanced MAPE: 7.26%
  MAPE Improvement: -1.27% (-21.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 366/464: SCHL
============================================================
ðŸ“Š Loading data for SCHL...
ðŸ“Š Loading data for SCHL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SCHL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SCHL...

==================================================
Training Baseline SCHL (SVM)
==================================================
Training SVM model...

Baseline SCHL Performance:
MAE: 139718.7951
RMSE: 195542.2230
MAPE: 14.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0028, rank=1
   2. Feature_1_t1: importance=0.0027, rank=2
   3. Feature_63_t1: importance=0.0018, rank=3
   4. Feature_65_t0: importance=0.0014, rank=4
   5. Feature_2_t3: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for SCHL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SCHL...

==================================================
Training Enhanced SCHL (SVM)
==================================================
Training SVM model...

Enhanced SCHL Performance:
MAE: 130953.6106
RMSE: 183273.6373
MAPE: 13.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0038, rank=1
   2. Feature_17_t0: importance=0.0016, rank=2
   3. Feature_4_t3: importance=0.0014, rank=3
   4. Feature_20_t1: importance=0.0014, rank=4
   5. Feature_1_t0: importance=0.0013, rank=5

ðŸ“Š SCHL Results:
  Baseline MAPE: 14.15%
  Enhanced MAPE: 13.60%
  MAPE Improvement: +0.55% (+3.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 367/464: SCL
============================================================
ðŸ“Š Loading data for SCL...
ðŸ“Š Loading data for SCL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SCL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SCL...

==================================================
Training Baseline SCL (SVM)
==================================================
Training SVM model...

Baseline SCL Performance:
MAE: 32866.0182
RMSE: 48361.9097
MAPE: 11.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0015, rank=1
   2. Feature_65_t0: importance=0.0015, rank=2
   3. Feature_0_t3: importance=0.0015, rank=3
   4. Feature_0_t2: importance=0.0012, rank=4
   5. Feature_64_t1: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for SCL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SCL...

==================================================
Training Enhanced SCL (SVM)
==================================================
Training SVM model...

Enhanced SCL Performance:
MAE: 30728.7548
RMSE: 43167.3112
MAPE: 9.99%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0014, rank=1
   2. Feature_7_t0: importance=0.0012, rank=2
   3. Feature_13_t3: importance=0.0011, rank=3
   4. Feature_6_t3: importance=0.0010, rank=4
   5. Feature_23_t2: importance=0.0010, rank=5

ðŸ“Š SCL Results:
  Baseline MAPE: 11.02%
  Enhanced MAPE: 9.99%
  MAPE Improvement: +1.03% (+9.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 368/464: SCSC
============================================================
ðŸ“Š Loading data for SCSC...
ðŸ“Š Loading data for SCSC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SCSC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SCSC...

==================================================
Training Baseline SCSC (SVM)
==================================================
Training SVM model...

Baseline SCSC Performance:
MAE: 147231.8427
RMSE: 227154.4157
MAPE: 10.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 125
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0019, rank=1
   2. Feature_65_t3: importance=0.0017, rank=2
   3. Feature_64_t0: importance=0.0011, rank=3
   4. Feature_0_t2: importance=0.0010, rank=4
   5. Feature_67_t1: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for SCSC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SCSC...

==================================================
Training Enhanced SCSC (SVM)
==================================================
Training SVM model...

Enhanced SCSC Performance:
MAE: 159948.8188
RMSE: 239199.7833
MAPE: 10.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0022, rank=1
   2. Feature_16_t1: importance=0.0016, rank=2
   3. Feature_11_t0: importance=0.0012, rank=3
   4. Feature_23_t0: importance=0.0010, rank=4
   5. Feature_15_t0: importance=0.0010, rank=5

ðŸ“Š SCSC Results:
  Baseline MAPE: 10.16%
  Enhanced MAPE: 10.94%
  MAPE Improvement: -0.78% (-7.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 369/464: SCVL
============================================================
ðŸ“Š Loading data for SCVL...
ðŸ“Š Loading data for SCVL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SCVL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SCVL...

==================================================
Training Baseline SCVL (SVM)
==================================================
Training SVM model...

Baseline SCVL Performance:
MAE: 233834.6816
RMSE: 306183.6264
MAPE: 7.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 149
   â€¢ Highly important features (top 5%): 88

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0003, rank=1
   2. Feature_1_t3: importance=0.0002, rank=2
   3. Feature_63_t0: importance=0.0002, rank=3
   4. Feature_67_t1: importance=0.0002, rank=4
   5. Feature_64_t3: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for SCVL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SCVL...

==================================================
Training Enhanced SCVL (SVM)
==================================================
Training SVM model...

Enhanced SCVL Performance:
MAE: 147348.2503
RMSE: 212282.6284
MAPE: 5.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 53
   â€¢ Highly important features (top 5%): 35

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0005, rank=1
   2. Feature_6_t0: importance=0.0004, rank=2
   3. Feature_12_t1: importance=0.0002, rank=3
   4. Feature_10_t3: importance=0.0002, rank=4
   5. Feature_3_t2: importance=0.0002, rank=5

ðŸ“Š SCVL Results:
  Baseline MAPE: 7.41%
  Enhanced MAPE: 5.12%
  MAPE Improvement: +2.29% (+30.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 370/464: SEDG
============================================================
ðŸ“Š Loading data for SEDG...
ðŸ“Š Loading data for SEDG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SEDG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SEDG...

==================================================
Training Baseline SEDG (SVM)
==================================================
Training SVM model...

Baseline SEDG Performance:
MAE: 1289755.5031
RMSE: 1645184.7837
MAPE: 8.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0012, rank=1
   2. Feature_2_t1: importance=0.0009, rank=2
   3. Feature_2_t3: importance=0.0007, rank=3
   4. Feature_1_t3: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for SEDG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SEDG...

==================================================
Training Enhanced SEDG (SVM)
==================================================
Training SVM model...

Enhanced SEDG Performance:
MAE: 1030077.5949
RMSE: 1409276.4635
MAPE: 6.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0012, rank=1
   2. Feature_1_t3: importance=0.0005, rank=2
   3. Feature_9_t1: importance=0.0005, rank=3
   4. Feature_15_t1: importance=0.0005, rank=4
   5. Feature_10_t2: importance=0.0005, rank=5

ðŸ“Š SEDG Results:
  Baseline MAPE: 8.07%
  Enhanced MAPE: 6.92%
  MAPE Improvement: +1.16% (+14.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 371/464: SEE
============================================================
ðŸ“Š Loading data for SEE...
ðŸ“Š Loading data for SEE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SEE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SEE...

==================================================
Training Baseline SEE (SVM)
==================================================
Training SVM model...

Baseline SEE Performance:
MAE: 628883.8881
RMSE: 802224.6411
MAPE: 10.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 13

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0010, rank=1
   2. Feature_65_t1: importance=0.0009, rank=2
   3. Feature_67_t0: importance=0.0006, rank=3
   4. Feature_1_t1: importance=0.0005, rank=4
   5. Feature_1_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for SEE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SEE...

==================================================
Training Enhanced SEE (SVM)
==================================================
Training SVM model...

Enhanced SEE Performance:
MAE: 522581.7343
RMSE: 662697.1472
MAPE: 9.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t3: importance=0.0009, rank=1
   2. Feature_4_t2: importance=0.0006, rank=2
   3. Feature_4_t1: importance=0.0005, rank=3
   4. Feature_16_t0: importance=0.0005, rank=4
   5. Feature_9_t3: importance=0.0004, rank=5

ðŸ“Š SEE Results:
  Baseline MAPE: 10.85%
  Enhanced MAPE: 9.18%
  MAPE Improvement: +1.67% (+15.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 372/464: SEM
============================================================
ðŸ“Š Loading data for SEM...
ðŸ“Š Loading data for SEM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SEM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SEM...

==================================================
Training Baseline SEM (SVM)
==================================================
Training SVM model...

Baseline SEM Performance:
MAE: 287759.3391
RMSE: 381153.4314
MAPE: 9.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 120
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0012, rank=1
   2. Feature_65_t3: importance=0.0005, rank=2
   3. Feature_1_t0: importance=0.0005, rank=3
   4. Feature_63_t1: importance=0.0005, rank=4
   5. Feature_1_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for SEM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SEM...

==================================================
Training Enhanced SEM (SVM)
==================================================
Training SVM model...

Enhanced SEM Performance:
MAE: 257434.6569
RMSE: 365405.7775
MAPE: 8.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t0: importance=0.0009, rank=1
   2. Feature_1_t2: importance=0.0006, rank=2
   3. Feature_22_t3: importance=0.0005, rank=3
   4. Feature_18_t3: importance=0.0005, rank=4
   5. Feature_15_t0: importance=0.0004, rank=5

ðŸ“Š SEM Results:
  Baseline MAPE: 9.74%
  Enhanced MAPE: 8.75%
  MAPE Improvement: +0.99% (+10.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 373/464: SFBS
============================================================
ðŸ“Š Loading data for SFBS...
ðŸ“Š Loading data for SFBS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SFBS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SFBS...

==================================================
Training Baseline SFBS (SVM)
==================================================
Training SVM model...

Baseline SFBS Performance:
MAE: 248220.1722
RMSE: 288369.6997
MAPE: 6.65%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0002, rank=1
   2. Feature_64_t1: importance=0.0002, rank=2
   3. Feature_65_t3: importance=0.0001, rank=3
   4. Feature_63_t1: importance=0.0001, rank=4
   5. Feature_64_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for SFBS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SFBS...

==================================================
Training Enhanced SFBS (SVM)
==================================================
Training SVM model...

Enhanced SFBS Performance:
MAE: 110114.0995
RMSE: 144731.4919
MAPE: 2.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 56
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t2: importance=0.0002, rank=1
   2. Feature_9_t3: importance=0.0002, rank=2
   3. Feature_12_t2: importance=0.0002, rank=3
   4. Feature_15_t3: importance=0.0001, rank=4
   5. Feature_18_t3: importance=0.0001, rank=5

ðŸ“Š SFBS Results:
  Baseline MAPE: 6.65%
  Enhanced MAPE: 2.98%
  MAPE Improvement: +3.67% (+55.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 374/464: SFNC
============================================================
ðŸ“Š Loading data for SFNC...
ðŸ“Š Loading data for SFNC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SFNC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SFNC...

==================================================
Training Baseline SFNC (SVM)
==================================================
Training SVM model...

Baseline SFNC Performance:
MAE: 198768.4749
RMSE: 290243.8518
MAPE: 7.44%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0010, rank=1
   2. Feature_1_t1: importance=0.0008, rank=2
   3. Feature_2_t1: importance=0.0008, rank=3
   4. Feature_67_t3: importance=0.0006, rank=4
   5. Feature_0_t2: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for SFNC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SFNC...

==================================================
Training Enhanced SFNC (SVM)
==================================================
Training SVM model...

Enhanced SFNC Performance:
MAE: 210237.9960
RMSE: 312475.5767
MAPE: 7.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 47

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0015, rank=1
   2. Feature_21_t3: importance=0.0011, rank=2
   3. Feature_18_t3: importance=0.0007, rank=3
   4. Feature_3_t3: importance=0.0006, rank=4
   5. Feature_10_t3: importance=0.0006, rank=5

ðŸ“Š SFNC Results:
  Baseline MAPE: 7.44%
  Enhanced MAPE: 7.98%
  MAPE Improvement: -0.54% (-7.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 375/464: SHAK
============================================================
ðŸ“Š Loading data for SHAK...
ðŸ“Š Loading data for SHAK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SHAK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SHAK...

==================================================
Training Baseline SHAK (SVM)
==================================================
Training SVM model...

Baseline SHAK Performance:
MAE: 243330.8323
RMSE: 320098.1354
MAPE: 5.85%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0003, rank=1
   2. Feature_0_t2: importance=0.0002, rank=2
   3. Feature_2_t3: importance=0.0002, rank=3
   4. Feature_1_t3: importance=0.0002, rank=4
   5. Feature_63_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for SHAK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SHAK...

==================================================
Training Enhanced SHAK (SVM)
==================================================
Training SVM model...

Enhanced SHAK Performance:
MAE: 249936.8414
RMSE: 291199.8882
MAPE: 6.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t0: importance=0.0004, rank=1
   2. Feature_20_t0: importance=0.0003, rank=2
   3. Feature_9_t1: importance=0.0003, rank=3
   4. Feature_15_t2: importance=0.0002, rank=4
   5. Feature_6_t0: importance=0.0002, rank=5

ðŸ“Š SHAK Results:
  Baseline MAPE: 5.85%
  Enhanced MAPE: 6.21%
  MAPE Improvement: -0.36% (-6.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 376/464: SHEN
============================================================
ðŸ“Š Loading data for SHEN...
ðŸ“Š Loading data for SHEN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SHEN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SHEN...

==================================================
Training Baseline SHEN (SVM)
==================================================
Training SVM model...

Baseline SHEN Performance:
MAE: 99516.7477
RMSE: 119604.4808
MAPE: 7.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 54
   â€¢ Highly important features (top 5%): 20

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0014, rank=1
   2. Feature_0_t3: importance=0.0008, rank=2
   3. Feature_0_t2: importance=0.0007, rank=3
   4. Feature_2_t0: importance=0.0007, rank=4
   5. Feature_67_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for SHEN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SHEN...

==================================================
Training Enhanced SHEN (SVM)
==================================================
Training SVM model...

Enhanced SHEN Performance:
MAE: 102523.1333
RMSE: 122027.5190
MAPE: 7.46%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 48
   â€¢ Highly important features (top 5%): 25

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t2: importance=0.0027, rank=1
   2. Feature_12_t0: importance=0.0025, rank=2
   3. Feature_6_t0: importance=0.0016, rank=3
   4. Feature_8_t3: importance=0.0014, rank=4
   5. Feature_12_t1: importance=0.0011, rank=5

ðŸ“Š SHEN Results:
  Baseline MAPE: 7.14%
  Enhanced MAPE: 7.46%
  MAPE Improvement: -0.31% (-4.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 377/464: SHO
============================================================
ðŸ“Š Loading data for SHO...
ðŸ“Š Loading data for SHO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SHO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SHO...

==================================================
Training Baseline SHO (SVM)
==================================================
Training SVM model...

Baseline SHO Performance:
MAE: 1360537.1510
RMSE: 1819386.6176
MAPE: 10.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 98
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t1: importance=0.0013, rank=1
   2. Feature_67_t2: importance=0.0010, rank=2
   3. Feature_63_t0: importance=0.0008, rank=3
   4. Feature_67_t1: importance=0.0008, rank=4
   5. Feature_0_t3: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for SHO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SHO...

==================================================
Training Enhanced SHO (SVM)
==================================================
Training SVM model...

Enhanced SHO Performance:
MAE: 1351378.1556
RMSE: 1759703.7959
MAPE: 10.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0013, rank=1
   2. Feature_19_t3: importance=0.0011, rank=2
   3. Feature_23_t2: importance=0.0010, rank=3
   4. Feature_13_t3: importance=0.0009, rank=4
   5. Feature_19_t2: importance=0.0009, rank=5

ðŸ“Š SHO Results:
  Baseline MAPE: 10.51%
  Enhanced MAPE: 10.58%
  MAPE Improvement: -0.07% (-0.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 378/464: SHOO
============================================================
ðŸ“Š Loading data for SHOO...
ðŸ“Š Loading data for SHOO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SHOO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SHOO...

==================================================
Training Baseline SHOO (SVM)
==================================================
Training SVM model...

Baseline SHOO Performance:
MAE: 439299.4208
RMSE: 592002.5533
MAPE: 12.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 140
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0008, rank=1
   2. Feature_2_t2: importance=0.0008, rank=2
   3. Feature_1_t2: importance=0.0007, rank=3
   4. Feature_6_t3: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for SHOO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SHOO...

==================================================
Training Enhanced SHOO (SVM)
==================================================
Training SVM model...

Enhanced SHOO Performance:
MAE: 376527.9507
RMSE: 537896.2306
MAPE: 10.66%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 79

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0009, rank=1
   2. Feature_20_t3: importance=0.0007, rank=2
   3. Feature_12_t1: importance=0.0006, rank=3
   4. Feature_7_t2: importance=0.0006, rank=4
   5. Feature_9_t1: importance=0.0006, rank=5

ðŸ“Š SHOO Results:
  Baseline MAPE: 12.07%
  Enhanced MAPE: 10.66%
  MAPE Improvement: +1.40% (+11.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 379/464: SIG
============================================================
ðŸ“Š Loading data for SIG...
ðŸ“Š Loading data for SIG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SIG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SIG...

==================================================
Training Baseline SIG (SVM)
==================================================
Training SVM model...

Baseline SIG Performance:
MAE: 351823.5810
RMSE: 444918.1186
MAPE: 7.37%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 23
   â€¢ Highly important features (top 5%): 14

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0007, rank=1
   2. Feature_2_t0: importance=0.0006, rank=2
   3. Feature_65_t0: importance=0.0005, rank=3
   4. Feature_63_t0: importance=0.0004, rank=4
   5. Feature_0_t2: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for SIG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SIG...

==================================================
Training Enhanced SIG (SVM)
==================================================
Training SVM model...

Enhanced SIG Performance:
MAE: 299242.7385
RMSE: 385226.9060
MAPE: 6.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 60

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0008, rank=1
   2. Feature_18_t3: importance=0.0005, rank=2
   3. Feature_21_t3: importance=0.0004, rank=3
   4. Feature_23_t2: importance=0.0002, rank=4
   5. Feature_22_t3: importance=0.0002, rank=5

ðŸ“Š SIG Results:
  Baseline MAPE: 7.37%
  Enhanced MAPE: 6.09%
  MAPE Improvement: +1.28% (+17.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 380/464: SKT
============================================================
ðŸ“Š Loading data for SKT...
ðŸ“Š Loading data for SKT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing SKT: 'SKT'

============================================================
TESTING TICKER 381/464: SKY
============================================================
ðŸ“Š Loading data for SKY...
ðŸ“Š Loading data for SKY from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing SKY: 'SKY'

============================================================
TESTING TICKER 382/464: SKYW
============================================================
ðŸ“Š Loading data for SKYW...
ðŸ“Š Loading data for SKYW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SKYW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SKYW...

==================================================
Training Baseline SKYW (SVM)
==================================================
Training SVM model...

Baseline SKYW Performance:
MAE: 166031.6187
RMSE: 199933.1884
MAPE: 10.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 130
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0006, rank=1
   2. Feature_64_t1: importance=0.0006, rank=2
   3. Feature_2_t3: importance=0.0006, rank=3
   4. Feature_67_t3: importance=0.0005, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for SKYW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SKYW...

==================================================
Training Enhanced SKYW (SVM)
==================================================
Training SVM model...

Enhanced SKYW Performance:
MAE: 155142.2885
RMSE: 194866.7251
MAPE: 10.41%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 82

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0007, rank=1
   2. Feature_11_t2: importance=0.0007, rank=2
   3. Feature_4_t0: importance=0.0006, rank=3
   4. Feature_2_t2: importance=0.0005, rank=4
   5. Feature_8_t0: importance=0.0005, rank=5

ðŸ“Š SKYW Results:
  Baseline MAPE: 10.98%
  Enhanced MAPE: 10.41%
  MAPE Improvement: +0.57% (+5.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 383/464: SLG
============================================================
ðŸ“Š Loading data for SLG...
ðŸ“Š Loading data for SLG from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SLG...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SLG...

==================================================
Training Baseline SLG (SVM)
==================================================
Training SVM model...

Baseline SLG Performance:
MAE: 472600.7003
RMSE: 583918.6526
MAPE: 5.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 123
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0003, rank=1
   2. Feature_65_t2: importance=0.0002, rank=2
   3. Feature_64_t0: importance=0.0002, rank=3
   4. Feature_2_t2: importance=0.0002, rank=4
   5. Feature_67_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for SLG...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SLG...

==================================================
Training Enhanced SLG (SVM)
==================================================
Training SVM model...

Enhanced SLG Performance:
MAE: 509162.2362
RMSE: 628730.4929
MAPE: 5.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_12_t0: importance=0.0004, rank=1
   2. Feature_16_t2: importance=0.0004, rank=2
   3. Feature_6_t2: importance=0.0002, rank=3
   4. Feature_20_t1: importance=0.0002, rank=4
   5. Feature_16_t0: importance=0.0002, rank=5

ðŸ“Š SLG Results:
  Baseline MAPE: 5.69%
  Enhanced MAPE: 5.83%
  MAPE Improvement: -0.14% (-2.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 384/464: SM
============================================================
ðŸ“Š Loading data for SM...
ðŸ“Š Loading data for SM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SM...

==================================================
Training Baseline SM (SVM)
==================================================
Training SVM model...

Baseline SM Performance:
MAE: 824827.0540
RMSE: 1065147.6464
MAPE: 8.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 156
   â€¢ Highly important features (top 5%): 85

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0009, rank=1
   2. Feature_1_t3: importance=0.0006, rank=2
   3. Feature_67_t3: importance=0.0006, rank=3
   4. Feature_67_t0: importance=0.0004, rank=4
   5. Feature_2_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for SM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SM...

==================================================
Training Enhanced SM (SVM)
==================================================
Training SVM model...

Enhanced SM Performance:
MAE: 848597.4927
RMSE: 1023067.2446
MAPE: 8.24%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t0: importance=0.0009, rank=1
   2. Feature_14_t2: importance=0.0008, rank=2
   3. Feature_15_t1: importance=0.0008, rank=3
   4. Feature_9_t3: importance=0.0006, rank=4
   5. Feature_11_t3: importance=0.0006, rank=5

ðŸ“Š SM Results:
  Baseline MAPE: 8.05%
  Enhanced MAPE: 8.24%
  MAPE Improvement: -0.19% (-2.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 385/464: SMP
============================================================
ðŸ“Š Loading data for SMP...
ðŸ“Š Loading data for SMP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SMP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SMP...

==================================================
Training Baseline SMP (SVM)
==================================================
Training SVM model...

Baseline SMP Performance:
MAE: 126366.3049
RMSE: 167594.1285
MAPE: 22.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 28

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0015, rank=1
   2. Feature_64_t0: importance=0.0015, rank=2
   3. Feature_1_t0: importance=0.0010, rank=3
   4. Feature_2_t3: importance=0.0008, rank=4
   5. Feature_65_t3: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for SMP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SMP...

==================================================
Training Enhanced SMP (SVM)
==================================================
Training SVM model...

Enhanced SMP Performance:
MAE: 117735.2361
RMSE: 155707.1172
MAPE: 19.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 89
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t2: importance=0.0015, rank=1
   2. Feature_24_t3: importance=0.0012, rank=2
   3. Feature_4_t1: importance=0.0012, rank=3
   4. Feature_0_t3: importance=0.0010, rank=4
   5. Feature_11_t3: importance=0.0010, rank=5

ðŸ“Š SMP Results:
  Baseline MAPE: 22.83%
  Enhanced MAPE: 19.75%
  MAPE Improvement: +3.08% (+13.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 386/464: SMPL
============================================================
ðŸ“Š Loading data for SMPL...
ðŸ“Š Loading data for SMPL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing SMPL: 'SMPL'

============================================================
TESTING TICKER 387/464: SMTC
============================================================
ðŸ“Š Loading data for SMTC...
ðŸ“Š Loading data for SMTC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SMTC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SMTC...

==================================================
Training Baseline SMTC (SVM)
==================================================
Training SVM model...

Baseline SMTC Performance:
MAE: 976368.5658
RMSE: 1586664.9172
MAPE: 11.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 122
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t2: importance=0.0023, rank=1
   2. Feature_63_t3: importance=0.0017, rank=2
   3. Feature_65_t0: importance=0.0013, rank=3
   4. Feature_65_t3: importance=0.0011, rank=4
   5. Feature_65_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for SMTC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SMTC...

==================================================
Training Enhanced SMTC (SVM)
==================================================
Training SVM model...

Enhanced SMTC Performance:
MAE: 1031162.4707
RMSE: 1482636.2187
MAPE: 12.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0015, rank=1
   2. Feature_24_t1: importance=0.0015, rank=2
   3. Feature_11_t2: importance=0.0012, rank=3
   4. Feature_19_t1: importance=0.0012, rank=4
   5. Feature_20_t2: importance=0.0010, rank=5

ðŸ“Š SMTC Results:
  Baseline MAPE: 11.19%
  Enhanced MAPE: 12.02%
  MAPE Improvement: -0.83% (-7.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 388/464: SNDR
============================================================
ðŸ“Š Loading data for SNDR...
ðŸ“Š Loading data for SNDR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing SNDR: 'SNDR'

============================================================
TESTING TICKER 389/464: SPSC
============================================================
ðŸ“Š Loading data for SPSC...
ðŸ“Š Loading data for SPSC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SPSC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SPSC...

==================================================
Training Baseline SPSC (SVM)
==================================================
Training SVM model...

Baseline SPSC Performance:
MAE: 109472.3715
RMSE: 148191.3171
MAPE: 11.54%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0018, rank=1
   2. Feature_63_t1: importance=0.0018, rank=2
   3. Feature_67_t3: importance=0.0017, rank=3
   4. Feature_0_t3: importance=0.0013, rank=4
   5. Feature_1_t1: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for SPSC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SPSC...

==================================================
Training Enhanced SPSC (SVM)
==================================================
Training SVM model...

Enhanced SPSC Performance:
MAE: 106500.9555
RMSE: 137497.7198
MAPE: 11.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t3: importance=0.0019, rank=1
   2. Feature_11_t3: importance=0.0019, rank=2
   3. Feature_22_t3: importance=0.0010, rank=3
   4. Feature_20_t2: importance=0.0009, rank=4
   5. Feature_17_t3: importance=0.0009, rank=5

ðŸ“Š SPSC Results:
  Baseline MAPE: 11.54%
  Enhanced MAPE: 11.98%
  MAPE Improvement: -0.44% (-3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 390/464: SPXC
============================================================
ðŸ“Š Loading data for SPXC...
ðŸ“Š Loading data for SPXC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SPXC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SPXC...

==================================================
Training Baseline SPXC (SVM)
==================================================
Training SVM model...

Baseline SPXC Performance:
MAE: 65611.6800
RMSE: 82001.6163
MAPE: 10.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 116
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t1: importance=0.0010, rank=1
   2. Feature_67_t3: importance=0.0007, rank=2
   3. Feature_0_t3: importance=0.0007, rank=3
   4. Feature_65_t1: importance=0.0007, rank=4
   5. Feature_63_t3: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for SPXC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SPXC...

==================================================
Training Enhanced SPXC (SVM)
==================================================
Training SVM model...

Enhanced SPXC Performance:
MAE: 63032.6922
RMSE: 79930.2208
MAPE: 9.67%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0011, rank=1
   2. Feature_12_t3: importance=0.0010, rank=2
   3. Feature_19_t3: importance=0.0009, rank=3
   4. Feature_11_t2: importance=0.0008, rank=4
   5. Feature_6_t1: importance=0.0007, rank=5

ðŸ“Š SPXC Results:
  Baseline MAPE: 10.26%
  Enhanced MAPE: 9.67%
  MAPE Improvement: +0.59% (+5.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 391/464: SRPT
============================================================
ðŸ“Š Loading data for SRPT...
ðŸ“Š Loading data for SRPT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SRPT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SRPT...

==================================================
Training Baseline SRPT (SVM)
==================================================
Training SVM model...

Baseline SRPT Performance:
MAE: 663654.1963
RMSE: 1132887.0055
MAPE: 8.84%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_66_t1: importance=0.0006, rank=1
   2. Feature_63_t3: importance=0.0002, rank=2
   3. Feature_67_t1: importance=0.0002, rank=3
   4. Feature_65_t3: importance=0.0002, rank=4
   5. Feature_63_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for SRPT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SRPT...

==================================================
Training Enhanced SRPT (SVM)
==================================================
Training SVM model...

Enhanced SRPT Performance:
MAE: 653741.5311
RMSE: 1154308.5246
MAPE: 8.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 41

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t3: importance=0.0005, rank=1
   2. Feature_5_t1: importance=0.0003, rank=2
   3. Feature_24_t1: importance=0.0002, rank=3
   4. Feature_5_t3: importance=0.0002, rank=4
   5. Feature_19_t2: importance=0.0001, rank=5

ðŸ“Š SRPT Results:
  Baseline MAPE: 8.84%
  Enhanced MAPE: 8.51%
  MAPE Improvement: +0.33% (+3.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 392/464: SSTK
============================================================
ðŸ“Š Loading data for SSTK...
ðŸ“Š Loading data for SSTK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SSTK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SSTK...

==================================================
Training Baseline SSTK (SVM)
==================================================
Training SVM model...

Baseline SSTK Performance:
MAE: 305796.9911
RMSE: 351519.4005
MAPE: 8.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 30

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0006, rank=1
   2. Feature_2_t0: importance=0.0003, rank=2
   3. Feature_66_t2: importance=0.0003, rank=3
   4. Feature_2_t2: importance=0.0002, rank=4
   5. Feature_66_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for SSTK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SSTK...

==================================================
Training Enhanced SSTK (SVM)
==================================================
Training SVM model...

Enhanced SSTK Performance:
MAE: 193491.0614
RMSE: 235802.3935
MAPE: 5.53%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0003, rank=1
   2. Feature_13_t1: importance=0.0003, rank=2
   3. Feature_19_t3: importance=0.0003, rank=3
   4. Feature_11_t3: importance=0.0003, rank=4
   5. Feature_5_t0: importance=0.0003, rank=5

ðŸ“Š SSTK Results:
  Baseline MAPE: 8.71%
  Enhanced MAPE: 5.53%
  MAPE Improvement: +3.18% (+36.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 393/464: STAA
============================================================
ðŸ“Š Loading data for STAA...
ðŸ“Š Loading data for STAA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for STAA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for STAA...

==================================================
Training Baseline STAA (SVM)
==================================================
Training SVM model...

Baseline STAA Performance:
MAE: 352878.2400
RMSE: 470512.4481
MAPE: 7.71%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0009, rank=1
   2. Feature_67_t3: importance=0.0009, rank=2
   3. Feature_65_t3: importance=0.0005, rank=3
   4. Feature_1_t1: importance=0.0005, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for STAA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for STAA...

==================================================
Training Enhanced STAA (SVM)
==================================================
Training SVM model...

Enhanced STAA Performance:
MAE: 386989.0890
RMSE: 489329.7494
MAPE: 8.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 70

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0007, rank=1
   2. Feature_6_t3: importance=0.0007, rank=2
   3. Feature_10_t2: importance=0.0005, rank=3
   4. Feature_19_t3: importance=0.0005, rank=4
   5. Feature_1_t2: importance=0.0005, rank=5

ðŸ“Š STAA Results:
  Baseline MAPE: 7.71%
  Enhanced MAPE: 8.63%
  MAPE Improvement: -0.93% (-12.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 394/464: STBA
============================================================
ðŸ“Š Loading data for STBA...
ðŸ“Š Loading data for STBA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for STBA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for STBA...

==================================================
Training Baseline STBA (SVM)
==================================================
Training SVM model...

Baseline STBA Performance:
MAE: 46720.9547
RMSE: 59638.6578
MAPE: 5.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 42

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0011, rank=1
   2. Feature_1_t2: importance=0.0007, rank=2
   3. Feature_64_t1: importance=0.0003, rank=3
   4. Feature_1_t0: importance=0.0002, rank=4
   5. Feature_1_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for STBA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for STBA...

==================================================
Training Enhanced STBA (SVM)
==================================================
Training SVM model...

Enhanced STBA Performance:
MAE: 47986.0349
RMSE: 64051.4035
MAPE: 5.82%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 70
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0006, rank=1
   2. Feature_15_t2: importance=0.0005, rank=2
   3. Feature_22_t3: importance=0.0004, rank=3
   4. Feature_4_t1: importance=0.0004, rank=4
   5. Feature_1_t2: importance=0.0004, rank=5

ðŸ“Š STBA Results:
  Baseline MAPE: 5.58%
  Enhanced MAPE: 5.82%
  MAPE Improvement: -0.24% (-4.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 395/464: STC
============================================================
ðŸ“Š Loading data for STC...
ðŸ“Š Loading data for STC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for STC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for STC...

==================================================
Training Baseline STC (SVM)
==================================================
Training SVM model...

Baseline STC Performance:
MAE: 125961.1264
RMSE: 198823.2226
MAPE: 19.21%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 174
   â€¢ Highly important features (top 5%): 95

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0023, rank=1
   2. Feature_65_t1: importance=0.0012, rank=2
   3. Feature_1_t1: importance=0.0012, rank=3
   4. Feature_67_t1: importance=0.0012, rank=4
   5. Feature_64_t2: importance=0.0011, rank=5

ðŸ”§ Applying universal feature engineering for STC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for STC...

==================================================
Training Enhanced STC (SVM)
==================================================
Training SVM model...

Enhanced STC Performance:
MAE: 121969.1536
RMSE: 201451.5388
MAPE: 18.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0032, rank=1
   2. Feature_15_t2: importance=0.0031, rank=2
   3. Feature_18_t3: importance=0.0021, rank=3
   4. Feature_4_t0: importance=0.0021, rank=4
   5. Feature_10_t2: importance=0.0018, rank=5

ðŸ“Š STC Results:
  Baseline MAPE: 19.21%
  Enhanced MAPE: 18.93%
  MAPE Improvement: +0.29% (+1.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 396/464: STRA
============================================================
ðŸ“Š Loading data for STRA...
ðŸ“Š Loading data for STRA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for STRA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for STRA...

==================================================
Training Baseline STRA (SVM)
==================================================
Training SVM model...

Baseline STRA Performance:
MAE: 50541.8190
RMSE: 66041.9173
MAPE: 11.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 152
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0013, rank=1
   2. Feature_1_t1: importance=0.0005, rank=2
   3. Feature_2_t2: importance=0.0004, rank=3
   4. Feature_67_t2: importance=0.0004, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for STRA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for STRA...

==================================================
Training Enhanced STRA (SVM)
==================================================
Training SVM model...

Enhanced STRA Performance:
MAE: 54368.0237
RMSE: 70322.9062
MAPE: 13.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 58
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_7_t2: importance=0.0054, rank=1
   2. Feature_23_t3: importance=0.0014, rank=2
   3. Feature_13_t3: importance=0.0014, rank=3
   4. Feature_12_t2: importance=0.0013, rank=4
   5. Feature_19_t3: importance=0.0011, rank=5

ðŸ“Š STRA Results:
  Baseline MAPE: 11.87%
  Enhanced MAPE: 13.00%
  MAPE Improvement: -1.13% (-9.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 397/464: STRL
============================================================
ðŸ“Š Loading data for STRL...
ðŸ“Š Loading data for STRL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for STRL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for STRL...

==================================================
Training Baseline STRL (SVM)
==================================================
Training SVM model...

Baseline STRL Performance:
MAE: 199891.2559
RMSE: 293456.3769
MAPE: 11.98%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 196
   â€¢ Highly important features (top 5%): 86

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0014, rank=1
   2. Feature_1_t2: importance=0.0006, rank=2
   3. Feature_0_t2: importance=0.0006, rank=3
   4. Feature_64_t0: importance=0.0006, rank=4
   5. Feature_65_t0: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for STRL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for STRL...

==================================================
Training Enhanced STRL (SVM)
==================================================
Training SVM model...

Enhanced STRL Performance:
MAE: 144918.5498
RMSE: 248553.5593
MAPE: 8.06%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0016, rank=1
   2. Feature_15_t3: importance=0.0010, rank=2
   3. Feature_6_t2: importance=0.0009, rank=3
   4. Feature_23_t2: importance=0.0009, rank=4
   5. Feature_20_t1: importance=0.0008, rank=5

ðŸ“Š STRL Results:
  Baseline MAPE: 11.98%
  Enhanced MAPE: 8.06%
  MAPE Improvement: +3.92% (+32.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 398/464: SUPN
============================================================
ðŸ“Š Loading data for SUPN...
ðŸ“Š Loading data for SUPN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SUPN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SUPN...

==================================================
Training Baseline SUPN (SVM)
==================================================
Training SVM model...

Baseline SUPN Performance:
MAE: 319815.5569
RMSE: 395101.8517
MAPE: 6.12%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0002, rank=1
   2. Feature_63_t1: importance=0.0001, rank=2
   3. Feature_65_t2: importance=0.0001, rank=3
   4. Feature_66_t3: importance=0.0001, rank=4
   5. Feature_66_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for SUPN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SUPN...

==================================================
Training Enhanced SUPN (SVM)
==================================================
Training SVM model...

Enhanced SUPN Performance:
MAE: 305386.4278
RMSE: 390415.5697
MAPE: 5.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0002, rank=1
   2. Feature_22_t2: importance=0.0002, rank=2
   3. Feature_5_t2: importance=0.0001, rank=3
   4. Feature_1_t2: importance=0.0001, rank=4
   5. Feature_12_t2: importance=0.0001, rank=5

ðŸ“Š SUPN Results:
  Baseline MAPE: 6.12%
  Enhanced MAPE: 5.92%
  MAPE Improvement: +0.21% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 399/464: SXC
============================================================
ðŸ“Š Loading data for SXC...
ðŸ“Š Loading data for SXC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SXC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SXC...

==================================================
Training Baseline SXC (SVM)
==================================================
Training SVM model...

Baseline SXC Performance:
MAE: 578532.1868
RMSE: 816214.1851
MAPE: 17.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 116
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t3: importance=0.0019, rank=1
   2. Feature_2_t2: importance=0.0016, rank=2
   3. Feature_2_t0: importance=0.0015, rank=3
   4. Feature_1_t3: importance=0.0013, rank=4
   5. Feature_0_t3: importance=0.0012, rank=5

ðŸ”§ Applying universal feature engineering for SXC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SXC...

==================================================
Training Enhanced SXC (SVM)
==================================================
Training SVM model...

Enhanced SXC Performance:
MAE: 496938.0133
RMSE: 754053.5870
MAPE: 13.93%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 79

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t1: importance=0.0028, rank=1
   2. Feature_9_t3: importance=0.0024, rank=2
   3. Feature_20_t2: importance=0.0024, rank=3
   4. Feature_8_t2: importance=0.0022, rank=4
   5. Feature_7_t2: importance=0.0018, rank=5

ðŸ“Š SXC Results:
  Baseline MAPE: 17.14%
  Enhanced MAPE: 13.93%
  MAPE Improvement: +3.21% (+18.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 400/464: SXI
============================================================
ðŸ“Š Loading data for SXI...
ðŸ“Š Loading data for SXI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SXI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SXI...

==================================================
Training Baseline SXI (SVM)
==================================================
Training SVM model...

Baseline SXI Performance:
MAE: 24967.3879
RMSE: 30692.2652
MAPE: 11.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 107
   â€¢ Highly important features (top 5%): 65

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0032, rank=1
   2. Feature_2_t2: importance=0.0032, rank=2
   3. Feature_65_t1: importance=0.0025, rank=3
   4. Feature_0_t3: importance=0.0023, rank=4
   5. Feature_67_t2: importance=0.0022, rank=5

ðŸ”§ Applying universal feature engineering for SXI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SXI...

==================================================
Training Enhanced SXI (SVM)
==================================================
Training SVM model...

Enhanced SXI Performance:
MAE: 36364.1642
RMSE: 41672.8573
MAPE: 16.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 89

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t2: importance=0.0014, rank=1
   2. Feature_12_t3: importance=0.0014, rank=2
   3. Feature_15_t3: importance=0.0014, rank=3
   4. Feature_24_t3: importance=0.0013, rank=4
   5. Feature_7_t3: importance=0.0012, rank=5

ðŸ“Š SXI Results:
  Baseline MAPE: 11.13%
  Enhanced MAPE: 16.33%
  MAPE Improvement: -5.21% (-46.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 401/464: SXT
============================================================
ðŸ“Š Loading data for SXT...
ðŸ“Š Loading data for SXT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for SXT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for SXT...

==================================================
Training Baseline SXT (SVM)
==================================================
Training SVM model...

Baseline SXT Performance:
MAE: 104583.2455
RMSE: 169960.2105
MAPE: 12.34%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 159
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0009, rank=1
   2. Feature_63_t0: importance=0.0009, rank=2
   3. Feature_65_t1: importance=0.0007, rank=3
   4. Feature_1_t1: importance=0.0006, rank=4
   5. Feature_64_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for SXT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for SXT...

==================================================
Training Enhanced SXT (SVM)
==================================================
Training SVM model...

Enhanced SXT Performance:
MAE: 85131.9899
RMSE: 130586.8100
MAPE: 11.04%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 83

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t3: importance=0.0008, rank=1
   2. Feature_14_t2: importance=0.0008, rank=2
   3. Feature_4_t3: importance=0.0007, rank=3
   4. Feature_20_t1: importance=0.0007, rank=4
   5. Feature_18_t1: importance=0.0007, rank=5

ðŸ“Š SXT Results:
  Baseline MAPE: 12.34%
  Enhanced MAPE: 11.04%
  MAPE Improvement: +1.30% (+10.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 402/464: TBBK
============================================================
ðŸ“Š Loading data for TBBK...
ðŸ“Š Loading data for TBBK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TBBK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TBBK...

==================================================
Training Baseline TBBK (SVM)
==================================================
Training SVM model...

Baseline TBBK Performance:
MAE: 278248.8176
RMSE: 363439.1532
MAPE: 4.80%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0024, rank=1
   2. Feature_65_t1: importance=0.0011, rank=2
   3. Feature_2_t2: importance=0.0011, rank=3
   4. Feature_2_t0: importance=0.0009, rank=4
   5. Feature_64_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for TBBK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TBBK...

==================================================
Training Enhanced TBBK (SVM)
==================================================
Training SVM model...

Enhanced TBBK Performance:
MAE: 296745.1263
RMSE: 362031.2257
MAPE: 5.08%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t2: importance=0.0020, rank=1
   2. Feature_4_t0: importance=0.0017, rank=2
   3. Feature_4_t2: importance=0.0015, rank=3
   4. Feature_15_t0: importance=0.0014, rank=4
   5. Feature_6_t0: importance=0.0013, rank=5

ðŸ“Š TBBK Results:
  Baseline MAPE: 4.80%
  Enhanced MAPE: 5.08%
  MAPE Improvement: -0.28% (-5.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 403/464: TDC
============================================================
ðŸ“Š Loading data for TDC...
ðŸ“Š Loading data for TDC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TDC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TDC...

==================================================
Training Baseline TDC (SVM)
==================================================
Training SVM model...

Baseline TDC Performance:
MAE: 333158.0524
RMSE: 426882.8910
MAPE: 8.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 154
   â€¢ Highly important features (top 5%): 68

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0004, rank=1
   2. Feature_2_t3: importance=0.0004, rank=2
   3. Feature_1_t3: importance=0.0003, rank=3
   4. Feature_8_t3: importance=0.0003, rank=4
   5. Feature_64_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for TDC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TDC...

==================================================
Training Enhanced TDC (SVM)
==================================================
Training SVM model...

Enhanced TDC Performance:
MAE: 391892.0353
RMSE: 500143.3105
MAPE: 9.91%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0004, rank=1
   2. Feature_23_t3: importance=0.0004, rank=2
   3. Feature_19_t2: importance=0.0004, rank=3
   4. Feature_4_t0: importance=0.0003, rank=4
   5. Feature_24_t3: importance=0.0003, rank=5

ðŸ“Š TDC Results:
  Baseline MAPE: 8.92%
  Enhanced MAPE: 9.91%
  MAPE Improvement: -0.99% (-11.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 404/464: TDS
============================================================
ðŸ“Š Loading data for TDS...
ðŸ“Š Loading data for TDS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TDS: 'TDS'

============================================================
TESTING TICKER 405/464: TDW
============================================================
ðŸ“Š Loading data for TDW...
ðŸ“Š Loading data for TDW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TDW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TDW...

==================================================
Training Baseline TDW (SVM)
==================================================
Training SVM model...

Baseline TDW Performance:
MAE: 391772.8238
RMSE: 483846.6489
MAPE: 7.26%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 25
   â€¢ Highly important features (top 5%): 13

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0016, rank=1
   2. Feature_67_t3: importance=0.0013, rank=2
   3. Feature_2_t0: importance=0.0009, rank=3
   4. Feature_65_t3: importance=0.0007, rank=4
   5. Feature_65_t2: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for TDW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TDW...

==================================================
Training Enhanced TDW (SVM)
==================================================
Training SVM model...

Enhanced TDW Performance:
MAE: 460576.8086
RMSE: 542593.2125
MAPE: 8.55%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 78
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t2: importance=0.0021, rank=1
   2. Feature_20_t2: importance=0.0018, rank=2
   3. Feature_5_t2: importance=0.0012, rank=3
   4. Feature_4_t1: importance=0.0011, rank=4
   5. Feature_5_t3: importance=0.0011, rank=5

ðŸ“Š TDW Results:
  Baseline MAPE: 7.26%
  Enhanced MAPE: 8.55%
  MAPE Improvement: -1.30% (-17.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 406/464: TFX
============================================================
ðŸ“Š Loading data for TFX...
ðŸ“Š Loading data for TFX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TFX...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TFX...

==================================================
Training Baseline TFX (SVM)
==================================================
Training SVM model...

Baseline TFX Performance:
MAE: 204902.3064
RMSE: 337989.4869
MAPE: 21.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 144
   â€¢ Highly important features (top 5%): 81

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0013, rank=1
   2. Feature_67_t1: importance=0.0010, rank=2
   3. Feature_2_t3: importance=0.0010, rank=3
   4. Feature_65_t1: importance=0.0006, rank=4
   5. Feature_66_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for TFX...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TFX...

==================================================
Training Enhanced TFX (SVM)
==================================================
Training SVM model...

Enhanced TFX Performance:
MAE: 202249.6193
RMSE: 330986.1160
MAPE: 21.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0011, rank=1
   2. Feature_13_t1: importance=0.0011, rank=2
   3. Feature_23_t1: importance=0.0011, rank=3
   4. Feature_16_t2: importance=0.0007, rank=4
   5. Feature_9_t3: importance=0.0006, rank=5

ðŸ“Š TFX Results:
  Baseline MAPE: 21.75%
  Enhanced MAPE: 21.31%
  MAPE Improvement: +0.44% (+2.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 407/464: TGNA
============================================================
ðŸ“Š Loading data for TGNA...
ðŸ“Š Loading data for TGNA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TGNA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TGNA...

==================================================
Training Baseline TGNA (SVM)
==================================================
Training SVM model...

Baseline TGNA Performance:
MAE: 860249.5997
RMSE: 1244999.9372
MAPE: 11.15%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 65
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0058, rank=1
   2. Feature_67_t2: importance=0.0023, rank=2
   3. Feature_0_t3: importance=0.0019, rank=3
   4. Feature_66_t3: importance=0.0014, rank=4
   5. Feature_2_t3: importance=0.0014, rank=5

ðŸ”§ Applying universal feature engineering for TGNA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TGNA...

==================================================
Training Enhanced TGNA (SVM)
==================================================
Training SVM model...

Enhanced TGNA Performance:
MAE: 892896.3289
RMSE: 1281053.4754
MAPE: 11.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 49
   â€¢ Highly important features (top 5%): 18

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0061, rank=1
   2. Feature_6_t0: importance=0.0059, rank=2
   3. Feature_23_t0: importance=0.0057, rank=3
   4. Feature_19_t0: importance=0.0027, rank=4
   5. Feature_11_t2: importance=0.0020, rank=5

ðŸ“Š TGNA Results:
  Baseline MAPE: 11.15%
  Enhanced MAPE: 11.50%
  MAPE Improvement: -0.34% (-3.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 408/464: TGTX
============================================================
ðŸ“Š Loading data for TGTX...
ðŸ“Š Loading data for TGTX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TGTX: 'TGTX'

============================================================
TESTING TICKER 409/464: THRM
============================================================
ðŸ“Š Loading data for THRM...
ðŸ“Š Loading data for THRM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for THRM...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for THRM...

==================================================
Training Baseline THRM (SVM)
==================================================
Training SVM model...

Baseline THRM Performance:
MAE: 90710.4001
RMSE: 125817.2428
MAPE: 9.72%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 71
   â€¢ Highly important features (top 5%): 31

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0012, rank=1
   2. Feature_65_t2: importance=0.0007, rank=2
   3. Feature_63_t1: importance=0.0006, rank=3
   4. Feature_65_t0: importance=0.0006, rank=4
   5. Feature_67_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for THRM...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for THRM...

==================================================
Training Enhanced THRM (SVM)
==================================================
Training SVM model...

Enhanced THRM Performance:
MAE: 103599.7661
RMSE: 132801.3997
MAPE: 11.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 79
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t2: importance=0.0007, rank=1
   2. Feature_22_t1: importance=0.0006, rank=2
   3. Feature_9_t3: importance=0.0006, rank=3
   4. Feature_16_t3: importance=0.0005, rank=4
   5. Feature_1_t2: importance=0.0004, rank=5

ðŸ“Š THRM Results:
  Baseline MAPE: 9.72%
  Enhanced MAPE: 11.18%
  MAPE Improvement: -1.46% (-15.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 410/464: THS
============================================================
ðŸ“Š Loading data for THS...
ðŸ“Š Loading data for THS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for THS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for THS...

==================================================
Training Baseline THS (SVM)
==================================================
Training SVM model...

Baseline THS Performance:
MAE: 227175.4555
RMSE: 327162.9543
MAPE: 10.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 91
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t1: importance=0.0005, rank=1
   2. Feature_64_t3: importance=0.0003, rank=2
   3. Feature_1_t0: importance=0.0003, rank=3
   4. Feature_64_t1: importance=0.0003, rank=4
   5. Feature_67_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for THS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for THS...

==================================================
Training Enhanced THS (SVM)
==================================================
Training SVM model...

Enhanced THS Performance:
MAE: 185425.8167
RMSE: 289794.5347
MAPE: 8.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t0: importance=0.0005, rank=1
   2. Feature_24_t2: importance=0.0004, rank=2
   3. Feature_1_t0: importance=0.0004, rank=3
   4. Feature_15_t0: importance=0.0004, rank=4
   5. Feature_10_t3: importance=0.0004, rank=5

ðŸ“Š THS Results:
  Baseline MAPE: 10.60%
  Enhanced MAPE: 8.51%
  MAPE Improvement: +2.10% (+19.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 411/464: TILE
============================================================
ðŸ“Š Loading data for TILE...
ðŸ“Š Loading data for TILE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TILE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TILE...

==================================================
Training Baseline TILE (SVM)
==================================================
Training SVM model...

Baseline TILE Performance:
MAE: 292929.9353
RMSE: 412319.6938
MAPE: 16.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 134
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0010, rank=1
   2. Feature_0_t1: importance=0.0009, rank=2
   3. Feature_65_t1: importance=0.0009, rank=3
   4. Feature_2_t0: importance=0.0009, rank=4
   5. Feature_2_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for TILE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TILE...

==================================================
Training Enhanced TILE (SVM)
==================================================
Training SVM model...

Enhanced TILE Performance:
MAE: 278744.0893
RMSE: 419707.5087
MAPE: 15.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 69

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0016, rank=1
   2. Feature_23_t2: importance=0.0013, rank=2
   3. Feature_6_t2: importance=0.0010, rank=3
   4. Feature_1_t0: importance=0.0009, rank=4
   5. Feature_20_t3: importance=0.0009, rank=5

ðŸ“Š TILE Results:
  Baseline MAPE: 16.86%
  Enhanced MAPE: 15.70%
  MAPE Improvement: +1.16% (+6.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 412/464: TMP
============================================================
ðŸ“Š Loading data for TMP...
ðŸ“Š Loading data for TMP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TMP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TMP...

==================================================
Training Baseline TMP (SVM)
==================================================
Training SVM model...

Baseline TMP Performance:
MAE: 21021.4304
RMSE: 29694.9596
MAPE: 12.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 16

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0002, rank=1
   2. Feature_1_t0: importance=0.0001, rank=2
   3. Feature_67_t3: importance=0.0001, rank=3
   4. Feature_65_t1: importance=0.0001, rank=4
   5. Feature_2_t2: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for TMP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TMP...

==================================================
Training Enhanced TMP (SVM)
==================================================
Training SVM model...

Enhanced TMP Performance:
MAE: 16681.0519
RMSE: 24337.9539
MAPE: 10.88%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 44

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t0: importance=0.0001, rank=1
   2. Feature_19_t0: importance=0.0001, rank=2
   3. Feature_21_t3: importance=0.0001, rank=3
   4. Feature_24_t2: importance=0.0001, rank=4
   5. Feature_11_t2: importance=0.0001, rank=5

ðŸ“Š TMP Results:
  Baseline MAPE: 12.59%
  Enhanced MAPE: 10.88%
  MAPE Improvement: +1.71% (+13.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 413/464: TNC
============================================================
ðŸ“Š Loading data for TNC...
ðŸ“Š Loading data for TNC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TNC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TNC...

==================================================
Training Baseline TNC (SVM)
==================================================
Training SVM model...

Baseline TNC Performance:
MAE: 48063.5169
RMSE: 56954.4139
MAPE: 10.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 121
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0009, rank=1
   2. Feature_2_t2: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_67_t1: importance=0.0004, rank=4
   5. Feature_67_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for TNC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TNC...

==================================================
Training Enhanced TNC (SVM)
==================================================
Training SVM model...

Enhanced TNC Performance:
MAE: 43514.1804
RMSE: 53109.4597
MAPE: 9.16%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t2: importance=0.0010, rank=1
   2. Feature_11_t2: importance=0.0006, rank=2
   3. Feature_10_t2: importance=0.0005, rank=3
   4. Feature_8_t0: importance=0.0004, rank=4
   5. Feature_20_t0: importance=0.0004, rank=5

ðŸ“Š TNC Results:
  Baseline MAPE: 10.19%
  Enhanced MAPE: 9.16%
  MAPE Improvement: +1.03% (+10.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 414/464: TNDM
============================================================
ðŸ“Š Loading data for TNDM...
ðŸ“Š Loading data for TNDM from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TNDM: 'TNDM'

============================================================
TESTING TICKER 415/464: TPH
============================================================
ðŸ“Š Loading data for TPH...
ðŸ“Š Loading data for TPH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TPH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TPH...

==================================================
Training Baseline TPH (SVM)
==================================================
Training SVM model...

Baseline TPH Performance:
MAE: 350999.3740
RMSE: 576088.3559
MAPE: 13.54%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 177
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0006, rank=1
   2. Feature_65_t1: importance=0.0004, rank=2
   3. Feature_64_t3: importance=0.0003, rank=3
   4. Feature_65_t0: importance=0.0003, rank=4
   5. Feature_67_t2: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for TPH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TPH...

==================================================
Training Enhanced TPH (SVM)
==================================================
Training SVM model...

Enhanced TPH Performance:
MAE: 361674.4866
RMSE: 584850.5578
MAPE: 13.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 67
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0004, rank=1
   2. Feature_22_t1: importance=0.0004, rank=2
   3. Feature_1_t2: importance=0.0003, rank=3
   4. Feature_9_t3: importance=0.0003, rank=4
   5. Feature_6_t2: importance=0.0003, rank=5

ðŸ“Š TPH Results:
  Baseline MAPE: 13.54%
  Enhanced MAPE: 13.38%
  MAPE Improvement: +0.16% (+1.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 416/464: TR
============================================================
ðŸ“Š Loading data for TR...
ðŸ“Š Loading data for TR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TR: 'TR'

============================================================
TESTING TICKER 417/464: TRIP
============================================================
ðŸ“Š Loading data for TRIP...
ðŸ“Š Loading data for TRIP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TRIP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TRIP...

==================================================
Training Baseline TRIP (SVM)
==================================================
Training SVM model...

Baseline TRIP Performance:
MAE: 1120775.1377
RMSE: 1627954.4479
MAPE: 12.14%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0020, rank=1
   2. Feature_63_t0: importance=0.0012, rank=2
   3. Feature_66_t1: importance=0.0009, rank=3
   4. Feature_65_t1: importance=0.0008, rank=4
   5. Feature_67_t0: importance=0.0007, rank=5

ðŸ”§ Applying universal feature engineering for TRIP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TRIP...

==================================================
Training Enhanced TRIP (SVM)
==================================================
Training SVM model...

Enhanced TRIP Performance:
MAE: 963774.3013
RMSE: 1712149.0612
MAPE: 9.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 77

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t1: importance=0.0008, rank=1
   2. Feature_19_t2: importance=0.0007, rank=2
   3. Feature_6_t3: importance=0.0006, rank=3
   4. Feature_17_t2: importance=0.0005, rank=4
   5. Feature_13_t2: importance=0.0005, rank=5

ðŸ“Š TRIP Results:
  Baseline MAPE: 12.14%
  Enhanced MAPE: 9.31%
  MAPE Improvement: +2.82% (+23.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 418/464: TRMK
============================================================
ðŸ“Š Loading data for TRMK...
ðŸ“Š Loading data for TRMK from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TRMK...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TRMK...

==================================================
Training Baseline TRMK (SVM)
==================================================
Training SVM model...

Baseline TRMK Performance:
MAE: 137074.8221
RMSE: 181228.9230
MAPE: 8.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 49
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t2: importance=0.0007, rank=1
   2. Feature_64_t0: importance=0.0007, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_67_t3: importance=0.0004, rank=4
   5. Feature_65_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for TRMK...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TRMK...

==================================================
Training Enhanced TRMK (SVM)
==================================================
Training SVM model...

Enhanced TRMK Performance:
MAE: 141828.4044
RMSE: 188773.4675
MAPE: 8.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 74
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_16_t3: importance=0.0006, rank=1
   2. Feature_11_t0: importance=0.0006, rank=2
   3. Feature_23_t1: importance=0.0006, rank=3
   4. Feature_16_t0: importance=0.0006, rank=4
   5. Feature_1_t2: importance=0.0005, rank=5

ðŸ“Š TRMK Results:
  Baseline MAPE: 8.57%
  Enhanced MAPE: 8.83%
  MAPE Improvement: -0.26% (-3.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 419/464: TRN
============================================================
ðŸ“Š Loading data for TRN...
ðŸ“Š Loading data for TRN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TRN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TRN...

==================================================
Training Baseline TRN (SVM)
==================================================
Training SVM model...

Baseline TRN Performance:
MAE: 184008.2310
RMSE: 268242.7153
MAPE: 8.94%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 199
   â€¢ Highly important features (top 5%): 145

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0007, rank=1
   2. Feature_1_t2: importance=0.0006, rank=2
   3. Feature_66_t1: importance=0.0005, rank=3
   4. Feature_65_t2: importance=0.0005, rank=4
   5. Feature_66_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for TRN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TRN...

==================================================
Training Enhanced TRN (SVM)
==================================================
Training SVM model...

Enhanced TRN Performance:
MAE: 184033.1610
RMSE: 258505.4659
MAPE: 9.03%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 51
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0074, rank=1
   2. Feature_23_t2: importance=0.0074, rank=2
   3. Feature_19_t2: importance=0.0073, rank=3
   4. Feature_11_t3: importance=0.0026, rank=4
   5. Feature_20_t3: importance=0.0007, rank=5

ðŸ“Š TRN Results:
  Baseline MAPE: 8.94%
  Enhanced MAPE: 9.03%
  MAPE Improvement: -0.08% (-0.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 420/464: TRNO
============================================================
ðŸ“Š Loading data for TRNO...
ðŸ“Š Loading data for TRNO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TRNO: 'TRNO'

============================================================
TESTING TICKER 421/464: TRST
============================================================
ðŸ“Š Loading data for TRST...
ðŸ“Š Loading data for TRST from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TRST: 'TRST'

============================================================
TESTING TICKER 422/464: TRUP
============================================================
ðŸ“Š Loading data for TRUP...
ðŸ“Š Loading data for TRUP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing TRUP: 'TRUP'

============================================================
TESTING TICKER 423/464: TTMI
============================================================
ðŸ“Š Loading data for TTMI...
ðŸ“Š Loading data for TTMI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TTMI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TTMI...

==================================================
Training Baseline TTMI (SVM)
==================================================
Training SVM model...

Baseline TTMI Performance:
MAE: 268531.3964
RMSE: 352112.3379
MAPE: 13.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 100
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0008, rank=1
   2. Feature_63_t0: importance=0.0006, rank=2
   3. Feature_1_t3: importance=0.0004, rank=3
   4. Feature_67_t1: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for TTMI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TTMI...

==================================================
Training Enhanced TTMI (SVM)
==================================================
Training SVM model...

Enhanced TTMI Performance:
MAE: 283189.1043
RMSE: 362671.7995
MAPE: 14.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_10_t3: importance=0.0008, rank=1
   2. Feature_9_t3: importance=0.0007, rank=2
   3. Feature_21_t1: importance=0.0006, rank=3
   4. Feature_7_t3: importance=0.0006, rank=4
   5. Feature_16_t0: importance=0.0005, rank=5

ðŸ“Š TTMI Results:
  Baseline MAPE: 13.87%
  Enhanced MAPE: 14.17%
  MAPE Improvement: -0.30% (-2.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 424/464: TWI
============================================================
ðŸ“Š Loading data for TWI...
ðŸ“Š Loading data for TWI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TWI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TWI...

==================================================
Training Baseline TWI (SVM)
==================================================
Training SVM model...

Baseline TWI Performance:
MAE: 470990.8127
RMSE: 590437.9281
MAPE: 13.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0022, rank=1
   2. Feature_1_t2: importance=0.0012, rank=2
   3. Feature_63_t2: importance=0.0010, rank=3
   4. Feature_0_t2: importance=0.0009, rank=4
   5. Feature_64_t0: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for TWI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TWI...

==================================================
Training Enhanced TWI (SVM)
==================================================
Training SVM model...

Enhanced TWI Performance:
MAE: 295950.6195
RMSE: 353575.9752
MAPE: 9.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0016, rank=1
   2. Feature_13_t3: importance=0.0012, rank=2
   3. Feature_22_t3: importance=0.0012, rank=3
   4. Feature_23_t3: importance=0.0012, rank=4
   5. Feature_16_t2: importance=0.0011, rank=5

ðŸ“Š TWI Results:
  Baseline MAPE: 13.09%
  Enhanced MAPE: 9.05%
  MAPE Improvement: +4.04% (+30.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 425/464: TWO
============================================================
ðŸ“Š Loading data for TWO...
ðŸ“Š Loading data for TWO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for TWO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for TWO...

==================================================
Training Baseline TWO (SVM)
==================================================
Training SVM model...

Baseline TWO Performance:
MAE: 454768.3079
RMSE: 602513.3945
MAPE: 11.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 107
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0022, rank=1
   2. Feature_65_t1: importance=0.0021, rank=2
   3. Feature_0_t2: importance=0.0013, rank=3
   4. Feature_1_t2: importance=0.0013, rank=4
   5. Feature_63_t1: importance=0.0010, rank=5

ðŸ”§ Applying universal feature engineering for TWO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for TWO...

==================================================
Training Enhanced TWO (SVM)
==================================================
Training SVM model...

Enhanced TWO Performance:
MAE: 446264.4500
RMSE: 600046.2155
MAPE: 11.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 61
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_17_t2: importance=0.0069, rank=1
   2. Feature_18_t1: importance=0.0050, rank=2
   3. Feature_17_t1: importance=0.0029, rank=3
   4. Feature_4_t1: importance=0.0027, rank=4
   5. Feature_17_t3: importance=0.0022, rank=5

ðŸ“Š TWO Results:
  Baseline MAPE: 11.96%
  Enhanced MAPE: 11.48%
  MAPE Improvement: +0.48% (+4.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 426/464: UCTT
============================================================
ðŸ“Š Loading data for UCTT...
ðŸ“Š Loading data for UCTT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UCTT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UCTT...

==================================================
Training Baseline UCTT (SVM)
==================================================
Training SVM model...

Baseline UCTT Performance:
MAE: 88087.4957
RMSE: 108067.3713
MAPE: 9.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 118
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0007, rank=1
   2. Feature_65_t2: importance=0.0005, rank=2
   3. Feature_63_t0: importance=0.0004, rank=3
   4. Feature_2_t2: importance=0.0003, rank=4
   5. Feature_65_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for UCTT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UCTT...

==================================================
Training Enhanced UCTT (SVM)
==================================================
Training SVM model...

Enhanced UCTT Performance:
MAE: 86107.4417
RMSE: 112218.5972
MAPE: 8.92%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 59

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0009, rank=1
   2. Feature_23_t3: importance=0.0007, rank=2
   3. Feature_19_t3: importance=0.0006, rank=3
   4. Feature_9_t2: importance=0.0006, rank=4
   5. Feature_3_t3: importance=0.0004, rank=5

ðŸ“Š UCTT Results:
  Baseline MAPE: 9.02%
  Enhanced MAPE: 8.92%
  MAPE Improvement: +0.10% (+1.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 427/464: UE
============================================================
ðŸ“Š Loading data for UE...
ðŸ“Š Loading data for UE from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UE...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UE...

==================================================
Training Baseline UE (SVM)
==================================================
Training SVM model...

Baseline UE Performance:
MAE: 413829.3399
RMSE: 571993.0232
MAPE: 12.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 137
   â€¢ Highly important features (top 5%): 86

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0008, rank=1
   2. Feature_0_t3: importance=0.0007, rank=2
   3. Feature_66_t3: importance=0.0006, rank=3
   4. Feature_64_t1: importance=0.0005, rank=4
   5. Feature_2_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for UE...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UE...

==================================================
Training Enhanced UE (SVM)
==================================================
Training SVM model...

Enhanced UE Performance:
MAE: 412477.9245
RMSE: 543643.6181
MAPE: 12.33%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0007, rank=1
   2. Feature_22_t3: importance=0.0006, rank=2
   3. Feature_8_t2: importance=0.0005, rank=3
   4. Feature_12_t1: importance=0.0005, rank=4
   5. Feature_13_t1: importance=0.0005, rank=5

ðŸ“Š UE Results:
  Baseline MAPE: 12.30%
  Enhanced MAPE: 12.33%
  MAPE Improvement: -0.03% (-0.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 428/464: UFCS
============================================================
ðŸ“Š Loading data for UFCS...
ðŸ“Š Loading data for UFCS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UFCS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UFCS...

==================================================
Training Baseline UFCS (SVM)
==================================================
Training SVM model...

Baseline UFCS Performance:
MAE: 44760.0624
RMSE: 63544.9194
MAPE: 14.49%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 69
   â€¢ Highly important features (top 5%): 29

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0030, rank=1
   2. Feature_67_t0: importance=0.0015, rank=2
   3. Feature_0_t1: importance=0.0011, rank=3
   4. Feature_67_t2: importance=0.0010, rank=4
   5. Feature_2_t2: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for UFCS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UFCS...

==================================================
Training Enhanced UFCS (SVM)
==================================================
Training SVM model...

Enhanced UFCS Performance:
MAE: 46292.3431
RMSE: 61992.4711
MAPE: 15.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0014, rank=1
   2. Feature_16_t3: importance=0.0010, rank=2
   3. Feature_15_t2: importance=0.0010, rank=3
   4. Feature_22_t2: importance=0.0010, rank=4
   5. Feature_21_t3: importance=0.0009, rank=5

ðŸ“Š UFCS Results:
  Baseline MAPE: 14.49%
  Enhanced MAPE: 15.39%
  MAPE Improvement: -0.90% (-6.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 429/464: UFPT
============================================================
ðŸ“Š Loading data for UFPT...
ðŸ“Š Loading data for UFPT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing UFPT: 'UFPT'

============================================================
TESTING TICKER 430/464: UHT
============================================================
ðŸ“Š Loading data for UHT...
ðŸ“Š Loading data for UHT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UHT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UHT...

==================================================
Training Baseline UHT (SVM)
==================================================
Training SVM model...

Baseline UHT Performance:
MAE: 44842.9984
RMSE: 67822.9038
MAPE: 26.69%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 62

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0047, rank=1
   2. Feature_2_t3: importance=0.0022, rank=2
   3. Feature_0_t2: importance=0.0018, rank=3
   4. Feature_63_t0: importance=0.0013, rank=4
   5. Feature_67_t2: importance=0.0012, rank=5

ðŸ”§ Applying universal feature engineering for UHT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UHT...

==================================================
Training Enhanced UHT (SVM)
==================================================
Training SVM model...

Enhanced UHT Performance:
MAE: 42904.9761
RMSE: 69689.9096
MAPE: 26.75%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 84
   â€¢ Highly important features (top 5%): 61

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0028, rank=1
   2. Feature_12_t1: importance=0.0019, rank=2
   3. Feature_22_t1: importance=0.0019, rank=3
   4. Feature_24_t1: importance=0.0018, rank=4
   5. Feature_16_t3: importance=0.0011, rank=5

ðŸ“Š UHT Results:
  Baseline MAPE: 26.69%
  Enhanced MAPE: 26.75%
  MAPE Improvement: -0.06% (-0.2%)
  Features: 68 -> 25

============================================================
TESTING TICKER 431/464: UNF
============================================================
ðŸ“Š Loading data for UNF...
ðŸ“Š Loading data for UNF from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UNF...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UNF...

==================================================
Training Baseline UNF (SVM)
==================================================
Training SVM model...

Baseline UNF Performance:
MAE: 23710.8677
RMSE: 30629.3711
MAPE: 14.07%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 202
   â€¢ Highly important features (top 5%): 96

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0018, rank=1
   2. Feature_63_t3: importance=0.0013, rank=2
   3. Feature_0_t3: importance=0.0011, rank=3
   4. Feature_65_t2: importance=0.0010, rank=4
   5. Feature_65_t0: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for UNF...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UNF...

==================================================
Training Enhanced UNF (SVM)
==================================================
Training SVM model...

Enhanced UNF Performance:
MAE: 24826.2784
RMSE: 30749.2488
MAPE: 14.73%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 78

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t3: importance=0.0020, rank=1
   2. Feature_4_t2: importance=0.0018, rank=2
   3. Feature_11_t2: importance=0.0012, rank=3
   4. Feature_9_t2: importance=0.0011, rank=4
   5. Feature_10_t2: importance=0.0010, rank=5

ðŸ“Š UNF Results:
  Baseline MAPE: 14.07%
  Enhanced MAPE: 14.73%
  MAPE Improvement: -0.66% (-4.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 432/464: UNFI
============================================================
ðŸ“Š Loading data for UNFI...
ðŸ“Š Loading data for UNFI from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UNFI...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UNFI...

==================================================
Training Baseline UNFI (SVM)
==================================================
Training SVM model...

Baseline UNFI Performance:
MAE: 292929.5051
RMSE: 381291.7955
MAPE: 8.02%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 99
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0032, rank=1
   2. Feature_64_t3: importance=0.0006, rank=2
   3. Feature_65_t0: importance=0.0005, rank=3
   4. Feature_65_t2: importance=0.0003, rank=4
   5. Feature_2_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for UNFI...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UNFI...

==================================================
Training Enhanced UNFI (SVM)
==================================================
Training SVM model...

Enhanced UNFI Performance:
MAE: 253735.8620
RMSE: 315979.6535
MAPE: 6.81%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 73
   â€¢ Highly important features (top 5%): 54

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0016, rank=1
   2. Feature_13_t3: importance=0.0012, rank=2
   3. Feature_23_t3: importance=0.0011, rank=3
   4. Feature_6_t3: importance=0.0010, rank=4
   5. Feature_16_t0: importance=0.0005, rank=5

ðŸ“Š UNFI Results:
  Baseline MAPE: 8.02%
  Enhanced MAPE: 6.81%
  MAPE Improvement: +1.22% (+15.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 433/464: UNIT
============================================================
ðŸ“Š Loading data for UNIT...
ðŸ“Š Loading data for UNIT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing UNIT: 'UNIT'

============================================================
TESTING TICKER 434/464: URBN
============================================================
ðŸ“Š Loading data for URBN...
ðŸ“Š Loading data for URBN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for URBN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for URBN...

==================================================
Training Baseline URBN (SVM)
==================================================
Training SVM model...

Baseline URBN Performance:
MAE: 765426.9529
RMSE: 949827.2773
MAPE: 13.05%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 157
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0010, rank=1
   2. Feature_0_t3: importance=0.0009, rank=2
   3. Feature_67_t3: importance=0.0008, rank=3
   4. Feature_64_t3: importance=0.0006, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for URBN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for URBN...

==================================================
Training Enhanced URBN (SVM)
==================================================
Training SVM model...

Enhanced URBN Performance:
MAE: 735463.5015
RMSE: 979855.3112
MAPE: 12.39%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 85
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t2: importance=0.0019, rank=1
   2. Feature_23_t2: importance=0.0016, rank=2
   3. Feature_19_t2: importance=0.0015, rank=3
   4. Feature_21_t3: importance=0.0009, rank=4
   5. Feature_13_t1: importance=0.0009, rank=5

ðŸ“Š URBN Results:
  Baseline MAPE: 13.05%
  Enhanced MAPE: 12.39%
  MAPE Improvement: +0.66% (+5.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 435/464: USNA
============================================================
ðŸ“Š Loading data for USNA...
ðŸ“Š Loading data for USNA from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for USNA...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for USNA...

==================================================
Training Baseline USNA (SVM)
==================================================
Training SVM model...

Baseline USNA Performance:
MAE: 48858.2763
RMSE: 68042.6869
MAPE: 9.34%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 165
   â€¢ Highly important features (top 5%): 107

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0011, rank=1
   2. Feature_2_t3: importance=0.0008, rank=2
   3. Feature_64_t3: importance=0.0007, rank=3
   4. Feature_1_t1: importance=0.0004, rank=4
   5. Feature_2_t0: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for USNA...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for USNA...

==================================================
Training Enhanced USNA (SVM)
==================================================
Training SVM model...

Enhanced USNA Performance:
MAE: 45383.9963
RMSE: 68020.8879
MAPE: 9.25%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 86
   â€¢ Highly important features (top 5%): 67

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0019, rank=1
   2. Feature_19_t2: importance=0.0016, rank=2
   3. Feature_21_t3: importance=0.0011, rank=3
   4. Feature_18_t3: importance=0.0009, rank=4
   5. Feature_10_t2: importance=0.0007, rank=5

ðŸ“Š USNA Results:
  Baseline MAPE: 9.34%
  Enhanced MAPE: 9.25%
  MAPE Improvement: +0.09% (+1.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 436/464: USPH
============================================================
ðŸ“Š Loading data for USPH...
ðŸ“Š Loading data for USPH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for USPH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for USPH...

==================================================
Training Baseline USPH (SVM)
==================================================
Training SVM model...

Baseline USPH Performance:
MAE: 42117.8107
RMSE: 52751.5770
MAPE: 7.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 88
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0006, rank=1
   2. Feature_67_t3: importance=0.0003, rank=2
   3. Feature_1_t3: importance=0.0001, rank=3
   4. Feature_2_t2: importance=0.0001, rank=4
   5. Feature_67_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for USPH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for USPH...

==================================================
Training Enhanced USPH (SVM)
==================================================
Training SVM model...

Enhanced USPH Performance:
MAE: 38880.4168
RMSE: 49556.4133
MAPE: 6.83%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 60
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0003, rank=1
   2. Feature_11_t3: importance=0.0003, rank=2
   3. Feature_9_t2: importance=0.0002, rank=3
   4. Feature_18_t3: importance=0.0002, rank=4
   5. Feature_21_t1: importance=0.0001, rank=5

ðŸ“Š USPH Results:
  Baseline MAPE: 7.29%
  Enhanced MAPE: 6.83%
  MAPE Improvement: +0.46% (+6.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 437/464: UTL
============================================================
ðŸ“Š Loading data for UTL...
ðŸ“Š Loading data for UTL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UTL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UTL...

==================================================
Training Baseline UTL (SVM)
==================================================
Training SVM model...

Baseline UTL Performance:
MAE: 25070.7168
RMSE: 37078.5071
MAPE: 13.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 188
   â€¢ Highly important features (top 5%): 122

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0031, rank=1
   2. Feature_2_t3: importance=0.0028, rank=2
   3. Feature_65_t3: importance=0.0014, rank=3
   4. Feature_0_t3: importance=0.0012, rank=4
   5. Feature_2_t2: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for UTL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UTL...

==================================================
Training Enhanced UTL (SVM)
==================================================
Training SVM model...

Enhanced UTL Performance:
MAE: 29444.1881
RMSE: 41605.3727
MAPE: 15.90%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 90
   â€¢ Highly important features (top 5%): 86

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0028, rank=1
   2. Feature_13_t3: importance=0.0019, rank=2
   3. Feature_23_t3: importance=0.0016, rank=3
   4. Feature_12_t2: importance=0.0014, rank=4
   5. Feature_19_t1: importance=0.0013, rank=5

ðŸ“Š UTL Results:
  Baseline MAPE: 13.70%
  Enhanced MAPE: 15.90%
  MAPE Improvement: -2.20% (-16.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 438/464: UVV
============================================================
ðŸ“Š Loading data for UVV...
ðŸ“Š Loading data for UVV from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for UVV...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for UVV...

==================================================
Training Baseline UVV (SVM)
==================================================
Training SVM model...

Baseline UVV Performance:
MAE: 79118.4305
RMSE: 145827.6983
MAPE: 19.76%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 107
   â€¢ Highly important features (top 5%): 63

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t0: importance=0.0011, rank=1
   2. Feature_65_t1: importance=0.0005, rank=2
   3. Feature_0_t3: importance=0.0005, rank=3
   4. Feature_65_t0: importance=0.0004, rank=4
   5. Feature_64_t3: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for UVV...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for UVV...

==================================================
Training Enhanced UVV (SVM)
==================================================
Training SVM model...

Enhanced UVV Performance:
MAE: 78637.5353
RMSE: 148649.9073
MAPE: 19.51%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0014, rank=1
   2. Feature_6_t2: importance=0.0005, rank=2
   3. Feature_20_t2: importance=0.0005, rank=3
   4. Feature_4_t1: importance=0.0005, rank=4
   5. Feature_7_t2: importance=0.0005, rank=5

ðŸ“Š UVV Results:
  Baseline MAPE: 19.76%
  Enhanced MAPE: 19.51%
  MAPE Improvement: +0.25% (+1.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 439/464: VBTX
============================================================
ðŸ“Š Loading data for VBTX...
ðŸ“Š Loading data for VBTX from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing VBTX: 'VBTX'

============================================================
TESTING TICKER 440/464: VCEL
============================================================
ðŸ“Š Loading data for VCEL...
ðŸ“Š Loading data for VCEL from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VCEL...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VCEL...

==================================================
Training Baseline VCEL (SVM)
==================================================
Training SVM model...

Baseline VCEL Performance:
MAE: 374239.9354
RMSE: 591051.5246
MAPE: 10.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 145
   â€¢ Highly important features (top 5%): 81

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0070, rank=1
   2. Feature_65_t3: importance=0.0007, rank=2
   3. Feature_67_t2: importance=0.0004, rank=3
   4. Feature_13_t3: importance=0.0004, rank=4
   5. Feature_0_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for VCEL...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VCEL...

==================================================
Training Enhanced VCEL (SVM)
==================================================
Training SVM model...

Enhanced VCEL Performance:
MAE: 341830.8764
RMSE: 541911.1265
MAPE: 10.00%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 68
   â€¢ Highly important features (top 5%): 58

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t3: importance=0.0090, rank=1
   2. Feature_14_t3: importance=0.0063, rank=2
   3. Feature_23_t3: importance=0.0058, rank=3
   4. Feature_19_t3: importance=0.0057, rank=4
   5. Feature_6_t3: importance=0.0042, rank=5

ðŸ“Š VCEL Results:
  Baseline MAPE: 10.63%
  Enhanced MAPE: 10.00%
  MAPE Improvement: +0.63% (+6.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 441/464: VCYT
============================================================
ðŸ“Š Loading data for VCYT...
ðŸ“Š Loading data for VCYT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing VCYT: 'VCYT'

============================================================
TESTING TICKER 442/464: VECO
============================================================
ðŸ“Š Loading data for VECO...
ðŸ“Š Loading data for VECO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VECO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VECO...

==================================================
Training Baseline VECO (SVM)
==================================================
Training SVM model...

Baseline VECO Performance:
MAE: 349989.8933
RMSE: 428575.4963
MAPE: 6.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 52
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t1: importance=0.0005, rank=1
   2. Feature_1_t1: importance=0.0004, rank=2
   3. Feature_65_t1: importance=0.0003, rank=3
   4. Feature_0_t0: importance=0.0003, rank=4
   5. Feature_63_t3: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for VECO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VECO...

==================================================
Training Enhanced VECO (SVM)
==================================================
Training SVM model...

Enhanced VECO Performance:
MAE: 353013.4047
RMSE: 460140.7009
MAPE: 6.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 43

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_20_t1: importance=0.0009, rank=1
   2. Feature_22_t0: importance=0.0004, rank=2
   3. Feature_9_t3: importance=0.0004, rank=3
   4. Feature_20_t2: importance=0.0004, rank=4
   5. Feature_11_t2: importance=0.0004, rank=5

ðŸ“Š VECO Results:
  Baseline MAPE: 6.48%
  Enhanced MAPE: 6.61%
  MAPE Improvement: -0.13% (-2.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 443/464: VIAV
============================================================
ðŸ“Š Loading data for VIAV...
ðŸ“Š Loading data for VIAV from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VIAV...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VIAV...

==================================================
Training Baseline VIAV (SVM)
==================================================
Training SVM model...

Baseline VIAV Performance:
MAE: 444566.1143
RMSE: 581114.6224
MAPE: 8.60%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 97
   â€¢ Highly important features (top 5%): 55

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_66_t2: importance=0.0006, rank=1
   2. Feature_2_t2: importance=0.0006, rank=2
   3. Feature_66_t1: importance=0.0004, rank=3
   4. Feature_64_t2: importance=0.0004, rank=4
   5. Feature_67_t1: importance=0.0004, rank=5

ðŸ”§ Applying universal feature engineering for VIAV...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VIAV...

==================================================
Training Enhanced VIAV (SVM)
==================================================
Training SVM model...

Enhanced VIAV Performance:
MAE: 436238.3114
RMSE: 571271.4673
MAPE: 8.31%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 62
   â€¢ Highly important features (top 5%): 40

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_5_t1: importance=0.0007, rank=1
   2. Feature_8_t2: importance=0.0006, rank=2
   3. Feature_5_t2: importance=0.0005, rank=3
   4. Feature_7_t2: importance=0.0004, rank=4
   5. Feature_0_t3: importance=0.0003, rank=5

ðŸ“Š VIAV Results:
  Baseline MAPE: 8.60%
  Enhanced MAPE: 8.31%
  MAPE Improvement: +0.29% (+3.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 444/464: VICR
============================================================
ðŸ“Š Loading data for VICR...
ðŸ“Š Loading data for VICR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VICR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VICR...

==================================================
Training Baseline VICR (SVM)
==================================================
Training SVM model...

Baseline VICR Performance:
MAE: 90216.4723
RMSE: 116456.8250
MAPE: 5.61%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 95
   â€¢ Highly important features (top 5%): 26

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0015, rank=1
   2. Feature_2_t1: importance=0.0005, rank=2
   3. Feature_67_t0: importance=0.0005, rank=3
   4. Feature_66_t0: importance=0.0005, rank=4
   5. Feature_63_t2: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for VICR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VICR...

==================================================
Training Enhanced VICR (SVM)
==================================================
Training SVM model...

Enhanced VICR Performance:
MAE: 90124.9333
RMSE: 110562.3221
MAPE: 5.50%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 72
   â€¢ Highly important features (top 5%): 50

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_6_t3: importance=0.0010, rank=1
   2. Feature_22_t3: importance=0.0009, rank=2
   3. Feature_12_t3: importance=0.0008, rank=3
   4. Feature_9_t3: importance=0.0007, rank=4
   5. Feature_19_t2: importance=0.0005, rank=5

ðŸ“Š VICR Results:
  Baseline MAPE: 5.61%
  Enhanced MAPE: 5.50%
  MAPE Improvement: +0.11% (+2.0%)
  Features: 68 -> 25

============================================================
TESTING TICKER 445/464: VIRT
============================================================
ðŸ“Š Loading data for VIRT...
ðŸ“Š Loading data for VIRT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VIRT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VIRT...

==================================================
Training Baseline VIRT (SVM)
==================================================
Training SVM model...

Baseline VIRT Performance:
MAE: 304846.0446
RMSE: 488537.7662
MAPE: 11.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 202
   â€¢ Highly important features (top 5%): 110

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_64_t3: importance=0.0009, rank=1
   2. Feature_67_t1: importance=0.0009, rank=2
   3. Feature_65_t0: importance=0.0007, rank=3
   4. Feature_63_t3: importance=0.0006, rank=4
   5. Feature_1_t2: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for VIRT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VIRT...

==================================================
Training Enhanced VIRT (SVM)
==================================================
Training SVM model...

Enhanced VIRT Performance:
MAE: 290666.0689
RMSE: 483748.2839
MAPE: 11.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 53

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0027, rank=1
   2. Feature_13_t3: importance=0.0026, rank=2
   3. Feature_23_t3: importance=0.0025, rank=3
   4. Feature_20_t2: importance=0.0015, rank=4
   5. Feature_12_t2: importance=0.0015, rank=5

ðŸ“Š VIRT Results:
  Baseline MAPE: 11.86%
  Enhanced MAPE: 11.17%
  MAPE Improvement: +0.68% (+5.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 446/464: VRTS
============================================================
ðŸ“Š Loading data for VRTS...
ðŸ“Š Loading data for VRTS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing VRTS: 'VRTS'

============================================================
TESTING TICKER 447/464: VSAT
============================================================
ðŸ“Š Loading data for VSAT...
ðŸ“Š Loading data for VSAT from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VSAT...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VSAT...

==================================================
Training Baseline VSAT (SVM)
==================================================
Training SVM model...

Baseline VSAT Performance:
MAE: 1317396.5524
RMSE: 1763569.9472
MAPE: 7.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0002, rank=1
   2. Feature_0_t2: importance=0.0002, rank=2
   3. Feature_64_t3: importance=0.0002, rank=3
   4. Feature_65_t2: importance=0.0002, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for VSAT...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VSAT...

==================================================
Training Enhanced VSAT (SVM)
==================================================
Training SVM model...

Enhanced VSAT Performance:
MAE: 1249207.0375
RMSE: 1625692.2980
MAPE: 7.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 43
   â€¢ Highly important features (top 5%): 24

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_4_t0: importance=0.0003, rank=1
   2. Feature_23_t3: importance=0.0002, rank=2
   3. Feature_13_t3: importance=0.0002, rank=3
   4. Feature_19_t3: importance=0.0001, rank=4
   5. Feature_9_t3: importance=0.0001, rank=5

ðŸ“Š VSAT Results:
  Baseline MAPE: 7.48%
  Enhanced MAPE: 7.23%
  MAPE Improvement: +0.25% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 448/464: VSH
============================================================
ðŸ“Š Loading data for VSH...
ðŸ“Š Loading data for VSH from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for VSH...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for VSH...

==================================================
Training Baseline VSH (SVM)
==================================================
Training SVM model...

Baseline VSH Performance:
MAE: 823469.8056
RMSE: 1189915.0944
MAPE: 7.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 109
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0002, rank=1
   2. Feature_67_t0: importance=0.0001, rank=2
   3. Feature_65_t1: importance=0.0001, rank=3
   4. Feature_67_t3: importance=0.0001, rank=4
   5. Feature_66_t2: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for VSH...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for VSH...

==================================================
Training Enhanced VSH (SVM)
==================================================
Training SVM model...

Enhanced VSH Performance:
MAE: 719294.2657
RMSE: 1102056.7101
MAPE: 6.09%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 59
   â€¢ Highly important features (top 5%): 45

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_21_t2: importance=0.0001, rank=1
   2. Feature_3_t2: importance=0.0001, rank=2
   3. Feature_1_t3: importance=0.0001, rank=3
   4. Feature_16_t2: importance=0.0001, rank=4
   5. Feature_6_t3: importance=0.0001, rank=5

ðŸ“Š VSH Results:
  Baseline MAPE: 7.09%
  Enhanced MAPE: 6.09%
  MAPE Improvement: +1.00% (+14.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 449/464: WABC
============================================================
ðŸ“Š Loading data for WABC...
ðŸ“Š Loading data for WABC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WABC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WABC...

==================================================
Training Baseline WABC (SVM)
==================================================
Training SVM model...

Baseline WABC Performance:
MAE: 34775.4844
RMSE: 45813.7859
MAPE: 11.95%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 122
   â€¢ Highly important features (top 5%): 33

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0003, rank=1
   2. Feature_1_t2: importance=0.0002, rank=2
   3. Feature_1_t1: importance=0.0002, rank=3
   4. Feature_1_t0: importance=0.0002, rank=4
   5. Feature_65_t0: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for WABC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WABC...

==================================================
Training Enhanced WABC (SVM)
==================================================
Training SVM model...

Enhanced WABC Performance:
MAE: 35031.4915
RMSE: 43156.6578
MAPE: 11.87%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 44
   â€¢ Highly important features (top 5%): 18

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t1: importance=0.0008, rank=1
   2. Feature_19_t0: importance=0.0007, rank=2
   3. Feature_13_t3: importance=0.0003, rank=3
   4. Feature_16_t0: importance=0.0002, rank=4
   5. Feature_6_t3: importance=0.0002, rank=5

ðŸ“Š WABC Results:
  Baseline MAPE: 11.95%
  Enhanced MAPE: 11.87%
  MAPE Improvement: +0.08% (+0.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 450/464: WAFD
============================================================
ðŸ“Š Loading data for WAFD...
ðŸ“Š Loading data for WAFD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WAFD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WAFD...

==================================================
Training Baseline WAFD (SVM)
==================================================
Training SVM model...

Baseline WAFD Performance:
MAE: 230863.9890
RMSE: 311584.0462
MAPE: 12.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 129
   â€¢ Highly important features (top 5%): 84

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_0_t3: importance=0.0018, rank=1
   2. Feature_65_t1: importance=0.0008, rank=2
   3. Feature_63_t3: importance=0.0007, rank=3
   4. Feature_64_t0: importance=0.0004, rank=4
   5. Feature_4_t2: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for WAFD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WAFD...

==================================================
Training Enhanced WAFD (SVM)
==================================================
Training SVM model...

Enhanced WAFD Performance:
MAE: 241698.1504
RMSE: 317033.8114
MAPE: 13.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0056, rank=1
   2. Feature_23_t3: importance=0.0027, rank=2
   3. Feature_22_t0: importance=0.0017, rank=3
   4. Feature_3_t3: importance=0.0008, rank=4
   5. Feature_13_t3: importance=0.0007, rank=5

ðŸ“Š WAFD Results:
  Baseline MAPE: 12.48%
  Enhanced MAPE: 13.58%
  MAPE Improvement: -1.09% (-8.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 451/464: WD
============================================================
ðŸ“Š Loading data for WD...
ðŸ“Š Loading data for WD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WD...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WD...

==================================================
Training Baseline WD (SVM)
==================================================
Training SVM model...

Baseline WD Performance:
MAE: 109968.5683
RMSE: 155898.4646
MAPE: 9.38%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 233
   â€¢ Highly important features (top 5%): 133

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t0: importance=0.0006, rank=1
   2. Feature_64_t3: importance=0.0006, rank=2
   3. Feature_67_t2: importance=0.0005, rank=3
   4. Feature_0_t1: importance=0.0005, rank=4
   5. Feature_0_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for WD...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WD...

==================================================
Training Enhanced WD (SVM)
==================================================
Training SVM model...

Enhanced WD Performance:
MAE: 87738.6141
RMSE: 142697.8945
MAPE: 7.59%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 82
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_15_t2: importance=0.0009, rank=1
   2. Feature_6_t0: importance=0.0008, rank=2
   3. Feature_9_t3: importance=0.0007, rank=3
   4. Feature_7_t0: importance=0.0006, rank=4
   5. Feature_16_t0: importance=0.0006, rank=5

ðŸ“Š WD Results:
  Baseline MAPE: 9.38%
  Enhanced MAPE: 7.59%
  MAPE Improvement: +1.79% (+19.1%)
  Features: 68 -> 25

============================================================
TESTING TICKER 452/464: WDFC
============================================================
ðŸ“Š Loading data for WDFC...
ðŸ“Š Loading data for WDFC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WDFC...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WDFC...

==================================================
Training Baseline WDFC (SVM)
==================================================
Training SVM model...

Baseline WDFC Performance:
MAE: 43114.9337
RMSE: 56725.6970
MAPE: 3.70%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 104
   â€¢ Highly important features (top 5%): 72

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t3: importance=0.0002, rank=1
   2. Feature_64_t1: importance=0.0001, rank=2
   3. Feature_1_t3: importance=0.0000, rank=3
   4. Feature_67_t1: importance=0.0000, rank=4
   5. Feature_67_t3: importance=0.0000, rank=5

ðŸ”§ Applying universal feature engineering for WDFC...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WDFC...

==================================================
Training Enhanced WDFC (SVM)
==================================================
Training SVM model...

Enhanced WDFC Performance:
MAE: 40373.7370
RMSE: 61846.0115
MAPE: 3.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 55
   â€¢ Highly important features (top 5%): 38

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_18_t3: importance=0.0001, rank=1
   2. Feature_4_t3: importance=0.0001, rank=2
   3. Feature_12_t1: importance=0.0001, rank=3
   4. Feature_24_t3: importance=0.0001, rank=4
   5. Feature_16_t3: importance=0.0001, rank=5

ðŸ“Š WDFC Results:
  Baseline MAPE: 3.70%
  Enhanced MAPE: 3.58%
  MAPE Improvement: +0.12% (+3.3%)
  Features: 68 -> 25

============================================================
TESTING TICKER 453/464: WEN
============================================================
ðŸ“Š Loading data for WEN...
ðŸ“Š Loading data for WEN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WEN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WEN...

==================================================
Training Baseline WEN (SVM)
==================================================
Training SVM model...

Baseline WEN Performance:
MAE: 1704106.7253
RMSE: 2262737.0290
MAPE: 12.96%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 104
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t0: importance=0.0009, rank=1
   2. Feature_65_t3: importance=0.0007, rank=2
   3. Feature_2_t2: importance=0.0006, rank=3
   4. Feature_67_t2: importance=0.0006, rank=4
   5. Feature_2_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for WEN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WEN...

==================================================
Training Enhanced WEN (SVM)
==================================================
Training SVM model...

Enhanced WEN Performance:
MAE: 1815222.7827
RMSE: 2247849.6557
MAPE: 14.48%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 76
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t2: importance=0.0013, rank=1
   2. Feature_4_t0: importance=0.0012, rank=2
   3. Feature_22_t3: importance=0.0011, rank=3
   4. Feature_12_t3: importance=0.0010, rank=4
   5. Feature_24_t1: importance=0.0009, rank=5

ðŸ“Š WEN Results:
  Baseline MAPE: 12.96%
  Enhanced MAPE: 14.48%
  MAPE Improvement: -1.52% (-11.7%)
  Features: 68 -> 25

============================================================
TESTING TICKER 454/464: WERN
============================================================
ðŸ“Š Loading data for WERN...
ðŸ“Š Loading data for WERN from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WERN...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WERN...

==================================================
Training Baseline WERN (SVM)
==================================================
Training SVM model...

Baseline WERN Performance:
MAE: 383687.0118
RMSE: 472847.7091
MAPE: 11.17%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 93
   â€¢ Highly important features (top 5%): 27

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0006, rank=1
   2. Feature_63_t0: importance=0.0004, rank=2
   3. Feature_67_t0: importance=0.0004, rank=3
   4. Feature_67_t3: importance=0.0004, rank=4
   5. Feature_1_t0: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for WERN...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WERN...

==================================================
Training Enhanced WERN (SVM)
==================================================
Training SVM model...

Enhanced WERN Performance:
MAE: 371124.6437
RMSE: 456225.1606
MAPE: 10.68%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 83
   â€¢ Highly important features (top 5%): 71

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_22_t3: importance=0.0005, rank=1
   2. Feature_9_t3: importance=0.0004, rank=2
   3. Feature_20_t3: importance=0.0004, rank=3
   4. Feature_10_t3: importance=0.0003, rank=4
   5. Feature_9_t1: importance=0.0003, rank=5

ðŸ“Š WERN Results:
  Baseline MAPE: 11.17%
  Enhanced MAPE: 10.68%
  MAPE Improvement: +0.49% (+4.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 455/464: WGO
============================================================
ðŸ“Š Loading data for WGO...
ðŸ“Š Loading data for WGO from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WGO...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WGO...

==================================================
Training Baseline WGO (SVM)
==================================================
Training SVM model...

Baseline WGO Performance:
MAE: 195630.7309
RMSE: 263218.1697
MAPE: 4.36%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 63
   â€¢ Highly important features (top 5%): 22

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0005, rank=1
   2. Feature_1_t2: importance=0.0005, rank=2
   3. Feature_1_t0: importance=0.0003, rank=3
   4. Feature_67_t2: importance=0.0002, rank=4
   5. Feature_1_t1: importance=0.0002, rank=5

ðŸ”§ Applying universal feature engineering for WGO...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WGO...

==================================================
Training Enhanced WGO (SVM)
==================================================
Training SVM model...

Enhanced WGO Performance:
MAE: 201016.5745
RMSE: 245759.9127
MAPE: 4.40%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 66
   â€¢ Highly important features (top 5%): 37

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t0: importance=0.0006, rank=1
   2. Feature_6_t3: importance=0.0004, rank=2
   3. Feature_18_t3: importance=0.0004, rank=3
   4. Feature_22_t3: importance=0.0003, rank=4
   5. Feature_10_t3: importance=0.0003, rank=5

ðŸ“Š WGO Results:
  Baseline MAPE: 4.36%
  Enhanced MAPE: 4.40%
  MAPE Improvement: -0.04% (-0.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 456/464: WOR
============================================================
ðŸ“Š Loading data for WOR...
ðŸ“Š Loading data for WOR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WOR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WOR...

==================================================
Training Baseline WOR (SVM)
==================================================
Training SVM model...

Baseline WOR Performance:
MAE: 153782.7816
RMSE: 217835.6621
MAPE: 12.30%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 21

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_1_t3: importance=0.0028, rank=1
   2. Feature_2_t0: importance=0.0013, rank=2
   3. Feature_67_t2: importance=0.0012, rank=3
   4. Feature_63_t3: importance=0.0009, rank=4
   5. Feature_67_t3: importance=0.0008, rank=5

ðŸ”§ Applying universal feature engineering for WOR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WOR...

==================================================
Training Enhanced WOR (SVM)
==================================================
Training SVM model...

Enhanced WOR Performance:
MAE: 161903.1361
RMSE: 219624.5059
MAPE: 13.13%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 77
   â€¢ Highly important features (top 5%): 48

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0034, rank=1
   2. Feature_19_t0: importance=0.0026, rank=2
   3. Feature_23_t3: importance=0.0021, rank=3
   4. Feature_14_t3: importance=0.0019, rank=4
   5. Feature_21_t2: importance=0.0019, rank=5

ðŸ“Š WOR Results:
  Baseline MAPE: 12.30%
  Enhanced MAPE: 13.13%
  MAPE Improvement: -0.83% (-6.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 457/464: WRLD
============================================================
ðŸ“Š Loading data for WRLD...
ðŸ“Š Loading data for WRLD from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing WRLD: 'WRLD'

============================================================
TESTING TICKER 458/464: WSC
============================================================
ðŸ“Š Loading data for WSC...
ðŸ“Š Loading data for WSC from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
âŒ Error testing WSC: 'WSC'

============================================================
TESTING TICKER 459/464: WSFS
============================================================
ðŸ“Š Loading data for WSFS...
ðŸ“Š Loading data for WSFS from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WSFS...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WSFS...

==================================================
Training Baseline WSFS (SVM)
==================================================
Training SVM model...

Baseline WSFS Performance:
MAE: 126740.7363
RMSE: 172328.1499
MAPE: 9.19%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 136
   â€¢ Highly important features (top 5%): 66

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0023, rank=1
   2. Feature_65_t3: importance=0.0016, rank=2
   3. Feature_63_t0: importance=0.0016, rank=3
   4. Feature_63_t3: importance=0.0013, rank=4
   5. Feature_65_t1: importance=0.0009, rank=5

ðŸ”§ Applying universal feature engineering for WSFS...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WSFS...

==================================================
Training Enhanced WSFS (SVM)
==================================================
Training SVM model...

Enhanced WSFS Performance:
MAE: 151818.8729
RMSE: 183503.8752
MAPE: 11.29%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 80
   â€¢ Highly important features (top 5%): 57

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_23_t3: importance=0.0025, rank=1
   2. Feature_14_t3: importance=0.0025, rank=2
   3. Feature_6_t3: importance=0.0025, rank=3
   4. Feature_4_t1: importance=0.0024, rank=4
   5. Feature_19_t3: importance=0.0023, rank=5

ðŸ“Š WSFS Results:
  Baseline MAPE: 9.19%
  Enhanced MAPE: 11.29%
  MAPE Improvement: -2.10% (-22.8%)
  Features: 68 -> 25

============================================================
TESTING TICKER 460/464: WSR
============================================================
ðŸ“Š Loading data for WSR...
ðŸ“Š Loading data for WSR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WSR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WSR...

==================================================
Training Baseline WSR (SVM)
==================================================
Training SVM model...

Baseline WSR Performance:
MAE: 100183.0432
RMSE: 126098.3510
MAPE: 14.58%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 128
   â€¢ Highly important features (top 5%): 80

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t2: importance=0.0030, rank=1
   2. Feature_67_t3: importance=0.0030, rank=2
   3. Feature_0_t3: importance=0.0018, rank=3
   4. Feature_1_t2: importance=0.0017, rank=4
   5. Feature_2_t3: importance=0.0014, rank=5

ðŸ”§ Applying universal feature engineering for WSR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WSR...

==================================================
Training Enhanced WSR (SVM)
==================================================
Training SVM model...

Enhanced WSR Performance:
MAE: 114256.9002
RMSE: 146073.1978
MAPE: 17.64%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 87
   â€¢ Highly important features (top 5%): 73

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_9_t3: importance=0.0028, rank=1
   2. Feature_6_t3: importance=0.0027, rank=2
   3. Feature_12_t0: importance=0.0024, rank=3
   4. Feature_6_t1: importance=0.0020, rank=4
   5. Feature_23_t3: importance=0.0017, rank=5

ðŸ“Š WSR Results:
  Baseline MAPE: 14.58%
  Enhanced MAPE: 17.64%
  MAPE Improvement: -3.05% (-20.9%)
  Features: 68 -> 25

============================================================
TESTING TICKER 461/464: WWW
============================================================
ðŸ“Š Loading data for WWW...
ðŸ“Š Loading data for WWW from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for WWW...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for WWW...

==================================================
Training Baseline WWW (SVM)
==================================================
Training SVM model...

Baseline WWW Performance:
MAE: 650058.4307
RMSE: 910755.0530
MAPE: 11.57%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 102
   â€¢ Highly important features (top 5%): 56

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_65_t2: importance=0.0008, rank=1
   2. Feature_65_t1: importance=0.0007, rank=2
   3. Feature_0_t3: importance=0.0006, rank=3
   4. Feature_67_t2: importance=0.0005, rank=4
   5. Feature_65_t3: importance=0.0005, rank=5

ðŸ”§ Applying universal feature engineering for WWW...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for WWW...

==================================================
Training Enhanced WWW (SVM)
==================================================
Training SVM model...

Enhanced WWW Performance:
MAE: 613337.1206
RMSE: 884485.2357
MAPE: 11.18%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 39

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_13_t0: importance=0.0009, rank=1
   2. Feature_13_t3: importance=0.0008, rank=2
   3. Feature_19_t3: importance=0.0007, rank=3
   4. Feature_23_t3: importance=0.0007, rank=4
   5. Feature_16_t1: importance=0.0007, rank=5

ðŸ“Š WWW Results:
  Baseline MAPE: 11.57%
  Enhanced MAPE: 11.18%
  MAPE Improvement: +0.39% (+3.4%)
  Features: 68 -> 25

============================================================
TESTING TICKER 462/464: XHR
============================================================
ðŸ“Š Loading data for XHR...
ðŸ“Š Loading data for XHR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for XHR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for XHR...

==================================================
Training Baseline XHR (SVM)
==================================================
Training SVM model...

Baseline XHR Performance:
MAE: 336052.8645
RMSE: 438831.2858
MAPE: 8.79%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 168
   â€¢ Highly important features (top 5%): 76

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_67_t3: importance=0.0053, rank=1
   2. Feature_1_t3: importance=0.0025, rank=2
   3. Feature_65_t1: importance=0.0008, rank=3
   4. Feature_2_t0: importance=0.0006, rank=4
   5. Feature_65_t0: importance=0.0006, rank=5

ðŸ”§ Applying universal feature engineering for XHR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for XHR...

==================================================
Training Enhanced XHR (SVM)
==================================================
Training SVM model...

Enhanced XHR Performance:
MAE: 395803.1360
RMSE: 482146.0889
MAPE: 10.78%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 75
   â€¢ Highly important features (top 5%): 51

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0053, rank=1
   2. Feature_13_t3: importance=0.0053, rank=2
   3. Feature_23_t3: importance=0.0053, rank=3
   4. Feature_6_t3: importance=0.0049, rank=4
   5. Feature_1_t3: importance=0.0035, rank=5

ðŸ“Š XHR Results:
  Baseline MAPE: 8.79%
  Enhanced MAPE: 10.78%
  MAPE Improvement: -1.99% (-22.6%)
  Features: 68 -> 25

============================================================
TESTING TICKER 463/464: XNCR
============================================================
ðŸ“Š Loading data for XNCR...
ðŸ“Š Loading data for XNCR from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for XNCR...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for XNCR...

==================================================
Training Baseline XNCR (SVM)
==================================================
Training SVM model...

Baseline XNCR Performance:
MAE: 411197.2744
RMSE: 490093.7339
MAPE: 7.23%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 105
   â€¢ Highly important features (top 5%): 52

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_63_t3: importance=0.0005, rank=1
   2. Feature_67_t3: importance=0.0003, rank=2
   3. Feature_65_t3: importance=0.0001, rank=3
   4. Feature_66_t3: importance=0.0001, rank=4
   5. Feature_64_t0: importance=0.0001, rank=5

ðŸ”§ Applying universal feature engineering for XNCR...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for XNCR...

==================================================
Training Enhanced XNCR (SVM)
==================================================
Training SVM model...

Enhanced XNCR Performance:
MAE: 422362.2413
RMSE: 521421.1504
MAPE: 7.63%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 50
   â€¢ Highly important features (top 5%): 36

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_19_t3: importance=0.0006, rank=1
   2. Feature_23_t3: importance=0.0005, rank=2
   3. Feature_13_t1: importance=0.0005, rank=3
   4. Feature_13_t3: importance=0.0005, rank=4
   5. Feature_7_t2: importance=0.0001, rank=5

ðŸ“Š XNCR Results:
  Baseline MAPE: 7.23%
  Enhanced MAPE: 7.63%
  MAPE Improvement: -0.40% (-5.5%)
  Features: 68 -> 25

============================================================
TESTING TICKER 464/464: YELP
============================================================
ðŸ“Š Loading data for YELP...
ðŸ“Š Loading data for YELP from parquet file...
Loading parquet file: ../data/price_data_multiindex_20250904_113138.parquet
Parquet file loaded. Shape: (6499, 46544)
Date range: 1999-11-01 00:00:00 to 2025-09-03 00:00:00
SI dates range: 2017-12-29 00:00:00 to 2025-07-15 00:00:00
Number of SI observations: 182
Extracting OHLC data for YELP...
Retrieved price data for 1918 trading days
Price data date range: 2017-11-29 00:00:00 to 2025-07-18 00:00:00
Creating price features with 15 days lookback...
Price features shape: (182, 60)
Combined features shape: (182, 68)
Training data shape: (106, 4, 68)
Validation data shape: (36, 4, 68)
Test data shape: (36, 4, 68)
âœ… Data loaded: Train+Val=(142, 4, 68), Test=(36, 4, 68)

ðŸŽ¯ Training baseline model for YELP...

==================================================
Training Baseline YELP (SVM)
==================================================
Training SVM model...

Baseline YELP Performance:
MAE: 296149.7940
RMSE: 363000.2339
MAPE: 7.86%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 272
   â€¢ Important features (top 10%): 211
   â€¢ Highly important features (top 5%): 121

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_2_t2: importance=0.0004, rank=1
   2. Feature_64_t2: importance=0.0003, rank=2
   3. Feature_63_t1: importance=0.0003, rank=3
   4. Feature_65_t0: importance=0.0003, rank=4
   5. Feature_64_t1: importance=0.0003, rank=5

ðŸ”§ Applying universal feature engineering for YELP...
âœ… Claude API client initialized successfully!
âœ… Feature selection applied successfully on attempt 1
âœ… Feature selection applied successfully on attempt 1
Feature engineering: (142, 4, 68) -> (142, 4, 25)

ðŸš€ Training enhanced model for YELP...

==================================================
Training Enhanced YELP (SVM)
==================================================
Training SVM model...

Enhanced YELP Performance:
MAE: 291927.6693
RMSE: 359449.8960
MAPE: 7.74%

ðŸ“Š Calculating SVM feature importance...
ðŸ“ˆ SVM Feature Importance Analysis:
   â€¢ Total features: 100
   â€¢ Important features (top 10%): 81
   â€¢ Highly important features (top 5%): 74

ðŸ” TOP 5 MOST IMPORTANT FEATURES:
   1. Feature_11_t2: importance=0.0006, rank=1
   2. Feature_1_t2: importance=0.0005, rank=2
   3. Feature_10_t2: importance=0.0005, rank=3
   4. Feature_22_t1: importance=0.0004, rank=4
   5. Feature_20_t2: importance=0.0004, rank=5

ðŸ“Š YELP Results:
  Baseline MAPE: 7.86%
  Enhanced MAPE: 7.74%
  MAPE Improvement: +0.12% (+1.6%)
  Features: 68 -> 25

================================================================================
UNIVERSAL FEATURE ENGINEERING PERFORMANCE REPORT
================================================================================

ðŸ“Š SUMMARY STATISTICS:
  Total tickers tested: 464
  Successful tests: 391
  Failed tests: 73
  Failed tickers: AAP, ACAD, ADMA, ADNT, AIN, APLE, AXL, BANF, BFS, BLFS, BOX, BTU, CABO, CALM, CARG, CARS, CASH, CLB, CPRX, DVAX, DXC, EFC, EXPI, EYE, FBK, FDP, FUN, GOGO, HCC, HCI, IAC, IIPR, INSW, JBGS, JJSF, KAR, KREF, KRYS, LPG, MC, MKTX, MRTN, MXL, NPK, NWL, PAHC, PENN, PFBC, PLUS, POWL, PTGX, QDEL, QTWO, REX, SAFE, SKT, SKY, SMPL, SNDR, TDS, TGTX, TNDM, TR, TRNO, TRST, TRUP, UFPT, UNIT, VBTX, VCYT, VRTS, WRLD, WSC

ðŸŽ¯ MAPE IMPROVEMENT STATISTICS:
  Average MAPE improvement: 0.21%
  Median MAPE improvement: 0.13%
  Std deviation: 1.59%
  Min improvement: -5.78%
  Max improvement: 7.40%

ðŸ“ˆ RELATIVE IMPROVEMENT STATISTICS:
  Average relative improvement: 1.0%
  Median relative improvement: 1.4%
  Std deviation: 16.0%

ðŸ† IMPROVEMENT DISTRIBUTION:
  Tickers with positive improvement: 221/391 (56.5%)
  Tickers with >0.5% improvement: 143/391 (36.6%)

ðŸ“‹ DETAILED RESULTS:
----------------------------------------------------------------------------------------------------
Ticker   Baseline MAPE Enhanced MAPE Improvement  Rel. Imp.  Features    
----------------------------------------------------------------------------------------------------
AAT      15.57        14.06         1.51         9.7        68->25
ABCB     12.32        12.02         0.31         2.5        68->25
ABG      4.91         4.74          0.17         3.4        68->25
ABM      12.59        14.74         -2.15        -17.0      68->25
ABR      4.80         7.50          -2.70        -56.3      68->25
ACHC     8.13         8.16          -0.04        -0.4       68->25
ACIW     6.29         5.68          0.61         9.7        68->25
ACLS     9.23         7.68          1.55         16.8       68->25
ADUS     10.50        12.18         -1.68        -16.0      68->25
AEIS     7.86         4.84          3.02         38.4       68->25
AEO      10.98        10.89         0.09         0.8        68->25
AGO      13.17        10.26         2.90         22.0       68->25
AGYS     11.99        12.29         -0.30        -2.5       68->25
AHH      15.36        15.24         0.12         0.8        68->25
AIR      16.94        12.54         4.40         26.0       68->25
AKR      21.80        20.85         0.95         4.4        68->25
AL       12.81        12.33         0.47         3.7        68->25
ALEX     12.78        12.72         0.06         0.5        68->25
ALG      8.35         9.67          -1.33        -15.9      68->25
ALGT     12.07        11.90         0.17         1.4        68->25
ALKS     8.21         8.77          -0.55        -6.7       68->25
ALRM     7.25         6.70          0.55         7.6        68->25
AMN      13.26        9.22          4.05         30.5       68->25
AMPH     5.26         6.87          -1.61        -30.7      68->25
AMSF     15.86        17.09         -1.23        -7.8       68->25
AMWD     11.34        15.16         -3.82        -33.7      68->25
ANDE     12.75        14.04         -1.29        -10.1      68->25
ANGI     26.85        31.18         -4.33        -16.1      68->25
ANIP     13.90        13.01         0.88         6.4        68->25
AOSL     8.18         10.59         -2.41        -29.5      68->25
APAM     7.07         8.41          -1.34        -19.0      68->25
APOG     11.76        11.05         0.71         6.0        68->25
ARCB     9.21         8.71          0.50         5.5        68->25
ARI      8.75         9.12          -0.37        -4.2       68->25
AROC     9.21         9.12          0.09         1.0        68->25
ARR      16.37        15.42         0.95         5.8        68->25
ARWR     5.79         5.67          0.12         2.1        68->25
ASIX     9.06         11.04         -1.98        -21.9      68->25
ASTE     12.04        12.24         -0.20        -1.7       68->25
ATEN     16.81        17.93         -1.11        -6.6       68->25
ATGE     14.28        14.84         -0.56        -4.0       68->25
AVA      7.74         7.61          0.13         1.7        68->25
AWI      18.48        15.60         2.89         15.6       68->25
AWR      11.04        12.12         -1.08        -9.8       68->25
AZZ      12.63        12.45         0.18         1.4        68->25
BANC     13.01        8.70          4.31         33.1       68->25
BANR     9.19         8.79          0.39         4.3        68->25
BCC      12.41        13.57         -1.15        -9.3       68->25
BCPC     8.86         9.05          -0.18        -2.1       68->25
BDN      5.83         6.01          -0.18        -3.1       68->25
BHE      12.90        10.84         2.06         15.9       68->25
BJRI     5.97         7.34          -1.37        -22.9      68->25
BKE      8.10         7.00          1.10         13.5       68->25
BKU      9.31         8.44          0.88         9.4        68->25
BL       9.11         9.22          -0.11        -1.2       68->25
BLMN     12.36        9.92          2.44         19.8       68->25
BMI      5.87         4.80          1.07         18.2       68->25
BOH      5.63         5.23          0.39         7.0        68->25
BOOT     11.75        10.47         1.28         10.9       68->25
BRC      21.03        19.75         1.29         6.1        68->25
BWA      9.00         10.16         -1.15        -12.8      68->25
BXMT     8.65         9.43          -0.78        -9.0       68->25
CAKE     5.18         4.98          0.20         3.8        68->25
CAL      8.68         8.61          0.07         0.8        68->25
CALX     9.00         8.51          0.48         5.4        68->25
CATY     8.78         8.94          -0.17        -1.9       68->25
CBRL     11.69        11.39         0.30         2.5        68->25
CBU      5.84         6.24          -0.40        -6.9       68->25
CC       6.83         5.82          1.01         14.8       68->25
CCOI     9.99         8.15          1.84         18.4       68->25
CCS      10.10        11.26         -1.17        -11.6      68->25
CE       9.41         8.36          1.05         11.2       68->25
CENT     7.92         6.75          1.17         14.8       68->25
CENTA    11.94        11.71         0.22         1.9        68->25
CENX     9.73         8.39          1.33         13.7       68->25
CEVA     9.61         10.04         -0.44        -4.6       68->25
CFFN     5.60         6.66          -1.07        -19.1      68->25
CHCO     2.66         4.53          -1.87        -70.4      68->25
CHEF     6.68         6.34          0.34         5.2        68->25
CNK      7.50         4.94          2.56         34.2       68->25
CNMD     7.19         7.50          -0.31        -4.3       68->25
CNS      8.95         8.56          0.39         4.4        68->25
CNXN     13.68        16.51         -2.83        -20.7      68->25
COHU     11.44        11.43         0.01         0.1        68->25
COLL     4.03         4.51          -0.48        -11.8      68->25
CORT     4.84         5.47          -0.64        -13.2      68->25
CPF      17.67        18.49         -0.81        -4.6       68->25
CPK      12.96        11.45         1.51         11.6       68->25
CRI      10.19        10.58         -0.38        -3.8       68->25
CRK      6.97         8.79          -1.82        -26.2      68->25
CRVL     13.15        8.42          4.72         35.9       68->25
CSGS     7.84         11.05         -3.22        -41.0      68->25
CTRE     17.40        16.56         0.83         4.8        68->25
CTS      11.99        9.10          2.89         24.1       68->25
CUBI     13.35        7.61          5.75         43.0       68->25
CVBF     11.82        10.59         1.22         10.4       68->25
CVCO     12.86        11.45         1.41         10.9       68->25
CVI      8.97         9.54          -0.57        -6.4       68->25
CWT      15.95        15.08         0.86         5.4        68->25
CXW      12.04        10.52         1.52         12.6       68->25
CZR      10.33        10.44         -0.11        -1.0       68->25
DAN      8.97         10.16         -1.19        -13.2      68->25
DCOM     6.17         7.42          -1.25        -20.2      68->25
DEA      17.28        18.44         -1.17        -6.8       68->25
DEI      9.06         7.28          1.78         19.6       68->25
DFIN     12.52        9.23          3.29         26.3       68->25
DGII     6.87         6.35          0.51         7.5        68->25
DIOD     8.45         7.12          1.33         15.7       68->25
DLX      3.77         3.96          -0.19        -5.2       68->25
DNOW     15.89        21.68         -5.78        -36.4      68->25
DORM     11.71        11.23         0.48         4.1        68->25
DRH      9.96         7.33          2.62         26.4       68->25
DXPE     10.40        9.92          0.48         4.6        68->25
DY       10.87        12.03         -1.16        -10.7      68->25
EAT      7.29         6.25          1.04         14.3       68->25
ECPG     6.26         4.73          1.53         24.5       68->25
EGBN     9.73         11.05         -1.33        -13.7      68->25
EIG      15.11        14.65         0.45         3.0        68->25
ENPH     8.13         7.87          0.26         3.1        68->25
ENR      6.23         7.29          -1.06        -17.0      68->25
ENVA     5.05         7.07          -2.02        -40.1      68->25
EPC      6.26         6.62          -0.36        -5.8       68->25
ESE      17.33        13.92         3.41         19.7       68->25
ETSY     7.42         5.79          1.63         21.9       68->25
EVTC     13.09        11.13         1.96         15.0       68->25
EXTR     9.46         10.57         -1.10        -11.7      68->25
EZPW     7.22         7.31          -0.09        -1.2       68->25
FBNC     8.62         7.67          0.95         11.0       68->25
FBP      20.27        17.17         3.10         15.3       68->25
FCF      10.66        11.56         -0.90        -8.4       68->25
FCPT     10.51        11.02         -0.51        -4.9       68->25
FELE     11.61        11.48         0.13         1.1        68->25
FFBC     9.41         9.52          -0.11        -1.1       68->25
FHB      7.77         9.02          -1.25        -16.1      68->25
FIZZ     5.74         4.76          0.97         17.0       68->25
FMC      6.76         6.76          0.01         0.1        68->25
FORM     9.78         8.37          1.41         14.4       68->25
FOXF     10.15        9.61          0.54         5.3        68->25
FRPT     5.66         4.88          0.78         13.8       68->25
FSS      13.59        13.42         0.17         1.3        68->25
FUL      8.85         8.59          0.25         2.9        68->25
FULT     9.13         7.40          1.74         19.0       68->25
FWRD     10.57        10.44         0.13         1.3        68->25
GBX      5.09         5.77          -0.69        -13.5      68->25
GDEN     9.22         12.58         -3.35        -36.4      68->25
GEO      13.98        12.56         1.42         10.1       68->25
GES      7.88         6.91          0.97         12.3       68->25
GFF      9.48         13.85         -4.37        -46.1      68->25
GIII     6.63         5.86          0.77         11.6       68->25
GKOS     12.83        12.29         0.54         4.2        68->25
GNL      14.08        14.15         -0.07        -0.5       68->25
GNW      9.92         9.25          0.67         6.8        68->25
GOLF     6.00         9.07          -3.07        -51.3      68->25
GPI      3.83         3.73          0.10         2.7        68->25
GRBK     10.91        10.27         0.64         5.8        68->25
GTY      11.50        11.69         -0.19        -1.7       68->25
GVA      7.13         5.69          1.44         20.2       68->25
HAFC     14.63        17.07         -2.44        -16.7      68->25
HASI     6.07         3.46          2.61         43.0       68->25
HBI      6.52         6.04          0.48         7.4        68->25
HCSG     7.61         8.14          -0.53        -6.9       68->25
HELE     15.74        8.33          7.40         47.0       68->25
HFWA     10.41        10.19         0.23         2.2        68->25
HI       10.49        10.68         -0.19        -1.8       68->25
HIW      16.20        16.25         -0.05        -0.3       68->25
HL       12.93        12.35         0.58         4.5        68->25
HLIT     14.87        9.49          5.39         36.2       68->25
HLX      12.77        10.50         2.26         17.7       68->25
HMN      15.30        15.04         0.26         1.7        68->25
HNI      11.55        10.60         0.95         8.3        68->25
HOPE     13.05        13.16         -0.10        -0.8       68->25
HP       8.30         7.24          1.06         12.8       68->25
HSII     16.24        14.39         1.84         11.4       68->25
HSTM     13.70        14.15         -0.45        -3.2       68->25
HTH      10.21        14.06         -3.86        -37.8      68->25
HTLD     10.55        10.08         0.47         4.5        68->25
HUBG     24.36        23.79         0.57         2.3        68->25
HWKN     18.48        18.56         -0.08        -0.4       68->25
HZO      7.01         6.89          0.12         1.7        68->25
IART     10.73        9.74          0.99         9.3        68->25
IBP      9.71         7.82          1.89         19.5       68->25
ICHR     11.01        11.91         -0.91        -8.2       68->25
ICUI     8.85         9.55          -0.71        -8.0       68->25
IDCC     7.22         5.80          1.42         19.7       68->25
IIIN     12.93        13.38         -0.45        -3.5       68->25
INDB     13.93        14.79         -0.86        -6.2       68->25
INN      15.27        13.86         1.41         9.2        68->25
INVA     3.72         3.71          0.02         0.4        68->25
IOSP     15.63        17.00         -1.37        -8.8       68->25
IPAR     13.56        11.02         2.54         18.7       68->25
ITGR     7.71         11.09         -3.39        -43.9      68->25
ITRI     8.00         7.85          0.16         2.0        68->25
JBLU     12.18        9.42          2.76         22.7       68->25
JBSS     16.44        15.24         1.20         7.3        68->25
JOE      9.43         9.84          -0.40        -4.3       68->25
KAI      8.88         10.02         -1.14        -12.8      68->25
KALU     13.48        13.99         -0.50        -3.7       68->25
KFY      15.99        16.47         -0.48        -3.0       68->25
KLIC     7.42         7.41          0.00         0.1        68->25
KMT      9.63         8.67          0.95         9.9        68->25
KN       11.80        11.12         0.68         5.7        68->25
KOP      13.59        15.30         -1.71        -12.6      68->25
KSS      7.00         7.15          -0.15        -2.1       68->25
KW       6.97         9.94          -2.97        -42.5      68->25
KWR      5.40         4.86          0.55         10.2       68->25
LCII     8.02         6.77          1.25         15.6       68->25
LEG      14.52        14.04         0.49         3.4        68->25
LGIH     2.85         3.29          -0.44        -15.5      68->25
LGND     10.16        10.14         0.02         0.2        68->25
LKFN     3.87         4.70          -0.83        -21.4      68->25
LMAT     8.42         7.43          0.99         11.8       68->25
LNC      7.88         6.93          0.95         12.1       68->25
LNN      7.80         9.76          -1.97        -25.2      68->25
LQDT     10.26        9.33          0.94         9.1        68->25
LRN      8.14         10.33         -2.19        -26.9      68->25
LTC      6.57         6.71          -0.14        -2.2       68->25
LXP      19.46        19.57         -0.11        -0.6       68->25
LZB      9.44         7.50          1.94         20.6       68->25
MAC      5.75         6.15          -0.40        -6.9       68->25
MAN      8.77         7.51          1.26         14.4       68->25
MARA     10.74        7.30          3.44         32.0       68->25
MATW     9.35         7.05          2.30         24.6       68->25
MATX     14.73        14.71         0.02         0.2        68->25
MCRI     19.94        20.69         -0.75        -3.8       68->25
MCY      13.25        14.24         -0.99        -7.5       68->25
MD       13.88        13.68         0.20         1.4        68->25
MDU      10.03        10.30         -0.27        -2.7       68->25
MGEE     13.37        10.81         2.56         19.2       68->25
MGPI     7.17         7.76          -0.59        -8.3       68->25
MHO      9.09         9.13          -0.04        -0.4       68->25
MMI      8.56         10.37         -1.81        -21.2      68->25
MMSI     6.75         8.74          -1.99        -29.5      68->25
MNRO     7.21         6.82          0.39         5.4        68->25
MPW      3.88         3.84          0.04         1.0        68->25
MRCY     5.29         6.00          -0.71        -13.5      68->25
MSEX     9.82         11.35         -1.53        -15.6      68->25
MTH      12.83        13.65         -0.82        -6.4       68->25
MTRN     8.67         9.16          -0.48        -5.6       68->25
MTX      9.79         11.76         -1.98        -20.2      68->25
MWA      13.56        13.62         -0.06        -0.5       68->25
MYGN     10.59        9.95          0.64         6.0        68->25
MYRG     10.69        11.46         -0.78        -7.3       68->25
NAVI     7.62         5.14          2.47         32.5       68->25
NBHC     12.59        10.80         1.79         14.2       68->25
NBTB     9.93         9.54          0.39         3.9        68->25
NEO      12.28        9.57          2.71         22.1       68->25
NEOG     10.26        7.70          2.57         25.0       68->25
NGVT     6.52         6.25          0.26         4.1        68->25
NHC      13.68        12.61         1.07         7.8        68->25
NMIH     11.17        13.99         -2.83        -25.3      68->25
NOG      6.45         6.23          0.22         3.4        68->25
NPO      11.72        10.99         0.73         6.3        68->25
NSIT     10.14        11.82         -1.68        -16.6      68->25
NTCT     12.42        13.07         -0.65        -5.2       68->25
NWBI     7.73         6.96          0.77         9.9        68->25
NWN      17.72        18.90         -1.18        -6.6       68->25
NX       13.05        13.02         0.03         0.2        68->25
NXRT     13.13        16.67         -3.54        -26.9      68->25
OFG      24.23        24.32         -0.09        -0.4       68->25
OI       9.53         9.23          0.31         3.2        68->25
OII      6.79         6.45          0.34         5.0        68->25
OMCL     10.98        8.53          2.45         22.3       68->25
OSIS     8.40         9.01          -0.61        -7.3       68->25
OTTR     4.94         5.80          -0.86        -17.4      68->25
OUT      12.35        10.29         2.06         16.7       68->25
OXM      7.85         6.50          1.35         17.2       68->25
PARR     13.04        10.86         2.17         16.7       68->25
PATK     7.08         7.35          -0.27        -3.8       68->25
PBH      9.09         8.27          0.83         9.1        68->25
PBI      9.55         9.07          0.48         5.1        68->25
PCRX     8.18         7.27          0.91         11.1       68->25
PDFS     9.71         9.94          -0.23        -2.4       68->25
PEB      7.03         7.14          -0.11        -1.6       68->25
PFS      7.22         9.77          -2.54        -35.2      68->25
PI       7.45         7.39          0.06         0.8        68->25
PINC     9.76         10.60         -0.84        -8.6       68->25
PJT      8.74         7.90          0.84         9.6        68->25
PLAB     8.58         8.39          0.19         2.3        68->25
PLAY     12.65        11.11         1.53         12.1       68->25
PLXS     10.49        12.79         -2.31        -22.0      68->25
PMT      7.90         8.19          -0.29        -3.7       68->25
PRA      13.17        12.73         0.44         3.3        68->25
PRAA     12.51        12.51         0.00         0.0        68->25
PRGS     5.73         7.72          -1.99        -34.8      68->25
PRK      8.52         10.06         -1.54        -18.1      68->25
PRLB     11.40        11.41         -0.00        -0.0       68->25
PSMT     9.38         9.68          -0.30        -3.2       68->25
PTEN     12.65        11.38         1.27         10.0       68->25
PZZA     17.48        14.85         2.62         15.0       68->25
QNST     13.19        11.87         1.32         10.0       68->25
QRVO     12.07        12.35         -0.29        -2.4       68->25
RDN      11.63        9.92          1.71         14.7       68->25
RDNT     8.04         6.26          1.78         22.2       68->25
RES      10.63        9.75          0.89         8.3        68->25
RGR      10.10        9.37          0.73         7.3        68->25
RHI      7.66         8.60          -0.94        -12.2      68->25
RHP      11.85        12.56         -0.71        -6.0       68->25
RNST     11.60        11.81         -0.21        -1.8       68->25
ROCK     14.11        12.84         1.28         9.1        68->25
ROG      11.28        10.66         0.62         5.5        68->25
RUN      6.20         5.11          1.09         17.5       68->25
RUSHA    8.24         7.76          0.48         5.8        68->25
RWT      10.25        10.40         -0.15        -1.5       68->25
SABR     7.19         7.45          -0.26        -3.6       68->25
SAFT     10.73        10.44         0.29         2.7        68->25
SAH      4.35         4.73          -0.37        -8.6       68->25
SANM     10.21        10.14         0.07         0.7        68->25
SBCF     9.49         9.67          -0.18        -1.9       68->25
SBH      9.63         9.25          0.38         4.0        68->25
SBSI     5.99         7.26          -1.27        -21.2      68->25
SCHL     14.15        13.60         0.55         3.9        68->25
SCL      11.02        9.99          1.03         9.3        68->25
SCSC     10.16        10.94         -0.78        -7.7       68->25
SCVL     7.41         5.12          2.29         30.9       68->25
SEDG     8.07         6.92          1.16         14.3       68->25
SEE      10.85        9.18          1.67         15.4       68->25
SEM      9.74         8.75          0.99         10.2       68->25
SFBS     6.65         2.98          3.67         55.2       68->25
SFNC     7.44         7.98          -0.54        -7.3       68->25
SHAK     5.85         6.21          -0.36        -6.2       68->25
SHEN     7.14         7.46          -0.31        -4.4       68->25
SHO      10.51        10.58         -0.07        -0.6       68->25
SHOO     12.07        10.66         1.40         11.6       68->25
SIG      7.37         6.09          1.28         17.4       68->25
SKYW     10.98        10.41         0.57         5.2        68->25
SLG      5.69         5.83          -0.14        -2.5       68->25
SM       8.05         8.24          -0.19        -2.4       68->25
SMP      22.83        19.75         3.08         13.5       68->25
SMTC     11.19        12.02         -0.83        -7.4       68->25
SPSC     11.54        11.98         -0.44        -3.8       68->25
SPXC     10.26        9.67          0.59         5.7        68->25
SRPT     8.84         8.51          0.33         3.8        68->25
SSTK     8.71         5.53          3.18         36.5       68->25
STAA     7.71         8.63          -0.93        -12.0      68->25
STBA     5.58         5.82          -0.24        -4.3       68->25
STC      19.21        18.93         0.29         1.5        68->25
STRA     11.87        13.00         -1.13        -9.5       68->25
STRL     11.98        8.06          3.92         32.7       68->25
SUPN     6.12         5.92          0.21         3.4        68->25
SXC      17.14        13.93         3.21         18.7       68->25
SXI      11.13        16.33         -5.21        -46.8      68->25
SXT      12.34        11.04         1.30         10.5       68->25
TBBK     4.80         5.08          -0.28        -5.9       68->25
TDC      8.92         9.91          -0.99        -11.0      68->25
TDW      7.26         8.55          -1.30        -17.9      68->25
TFX      21.75        21.31         0.44         2.0        68->25
TGNA     11.15        11.50         -0.34        -3.1       68->25
THRM     9.72         11.18         -1.46        -15.0      68->25
THS      10.60        8.51          2.10         19.8       68->25
TILE     16.86        15.70         1.16         6.9        68->25
TMP      12.59        10.88         1.71         13.6       68->25
TNC      10.19        9.16          1.03         10.1       68->25
TPH      13.54        13.38         0.16         1.2        68->25
TRIP     12.14        9.31          2.82         23.3       68->25
TRMK     8.57         8.83          -0.26        -3.0       68->25
TRN      8.94         9.03          -0.08        -0.9       68->25
TTMI     13.87        14.17         -0.30        -2.2       68->25
TWI      13.09        9.05          4.04         30.9       68->25
TWO      11.96        11.48         0.48         4.1        68->25
UCTT     9.02         8.92          0.10         1.1        68->25
UE       12.30        12.33         -0.03        -0.3       68->25
UFCS     14.49        15.39         -0.90        -6.2       68->25
UHT      26.69        26.75         -0.06        -0.2       68->25
UNF      14.07        14.73         -0.66        -4.7       68->25
UNFI     8.02         6.81          1.22         15.1       68->25
URBN     13.05        12.39         0.66         5.1        68->25
USNA     9.34         9.25          0.09         1.0        68->25
USPH     7.29         6.83          0.46         6.3        68->25
UTL      13.70        15.90         -2.20        -16.1      68->25
UVV      19.76        19.51         0.25         1.3        68->25
VCEL     10.63        10.00         0.63         6.0        68->25
VECO     6.48         6.61          -0.13        -2.0       68->25
VIAV     8.60         8.31          0.29         3.3        68->25
VICR     5.61         5.50          0.11         2.0        68->25
VIRT     11.86        11.17         0.68         5.8        68->25
VSAT     7.48         7.23          0.25         3.4        68->25
VSH      7.09         6.09          1.00         14.1       68->25
WABC     11.95        11.87         0.08         0.7        68->25
WAFD     12.48        13.58         -1.09        -8.7       68->25
WD       9.38         7.59          1.79         19.1       68->25
WDFC     3.70         3.58          0.12         3.3        68->25
WEN      12.96        14.48         -1.52        -11.7      68->25
WERN     11.17        10.68         0.49         4.4        68->25
WGO      4.36         4.40          -0.04        -0.9       68->25
WOR      12.30        13.13         -0.83        -6.8       68->25
WSFS     9.19         11.29         -2.10        -22.8      68->25
WSR      14.58        17.64         -3.05        -20.9      68->25
WWW      11.57        11.18         0.39         3.4        68->25
XHR      8.79         10.78         -1.99        -22.6      68->25
XNCR     7.23         7.63          -0.40        -5.5       68->25
YELP     7.86         7.74          0.12         1.6        68->25
----------------------------------------------------------------------------------------------------

ðŸ’¾ Detailed report saved to: cache/universal_feature_engineering_validation_report.txt
ðŸ’¾ Validation results saved to: cache/universal_feature_engineering_validation_results.pkl

ðŸŽ¯ VALIDATION SUMMARY:
  Successfully tested: 391/464 tickers
  Average MAPE improvement: 0.21%
  Tickers with positive improvement: 221/391 (56.5%)
  Tickers with >0.5% improvement: 143/391 (36.6%)

ðŸŽ‰ Complete multi-ticker process completed successfully!
Processed 12 tickers for iterative feature engineering
Generated and validated universal feature engineering code on 464 tickers

âœ… SVM Multi-ticker results:
  Processed 12 tickers for iterative engineering
  Generated universal feature engineering function
  Validated on 391 tickers

ðŸŽ‰ Examples completed!

To run the full pipeline:
  python code/main.py --config development --single-ticker AAPL
  python code/main.py --config production --tickers AAPL TSLA
  python code/main.py --config quick_test --max-tickers 3
