
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 62)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2:62]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 2:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.

Total: 1 + 1 + 60 = 62 features per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep ALL 62 raw features** and add **new features** so that **(raw + new) ≤ 85 total columns**.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 9.77%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0241), Feature_1 (importance=0.0058), Feature_52 (importance=0.0011), Feature_58 (importance=0.0011), Feature_12 (importance=0.0008)

Iteration 1: Iteration 1 - MAPE: 9.09% (Improvement Over Baseline: +0.7%) (Improvement Over Last: +0.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0323), Feature_1 (importance=0.0046), Feature_14 (importance=0.0016), Feature_17 (importance=0.0008), Feature_46 (importance=0.0007)

Iteration 2: Iteration 2 - MAPE: 7.98% (Improvement Over Baseline: +1.1%) (Improvement Over Last: +1.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0368), Feature_1 (importance=0.0078), Feature_27 (importance=0.0009), Feature_14 (importance=0.0005), Feature_25 (importance=0.0005)









PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 2):

```python
def construct_features(data):
    """
    Construct features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    RAW_DIM = 62
    MAX_TOTAL = 85
    MAX_NEW = MAX_TOTAL - RAW_DIM
    
    lookback_window = data.shape[0]
    result = []
    
    for t in range(lookback_window):
        # Keep all raw features
        raw_keep = list(data[t])
        
        # Initialize engineered features list
        eng = []
        
        # Extract short interest and volume
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        
        # Reshape OHLC data
        ohlc = data[t, 2:].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # 1. Days to Cover - consistently high importance in previous iterations
        # Measures how many days of average volume would be needed to cover all short positions
        days_to_cover = short_interest / max(avg_volume, 1e-8)
        eng.append(days_to_cover)
        
        # 2. Short Interest Change Rate - if we have previous data
        # Captures momentum in short interest changes
        si_change = 0
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
        eng.append(si_change)
        
        # 3. Short Interest Acceleration - second derivative of short interest
        # Captures acceleration/deceleration in short interest changes
        si_accel = 0
        if t > 1:
            prev_si_change = (data[t-1, 0] - data[t-2, 0]) / max(data[t-2, 0], 1e-8)
            si_accel = si_change - prev_si_change
        eng.append(si_accel)
        
        # 4. Normalized Short Interest - relative to historical levels
        # Provides context for current short interest relative to recent history
        si_history = data[:t+1, 0]
        normalized_si = short_interest / max(np.mean(si_history), 1e-8)
        eng.append(normalized_si)
        
        # 5. Short Interest Percentile Rank
        # Provides relative position of current SI in historical distribution
        si_rank = 0.5  # Default to middle if not enough history
        if t >= 4:  # Need some history to make this meaningful
            si_history = data[:t+1, 0]
            si_rank = np.sum(si_history < short_interest) / max(len(si_history), 1)
        eng.append(si_rank)
        
        # 6. Price Volatility (15-day)
        # Volatility is often correlated with short interest
        returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
        volatility = np.std(returns) if len(returns) > 1 else 0
        eng.append(volatility)
        
        # 7. Average True Range (ATR) - measure of volatility that accounts for gaps
        tr_values = []
        for i in range(1, len(close_prices)):
            high_low = high_prices[i] - low_prices[i]
            high_close_prev = abs(high_prices[i] - close_prices[i-1])
            low_close_prev = abs(low_prices[i] - close_prices[i-1])
            tr = max(high_low, high_close_prev, low_close_prev)
            tr_values.append(tr)
        atr = np.mean(tr_values) if tr_values else 0
        eng.append(atr)
        
        # 8. ATR to Price Ratio - normalized volatility
        # Makes ATR comparable across different price levels
        avg_price = np.mean(close_prices) if len(close_prices) > 0 else 1
        atr_price_ratio = atr / max(avg_price, 1e-8)
        eng.append(atr_price_ratio)
        
        # 9. RSI (Relative Strength Index) - momentum indicator
        # Improved calculation with proper up/down day separation
        up_moves = np.zeros(len(returns))
        down_moves = np.zeros(len(returns))
        
        if len(returns) > 0:
            up_moves[returns > 0] = returns[returns > 0]
            down_moves[returns < 0] = -returns[returns < 0]
            
            avg_up = np.mean(up_moves) if np.sum(up_moves > 0) > 0 else 0
            avg_down = np.mean(down_moves) if np.sum(down_moves > 0) > 0 else 0
            
            rs = avg_up / max(avg_down, 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral if not enough data
        eng.append(rsi)
        
        # 10. Price Trend (linear regression slope)
        # Trend direction can influence short interest
        if len(close_prices) > 3:  # Need at least a few points for meaningful slope
            x = np.arange(len(close_prices))
            slope, _ = np.polyfit(x, close_prices, 1) if len(close_prices) > 1 else (0, 0)
            # Normalize slope by average price to make comparable across stocks
            norm_slope = slope / max(avg_price, 1e-8)
        else:
            norm_slope = 0
        eng.append(norm_slope)
        
        # 11. Price to Moving Average Ratio - price relative to trend
        # Indicates overbought/oversold conditions
        sma_5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else np.mean(close_prices)
        price_to_sma = close_prices[-1] / max(sma_5, 1e-8)
        eng.append(price_to_sma)
        
        # 12. Volume Trend - volume changes can signal short interest changes
        # Improved calculation using actual volume trend
        vol_trend = 0
        if len(close_prices) > 5:
            # Proxy for volume trend using price-volume relationship
            vol_trend = (avg_volume - np.mean(close_prices[-5:] * avg_volume / max(close_prices[-1], 1e-8))) / max(avg_volume, 1e-8)
        eng.append(vol_trend)
        
        # 13. Bollinger Band Width - volatility measure
        # Wider bands indicate higher volatility
        if len(close_prices) > 3:
            sma_all = np.mean(close_prices)
            std_all = np.std(close_prices)
            bb_width = (2 * std_all) / max(sma_all, 1e-8)
        else:
            bb_width = 0
        eng.append(bb_width)
        
        # 14. Bollinger Band Position - where price is within the bands
        # Indicates potential mean reversion or trend continuation
        if len(close_prices) > 3:
            upper_band = sma_all + 2 * std_all
            lower_band = sma_all - 2 * std_all
            band_range = upper_band - lower_band
            if band_range > 0:
                bb_position = (close_prices[-1] - lower_band) / max(band_range, 1e-8)
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
        eng.append(bb_position)
        
        # 15. MACD Line - momentum indicator
        # Simplified but effective calculation
        if len(close_prices) >= 12:
            ema_12 = np.mean(close_prices[-12:])  # Simplified EMA
            ema_26 = np.mean(close_prices)  # Use all available as longer EMA
            macd_line = ema_12 - ema_26
            # Normalize by price
            norm_macd = macd_line / max(avg_price, 1e-8)
        else:
            norm_macd = 0
        eng.append(norm_macd)
        
        # 16. Price Gap Ratio - large gaps can indicate short interest changes
        # Improved calculation focusing on overnight gaps
        gaps = []
        for i in range(1, len(open_prices)):
            gap = abs(open_prices[i] - close_prices[i-1]) / max(close_prices[i-1], 1e-8)
            gaps.append(gap)
        avg_gap = np.mean(gaps) if gaps else 0
        eng.append(avg_gap)
        
        # 17. Close Location Value - where close price is relative to the day's range
        # Indicates buying/selling pressure within the day
        clv_values = []
        for i in range(len(close_prices)):
            if high_prices[i] == low_prices[i]:
                clv_values.append(0)
            else:
                clv = ((close_prices[i] - low_prices[i]) - (high_prices[i] - close_prices[i])) / max(high_prices[i] - low_prices[i], 1e-8)
                clv_values.append(clv)
        avg_clv = np.mean(clv_values) if clv_values else 0
        eng.append(avg_clv)
        
        # 18. Price Momentum - rate of price change
        # Captures recent price direction and strength
        momentum = (close_prices[-1] / max(close_prices[0], 1e-8) - 1) if len(close_prices) > 1 else 0
        eng.append(momentum)
        
        # 19. Volume-Adjusted Price Change - price moves weighted by volume
        # Gives more weight to price moves with higher volume
        vol_adj_price_change = momentum * avg_volume
        eng.append(vol_adj_price_change)
        
        # 20. Chaikin Money Flow - volume-weighted accumulation/distribution
        # Indicates buying/selling pressure with volume confirmation
        cmf_values = []
        for i in range(len(close_prices)):
            if high_prices[i] == low_prices[i]:
                mf_multiplier = 0
            else:
                mf_multiplier = ((close_prices[i] - low_prices[i]) - (high_prices[i] - close_prices[i])) / (high_prices[i] - low_prices[i])
            money_flow_volume = mf_multiplier * avg_volume
            cmf_values.append(money_flow_volume)
        cmf = np.sum(cmf_values) / max(avg_volume * len(cmf_values), 1e-8) if cmf_values else 0
        eng.append(cmf)
        
        # 21. Short Interest to Free Float Ratio (approximated by volume)
        # Measures how much of available float is being shorted
        si_float_ratio = short_interest / max(avg_volume * 15, 1e-8)  # 15 days of volume as proxy for float
        eng.append(si_float_ratio)
        
        # 22. Recent Price Reversal - captures potential short covering events
        # Short covering often happens after price reversals
        price_reversal = 0
        if len(close_prices) > 5:
            # Check if recent price movement is opposite to the previous trend
            recent_trend = close_prices[-1] - close_prices[-3]
            previous_trend = close_prices[-3] - close_prices[-5]
            if previous_trend != 0:
                price_reversal = -1 * (recent_trend * previous_trend) / max(abs(previous_trend), 1e-8)
        eng.append(price_reversal)
        
        # 23. Short Interest to Volatility Ratio
        # Relates short interest to market volatility
        si_vol_ratio = short_interest / max(volatility * avg_price, 1e-8)
        eng.append(si_vol_ratio)
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result.append(row)
    
    # Stack all rows and handle NaN values
    result_array = np.stack(result)
    result_array = np.nan_to_num(result_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result_array
```

Performance of this code: MAPE = 7.98%
Improvement over previous: +1.11%
Statistical Analysis: 2/62 features were significant (p < 0.05), 2 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 3):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 7.98%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, but keep all raw channels this time.
- Use **financial domain knowledge** for engineered features.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 62`
  - `MAX_TOTAL = 85`
  - `MAX_NEW = MAX_TOTAL - RAW_DIM`
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Start with `raw_keep = list(data[t])` (this includes **all 62 raw features**).
  2) Build `eng = []` for engineered features.
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 62+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve all 62 raw features**.
   - Add **new, diverse features** while enforcing **(raw + new) ≤ 85**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
