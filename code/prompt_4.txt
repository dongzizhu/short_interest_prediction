
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 62)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2:62]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 2:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.

Total: 1 + 1 + 60 = 62 features per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 25 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy after the second iteration.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 7.37%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_6_t3 (importance=0.0009), Feature_1_t3 (importance=0.0009), Feature_35_t3 (importance=0.0007), Feature_3_t3 (importance=0.0007), Feature_0_t0 (importance=0.0006)

Iteration 1: Iteration 1 - MAPE: 8.67% (Improvement Over Baseline: -1.3%) (Improvement Over Last: -1.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_6_t3 (importance=0.0079), Feature_14_t1 (importance=0.0042), Feature_1_t1 (importance=0.0035), Feature_18_t0 (importance=0.0027), Feature_11_t0 (importance=0.0025)

Iteration 2: Iteration 2 - MAPE: 9.52% (Improvement Over Baseline: -2.1%) (Improvement Over Last: -0.8%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_10_t3 (importance=0.0144), Feature_17_t3 (importance=0.0089), Feature_9_t3 (importance=0.0049), Feature_2_t1 (importance=0.0044), Feature_16_t1 (importance=0.0039)

Iteration 3: Iteration 3 - MAPE: 8.22% (Improvement Over Baseline: -0.8%) (Improvement Over Last: +1.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_17_t2 (importance=0.0072), Feature_15_t0 (importance=0.0049), Feature_8_t3 (importance=0.0032), Feature_19_t3 (importance=0.0030), Feature_15_t1 (importance=0.0028)









PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 3):

```python
def construct_features(data):
    RAW_DIM = 62
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Extract raw data for this timestep
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        ohlc = data[t, 2:].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep essential raw features based on importance analysis
        # Focusing on the most recent price data which showed higher importance
        raw_keep = [
            short_interest,  # Always keep short interest (highest importance)
            avg_volume,      # Always keep average volume (high importance)
            close_prices[-1],  # Most recent close price
            high_prices[-1],   # Most recent high price
            low_prices[-1],    # Most recent low price
        ]
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short Interest Specific Features (highest importance in previous iterations)
        
        # Short interest to volume ratio (consistently high importance)
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # Days to cover (short interest / avg daily volume) - key metric for short squeeze potential
        days_to_cover = short_interest / max(avg_volume / 15, 1e-8)  # Divide by 15 to get daily avg
        eng.append(days_to_cover)
        
        # Short interest momentum (change over time if we have previous data)
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            eng.append(si_change)
        else:
            eng.append(0.0)  # No previous data
        
        # 2. Price Action Features (refined based on importance analysis)
        
        # Recent price trend (ratio of latest close to average of previous 5 days)
        if len(close_prices) >= 5:
            avg_price_5d = np.mean(close_prices[-5:])
            price_trend = close_prices[-1] / max(avg_price_5d, 1e-8) - 1.0
            eng.append(price_trend)
        else:
            eng.append(0.0)
        
        # Price volatility: normalized true range over last 5 days
        if len(close_prices) >= 5:
            true_ranges = []
            for i in range(-5, 0):
                if i > -len(close_prices):
                    if i == -len(close_prices) + 1:
                        tr = high_prices[i] - low_prices[i]
                    else:
                        tr1 = high_prices[i] - low_prices[i]
                        tr2 = abs(high_prices[i] - close_prices[i-1])
                        tr3 = abs(low_prices[i] - close_prices[i-1])
                        tr = max(tr1, tr2, tr3)
                    true_ranges.append(tr)
            
            avg_tr = np.mean(true_ranges) if true_ranges else 0
            normalized_atr = avg_tr / max(close_prices[-1], 1e-8)
            eng.append(normalized_atr)
        else:
            eng.append(0.0)
        
        # 3. Volume-Price Relationship Features
        
        # Volume trend (ratio of recent volume to longer-term average)
        volume_trend = avg_volume / max(np.mean(data[max(0, t-3):t+1, 1]) if t > 0 else avg_volume, 1e-8)
        eng.append(volume_trend)
        
        # Price-volume correlation (simplified)
        if len(close_prices) >= 5 and t > 0:
            # Use previous volumes as proxy
            prev_volumes = data[max(0, t-5):t+1, 1]
            price_changes = np.diff(close_prices[-min(6, len(close_prices)):])
            
            # Ensure equal lengths for correlation
            min_len = min(len(prev_volumes), len(price_changes))
            if min_len >= 2:
                # Calculate sign correlation (direction alignment)
                vol_signs = np.sign(np.diff(prev_volumes[-min_len:]))
                price_signs = np.sign(price_changes[-min_len+1:])
                sign_match = np.mean(vol_signs == price_signs)
                eng.append(sign_match)
            else:
                eng.append(0.5)  # Neutral value
        else:
            eng.append(0.5)
        
        # 4. Technical Indicators (refined based on importance)
        
        # RSI (14-day) - improved implementation
        if len(close_prices) >= 14:
            deltas = np.diff(close_prices[-14:])
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            # RSI distance from neutral (50) - more informative than raw RSI
            rsi_dist_from_neutral = abs(rsi - 50) / 50
            eng.append(rsi_dist_from_neutral)
        else:
            eng.append(0.0)
        
        # MACD signal line crossover (simplified)
        if len(close_prices) >= 12:
            # Simple implementation using SMA instead of EMA for stability
            fast_ma = np.mean(close_prices[-6:])
            slow_ma = np.mean(close_prices[-12:])
            macd = (fast_ma - slow_ma) / max(slow_ma, 1e-8)
            eng.append(macd)
        else:
            eng.append(0.0)
        
        # 5. Price Pattern Features
        
        # Candlestick pattern: Doji detection (open close proximity relative to range)
        if len(close_prices) >= 1:
            latest_range = high_prices[-1] - low_prices[-1]
            open_close_diff = abs(open_prices[-1] - close_prices[-1])
            doji_score = 1.0 - (open_close_diff / max(latest_range, 1e-8))
            eng.append(doji_score)
        else:
            eng.append(0.0)
        
        # Momentum oscillator (simplified)
        if len(close_prices) >= 10:
            momentum = (close_prices[-1] / max(close_prices[-10], 1e-8) - 1.0)
            # Normalize to typical range
            norm_momentum = np.clip(momentum / 0.1, -1.0, 1.0)
            eng.append(norm_momentum)
        else:
            eng.append(0.0)
        
        # 6. Advanced Short Interest Predictors
        
        # Short interest relative to recent price movement
        if len(close_prices) >= 5:
            price_range = np.max(high_prices[-5:]) - np.min(low_prices[-5:])
            si_price_movement_ratio = short_interest / max(price_range * avg_volume, 1e-8)
            eng.append(si_price_movement_ratio)
        else:
            eng.append(0.0)
        
        # Short interest concentration (relative to market cap proxy)
        market_cap_proxy = close_prices[-1] * avg_volume * 20  # Rough estimate
        si_concentration = short_interest / max(market_cap_proxy, 1e-8)
        eng.append(si_concentration)
        
        # 7. Specialized Predictive Features
        
        # Price gap analysis (overnight gaps as volatility indicator)
        if len(close_prices) >= 5 and len(open_prices) >= 5:
            gaps = []
            for i in range(-5, 0):
                if i > -len(close_prices)+1:
                    gap = (open_prices[i] - close_prices[i-1]) / max(close_prices[i-1], 1e-8)
                    gaps.append(abs(gap))
            
            avg_gap = np.mean(gaps) if gaps else 0
            eng.append(avg_gap)
        else:
            eng.append(0.0)
        
        # Bollinger Band position (where price is within the bands)
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            # Position within bands (-1 to +1 scale)
            bb_position = (close_prices[-1] - sma) / max(std, 1e-8)
            # Normalize to typical range
            bb_position = np.clip(bb_position / 2, -1.0, 1.0)
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # 8. Mean Reversion vs Trend Following Features
        
        # Mean reversion potential (distance from moving average)
        if len(close_prices) >= 10:
            ma10 = np.mean(close_prices[-10:])
            dist_from_ma = (close_prices[-1] / max(ma10, 1e-8)) - 1.0
            # Normalize to typical range
            norm_dist = np.clip(dist_from_ma / 0.05, -1.0, 1.0)
            eng.append(norm_dist)
        else:
            eng.append(0.0)
        
        # Trend strength indicator (simplified ADX)
        if len(close_prices) >= 10:
            up_moves = []
            down_moves = []
            
            for i in range(-9, 0):
                if i > -len(close_prices)+1:
                    up_move = high_prices[i] - high_prices[i-1]
                    down_move = low_prices[i-1] - low_prices[i]
                    
                    up_moves.append(max(up_move, 0))
                    down_moves.append(max(down_move, 0))
            
            avg_up = np.mean(up_moves) if up_moves else 0
            avg_down = np.mean(down_moves) if down_moves else 0
            
            # Directional movement index
            di_diff = abs(avg_up - avg_down)
            di_sum = avg_up + avg_down
            trend_strength = di_diff / max(di_sum, 1e-8)
            eng.append(trend_strength)
        else:
            eng.append(0.0)
        
        # 9. Volatility Features
        
        # Historical volatility (annualized)
        if len(close_prices) >= 10:
            returns = np.diff(np.log(close_prices[-10:]))
            vol = np.std(returns) * np.sqrt(252)  # Annualized
            # Normalize to typical range
            norm_vol = np.clip(vol / 0.5, 0.0, 1.0)
            eng.append(norm_vol)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
            
        features[t, :] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

Performance of this code: MAPE = 8.22%
Change from previous: -0.84%
Statistical Analysis: 51/100 features were significant (p < 0.05), 30 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 4):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 7.37%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 62`
  - `MAX_TOTAL = 25`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 62+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve the raw features**.
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 25**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
