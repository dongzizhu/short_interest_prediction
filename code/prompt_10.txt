
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 97)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2]` → **days to cover** The number of days it would take to cover all short positions based on average daily trading volume.
  - `data[t, 3:63]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 3:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.
  - `data[t, 63]` → **options_put_call_volume_ratio** The ratio of the volume of put options to call options traded on that day.
  - `data[t, 64]` → **options_synthetic_short_cost** The cost associated with creating a synthetic short position using options.
  - `data[t, 65]` → **options_avg_implied_volatility** The average implied volatility of options, reflecting market expectations of future stock price volatility.
  - `data[t, 66]` → **shares_outstanding** The total number of shares of a company that are currently owned by all its shareholders, including restricted shares owned by company insiders and institutional investors.
  - `data[t, 67:82]` → Daily short interest volume for the stock over the past 15 days, flattened as **15 days × 1 columns**. Represents the total number of shares that were sold short on each day.
  - `data[t, 82:97]` → Daily total trading volume for the stock over the past 15 days, flattened as **15 days × 1 columns**.  Represents the total number of shares traded (buy + sell) on that trading day.

Total: 3 + 60 + 4 + 30 = 97 features(dimensions) per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 25 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy after the second iteration.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 10.03%
  Features: All 97 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_88_t2 (importance=0.0003), Feature_91_t1 (importance=0.0003), Feature_85_t0 (importance=0.0002), Feature_84_t2 (importance=0.0002), Feature_92_t3 (importance=0.0002)

Iteration 9: Iteration 9 - MAPE: 8.63% (Improvement Over Previous Best: -0.1%) (Improvement Over Last Iteration: -0.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_5_t3 (importance=0.0008), Feature_12_t2 (importance=0.0006), Feature_17_t2 (importance=0.0005), Feature_17_t3 (importance=0.0004), Feature_12_t0 (importance=0.0004)







PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 9):

```python
def construct_features(data):
    RAW_DIM = 97
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    result = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Extract key raw features
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        days_to_cover = data[t, 2]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract options data
        options_put_call_ratio = data[t, 63]
        options_synthetic_short_cost = data[t, 64]
        options_avg_implied_volatility = data[t, 65]
        
        # Extract shares outstanding
        shares_outstanding = data[t, 66]
        
        # Extract short volume and total volume data
        short_volume = data[t, 67:82]
        total_volume = data[t, 82:97]
        
        # Keep critical raw features based on previous importance analysis
        # Feature_18_t3, Feature_16_t1, Feature_5_t3, Feature_18_t2, Feature_8_t3 were top performers
        raw_keep.extend([
            short_interest,                  # Always keep short interest (critical target-related feature)
            avg_volume,                      # Always keep average volume (critical liquidity indicator)
            days_to_cover,                   # Important for short interest dynamics
            close_prices[-1],                # Most recent close price
            options_put_call_ratio,          # Important options sentiment indicator
            options_avg_implied_volatility,  # Market's expectation of volatility
        ])
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Interest to Shares Outstanding Ratio
        # Normalized measure of short interest relative to total shares
        # High importance in previous iterations (Feature_18_t3 was top performer)
        si_ratio = short_interest / max(shares_outstanding, 1e-8)
        eng.append(si_ratio)
        
        # 2. Short Volume Ratio (average of daily short volume / total volume)
        # Daily shorting intensity - high importance in previous iterations
        daily_short_ratio = short_volume / np.maximum(total_volume, 1e-8)
        avg_short_ratio = np.mean(daily_short_ratio)
        eng.append(avg_short_ratio)
        
        # 3. Short Interest to Implied Volatility Ratio
        # Relates short interest to market's expectation of volatility
        # This was Feature_18_t2 in previous iteration - high importance
        si_iv_ratio = short_interest / max(options_avg_implied_volatility, 1e-8)
        eng.append(si_iv_ratio)
        
        # 4. Short Interest to Price Ratio
        # Normalizes short interest by current price level
        # This was Feature_16_t1 in previous iteration - high importance
        si_price_ratio = short_interest / max(close_prices[-1], 1e-8)
        eng.append(si_price_ratio)
        
        # 5. Short Interest Growth Rate - high importance in previous iteration
        si_growth = 0
        if t > 0 and data[t-1, 0] > 0:
            si_growth = (short_interest / max(data[t-1, 0], 1e-8)) - 1
        eng.append(si_growth)
        
        # 6. Short Interest to Volume Ratio (Days to Cover alternative)
        # Measures how many days of average volume the short interest represents
        # This was Feature_5_t3 in previous iteration - high importance
        si_volume_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_volume_ratio)
        
        # 7. Recent Short Volume Trend (last 5 days vs previous 10)
        # Captures acceleration in shorting activity
        if len(daily_short_ratio) >= 10:
            recent_short_ratio = np.mean(daily_short_ratio[-5:])
            earlier_short_ratio = np.mean(daily_short_ratio[-10:-5])
            short_trend = recent_short_ratio / max(earlier_short_ratio, 1e-8) - 1
        else:
            short_trend = 0
        eng.append(short_trend)
        
        # 8. Volatility-Adjusted Short Interest
        # Combines volatility with short interest - better signal in volatile conditions
        # This was Feature_8_t3 in previous iteration - high importance
        vol_adj_si = 0
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            vol_adj_si = short_interest * price_volatility
        eng.append(vol_adj_si)
        
        # 9. Short Interest Acceleration (second derivative)
        # Second derivative of short interest - acceleration of short interest changes
        si_acceleration = 0
        if t >= 2 and data[t-1, 0] > 0 and data[t-2, 0] > 0:
            prev_si_growth = (data[t-1, 0] / max(data[t-2, 0], 1e-8)) - 1
            current_si_growth = (short_interest / max(data[t-1, 0], 1e-8)) - 1
            si_acceleration = current_si_growth - prev_si_growth
        eng.append(si_acceleration)
        
        # 10. NEW: Short Interest Relative to Historical Range
        # Measures where current short interest sits within its recent historical range
        si_range_position = 0.5  # Default to middle if not enough history
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_min, si_max = np.min(si_history), np.max(si_history)
            si_range = si_max - si_min
            if si_range > 0:
                si_range_position = (short_interest - si_min) / max(si_range, 1e-8)
        eng.append(si_range_position)
        
        # 11. NEW: Short Interest Momentum Divergence
        # Measures divergence between short interest momentum and price momentum
        si_price_divergence = 0
        if t > 0 and len(close_prices) >= 5:
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            si_momentum = (short_interest / max(data[t-1, 0], 1e-8)) - 1 if data[t-1, 0] > 0 else 0
            si_price_divergence = si_momentum - price_momentum
        eng.append(si_price_divergence)
        
        # 12. NEW: Short Volume Concentration Index
        # Measures if short volume is concentrated in specific days (improved calculation)
        sv_concentration = 0
        if np.sum(short_volume) > 0:
            # Calculate normalized short volume (as proportion of total)
            norm_sv = short_volume / max(np.sum(short_volume), 1e-8)
            # Calculate Herfindahl-Hirschman Index (HHI) - measure of concentration
            sv_concentration = np.sum(norm_sv**2)
        eng.append(sv_concentration)
        
        # 13. NEW: Options-Adjusted Short Interest Trend
        # Combines short interest trend with options market sentiment trend
        options_adj_si_trend = 0
        if t > 0 and data[t-1, 63] > 0:
            si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1 if data[t-1, 0] > 0 else 0
            options_change = (options_put_call_ratio / max(data[t-1, 63], 1e-8)) - 1
            # Positive when both move in same direction (alignment of signals)
            options_adj_si_trend = si_change * options_change
        eng.append(options_adj_si_trend)
        
        # 14. NEW: Short Interest Efficiency Ratio
        # Measures how effectively short positions are being established relative to price movement
        si_efficiency = 0
        if len(close_prices) >= 5 and t > 0 and data[t-1, 0] > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1
            # Negative correlation is more efficient for shorts (improved calculation)
            if abs(price_change) > 1e-8:
                si_efficiency = -1 * si_change / max(abs(price_change), 1e-8)
        eng.append(si_efficiency)
        
        # 15. NEW: Short Interest to Synthetic Short Cost Ratio with Trend
        # Compares actual short interest to the cost of creating synthetic shorts, with trend component
        si_synthetic_ratio = short_interest / max(options_synthetic_short_cost, 1e-8)
        si_synthetic_trend = 0
        if t > 0 and data[t-1, 64] > 0 and data[t-1, 0] > 0:
            prev_ratio = data[t-1, 0] / max(data[t-1, 64], 1e-8)
            si_synthetic_trend = (si_synthetic_ratio / max(prev_ratio, 1e-8)) - 1
        eng.append(si_synthetic_ratio * (1 + si_synthetic_trend))
        
        # 16. NEW: Short Interest Volatility
        # Measures the volatility of short interest over time
        si_volatility = 0
        if t >= 5:
            si_history = np.array([data[max(0, t-i), 0] for i in range(5)])
            si_mean = np.mean(si_history)
            if si_mean > 0:
                si_volatility = np.std(si_history) / max(si_mean, 1e-8)
        eng.append(si_volatility)
        
        # 17. NEW: Short Volume Trend Strength with Momentum
        # Measures strength and consistency of short volume trend using regression with momentum
        sv_trend_strength = 0
        if len(short_volume) >= 5:
            x = np.arange(5)
            y = short_volume[-5:]
            # Simple linear regression slope calculation
            slope = (np.mean(x*y) - np.mean(x)*np.mean(y)) / max(np.var(x), 1e-8)
            sv_trend_strength = slope / max(np.mean(y), 1e-8)  # Normalized slope
            
            # Add momentum component
            if len(short_volume) >= 10:
                prev_y = short_volume[-10:-5]
                prev_slope = (np.mean(x*prev_y) - np.mean(x)*np.mean(prev_y)) / max(np.var(x), 1e-8)
                prev_norm_slope = prev_slope / max(np.mean(prev_y), 1e-8)
                # Combine current trend with acceleration
                sv_trend_strength = sv_trend_strength * (1 + (sv_trend_strength - prev_norm_slope))
        eng.append(sv_trend_strength)
        
        # 18. NEW: Short Interest to Price Volatility Ratio
        # Relates short interest to recent price volatility
        si_price_vol_ratio = 0
        if len(close_prices) >= 5:
            price_volatility = np.std(close_prices[-5:]) / max(np.mean(close_prices[-5:]), 1e-8)
            si_price_vol_ratio = short_interest / max(price_volatility, 1e-8)
        eng.append(si_price_vol_ratio)
        
        # 19. NEW: Composite Short Pressure Index
        # Combines multiple short indicators into a single composite measure
        short_pressure = 0
        components = [
            si_ratio,                    # Short interest to shares outstanding
            avg_short_ratio,             # Average short volume ratio
            options_put_call_ratio,      # Options put/call ratio
            si_volume_ratio              # Short interest to volume ratio
        ]
        # Normalize and combine components
        short_pressure = np.mean(components)
        eng.append(short_pressure)
        
        # Ensure we don't exceed MAX_NEW engineered features
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to ensure consistent size
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result[t] = row
    
    # Handle any NaN or infinite values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result
```

Performance of this code: MAPE = 8.63%
Change from previous: -0.10%
Statistical Analysis: 77/100 features were significant (p < 0.05), 66 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 10):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 8.51%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 97`
  - `MAX_TOTAL = 25`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 97+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 97) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve the useful raw features**.
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 25**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
