
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 62)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2:62]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 2:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.

Total: 1 + 1 + 60 = 62 features per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 85 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 8.33%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0005), Feature_1 (importance=0.0003), Feature_54 (importance=0.0002), Feature_31 (importance=0.0001), Feature_53 (importance=0.0001)

Iteration 1: Iteration 1 - MAPE: 10.23% (Improvement Over Baseline: -1.9%) (Improvement Over Last: -1.9%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_23 (importance=0.0039), Feature_13 (importance=0.0028), Feature_24 (importance=0.0012), Feature_8 (importance=0.0011), Feature_5 (importance=0.0009)

Iteration 2: Iteration 2 - MAPE: 8.91% (Improvement Over Baseline: -0.6%) (Improvement Over Last: +1.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_14 (importance=0.0029), Feature_9 (importance=0.0017), Feature_3 (importance=0.0013), Feature_18 (importance=0.0013), Feature_41 (importance=0.0012)

Iteration 3: Iteration 3 - MAPE: 10.11% (Improvement Over Baseline: -1.8%) (Improvement Over Last: -1.2%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_45 (importance=0.0010), Feature_38 (importance=0.0006), Feature_46 (importance=0.0006), Feature_1 (importance=0.0005), Feature_49 (importance=0.0005)

Iteration 4: Iteration 4 - MAPE: 12.37% (Improvement Over Baseline: -4.0%) (Improvement Over Last: -2.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_35 (importance=0.0012), Feature_34 (importance=0.0009), Feature_19 (importance=0.0005), Feature_31 (importance=0.0005), Feature_38 (importance=0.0004)









PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 4):

```python
def construct_features(data):
    RAW_DIM = 62
    MAX_TOTAL = 80
    
    lookback_window = data.shape[0]
    result = []
    
    for t in range(lookback_window):
        # Initialize lists for raw features to keep and engineered features
        raw_keep = []
        eng = []
        
        # Always keep short interest and average volume (consistently important)
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        raw_keep.extend([short_interest, avg_volume])
        
        # Reshape OHLC data for easier access
        ohlc = data[t, 2:].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep only the most recent OHLC values (last 5 days)
        # Based on feature importance analysis showing recent price data matters most
        if len(close_prices) >= 5:
            raw_keep.extend(close_prices[-5:].tolist())  # Last 5 closes
            raw_keep.append(open_prices[-1])  # Most recent open
            raw_keep.append(high_prices[-1])  # Most recent high
            raw_keep.append(low_prices[-1])   # Most recent low
        else:
            # Pad with zeros if not enough data
            raw_keep.extend(close_prices.tolist())
            padding_needed = 5 - len(close_prices)
            raw_keep.extend([0] * padding_needed)
            
            if len(open_prices) > 0:
                raw_keep.append(open_prices[-1])
            else:
                raw_keep.append(0)
                
            if len(high_prices) > 0:
                raw_keep.append(high_prices[-1])
            else:
                raw_keep.append(0)
                
            if len(low_prices) > 0:
                raw_keep.append(low_prices[-1])
            else:
                raw_keep.append(0)
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short interest relative metrics (focus on target variable)
        # Short interest to volume ratio (key relationship that showed high importance)
        si_vol_ratio = short_interest / max(avg_volume, 1e-8)
        eng.append(si_vol_ratio)
        
        # Days to cover (short interest / daily volume) - important for short squeeze potential
        days_to_cover = short_interest / max(avg_volume, 1e-8)
        eng.append(days_to_cover)
        
        # Short interest momentum (change from previous period)
        if t > 0:
            prev_short_interest = data[t-1, 0]
            si_change = (short_interest - prev_short_interest) / max(prev_short_interest, 1e-8)
            eng.append(si_change)
            
            # Short interest acceleration (second derivative)
            if t > 1:
                prev_prev_short_interest = data[t-2, 0]
                prev_si_change = (prev_short_interest - prev_prev_short_interest) / max(prev_prev_short_interest, 1e-8)
                si_acceleration = si_change - prev_si_change
                eng.append(si_acceleration)
            else:
                eng.append(0)  # Placeholder
        else:
            eng.extend([0, 0])  # Placeholders
        
        # 2. Price-based features with focus on recent price action
        if len(close_prices) > 1:
            # Daily returns (more granular than previous implementation)
            daily_returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            
            # Most recent return (1-day)
            eng.append(daily_returns[-1] if len(daily_returns) > 0 else 0)
            
            # 3-day cumulative return
            cum_ret_3d = np.sum(daily_returns[-3:]) if len(daily_returns) >= 3 else np.sum(daily_returns)
            eng.append(cum_ret_3d)
            
            # Return volatility (standard deviation) - showed high importance
            ret_vol = np.std(daily_returns) if len(daily_returns) > 1 else 0
            eng.append(ret_vol)
            
            # Skewness of returns (asymmetry measure)
            if len(daily_returns) >= 3:
                mean_ret = np.mean(daily_returns)
                skew_numerator = np.mean((daily_returns - mean_ret) ** 3)
                skew_denominator = max(np.std(daily_returns) ** 3, 1e-8)
                skewness = skew_numerator / skew_denominator
                eng.append(skewness)
            else:
                eng.append(0)  # Placeholder
        else:
            eng.extend([0, 0, 0, 0])  # Placeholders
        
        # 3. Price range and volatility metrics
        if len(close_prices) >= 3:
            # True Range (TR) - captures volatility
            tr_values = []
            for i in range(1, len(close_prices)):
                high_low = high_prices[i] - low_prices[i]
                high_close = abs(high_prices[i] - close_prices[i-1])
                low_close = abs(low_prices[i] - close_prices[i-1])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            
            # Average True Range (ATR) - normalized by price
            atr = np.mean(tr_values) / max(np.mean(close_prices[-3:]), 1e-8)
            eng.append(atr)
            
            # ATR to short interest ratio (volatility relative to short interest)
            atr_si_ratio = atr / max(short_interest, 1e-8)
            eng.append(atr_si_ratio)
            
            # Normalized price range (high-low spread)
            norm_range = np.mean((high_prices[-3:] - low_prices[-3:]) / np.maximum(close_prices[-3:], 1e-8))
            eng.append(norm_range)
        else:
            eng.extend([0, 0, 0])  # Placeholders
        
        # 4. Moving averages and trend indicators
        if len(close_prices) >= 5:
            # 5-day SMA
            sma5 = np.mean(close_prices[-5:])
            # Price to SMA ratio (momentum indicator)
            price_to_sma5 = close_prices[-1] / max(sma5, 1e-8)
            eng.append(price_to_sma5)
            
            if len(close_prices) >= 10:
                # 10-day SMA
                sma10 = np.mean(close_prices[-10:])
                # SMA crossover indicator (trend signal)
                sma_crossover = sma5 / max(sma10, 1e-8) - 1
                eng.append(sma_crossover)
                
                # Price trend strength
                price_trend = (close_prices[-1] - close_prices[-10]) / max(close_prices[-10], 1e-8)
                eng.append(price_trend)
            else:
                eng.extend([0, 0])  # Placeholders
        else:
            eng.extend([0, 0, 0])  # Placeholders
        
        # 5. RSI (Relative Strength Index) - improved implementation
        if len(daily_returns) >= 14:
            gains = np.maximum(daily_returns, 0)
            losses = np.maximum(-daily_returns, 0)
            avg_gain = np.mean(gains[-14:])
            avg_loss = np.mean(losses[-14:])
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
            
            # RSI extremes (overbought/oversold indicator)
            rsi_extreme = 0
            if rsi > 70:  # Overbought
                rsi_extreme = 1
            elif rsi < 30:  # Oversold
                rsi_extreme = -1
            eng.append(rsi_extreme)
        else:
            eng.extend([50, 0])  # Default RSI value and placeholders
        
        # 6. Volume analysis - showed high importance in previous iterations
        # Volume trend (normalized)
        if t > 0:
            prev_avg_volume = data[t-1, 1]
            volume_change = (avg_volume - prev_avg_volume) / max(prev_avg_volume, 1e-8)
            eng.append(volume_change)
            
            # Volume acceleration
            if t > 1:
                prev_prev_avg_volume = data[t-2, 1]
                prev_volume_change = (prev_avg_volume - prev_prev_avg_volume) / max(prev_prev_avg_volume, 1e-8)
                volume_acceleration = volume_change - prev_volume_change
                eng.append(volume_acceleration)
            else:
                eng.append(0)  # Placeholder
            
            # Volume to price ratio change
            if len(close_prices) > 0:
                vol_price_ratio = avg_volume / max(close_prices[-1], 1e-8)
                if t > 1 and len(data[t-1, 2:].reshape(15, 4)[:, 3]) > 0:
                    prev_close = data[t-1, 2:].reshape(15, 4)[:, 3][-1]
                    prev_vol_price_ratio = prev_avg_volume / max(prev_close, 1e-8)
                    vol_price_ratio_change = vol_price_ratio / max(prev_vol_price_ratio, 1e-8) - 1
                    eng.append(vol_price_ratio_change)
                else:
                    eng.append(0)  # Placeholder
            else:
                eng.append(0)  # Placeholder
        else:
            eng.extend([0, 0, 0])  # Placeholders
        
        # 7. Bollinger Bands - simplified and focused on most important aspects
        if len(close_prices) >= 10:
            sma20 = np.mean(close_prices[-10:])  # Using 10 instead of 20 due to data constraints
            std20 = np.std(close_prices[-10:])
            
            # %B indicator (position within Bollinger Bands)
            upper_band = sma20 + (2 * std20)
            lower_band = sma20 - (2 * std20)
            percent_b = (close_prices[-1] - lower_band) / max((upper_band - lower_band), 1e-8)
            eng.append(percent_b)
            
            # Bollinger Band width (volatility indicator)
            bb_width = (upper_band - lower_band) / max(sma20, 1e-8)
            eng.append(bb_width)
        else:
            eng.extend([0.5, 0])  # Default %B is 0.5 (middle of the band)
        
        # 8. Short interest to price relationship
        if len(close_prices) > 0:
            # Short interest to price ratio
            si_price_ratio = short_interest / max(close_prices[-1], 1e-8)
            eng.append(si_price_ratio)
            
            # Short interest to price volatility ratio
            if len(daily_returns) > 1:
                price_volatility = np.std(daily_returns)
                si_vol_ratio = short_interest / max(price_volatility * 100, 1e-8)  # Scaled for numerical stability
                eng.append(si_vol_ratio)
            else:
                eng.append(0)  # Placeholder
        else:
            eng.extend([0, 0])  # Placeholders
        
        # 9. Mean reversion indicators
        if len(close_prices) >= 10:
            # Z-score (price deviation from mean)
            mean_price = np.mean(close_prices[-10:])
            std_price = np.std(close_prices[-10:])
            z_score = (close_prices[-1] - mean_price) / max(std_price, 1e-8)
            eng.append(z_score)
        else:
            eng.append(0)  # Placeholder
        
        # 10. Volatility regime
        if len(daily_returns) >= 10:
            # Current volatility vs historical
            recent_vol = np.std(daily_returns[-5:])
            historical_vol = np.std(daily_returns[-10:])
            vol_regime = recent_vol / max(historical_vol, 1e-8) - 1
            eng.append(vol_regime)
        else:
            eng.append(0)  # Placeholder
        
        # 11. Short squeeze potential indicators
        # Short squeeze pressure (days to cover * recent return)
        if len(daily_returns) > 0:
            squeeze_pressure = days_to_cover * max(daily_returns[-1], 0)  # Only positive returns create pressure
            eng.append(squeeze_pressure)
        else:
            eng.append(0)  # Placeholder
        
        # 12. New: Relative Volume Indicator
        # Compare current volume to historical average
        if t > 0 and len(data) > 5:
            historical_volumes = [data[max(0, t-i), 1] for i in range(1, min(6, t+1))]
            avg_historical_volume = np.mean(historical_volumes)
            relative_volume = avg_volume / max(avg_historical_volume, 1e-8)
            eng.append(relative_volume)
        else:
            eng.append(1.0)  # Default to neutral
        
        # 13. New: Short Interest Trend Strength
        # Measure consistency of short interest direction
        if t >= 3:
            si_values = [data[t-i, 0] for i in range(4)]
            si_changes = [si_values[i] - si_values[i+1] for i in range(3)]
            # Count consistent direction changes
            consistent_up = sum(1 for change in si_changes if change > 0)
            consistent_down = sum(1 for change in si_changes if change < 0)
            # Convert to a normalized indicator (-1 to 1)
            si_trend_strength = (consistent_up - consistent_down) / 3.0
            eng.append(si_trend_strength)
        else:
            eng.append(0)  # Placeholder
        
        # 14. New: Price Momentum Oscillator
        # Similar to Rate of Change but normalized
        if len(close_prices) >= 10:
            price_10d_ago = close_prices[-10]
            roc_10 = (close_prices[-1] / max(price_10d_ago, 1e-8)) - 1
            # Normalize to typical range
            norm_roc = min(max(roc_10 * 10, -1), 1)  # Scale and clamp to [-1, 1]
            eng.append(norm_roc)
        else:
            eng.append(0)  # Placeholder
        
        # 15. New: Short Interest to Float Ratio Proxy
        # We don't have float data, but can approximate using volume as proxy
        if len(close_prices) >= 10 and avg_volume > 0:
            # Use 10-day total volume as proxy for tradable float
            total_volume_proxy = avg_volume * 10
            si_float_ratio_proxy = short_interest / max(total_volume_proxy, 1e-8)
            eng.append(si_float_ratio_proxy)
        else:
            eng.append(0)  # Placeholder
        
        # 16. New: Short Interest Volatility
        # Measure how much short interest fluctuates
        if t >= 5:
            si_history = [data[t-i, 0] for i in range(5)]
            si_volatility = np.std(si_history) / max(np.mean(si_history), 1e-8)
            eng.append(si_volatility)
        else:
            eng.append(0)  # Placeholder
        
        # 17. New: Price-Volume Divergence
        # Detect when price and volume move in opposite directions
        if len(close_prices) >= 5 and t > 0:
            price_change = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1
            prev_avg_volume = data[t-1, 1] if t > 0 else avg_volume
            volume_change = (avg_volume / max(prev_avg_volume, 1e-8)) - 1
            
            # Divergence occurs when signs differ
            divergence = 0
            if price_change > 0 and volume_change < 0:
                divergence = 1  # Bullish divergence
            elif price_change < 0 and volume_change > 0:
                divergence = -1  # Bearish divergence
            eng.append(divergence)
        else:
            eng.append(0)  # Placeholder
        
        # 18. New: Short Interest Reversal Signal
        # Detect potential reversals in short interest trend
        if t >= 3:
            si_values = [data[t-i, 0] for i in range(4)]
            # Check for pattern: 3 consistent moves followed by reversal
            consistent_up = all(si_values[i] > si_values[i+1] for i in range(2))
            consistent_down = all(si_values[i] < si_values[i+1] for i in range(2))
            
            reversal_signal = 0
            if consistent_up and si_values[0] < si_values[1]:
                reversal_signal = -1  # Uptrend potentially reversing down
            elif consistent_down and si_values[0] > si_values[1]:
                reversal_signal = 1  # Downtrend potentially reversing up
            eng.append(reversal_signal)
        else:
            eng.append(0)  # Placeholder
        
        # 19. New: Normalized Short Interest Change
        # Measure rate of change in short interest relative to its level
        if t > 0:
            prev_si = data[t-1, 0]
            norm_si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            # Scale to typical range
            norm_si_change = min(max(norm_si_change * 5, -1), 1)  # Scale and clamp
            eng.append(norm_si_change)
        else:
            eng.append(0)  # Placeholder
        
        # 20. New: Short Interest Acceleration Normalized
        # Second derivative of short interest, normalized
        if t > 1:
            si_t = short_interest
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            change_t = si_t - si_t1
            change_t1 = si_t1 - si_t2
            
            si_accel = change_t - change_t1
            norm_si_accel = si_accel / max(abs(si_t2), 1e-8)
            # Scale to typical range
            norm_si_accel = min(max(norm_si_accel * 10, -1), 1)  # Scale and clamp
            eng.append(norm_si_accel)
        else:
            eng.append(0)  # Placeholder
        
        # Ensure we don't exceed MAX_NEW features
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        result.append(row)
    
    # Stack all rows and handle NaN values
    result_array = np.stack(result)
    result_array = np.nan_to_num(result_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return result_array
```

Performance of this code: MAPE = 12.37%
Change from previous: -4.04%
Statistical Analysis: 38/80 features were significant (p < 0.05), 29 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 5):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 8.33%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 62`
  - `MAX_TOTAL = 80`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 62+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Select and preserve only the useful raw features** (you may drop low-importance raw channels).
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 80**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
