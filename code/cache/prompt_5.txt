
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 10.13%
  Features: All 62 original features
  Statistical Analysis:
    • Total features: 62
    • Significant features (p < 0.05): 8
    • Highly significant features (p < 0.01): 3
    • Top significant features: Feature_0, Feature_15, Feature_32
    • Most significant feature: Feature_0 (p=0.0000)

Iteration 1: Iteration 1 - MAPE: 10.64% (Improvement: -0.5%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 20
    • Significant features (p < 0.05): 3
    • Highly significant features (p < 0.01): 3
    • Top significant features: Feature_0, Feature_1, Feature_19
    • Most significant feature: Feature_19 (p=0.0009)

Iteration 2: Iteration 2 - MAPE: 8.86% (Improvement: +1.3%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 42
    • Significant features (p < 0.05): 4
    • Highly significant features (p < 0.01): 2
    • Top significant features: Feature_1, Feature_8, Feature_30
    • Most significant feature: Feature_30 (p=0.0013)

Iteration 3: Iteration 3 - MAPE: 9.68% (Improvement: -0.8%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 64
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 4: Iteration 4 - MAPE: 9.45% (Improvement: -0.6%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 38
    • Significant features (p < 0.05): 4
    • Highly significant features (p < 0.01): 3
    • Top significant features: Feature_23, Feature_25, Feature_26
    • Most significant feature: Feature_26 (p=0.0000)




STATISTICAL INSIGHTS FROM BEST MODEL (MAPE: 8.86%):
- Most predictive features (lowest p-values): Feature_30 (p=0.0013), Feature_1 (p=0.0021), Feature_31 (p=0.0193), Feature_8 (p=0.0270), Feature_38 (p=0.0838)
- Least predictive features (highest p-values): Feature_41 (p=0.9931), Feature_24 (p=0.9922), Feature_28 (p=0.9367)
- Feature significance ratio: 4/42 features are statistically significant




ERROR FEEDBACK FROM PREVIOUS ATTEMPTS:
The following errors occurred in previous attempts. Please analyze these errors and ensure your code avoids these issues:

Error 1:
  • Error Type: ExecutionError
  • Error Message: Function execution failed (attempt 1/3)
  • Problematic Code: def construct_features(data):
    """
    Constructs engineered features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature_0:...

IMPORTANT: Your new code must avoid these specific errors. Pay special attention to:
- Array dimension mismatches and shape issues
- Proper handling of edge cases and NaN values
- Correct return value format (2D numpy array)
- Robust error handling within the function




PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 4):

```python
def construct_features(data):
    """
    Constructs engineered features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature_0: Short interest
            - Feature_1: Average daily volume
            - Feature_2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    features_list = []
    
    # Analysis of previous iterations:
    # - Iteration 2 had best MAPE (8.86%) with 42 features
    # - Most significant features were Feature_30 (p=0.0013), Feature_1 (p=0.0021), 
    #   Feature_31 (p=0.0193), Feature_8 (p=0.0270)
    # - Iteration 3 had 0 significant features and worse performance (9.68%)
    # - Iteration 3 likely had too many features (64) causing overfitting
    
    # Strategy for Iteration 4:
    # 1. Return to a more focused feature set (closer to Iteration 2's 42 features)
    # 2. Emphasize features related to volume (Feature_1 was significant)
    # 3. Focus on the most predictive technical indicators
    # 4. Reduce noise by eliminating redundant features
    # 5. Add more robust financial indicators with stronger theoretical basis
    
    for t in range(lookback_window):
        # Extract features for current timestamp
        current_data = np.nan_to_num(data[t], nan=0.0)
        
        # Extract key components
        short_interest = current_data[0]  # Feature_0
        avg_volume = current_data[1]      # Feature_1 (highly significant in previous iterations)
        
        # Reshape OHLC data to 15 days x 4 OHLC values
        ohlc_data = current_data[2:].reshape(15, 4)
        opens = ohlc_data[:, 0]
        highs = ohlc_data[:, 1]
        lows = ohlc_data[:, 2]
        closes = ohlc_data[:, 3]
        
        # Initialize features for this timestamp
        timestamp_features = []
        
        # 1. CORE FEATURES - Keep the original significant features
        timestamp_features.append(short_interest)  # Original short interest
        timestamp_features.append(avg_volume)      # Average daily volume (highly significant)
        
        # 2. SHORT INTEREST DYNAMICS - More focused than previous iteration
        if t > 0:
            prev_data = np.nan_to_num(data[t-1], nan=0.0)
            prev_short_interest = prev_data[0]
            si_change = short_interest - prev_short_interest
            si_pct_change = si_change / max(prev_short_interest, 1e-10)
            timestamp_features.append(si_change)
            timestamp_features.append(si_pct_change)
            
            # Short interest momentum (rate of change)
            if t > 1:
                prev2_data = np.nan_to_num(data[t-2], nan=0.0)
                prev2_short_interest = prev2_data[0]
                prev_si_change = prev_short_interest - prev2_short_interest
                si_momentum = si_change / max(prev_si_change, 1e-10) if prev_si_change != 0 else 0
                timestamp_features.append(si_momentum)
            else:
                timestamp_features.append(0)
        else:
            timestamp_features.extend([0, 0, 0])
        
        # 3. VOLUME ANALYSIS - Enhanced focus on volume (Feature_1 was significant)
        # Days to cover (key short interest metric)
        days_to_cover = short_interest / max(avg_volume, 1)
        timestamp_features.append(days_to_cover)
        
        # Log transformation of days to cover
        log_days_to_cover = np.log1p(days_to_cover)
        timestamp_features.append(log_days_to_cover)
        
        # Volume dynamics
        if t > 0:
            prev_data = np.nan_to_num(data[t-1], nan=0.0)
            prev_volume = prev_data[1]
            volume_change = avg_volume - prev_volume
            volume_pct_change = volume_change / max(prev_volume, 1e-10)
            timestamp_features.append(volume_pct_change)
            
            # Volume acceleration (second derivative)
            if t > 1:
                prev2_data = np.nan_to_num(data[t-2], nan=0.0)
                prev2_volume = prev2_data[1]
                prev_volume_change = prev_volume - prev2_volume
                volume_acceleration = volume_change - prev_volume_change
                timestamp_features.append(volume_acceleration)
            else:
                timestamp_features.append(0)
        else:
            timestamp_features.extend([0, 0])
        
        # 4. PRICE DYNAMICS - More focused on significant price metrics
        # Calculate returns
        daily_returns = np.zeros(len(closes)-1)
        for i in range(len(closes)-1):
            if closes[i] > 0:
                daily_returns[i] = (closes[i+1] - closes[i]) / closes[i]
            else:
                daily_returns[i] = 0
        
        # Return statistics
        if len(daily_returns) > 1:
            mean_return = np.mean(daily_returns)
            std_return = np.std(daily_returns)
            timestamp_features.extend([mean_return, std_return])
            
            # Upside/downside volatility
            pos_returns = daily_returns[daily_returns > 0]
            neg_returns = daily_returns[daily_returns < 0]
            up_vol = np.std(pos_returns) if len(pos_returns) > 1 else 0
            down_vol = np.std(neg_returns) if len(neg_returns) > 1 else 0
            vol_ratio = up_vol / max(down_vol, 1e-10)
            timestamp_features.append(vol_ratio)
        else:
            timestamp_features.extend([0, 0, 0])
        
        # 5. TECHNICAL INDICATORS - Focused on the most predictive ones
        # Moving Averages
        if len(closes) >= 5:
            sma5 = np.mean(closes[-5:])
            price_to_sma5 = closes[-1] / max(sma5, 1e-10) - 1
            timestamp_features.append(price_to_sma5)
            
            if len(closes) >= 10:
                sma10 = np.mean(closes[-10:])
                # Moving average crossover
                ma_crossover = sma5 / max(sma10, 1e-10) - 1
                timestamp_features.append(ma_crossover)
            else:
                timestamp_features.append(0)
        else:
            timestamp_features.extend([0, 0])
        
        # Relative Strength Index (RSI) - Simplified for limited data
        if len(daily_returns) >= 5:
            gains = np.sum(daily_returns[daily_returns > 0])
            losses = np.abs(np.sum(daily_returns[daily_returns < 0]))
            
            if losses > 0:
                rs = gains / losses
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100 if gains > 0 else 50
                
            # RSI-based features
            timestamp_features.append(rsi)
            timestamp_features.append((rsi - 50) / 50)  # Normalized RSI
        else:
            timestamp_features.extend([50, 0])
        
        # 6. VOLATILITY MEASURES - Focused on the most predictive ones
        # True Range calculation
        true_ranges = np.zeros(len(highs))
        for i in range(len(highs)):
            if i == 0:
                true_ranges[i] = highs[i] - lows[i]
            else:
                true_ranges[i] = max(
                    highs[i] - lows[i],
                    abs(highs[i] - closes[i-1]),
                    abs(lows[i] - closes[i-1])
                )
        
        # Average True Range (ATR)
        atr = np.mean(true_ranges) if len(true_ranges) > 0 else 0
        timestamp_features.append(atr)
        
        # Normalized ATR (by price)
        if len(closes) > 0:
            norm_atr = atr / max(closes[-1], 1e-10)
            timestamp_features.append(norm_atr)
        else:
            timestamp_features.append(0)
        
        # 7. VOLUME-PRICE RELATIONSHIP - Enhanced from previous iteration
        # On-Balance Volume (OBV) concept
        if len(closes) > 1 and avg_volume > 0:
            obv_indicator = 1 if closes[-1] > closes[-2] else (-1 if closes[-1] < closes[-2] else 0)
            obv_value = obv_indicator * avg_volume
            timestamp_features.append(obv_value)
            
            # Volume Price Trend (VPT)
            vpt = avg_volume * (closes[-1] - closes[-2]) / max(closes[-2], 1e-10)
            timestamp_features.append(vpt)
        else:
            timestamp_features.extend([0, 0])
        
        # 8. SHORT INTEREST RELATIVE METRICS - Refined from previous iteration
        # Short interest to price ratio
        if len(closes) > 0:
            si_to_price = short_interest / max(closes[-1], 1e-10)
            timestamp_features.append(si_to_price)
            
            # Short interest to trading range ratio
            price_range = np.max(highs) - np.min(lows) if len(highs) > 0 else 0
            si_to_range = short_interest / max(price_range, 1e-10)
            timestamp_features.append(si_to_range)
        else:
            timestamp_features.extend([0, 0])
        
        # 9. KEY TRANSFORMATIONS AND INTERACTIONS - Focused on significant features
        # Transformations of short interest
        timestamp_features.append(np.log1p(short_interest))
        timestamp_features.append(np.sqrt(short_interest))
        
        # Transformations of volume (Feature_1 was significant)
        timestamp_features.append(np.log1p(avg_volume))
        timestamp_features.append(np.sqrt(avg_volume))
        
        # Interaction between short interest and volume
        si_vol_interaction = short_interest * avg_volume
        timestamp_features.append(si_vol_interaction)
        timestamp_features.append(np.log1p(si_vol_interaction))
        
        # Interaction between short interest and days to cover
        si_dtc_interaction = short_interest * days_to_cover
        timestamp_features.append(si_dtc_interaction)
        
        # 10. MOMENTUM INDICATORS - Refined from previous iteration
        # Price momentum at different timeframes
        for lookback in [3, 5, 10]:
            if len(closes) > lookback:
                momentum = (closes[-1] / max(closes[-lookback-1], 1e-10)) - 1
                timestamp_features.append(momentum)
            else:
                timestamp_features.append(0)
        
        # 11. BOLLINGER BANDS - Simplified from previous iteration
        if len(closes) >= 10:
            sma = np.mean(closes[-10:])
            std = np.std(closes[-10:])
            
            # Bollinger Band position
            bb_position = (closes[-1] - sma) / max(std, 1e-10)
            timestamp_features.append(bb_position)
            
            # Bollinger Band width
            bb_width = (2 * std) / max(sma, 1e-10)
            timestamp_features.append(bb_width)
        else:
            timestamp_features.extend([0, 0])
        
        # 12. FEATURE COMBINATIONS BASED ON PREVIOUS SIGNIFICANCE
        # Feature_30 and Feature_31 were significant in Iteration 2
        # These likely related to technical indicators or volume metrics
        
        # Days to cover variations (potentially related to significant features)
        timestamp_features.append(days_to_cover ** 2)
        timestamp_features.append(days_to_cover * np.log1p(avg_volume))
        
        # Volume and short interest combined metrics
        si_to_vol_ratio = short_interest / max(avg_volume, 1e-10)
        timestamp_features.append(si_to_vol_ratio)
        timestamp_features.append(np.log1p(si_to_vol_ratio))
        
        features_list.append(timestamp_features)
    
    # Convert to numpy array
    result = np.array(features_list)
    
    # Handle any NaN or inf values
    result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Final feature count: 42 (matching the count from the best performing iteration)
    return result
```

Performance of this code: MAPE = 9.45%
Change from previous: -0.59%
Statistical Analysis: 4/38 features were significant (p < 0.05), 3 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 5):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 8.86%.

Based on the performance history, statistical analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses p-value insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using statistical significance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and statistical insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
