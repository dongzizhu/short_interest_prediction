============================================================
ITERATIVE AGENT-BASED FEATURE SELECTION SUMMARY
============================================================
Stock: YORW
Date: 2025-09-26 01:28:11
Total Iterations: 5

PERFORMANCE TREND:
----------------------------------------
Iteration 0: Baseline - MAPE: 12.11% (Baseline)
Iteration 1: Iteration 1 - MAPE: 19.06% (-6.95%)
Iteration 2: Iteration 2 - MAPE: 18.37% (-6.26%)
Iteration 3: Iteration 3 - MAPE: 23.25% (-11.14%)
Iteration 4: Iteration 4 - MAPE: 16.20% (-4.09%)
Iteration 5: Iteration 5 - MAPE: 16.43% (-4.32%)

Best Model: Baseline - MAPE: 12.11%

============================================================
FEATURE ENGINEERING CODES
============================================================

ITERATION 1:
Performance: MAPE = 19.06%
Improvement: -6.95%
Features: 23
----------------------------------------
def construct_features(data):
    """
    Constructs features for short interest prediction based on financial time series data.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    
    # Initialize the output array
    # We'll start with a reasonable number of features and adjust as needed
    output_features = []
    
    for t in range(lookback_window):
        features_t = []
        
        # Extract key components from the data
        short_interest = data[t, 0]  # Feature_0 (highest importance)
        avg_volume = data[t, 1]
        
        # Extract OHLC data - reshape to (15, 4) for easier manipulation
        ohlc_data = data[t, 2:62].reshape(15, 4)  # 15 days, 4 values (OHLC)
        
        # 1. Keep the original short interest (highest importance feature)
        features_t.append(short_interest)
        
        # 2. Short interest momentum (change)
        if t > 0:
            si_change = short_interest - data[t-1, 0]
            si_pct_change = si_change / (data[t-1, 0] + 1e-8)  # Avoid division by zero
        else:
            si_change = 0
            si_pct_change = 0
        features_t.extend([si_change, si_pct_change])
        
        # 3. Volume features (normalized and relative)
        features_t.append(avg_volume)
        features_t.append(np.log1p(avg_volume))  # Log-transformed volume
        
        # 4. Price-based features from OHLC data
        
        # 4.1 Extract close prices (most recent is last)
        close_prices = ohlc_data[:, 3]
        
        # 4.2 Calculate returns at different timeframes
        returns_1d = (close_prices[-1] / close_prices[-2] - 1) if len(close_prices) > 1 else 0
        returns_5d = (close_prices[-1] / close_prices[-6] - 1) if len(close_prices) > 5 else 0
        returns_15d = (close_prices[-1] / close_prices[0] - 1) if len(close_prices) > 0 else 0
        features_t.extend([returns_1d, returns_5d, returns_15d])
        
        # 4.3 Volatility measures (important for short interest)
        daily_returns = np.diff(close_prices) / close_prices[:-1]
        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0
        features_t.append(volatility)
        
        # 4.4 Price ranges and gaps
        high_low_ratio = np.mean(ohlc_data[:, 1] / (ohlc_data[:, 2] + 1e-8))
        open_close_ratio = np.mean(ohlc_data[:, 0] / (ohlc_data[:, 3] + 1e-8))
        features_t.extend([high_low_ratio, open_close_ratio])
        
        # 5. Technical indicators based on Feature_28, Feature_59, Feature_26, Feature_38 (high importance)
        # These likely correspond to specific days/metrics in the OHLC data
        
        # 5.1 Moving averages of close prices
        ma_5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else np.mean(close_prices)
        ma_10 = np.mean(close_prices[-10:]) if len(close_prices) >= 10 else np.mean(close_prices)
        ma_ratio = ma_5 / (ma_10 + 1e-8)
        features_t.extend([ma_5, ma_10, ma_ratio])
        
        # 5.2 Relative strength indicator (simplified)
        gains = np.sum(daily_returns[daily_returns > 0]) if len(daily_returns) > 0 else 0
        losses = np.abs(np.sum(daily_returns[daily_returns < 0])) if len(daily_returns) > 0 else 0
        rsi = 100 - (100 / (1 + gains/(losses + 1e-8)))
        features_t.append(rsi)
        
        # 5.3 Volume-price relationship
        vol_price_ratio = avg_volume / (close_prices[-1] + 1e-8)
        features_t.append(vol_price_ratio)
        
        # 5.4 Bollinger Bands
        bb_std = np.std(close_prices)
        bb_upper = ma_10 + 2 * bb_std
        bb_lower = ma_10 - 2 * bb_std
        bb_width = (bb_upper - bb_lower) / (ma_10 + 1e-8)
        bb_position = (close_prices[-1] - bb_lower) / ((bb_upper - bb_lower) + 1e-8)
        features_t.extend([bb_width, bb_position])
        
        # 6. Interaction terms between high-importance features
        features_t.append(short_interest * vol_price_ratio)  # SI and volume-price relationship
        features_t.append(short_interest * volatility)  # SI and volatility
        features_t.append(short_interest * returns_15d)  # SI and medium-term returns
        
        # 7. Non-linear transformations of important features
        features_t.append(np.square(si_pct_change))  # Squared SI change
        features_t.append(np.sign(returns_5d) * np.square(returns_5d))  # Signed squared returns
        
        # Add the features for this timestamp to the output
        output_features.append(features_t)
    
    # Convert to numpy array and handle NaN values
    output_array = np.array(output_features)
    output_array = np.nan_to_num(output_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return output_array
============================================================

ITERATION 2:
Performance: MAPE = 18.37%
Improvement: -6.26%
Features: 49
----------------------------------------
def construct_features(data):
    """
    Constructs features for short interest prediction based on financial time series data.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    output_features = []
    
    # Based on feature importance analysis, we need to focus on:
    # - Feature_0 (Short interest) - highest importance in both iterations
    # - Feature_28, Feature_59, Feature_26, Feature_38 - high importance in baseline
    
    # Map feature indices to their positions in the OHLC data
    # Feature_28 corresponds to a specific day's OHLC value
    # Assuming features 2-61 are OHLC for 15 days (4 values per day)
    # Feature_28 would be: day_idx = (28-2)//4 = 6, value_idx = (28-2)%4 = 2
    
    for t in range(lookback_window):
        features_t = []
        
        # Extract key components from the data
        short_interest = data[t, 0]  # Feature_0 (highest importance)
        avg_volume = data[t, 1]
        
        # Extract OHLC data - reshape to (15, 4) for easier manipulation
        ohlc_data = data[t, 2:62].reshape(15, 4)  # 15 days, 4 values (OHLC)
        
        # Extract specific high-importance features from OHLC
        # Feature_28 = day 6, value 2 (Low price)
        feature_28_day, feature_28_val = 6, 2
        feature_28 = ohlc_data[feature_28_day, feature_28_val] if ohlc_data.shape[0] > feature_28_day else 0
        
        # Feature_59 = day 14, value 3 (Close price)
        feature_59_day, feature_59_val = 14, 3
        feature_59 = ohlc_data[feature_59_day, feature_59_val] if ohlc_data.shape[0] > feature_59_day else 0
        
        # Feature_26 = day 6, value 0 (Open price)
        feature_26_day, feature_26_val = 6, 0
        feature_26 = ohlc_data[feature_26_day, feature_26_val] if ohlc_data.shape[0] > feature_26_day else 0
        
        # Feature_38 = day 9, value 2 (Low price)
        feature_38_day, feature_38_val = 9, 2
        feature_38 = ohlc_data[feature_38_day, feature_38_val] if ohlc_data.shape[0] > feature_38_day else 0
        
        # 1. Original high-importance features
        # Keep the original short interest (highest importance feature)
        features_t.append(short_interest)
        
        # Add the specific high-importance OHLC features
        features_t.extend([feature_28, feature_59, feature_26, feature_38])
        
        # 2. Short interest momentum features
        # Previous iteration showed these were significant
        if t > 0:
            # Simple difference
            si_change = short_interest - data[t-1, 0]
            # Percentage change
            si_pct_change = si_change / (data[t-1, 0] + 1e-8)
            
            # Add exponential smoothing for short interest trend
            if t > 1:
                si_exp_smooth = 0.7 * si_change + 0.3 * (data[t-1, 0] - data[t-2, 0])
            else:
                si_exp_smooth = si_change
                
            # Add acceleration (change in change)
            if t > 1:
                si_accel = si_change - (data[t-1, 0] - data[t-2, 0])
            else:
                si_accel = 0
        else:
            si_change = 0
            si_pct_change = 0
            si_exp_smooth = 0
            si_accel = 0
            
        features_t.extend([si_change, si_pct_change, si_exp_smooth, si_accel])
        
        # 3. Volume features
        # Keep raw volume but add more sophisticated volume indicators
        features_t.append(avg_volume)
        
        # Log-transformed volume (reduces skew)
        log_volume = np.log1p(avg_volume)
        features_t.append(log_volume)
        
        # Volume z-score (if we have enough history)
        if t >= 3:
            vol_history = np.array([data[max(0, t-i), 1] for i in range(3)])
            vol_mean = np.mean(vol_history)
            vol_std = np.std(vol_history) + 1e-8
            vol_zscore = (avg_volume - vol_mean) / vol_std
        else:
            vol_zscore = 0
        features_t.append(vol_zscore)
        
        # 4. Price-based features
        # Extract price series
        open_prices = ohlc_data[:, 0]
        high_prices = ohlc_data[:, 1]
        low_prices = ohlc_data[:, 2]
        close_prices = ohlc_data[:, 3]
        
        # 4.1 Basic price statistics
        # These capture the overall price level and recent movement
        last_close = close_prices[-1] if len(close_prices) > 0 else 0
        price_mean = np.mean(close_prices) if len(close_prices) > 0 else 0
        price_std = np.std(close_prices) if len(close_prices) > 0 else 0
        price_min = np.min(low_prices) if len(low_prices) > 0 else 0
        price_max = np.max(high_prices) if len(high_prices) > 0 else 0
        price_range = price_max - price_min
        
        features_t.extend([last_close, price_mean, price_std, price_range])
        
        # 4.2 Returns at different timeframes
        # Short-term returns
        returns_1d = (close_prices[-1] / close_prices[-2] - 1) if len(close_prices) > 1 else 0
        # Medium-term returns
        returns_5d = (close_prices[-1] / close_prices[-6] - 1) if len(close_prices) > 5 else 0
        # Long-term returns
        returns_15d = (close_prices[-1] / close_prices[0] - 1) if len(close_prices) > 0 else 0
        
        features_t.extend([returns_1d, returns_5d, returns_15d])
        
        # 4.3 Advanced return features
        # Calculate daily returns
        daily_returns = np.diff(close_prices) / close_prices[:-1] if len(close_prices) > 1 else np.array([0])
        
        # Volatility (standard deviation of returns)
        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0
        
        # Skewness of returns (measure of asymmetry)
        if len(daily_returns) > 2:
            returns_mean = np.mean(daily_returns)
            returns_std = np.std(daily_returns) + 1e-8
            skew = np.mean(((daily_returns - returns_mean) / returns_std) ** 3) if returns_std > 0 else 0
        else:
            skew = 0
            
        # Kurtosis of returns (measure of "tailedness")
        if len(daily_returns) > 2:
            returns_mean = np.mean(daily_returns)
            returns_std = np.std(daily_returns) + 1e-8
            kurt = np.mean(((daily_returns - returns_mean) / returns_std) ** 4) - 3 if returns_std > 0 else 0
        else:
            kurt = 0
            
        features_t.extend([volatility, skew, kurt])
        
        # 5. Technical indicators
        # 5.1 Moving averages
        ma_5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else np.mean(close_prices)
        ma_10 = np.mean(close_prices[-10:]) if len(close_prices) >= 10 else np.mean(close_prices)
        ma_15 = np.mean(close_prices) if len(close_prices) > 0 else 0
        
        # Moving average ratios
        ma_5_10_ratio = ma_5 / (ma_10 + 1e-8)
        ma_5_15_ratio = ma_5 / (ma_15 + 1e-8)
        
        # Price to moving average ratios
        price_to_ma5 = last_close / (ma_5 + 1e-8)
        price_to_ma10 = last_close / (ma_10 + 1e-8)
        
        features_t.extend([ma_5, ma_10, ma_15, ma_5_10_ratio, ma_5_15_ratio, price_to_ma5, price_to_ma10])
        
        # 5.2 RSI (Relative Strength Index)
        if len(daily_returns) > 0:
            gains = daily_returns.copy()
            losses = daily_returns.copy()
            gains[gains < 0] = 0
            losses[losses > 0] = 0
            losses = np.abs(losses)
            
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            rs = avg_gain / (avg_loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral RSI when no data
            
        features_t.append(rsi)
        
        # 5.3 MACD (Moving Average Convergence Divergence) - simplified
        if len(close_prices) >= 12:
            ema_12 = np.mean(close_prices[-12:])  # Using simple MA as approximation
            ema_26 = np.mean(close_prices[-min(26, len(close_prices)):])
            macd = ema_12 - ema_26
        else:
            macd = 0
            
        features_t.append(macd)
        
        # 5.4 Bollinger Bands
        bb_std = np.std(close_prices) if len(close_prices) > 0 else 0
        bb_upper = ma_15 + 2 * bb_std
        bb_lower = ma_15 - 2 * bb_std
        
        # Bollinger Band width (normalized)
        bb_width = (bb_upper - bb_lower) / (ma_15 + 1e-8)
        
        # Position within Bollinger Bands (0 = lower band, 1 = upper band)
        bb_position = (last_close - bb_lower) / ((bb_upper - bb_lower) + 1e-8)
        
        features_t.extend([bb_width, bb_position])
        
        # 6. Volume-price relationships
        # Volume-price ratio
        vol_price_ratio = avg_volume / (last_close + 1e-8)
        
        # Volume-volatility relationship
        vol_volatility_ratio = avg_volume / (volatility + 1e-8)
        
        # On-balance volume (simplified)
        if t > 0 and len(close_prices) > 1 and len(close_prices) > 1:
            if close_prices[-1] > close_prices[-2]:
                obv_change = avg_volume
            elif close_prices[-1] < close_prices[-2]:
                obv_change = -avg_volume
            else:
                obv_change = 0
        else:
            obv_change = 0
            
        features_t.extend([vol_price_ratio, vol_volatility_ratio, obv_change])
        
        # 7. Interaction terms between high-importance features
        # Short interest and price features
        si_price_ratio = short_interest / (last_close + 1e-8)
        si_volatility = short_interest * volatility
        si_returns = short_interest * returns_15d
        
        # Short interest and volume
        si_volume_ratio = short_interest / (avg_volume + 1e-8)
        
        # Short interest and technical indicators
        si_rsi = short_interest * (rsi / 100)  # Normalize RSI to 0-1
        si_bb_position = short_interest * bb_position
        
        features_t.extend([si_price_ratio, si_volatility, si_returns, si_volume_ratio, si_rsi, si_bb_position])
        
        # 8. Non-linear transformations
        # Squared terms for important features
        si_squared = np.square(short_interest)
        vol_squared = np.square(log_volume)
        returns_squared = np.sign(returns_15d) * np.square(returns_15d)  # Preserve sign
        
        # Exponential terms
        si_exp = np.exp(np.minimum(short_interest, 5)) - 1  # Cap to prevent overflow
        
        # Logarithmic terms (with offset to handle zeros/negatives)
        si_log = np.log1p(np.abs(short_interest))
        
        features_t.extend([si_squared, vol_squared, returns_squared, si_exp, si_log])
        
        # 9. Temporal features specific to the high-importance features
        # Rate of change for Feature_28, Feature_59, etc.
        if t > 0:
            prev_ohlc = data[t-1, 2:62].reshape(15, 4)
            
            # Calculate rate of change for important features
            f28_prev = prev_ohlc[feature_28_day, feature_28_val] if prev_ohlc.shape[0] > feature_28_day else 0
            f28_change = (feature_28 - f28_prev) / (f28_prev + 1e-8)
            
            f59_prev = prev_ohlc[feature_59_day, feature_59_val] if prev_ohlc.shape[0] > feature_59_day else 0
            f59_change = (feature_59 - f59_prev) / (f59_prev + 1e-8)
        else:
            f28_change = 0
            f59_change = 0
            
        features_t.extend([f28_change, f59_change])
        
        # Add the features for this timestamp to the output
        output_features.append(features_t)
    
    # Convert to numpy array and handle NaN values
    output_array = np.array(output_features)
    output_array = np.nan_to_num(output_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return output_array
============================================================

ITERATION 3:
Performance: MAPE = 23.25%
Improvement: -11.14%
Features: 31
----------------------------------------
def construct_features(data):
    """
    Constructs features for short interest prediction based on financial time series data.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    output_features = []
    
    # Analysis of previous iterations:
    # 1. The baseline model (MAPE 12.11%) outperformed subsequent iterations
    # 2. Feature_0 (short interest) consistently shows highest importance across all iterations
    # 3. Feature_28, Feature_59, Feature_26, Feature_38 were important in the baseline model
    # 4. Previous iterations may have overengineered features, diluting signal
    
    # Strategy for this iteration:
    # 1. Return to basics - focus on the most important features from baseline
    # 2. Add minimal, carefully selected derived features
    # 3. Reduce feature count to avoid noise and overfitting
    # 4. Emphasize short interest dynamics and its relationship with key price/volume metrics
    
    for t in range(lookback_window):
        features_t = []
        
        # Extract key components from the data
        short_interest = data[t, 0]  # Feature_0 (highest importance)
        avg_volume = data[t, 1]
        
        # Extract OHLC data - reshape to (15, 4) for easier manipulation
        ohlc_data = data[t, 2:62].reshape(15, 4)  # 15 days, 4 values (OHLC)
        
        # Extract specific high-importance features from OHLC based on baseline model
        # Feature_28 = day 6, value 2 (Low price)
        feature_28_day, feature_28_val = 6, 2
        feature_28 = ohlc_data[feature_28_day, feature_28_val] if ohlc_data.shape[0] > feature_28_day else 0
        
        # Feature_59 = day 14, value 3 (Close price)
        feature_59_day, feature_59_val = 14, 3
        feature_59 = ohlc_data[feature_59_day, feature_59_val] if ohlc_data.shape[0] > feature_59_day else 0
        
        # Feature_26 = day 6, value 0 (Open price)
        feature_26_day, feature_26_val = 6, 0
        feature_26 = ohlc_data[feature_26_day, feature_26_val] if ohlc_data.shape[0] > feature_26_day else 0
        
        # Feature_38 = day 9, value 2 (Low price)
        feature_38_day, feature_38_val = 9, 2
        feature_38 = ohlc_data[feature_38_day, feature_38_val] if ohlc_data.shape[0] > feature_38_day else 0
        
        # 1. CORE FEATURES - Keep all original high-importance features
        # These were the most important in the baseline model
        features_t.append(short_interest)  # Feature_0
        features_t.append(feature_28)      # Feature_28
        features_t.append(feature_59)      # Feature_59
        features_t.append(feature_26)      # Feature_26
        features_t.append(feature_38)      # Feature_38
        
        # 2. SHORT INTEREST DYNAMICS - Focused, minimal transformations
        # Previous iterations showed short interest momentum was important
        if t > 0:
            # Simple difference (momentum)
            si_change = short_interest - data[t-1, 0]
            # Percentage change
            si_pct_change = si_change / (data[t-1, 0] + 1e-8)
            
            # Add acceleration (second derivative) if we have enough history
            if t > 1:
                prev_change = data[t-1, 0] - data[t-2, 0]
                si_accel = si_change - prev_change
            else:
                si_accel = 0
        else:
            si_change = 0
            si_pct_change = 0
            si_accel = 0
            
        features_t.extend([si_change, si_pct_change, si_accel])
        
        # 3. VOLUME FEATURES - Keep minimal but informative
        # Raw volume is important for liquidity assessment
        features_t.append(avg_volume)
        
        # Log-transformed volume (reduces skew)
        log_volume = np.log1p(avg_volume)
        features_t.append(log_volume)
        
        # 4. PRICE FEATURES - Extract key information from OHLC
        # Extract price series
        open_prices = ohlc_data[:, 0]
        high_prices = ohlc_data[:, 1]
        low_prices = ohlc_data[:, 2]
        close_prices = ohlc_data[:, 3]
        
        # Most recent close price (end of 15-day period)
        last_close = close_prices[-1] if len(close_prices) > 0 else 0
        features_t.append(last_close)
        
        # Price volatility - important for risk assessment
        price_std = np.std(close_prices) if len(close_prices) > 0 else 0
        features_t.append(price_std)
        
        # Price range - captures extremes
        price_min = np.min(low_prices) if len(low_prices) > 0 else 0
        price_max = np.max(high_prices) if len(high_prices) > 0 else 0
        price_range = price_max - price_min
        features_t.append(price_range)
        
        # 5. RETURNS - Different timeframes capture different market dynamics
        # Short-term returns (1 day)
        returns_1d = (close_prices[-1] / close_prices[-2] - 1) if len(close_prices) > 1 else 0
        # Medium-term returns (5 days)
        returns_5d = (close_prices[-1] / close_prices[-6] - 1) if len(close_prices) > 5 else 0
        # Long-term returns (15 days)
        returns_15d = (close_prices[-1] / close_prices[0] - 1) if len(close_prices) > 0 else 0
        
        features_t.extend([returns_1d, returns_5d, returns_15d])
        
        # 6. TECHNICAL INDICATORS - Carefully selected subset
        # Moving averages - capture trend information
        ma_5 = np.mean(close_prices[-5:]) if len(close_prices) >= 5 else np.mean(close_prices)
        ma_15 = np.mean(close_prices) if len(close_prices) > 0 else 0
        
        # Price to moving average ratio - indicates momentum/trend strength
        price_to_ma5 = last_close / (ma_5 + 1e-8)
        price_to_ma15 = last_close / (ma_15 + 1e-8)
        
        features_t.extend([ma_5, ma_15, price_to_ma5, price_to_ma15])
        
        # RSI (Relative Strength Index) - momentum oscillator
        # Calculate daily returns for RSI
        daily_returns = np.diff(close_prices) / close_prices[:-1] if len(close_prices) > 1 else np.array([0])
        
        if len(daily_returns) > 0:
            gains = daily_returns.copy()
            losses = daily_returns.copy()
            gains[gains < 0] = 0
            losses[losses > 0] = 0
            losses = np.abs(losses)
            
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0
            
            rs = avg_gain / (avg_loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
        else:
            rsi = 50  # Neutral RSI when no data
            
        features_t.append(rsi)
        
        # 7. KEY RELATIONSHIPS - Focus on relationships between important features
        # Short interest to volume ratio - indicates how many days to cover shorts
        si_volume_ratio = short_interest / (avg_volume + 1e-8)
        features_t.append(si_volume_ratio)
        
        # Short interest to price ratio - normalized short interest
        si_price_ratio = short_interest / (last_close + 1e-8)
        features_t.append(si_price_ratio)
        
        # Short interest change relative to price change - correlation between shorts and price
        if t > 0 and len(close_prices) > 1 and len(close_prices) > 1:
            prev_close = data[t-1, 2:62].reshape(15, 4)[-1, 3]
            price_change = (last_close / (prev_close + 1e-8)) - 1
            si_price_change_ratio = si_pct_change / (np.abs(price_change) + 1e-8) * np.sign(price_change)
        else:
            si_price_change_ratio = 0
            
        features_t.append(si_price_change_ratio)
        
        # 8. FEATURE INTERACTIONS - Carefully selected based on financial intuition
        # Interaction between short interest and volatility
        si_volatility = short_interest * price_std
        features_t.append(si_volatility)
        
        # Interaction between short interest and RSI
        si_rsi = short_interest * (rsi / 100)  # Normalize RSI to 0-1
        features_t.append(si_rsi)
        
        # 9. TEMPORAL FEATURES - Changes in key high-importance features
        # Rate of change for important features from baseline model
        if t > 0:
            prev_ohlc = data[t-1, 2:62].reshape(15, 4)
            
            # Calculate rate of change for important features
            f28_prev = prev_ohlc[feature_28_day, feature_28_val] if prev_ohlc.shape[0] > feature_28_day else 0
            f28_change = (feature_28 - f28_prev) / (f28_prev + 1e-8)
            
            f59_prev = prev_ohlc[feature_59_day, feature_59_val] if prev_ohlc.shape[0] > feature_59_day else 0
            f59_change = (feature_59 - f59_prev) / (f59_prev + 1e-8)
        else:
            f28_change = 0
            f59_change = 0
            
        features_t.extend([f28_change, f59_change])
        
        # 10. ORIGINAL FEATURES - Include a subset of original OHLC data
        # Last 3 days of close prices (most recent price action)
        for i in range(1, 4):
            if len(close_prices) >= i:
                features_t.append(close_prices[-i])
            else:
                features_t.append(0)
        
        # Add the features for this timestamp to the output
        output_features.append(features_t)
    
    # Convert to numpy array and handle NaN values
    output_array = np.array(output_features)
    output_array = np.nan_to_num(output_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Key differences from previous iteration:
    # 1. Reduced feature count to focus on signal, not noise
    # 2. Prioritized original high-importance features from baseline model
    # 3. Added minimal, carefully selected derived features
    # 4. Focused on relationships between short interest and other key metrics
    # 5. Included temporal dynamics of high-importance features
    # 6. Removed many complex features that may have added noise
    # 7. Maintained original time series structure for LSTM compatibility
    
    return output_array
============================================================

ITERATION 4:
Performance: MAPE = 16.20%
Improvement: -4.09%
Features: 75
----------------------------------------
def construct_features(data):
    """
    Constructs features for short interest prediction based on financial time series data.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    output_features = []
    
    # Key insights from performance history:
    # 1. Baseline model (MAPE 12.11%) outperformed all subsequent iterations
    # 2. Feature_0 (short interest) consistently shows high importance
    # 3. Feature_28, Feature_59, Feature_26, Feature_38 were important in baseline
    # 4. Previous iterations likely overengineered features, diluting signal
    # 5. Adding too many features worsened performance
    
    # Strategy for this iteration:
    # 1. Return to baseline approach but with minimal, targeted enhancements
    # 2. Keep ALL original features (since baseline performed best)
    # 3. Add only a few carefully selected derived features that provide new information
    # 4. Focus on preserving signal from important features rather than transforming them
    
    for t in range(lookback_window):
        features_t = []
        
        # 1. PRESERVE ALL ORIGINAL FEATURES (baseline approach)
        # This is critical since baseline outperformed all other iterations
        original_features = data[t, :].copy()
        features_t.extend(original_features)
        
        # Extract key components for derived features
        short_interest = data[t, 0]  # Feature_0 (highest importance)
        avg_volume = data[t, 1]
        
        # Extract OHLC data - reshape to (15, 4) for easier manipulation
        ohlc_data = data[t, 2:62].reshape(15, 4)  # 15 days, 4 values (OHLC)
        open_prices = ohlc_data[:, 0]
        high_prices = ohlc_data[:, 1]
        low_prices = ohlc_data[:, 2]
        close_prices = ohlc_data[:, 3]
        
        # 2. ADD MINIMAL HIGH-VALUE DERIVED FEATURES
        # These are carefully selected to add information without diluting signal
        
        # 2.1 Short Interest Dynamics (minimal transformations)
        if t > 0:
            # Rate of change in short interest (momentum)
            si_change = short_interest - data[t-1, 0]
            si_pct_change = si_change / (data[t-1, 0] + 1e-8)
        else:
            si_change = 0
            si_pct_change = 0
            
        features_t.extend([si_change, si_pct_change])
        
        # 2.2 Days to Cover - key metric for short squeeze potential
        days_to_cover = short_interest / (avg_volume + 1e-8)
        features_t.append(days_to_cover)
        
        # 2.3 Price Volatility - important for risk assessment
        # Use only recent data (last 5 days) to capture current volatility
        recent_close = close_prices[-5:] if len(close_prices) >= 5 else close_prices
        if len(recent_close) > 1:
            price_std = np.std(recent_close)
            price_range = np.max(recent_close) - np.min(recent_close)
            normalized_range = price_range / (np.mean(recent_close) + 1e-8)
        else:
            price_std = 0
            price_range = 0
            normalized_range = 0
            
        features_t.extend([price_std, normalized_range])
        
        # 2.4 Recent Price Trend - direction and strength
        if len(close_prices) >= 5:
            # Simple 5-day trend (slope of linear fit)
            x = np.arange(5)
            y = close_prices[-5:]
            if len(y) == 5:  # Ensure we have exactly 5 points
                # Simple linear regression slope calculation
                slope = (5 * np.sum(x * y) - np.sum(x) * np.sum(y)) / (5 * np.sum(x**2) - np.sum(x)**2 + 1e-8)
                # Normalize by average price
                norm_slope = slope / (np.mean(y) + 1e-8)
            else:
                slope = 0
                norm_slope = 0
        else:
            slope = 0
            norm_slope = 0
            
        features_t.extend([slope, norm_slope])
        
        # 2.5 Volume Trend - changes in trading activity
        if t > 0:
            prev_volume = data[t-1, 1]
            volume_change = avg_volume - prev_volume
            volume_pct_change = volume_change / (prev_volume + 1e-8)
        else:
            volume_change = 0
            volume_pct_change = 0
            
        features_t.extend([volume_change, volume_pct_change])
        
        # 2.6 Short Interest to Price Ratio - normalized short interest
        last_close = close_prices[-1] if len(close_prices) > 0 else 1
        si_price_ratio = short_interest / (last_close + 1e-8)
        features_t.append(si_price_ratio)
        
        # 2.7 Bollinger Band Position - indicates overbought/oversold conditions
        if len(close_prices) >= 5:
            ma_5 = np.mean(close_prices[-5:])
            std_5 = np.std(close_prices[-5:])
            upper_band = ma_5 + 2 * std_5
            lower_band = ma_5 - 2 * std_5
            # Position within bands (0 = lower band, 1 = upper band)
            if upper_band - lower_band > 0:
                bb_position = (last_close - lower_band) / (upper_band - lower_band + 1e-8)
            else:
                bb_position = 0.5
        else:
            bb_position = 0.5
            
        features_t.append(bb_position)
        
        # 2.8 Interaction between important features
        # Focus on interactions between Feature_0 and other important features
        feature_28 = data[t, 28]  # Important in baseline
        feature_59 = data[t, 59]  # Important in baseline
        
        # Interaction terms
        si_f28_interaction = short_interest * feature_28
        si_f59_interaction = short_interest * feature_59
        
        features_t.extend([si_f28_interaction, si_f59_interaction])
        
        # Add the features for this timestamp to the output
        output_features.append(features_t)
    
    # Convert to numpy array and handle NaN values
    output_array = np.array(output_features)
    output_array = np.nan_to_num(output_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Key differences from previous iterations:
    # 1. Preserved ALL original features (baseline approach) instead of selecting subset
    # 2. Added only 14 carefully selected derived features (vs. many in previous iterations)
    # 3. Focused on enhancing signal from important features rather than replacing them
    # 4. Used simpler, more robust calculations that are less prone to noise
    # 5. Emphasized short interest dynamics and its relationship with key price/volume metrics
    # 6. Avoided complex technical indicators that may add noise
    # 7. Maintained original time series structure for LSTM compatibility
    
    return output_array
============================================================

ITERATION 5:
Performance: MAPE = 16.43%
Improvement: -4.32%
Features: 73
----------------------------------------
def construct_features(data):
    """
    Constructs features for short interest prediction based on financial time series data.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    output_features = []
    
    # KEY INSIGHTS FROM PERFORMANCE HISTORY:
    # 1. Baseline model (MAPE 12.11%) outperformed all subsequent iterations
    # 2. Feature engineering attempts have consistently worsened performance
    # 3. Feature_0 (short interest), Feature_28, Feature_59, Feature_26, Feature_38 
    #    were most important in the best model
    # 4. Adding too many derived features dilutes the signal
    
    # STRATEGY FOR THIS ITERATION:
    # 1. Return to baseline approach (all original features) as foundation
    # 2. Add minimal, carefully selected features that enhance rather than replace signal
    # 3. Focus on enhancing the most important features identified by DL analysis
    # 4. Reduce feature count compared to previous iterations
    # 5. Avoid complex transformations that might introduce noise
    
    for t in range(lookback_window):
        features_t = []
        
        # 1. PRESERVE ALL ORIGINAL FEATURES (baseline approach)
        # This is critical since baseline outperformed all other iterations
        original_features = data[t, :].copy()
        features_t.extend(original_features)
        
        # Extract key components for derived features
        short_interest = data[t, 0]  # Feature_0 (highest importance)
        avg_volume = data[t, 1]
        
        # Extract OHLC data - reshape to (15, 4) for easier manipulation
        ohlc_data = data[t, 2:62].reshape(15, 4)  # 15 days, 4 values (OHLC)
        open_prices = ohlc_data[:, 0]
        high_prices = ohlc_data[:, 1]
        low_prices = ohlc_data[:, 2]
        close_prices = ohlc_data[:, 3]
        
        # 2. ENHANCE IMPORTANT FEATURES IDENTIFIED BY DL ANALYSIS
        # Feature_28, Feature_59, Feature_26, Feature_38 were important in baseline
        
        # Identify what these features actually represent in the data
        # Feature_28 is in the OHLC data (index 28-2=26)
        # This corresponds to day 6, index 2 (Low price) in the OHLC data
        feature_28_value = data[t, 28]
        
        # Feature_59 is in the OHLC data (index 59-2=57)
        # This corresponds to day 14, index 1 (High price) in the OHLC data
        feature_59_value = data[t, 59]
        
        # Feature_26 is in the OHLC data (index 26-2=24)
        # This corresponds to day 6, index 0 (Open price) in the OHLC data
        feature_26_value = data[t, 26]
        
        # Feature_38 is in the OHLC data (index 38-2=36)
        # This corresponds to day 9, index 0 (Open price) in the OHLC data
        feature_38_value = data[t, 38]
        
        # 3. ADD MINIMAL HIGH-VALUE DERIVED FEATURES
        
        # 3.1 Short Interest Relative to Important Price Points
        # Ratio of short interest to important price points identified by DL
        si_to_f28_ratio = short_interest / (feature_28_value + 1e-8)
        si_to_f59_ratio = short_interest / (feature_59_value + 1e-8)
        features_t.extend([si_to_f28_ratio, si_to_f59_ratio])
        
        # 3.2 Days to Cover - key metric for short squeeze potential
        days_to_cover = short_interest / (avg_volume + 1e-8)
        features_t.append(days_to_cover)
        
        # 3.3 Short Interest Change (if we have previous data)
        if t > 0:
            si_change = short_interest - data[t-1, 0]
            si_pct_change = si_change / (data[t-1, 0] + 1e-8)
        else:
            si_change = 0
            si_pct_change = 0
        features_t.extend([si_change, si_pct_change])
        
        # 3.4 Price Relationships Between Important Features
        # Ratios between important price points identified by DL
        f28_to_f59_ratio = feature_28_value / (feature_59_value + 1e-8)
        f26_to_f38_ratio = feature_26_value / (feature_38_value + 1e-8)
        features_t.extend([f28_to_f59_ratio, f26_to_f38_ratio])
        
        # 3.5 Recent Price Trend Using Important Features
        # Calculate trend using the important price points
        if t >= 2:
            f28_trend = (feature_28_value - data[t-2, 28]) / (data[t-2, 28] + 1e-8)
            f59_trend = (feature_59_value - data[t-2, 59]) / (data[t-2, 59] + 1e-8)
        else:
            f28_trend = 0
            f59_trend = 0
        features_t.extend([f28_trend, f59_trend])
        
        # 3.6 Volume-Adjusted Short Interest
        # Short interest normalized by trading volume
        vol_adjusted_si = short_interest * (1 / (avg_volume + 1e-8))
        features_t.append(vol_adjusted_si)
        
        # 3.7 Recent Price Volatility (using last 5 days of close prices)
        if len(close_prices) >= 5:
            recent_close = close_prices[-5:]
            price_std = np.std(recent_close)
            price_range = np.max(recent_close) - np.min(recent_close)
            normalized_volatility = price_std / (np.mean(recent_close) + 1e-8)
        else:
            price_std = 0
            price_range = 0
            normalized_volatility = 0
        features_t.append(normalized_volatility)
        
        # Add the features for this timestamp to the output
        output_features.append(features_t)
    
    # Convert to numpy array and handle NaN values
    output_array = np.array(output_features)
    output_array = np.nan_to_num(output_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    # DIFFERENCES FROM PREVIOUS ITERATIONS:
    # 1. Preserved ALL original features like baseline (which had best performance)
    # 2. Added only 11 carefully selected derived features (fewer than previous iterations)
    # 3. Focused specifically on enhancing the signal from features identified as important by DL
    # 4. Created relationships between important features rather than generic technical indicators
    # 5. Avoided complex transformations that might introduce noise
    # 6. Used simpler calculations that are more robust to outliers
    # 7. Maintained original time series structure for LSTM compatibility
    
    return output_array
============================================================

