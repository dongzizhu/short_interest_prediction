
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 97)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2]` → **days to cover** The number of days it would take to cover all short positions based on average daily trading volume.
  - `data[t, 3:63]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 3:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.
  - `data[t, 63]` → **options_put_call_volume_ratio** The ratio of the volume of put options to call options traded on that day.
  - `data[t, 64]` → **options_synthetic_short_cost** The cost associated with creating a synthetic short position using options.
  - `data[t, 65]` → **options_avg_implied_volatility** The average implied volatility of options, reflecting market expectations of future stock price volatility.
  - `data[t, 66]` → **shares_outstanding** The total number of shares of a company that are currently owned by all its shareholders, including restricted shares owned by company insiders and institutional investors.
  - `data[t, 67:82]` → Daily short interest volume for the stock over the past 15 days, flattened as **15 days × 1 columns**. Represents the total number of shares that were sold short on each day.
  - `data[t, 82:97]` → Daily total trading volume for the stock over the past 15 days, flattened as **15 days × 1 columns**.  Represents the total number of shares traded (buy + sell) on that trading day.

Total: 3 + 60 + 4 + 30 = 97 features(dimensions) per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 25 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy after the second iteration.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 10.54%
  Features: All 97 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_69_t3 (importance=0.0018), Feature_71_t3 (importance=0.0017), Feature_70_t3 (importance=0.0015), Feature_72_t3 (importance=0.0014), Feature_74_t3 (importance=0.0008)

Iteration 4: Iteration 4 - MAPE: 9.50% (Improvement Over Previous Best: -0.6%) (Improvement Over Last Iteration: -0.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_22_t3 (importance=0.0066), Feature_1_t3 (importance=0.0038), Feature_7_t1 (importance=0.0022), Feature_24_t1 (importance=0.0021), Feature_3_t0 (importance=0.0017)







PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 4):

```python
def construct_features(data):
    RAW_DIM = 97
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Start with essential raw features to keep
        # Based on feature importance analysis, keeping the most important raw features
        raw_keep = [
            data[t, 0],  # short interest - critical target-related feature
            data[t, 1],  # average daily volume - high importance in previous iteration
            data[t, 2],  # days to cover - key short interest metric
            data[t, 63], # options_put_call_volume_ratio - options sentiment indicator
            data[t, 64], # options_synthetic_short_cost - cost of shorting
            data[t, 65], # options_avg_implied_volatility - market's volatility expectation
            data[t, 66], # shares_outstanding - needed for float-based calculations
        ]
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract short interest volume and total trading volume
        short_volume = data[t, 67:82]
        total_volume = data[t, 82:97]
        shares_outstanding = data[t, 66]
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Short interest to float ratio - key metric for short squeeze potential
        # High feature importance in previous iteration (Feature_18_t3)
        si_float_ratio = data[t, 0] / max(shares_outstanding, 1e-8)
        eng.append(si_float_ratio)
        
        # 2. Short interest to average daily volume ratio - improved version of days to cover
        # High feature importance in previous iteration (Feature_1_t3)
        si_adv_ratio = data[t, 0] / max(data[t, 1], 1e-8)
        eng.append(si_adv_ratio)
        
        # 3. Weighted short volume ratio - recent shorting intensity with more weight on recent days
        # Improved version with exponential weighting for better recency bias
        if len(short_volume) >= 5 and len(total_volume) >= 5:
            # Exponential weights (sum to 1)
            weights = np.array([0.05, 0.1, 0.15, 0.3, 0.4])
            short_ratios = short_volume[-5:] / np.maximum(total_volume[-5:], 1e-8)
            weighted_short_ratio = np.sum(weights * short_ratios) if len(short_ratios) == 5 else np.mean(short_ratios)
        else:
            weighted_short_ratio = 0
        eng.append(weighted_short_ratio)
        
        # 4. Short interest growth acceleration - second derivative of short interest
        # Captures acceleration in short interest buildup, which is more predictive than simple growth
        if t > 2:
            si_t = data[t, 0]
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            growth_current = (si_t / max(si_t1, 1e-8)) - 1
            growth_prev = (si_t1 / max(si_t2, 1e-8)) - 1
            
            si_acceleration = growth_current - growth_prev
        else:
            si_acceleration = 0
        eng.append(si_acceleration)
        
        # 5. Relative short volume trend - normalized short volume trend
        # Improved to use ratio of ratios for better normalization
        if len(short_volume) >= 10 and len(total_volume) >= 10:
            recent_ratio = np.mean(short_volume[-5:]) / max(np.mean(total_volume[-5:]), 1e-8)
            prev_ratio = np.mean(short_volume[-10:-5]) / max(np.mean(total_volume[-10:-5]), 1e-8)
            rel_short_trend = recent_ratio / max(prev_ratio, 1e-8) - 1
        else:
            rel_short_trend = 0
        eng.append(rel_short_trend)
        
        # 6. Yang-Zhang Volatility - most comprehensive volatility estimator using OHLC
        # Superior to both Parkinson and Garman-Klass for capturing overnight jumps
        if len(open_prices) >= 5 and len(high_prices) >= 5 and len(low_prices) >= 5 and len(close_prices) >= 5:
            # Calculate overnight (close-to-open) volatility
            if len(close_prices) > 5:
                overnight_returns = np.log(open_prices[-5:] / np.maximum(np.roll(close_prices, 1)[-5:], 1e-8))
                overnight_vol = np.sum(overnight_returns**2) / 4  # n-1 degrees of freedom
            else:
                overnight_vol = 0
                
            # Calculate open-to-close volatility
            open_close_returns = np.log(close_prices[-5:] / np.maximum(open_prices[-5:], 1e-8))
            open_close_vol = np.sum(open_close_returns**2) / 4  # n-1 degrees of freedom
            
            # Calculate high-low volatility (Parkinson component)
            hl_ratio = np.log(high_prices[-5:] / np.maximum(low_prices[-5:], 1e-8))**2
            hl_vol = np.sum(hl_ratio) / (4 * np.log(2) * 5)
            
            # Yang-Zhang combines these components with optimal weights
            k = 0.34 / (1.34 + (5 + 1) / (5 - 1))  # Optimal k value
            yz_vol = np.sqrt(overnight_vol + k * open_close_vol + (1 - k) * hl_vol)
        else:
            yz_vol = 0
        eng.append(yz_vol)
        
        # 7. Implied volatility to Yang-Zhang volatility ratio - market vs sophisticated realized volatility
        # More accurate than previous IV/HV ratio by using better historical volatility estimate
        iv_yz_ratio = data[t, 65] / max(yz_vol, 1e-8)
        eng.append(iv_yz_ratio)
        
        # 8. Normalized short cost - short cost relative to stock price
        # Better normalization than previous cost efficiency metric
        if len(close_prices) > 0:
            norm_short_cost = data[t, 64] / max(close_prices[-1], 1e-8)
        else:
            norm_short_cost = data[t, 64]
        eng.append(norm_short_cost)
        
        # 9. Connors RSI - enhanced RSI that incorporates streak and percentile rank
        # More responsive than standard RSI for detecting potential reversals
        if len(close_prices) >= 14:
            # Standard RSI calculation
            delta = np.diff(np.append([close_prices[0]], close_prices))
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain[-14:])
            avg_loss = np.mean(loss[-14:])
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            
            # Streak calculation
            streak = 0
            for i in range(len(close_prices)-1, 0, -1):
                if close_prices[i] > close_prices[i-1]:
                    if streak >= 0:
                        streak += 1
                    else:
                        streak = 1
                elif close_prices[i] < close_prices[i-1]:
                    if streak <= 0:
                        streak -= 1
                    else:
                        streak = -1
                else:
                    streak = 0
                    
            # Normalize streak to 0-100 scale
            streak_rsi = 50 * (streak + 5) / 10 if abs(streak) <= 5 else 100 if streak > 5 else 0
            
            # Percentile rank of current close
            if len(close_prices) >= 100:
                lookback = 100
            else:
                lookback = len(close_prices)
            
            sorted_closes = np.sort(close_prices[-lookback:])
            rank = np.searchsorted(sorted_closes, close_prices[-1])
            pct_rank = 100 * rank / max(lookback - 1, 1)
            
            # Combine the components (standard formula for Connors RSI)
            connors_rsi = (rsi + streak_rsi + pct_rank) / 3
        else:
            connors_rsi = 50  # Default neutral value
        eng.append(connors_rsi)
        
        # 10. Bollinger Band Width Percentile - relative volatility contraction/expansion
        # Improved version of BB squeeze that provides historical context
        if len(close_prices) >= 20:
            # Calculate current BB width
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            current_bb_width = (2 * std20) / max(sma20, 1e-8)
            
            # Calculate historical BB widths for percentile ranking
            if len(close_prices) >= 60:  # Need enough history for meaningful percentile
                hist_widths = []
                for i in range(min(40, len(close_prices)-20)):
                    window = close_prices[-(20+i+1):-(i+1)]
                    sma = np.mean(window)
                    std = np.std(window)
                    hist_widths.append((2 * std) / max(sma, 1e-8))
                
                # Calculate percentile of current width
                if hist_widths:
                    sorted_widths = np.sort(hist_widths)
                    rank = np.searchsorted(sorted_widths, current_bb_width)
                    bb_width_percentile = 100 * rank / max(len(hist_widths), 1)
                else:
                    bb_width_percentile = 50
            else:
                bb_width_percentile = 50
        else:
            bb_width_percentile = 50  # Neutral value
        eng.append(bb_width_percentile)
        
        # 11. Anchored VWAP distance - price relative to significant VWAP level
        # Uses more significant price points for VWAP calculation
        if len(close_prices) >= 10 and len(total_volume) >= 10:
            # Find significant price point (local min/max) in the first 5 days
            first_5_high = np.max(high_prices[-10:-5])
            first_5_low = np.min(low_prices[-10:-5])
            last_5_close = close_prices[-5]
            
            # Determine if we should anchor to high or low based on current price
            if last_5_close > first_5_high:
                anchor_price = first_5_high  # Breakout above resistance
            elif last_5_close < first_5_low:
                anchor_price = first_5_low   # Breakdown below support
            else:
                anchor_price = (first_5_high + first_5_low) / 2  # In range
            
            # Calculate VWAP from the anchor point
            vwap_sum = anchor_price * total_volume[-10]  # Initialize with anchor
            vol_sum = total_volume[-10]
            
            for i in range(-9, 0):
                vwap_sum += close_prices[i] * total_volume[i]
                vol_sum += total_volume[i]
            
            anchored_vwap = vwap_sum / max(vol_sum, 1e-8)
            
            # Normalize by ATR
            atr10 = np.mean(high_prices[-10:] - low_prices[-10:])
            anchored_vwap_distance = (close_prices[-1] - anchored_vwap) / max(atr10, 1e-8)
        else:
            anchored_vwap_distance = 0
        eng.append(anchored_vwap_distance)
        
        # 12. Short volume intensity relative to price movement - detects divergence
        # Improved to incorporate price direction for better signal quality
        if len(short_volume) >= 10 and len(close_prices) >= 10:
            recent_short_vol = np.mean(short_volume[-5:])
            prev_short_vol = np.mean(short_volume[-10:-5])
            short_vol_change = recent_short_vol / max(prev_short_vol, 1e-8) - 1
            
            price_change = close_prices[-1] / max(close_prices[-5], 1e-8) - 1
            
            # Divergence occurs when short volume increases but price also increases
            # or when short volume decreases but price also decreases
            short_price_divergence = short_vol_change * price_change
        else:
            short_price_divergence = 0
        eng.append(short_price_divergence)
        
        # 13. Options sentiment change velocity - rate of change in options sentiment
        # Improved to use ratio of ratios for better normalization
        if t > 1:
            current_pc = data[t, 63]
            prev_pc = data[t-1, 63]
            
            # Use ratio instead of difference for better scaling
            pc_velocity = current_pc / max(prev_pc, 1e-8) - 1
        else:
            pc_velocity = 0
        eng.append(pc_velocity)
        
        # 14. Volume-weighted price momentum - price trend with volume confirmation
        # Improved to use log returns and better volume weighting
        if len(close_prices) >= 10 and len(total_volume) >= 10:
            # Calculate log returns
            returns = np.log(close_prices[-10:] / np.maximum(np.roll(close_prices[-10:], 1), 1e-8))[1:]
            
            # Weight returns by relative volume
            vol_weights = total_volume[-9:] / max(np.mean(total_volume[-9:]), 1e-8)
            weighted_returns = returns * vol_weights
            
            # Sum weighted returns for momentum
            vw_momentum = np.sum(weighted_returns)
        else:
            vw_momentum = 0
        eng.append(vw_momentum)
        
        # 15. Short squeeze potential composite - improved composite indicator
        # Refined weights based on feature importance analysis
        short_squeeze_potential = (
            si_float_ratio * 0.35 +           # Short interest to float (high importance)
            si_adv_ratio * 0.25 +             # Short interest to volume (high importance)
            weighted_short_ratio * 0.15 +     # Recent short volume ratio
            (100 - connors_rsi) / 100 * 0.15 + # Oversold indicator (inverted RSI)
            vw_momentum * 0.1                 # Price momentum
        )
        eng.append(short_squeeze_potential)
        
        # 16. Institutional ownership pressure - measures potential for institutional action
        # Improved to incorporate shares outstanding and average volume
        inst_ownership_est = 0.7 * shares_outstanding  # Estimated institutional ownership
        inst_pressure = (data[t, 0] / max(inst_ownership_est, 1e-8)) * (data[t, 1] / max(shares_outstanding, 1e-8))
        eng.append(inst_pressure)
        
        # 17. Liquidity-adjusted short ratio with volatility - short interest adjusted for market conditions
        # Improved to incorporate both volume and volatility
        liquidity_adj_short = si_adv_ratio * (1 + yz_vol) * (1 + weighted_short_ratio)
        eng.append(liquidity_adj_short)
        
        # 18. Short interest to market cap ratio - economic significance of short position
        # New feature that normalizes short interest by company value
        if len(close_prices) > 0:
            market_cap = shares_outstanding * close_prices[-1]
            si_mcap_ratio = data[t, 0] * close_prices[-1] / max(market_cap, 1e-8)
        else:
            si_mcap_ratio = 0
        eng.append(si_mcap_ratio)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Pad or truncate to MAX_TOTAL
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

Performance of this code: MAPE = 9.50%
Change from previous: -0.56%
Statistical Analysis: 71/100 features were significant (p < 0.05), 46 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 5):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 8.94%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 97`
  - `MAX_TOTAL = 25`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 97+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 97) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve the useful raw features**.
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 25**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
