
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 62)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2:62]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 2:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.

Total: 1 + 1 + 60 = 62 features per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 25 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy after the second iteration.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 7.37%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_6_t3 (importance=0.0009), Feature_1_t3 (importance=0.0009), Feature_35_t3 (importance=0.0007), Feature_3_t3 (importance=0.0007), Feature_0_t0 (importance=0.0006)

Iteration 1: Iteration 1 - MAPE: 8.67% (Improvement Over Baseline: -1.3%) (Improvement Over Last: -1.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_6_t3 (importance=0.0079), Feature_14_t1 (importance=0.0042), Feature_1_t1 (importance=0.0035), Feature_18_t0 (importance=0.0027), Feature_11_t0 (importance=0.0025)

Iteration 2: Iteration 2 - MAPE: 9.52% (Improvement Over Baseline: -2.1%) (Improvement Over Last: -0.8%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_10_t3 (importance=0.0144), Feature_17_t3 (importance=0.0089), Feature_9_t3 (importance=0.0049), Feature_2_t1 (importance=0.0044), Feature_16_t1 (importance=0.0039)

Iteration 3: Iteration 3 - MAPE: 8.22% (Improvement Over Baseline: -0.8%) (Improvement Over Last: +1.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_17_t2 (importance=0.0072), Feature_15_t0 (importance=0.0049), Feature_8_t3 (importance=0.0032), Feature_19_t3 (importance=0.0030), Feature_15_t1 (importance=0.0028)

Iteration 4: Iteration 4 - MAPE: 6.60% (Improvement Over Baseline: +0.8%) (Improvement Over Last: +1.6%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_8_t2 (importance=0.0042), Feature_17_t3 (importance=0.0042), Feature_11_t3 (importance=0.0040), Feature_15_t0 (importance=0.0038), Feature_19_t3 (importance=0.0036)

Iteration 5: Iteration 5 - MAPE: 6.89% (Improvement Over Baseline: -0.3%) (Improvement Over Last: -0.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_21_t3 (importance=0.0054), Feature_17_t3 (importance=0.0039), Feature_20_t1 (importance=0.0036), Feature_15_t3 (importance=0.0030), Feature_13_t3 (importance=0.0025)

Iteration 6: Iteration 6 - MAPE: 7.64% (Improvement Over Baseline: -1.0%) (Improvement Over Last: -0.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_11_t3 (importance=0.0099), Feature_18_t2 (importance=0.0063), Feature_9_t0 (importance=0.0060), Feature_6_t3 (importance=0.0050), Feature_5_t2 (importance=0.0034)

Iteration 7: Iteration 7 - MAPE: 7.20% (Improvement Over Baseline: -0.6%) (Improvement Over Last: +0.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_14_t3 (importance=0.0050), Feature_11_t2 (importance=0.0048), Feature_23_t3 (importance=0.0036), Feature_7_t3 (importance=0.0035), Feature_19_t3 (importance=0.0035)

Iteration 8: Iteration 8 - MAPE: 7.28% (Improvement Over Baseline: -0.7%) (Improvement Over Last: -0.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_12_t2 (importance=0.0085), Feature_10_t2 (importance=0.0054), Feature_9_t3 (importance=0.0050), Feature_9_t0 (importance=0.0046), Feature_4_t2 (importance=0.0043)









PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 8):

```python
def construct_features(data):
    RAW_DIM = 62
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Extract raw data for this timestep
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        ohlc = data[t, 2:].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep essential raw features based on importance analysis
        # Always include short interest and volume (consistently high importance)
        # Include most recent close price (high importance in multiple iterations)
        raw_keep = [
            short_interest,  # Current short interest
            avg_volume,      # Average daily volume
            close_prices[-1],  # Most recent close price
        ]
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # Initialize engineered features list
        eng = []
        
        # 1. Days to cover - consistently high importance across all iterations
        # This is a fundamental metric in short interest analysis
        days_to_cover = short_interest / max(avg_volume, 1e-8)
        # Apply log transformation to handle skewness
        days_to_cover_log = np.log1p(days_to_cover) if days_to_cover > 0 else 0
        eng.append(days_to_cover_log)
        
        # 2. Short interest momentum with adaptive lookback
        # High importance in iterations 4-7 (Feature_17_t3, Feature_11_t3)
        if t > 0:
            prev_si = data[t-1, 0]
            si_change = (short_interest - prev_si) / max(prev_si, 1e-8)
            
            # Add exponential weighting if we have more history
            if t >= 3:
                si_history = [data[max(0, t-i), 0] for i in range(3, 0, -1)]
                weights = np.exp(np.linspace(0, 1, len(si_history)))
                weights = weights / np.sum(weights)
                
                # Calculate weighted momentum
                si_changes = []
                for i in range(1, len(si_history)):
                    change = (si_history[i] - si_history[i-1]) / max(si_history[i-1], 1e-8)
                    si_changes.append(change)
                
                if si_changes:
                    weighted_si_change = np.sum(weights[1:] * np.array(si_changes))
                    # Blend recent and weighted with higher weight on recent (improved from iteration 7)
                    si_change = 0.8 * si_change + 0.2 * weighted_si_change
            
            # Apply sigmoid-like transformation to handle outliers while preserving sign
            si_change_transformed = np.sign(si_change) * np.log1p(abs(si_change) * 10)
            eng.append(si_change_transformed)
        else:
            eng.append(0.0)
        
        # 3. Short interest acceleration - captures rate of change in momentum
        # High importance in iterations 4-7 (Feature_11_t3, Feature_17_t3)
        if t >= 2:
            si_t = short_interest
            si_t1 = data[t-1, 0]
            si_t2 = data[t-2, 0]
            
            # Calculate first derivatives (velocity)
            velocity_t = (si_t - si_t1) / max(si_t1, 1e-8)
            velocity_t1 = (si_t1 - si_t2) / max(si_t2, 1e-8)
            
            # Calculate second derivative (acceleration)
            acceleration = velocity_t - velocity_t1
            
            # Normalize by average short interest to make more comparable across stocks
            avg_si = (si_t + si_t1 + si_t2) / 3
            norm_acceleration = acceleration * avg_si / max(si_t, 1e-8)
            
            # Apply sigmoid-like transformation to handle outliers while preserving sign
            # Increased scaling factor from 20 to 30 to better capture significant changes
            accel_transformed = np.sign(norm_acceleration) * np.log1p(abs(norm_acceleration) * 30)
            
            eng.append(accel_transformed)
        else:
            eng.append(0.0)
        
        # 4. Short squeeze potential - combining days to cover with price momentum and volume
        # High importance in iterations 4-5 (Feature_8_t2, Feature_17_t3)
        if len(close_prices) >= 5 and t >= 3:
            # Price momentum - exponentially weighted to emphasize recent movement
            weights = np.exp(np.linspace(0, 1, 5))
            weights = weights / np.sum(weights)
            
            price_returns = []
            for i in range(1, 5):
                ret = (close_prices[-i] / max(close_prices[-i-1], 1e-8)) - 1.0
                price_returns.append(ret)
            
            price_momentum = np.sum(weights[:-1] * np.array(price_returns))
            
            # Volume acceleration (increasing volume trend)
            volume_history = [data[max(0, t-3+i), 1] for i in range(4)]
            volume_changes = np.diff(volume_history)
            volume_accel = np.mean(volume_changes) / max(volume_history[0], 1e-8)
            
            # Combine days to cover with price momentum and volume acceleration
            # Higher score when days to cover is high, price momentum is positive, and volume is accelerating
            # Improved formula with higher weight on days to cover (most important factor)
            squeeze_potential = days_to_cover * (1 + max(0, price_momentum)) * (1 + max(0, volume_accel))
            
            # Log transform to handle skewness
            squeeze_potential_log = np.log1p(squeeze_potential) if squeeze_potential > 0 else 0
            eng.append(squeeze_potential_log)
        else:
            eng.append(0.0)
        
        # 5. Short interest relative to historical range
        # High importance in iterations 4-5 (Feature_15_t0, Feature_15_t3)
        if t >= 5:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(5, -1, -1)]
            
            # Calculate min, max, and range
            si_min = min(si_history)
            si_max = max(si_history)
            si_range = max(si_max - si_min, 1e-8)
            
            # Position within historical range (0 to 1)
            si_position = (short_interest - si_min) / si_range
            
            # Transform to highlight extremes (-1 to 1 scale)
            # -1: at historical low, 0: at middle of range, 1: at historical high
            si_position_transformed = 2 * si_position - 1
            
            # Apply non-linear transformation to emphasize extremes (new in iteration 8)
            si_position_transformed = np.sign(si_position_transformed) * np.power(abs(si_position_transformed), 0.7)
            
            eng.append(si_position_transformed)
        else:
            eng.append(0.0)
        
        # 6. Bollinger Band position with volatility adjustment
        # High importance in iterations 4-5 (Feature_15_t0, Feature_15_t1)
        if len(close_prices) >= 10:
            sma = np.mean(close_prices[-10:])
            std = np.std(close_prices[-10:])
            
            # Position within Bollinger Bands (-1 to +1 scale)
            bb_position = (close_prices[-1] - sma) / max(2 * std, 1e-8)
            bb_position = np.clip(bb_position, -1.0, 1.0)
            
            # Adjust signal strength based on BB width (volatility)
            bb_width = (2 * std) / max(sma, 1e-8)
            
            # Stronger signal in low volatility environments (tighter bands)
            # Improved scaling logic from iteration 7
            if bb_width < 0.05:  # Very tight bands
                bb_position *= 1.5
            elif bb_width > 0.15:  # Very wide bands
                bb_position *= 0.7  # Reduce signal strength in high volatility
            
            eng.append(bb_position)
        else:
            eng.append(0.0)
        
        # 7. RSI with adaptive lookback and extremes emphasis
        # High importance in iterations 4-7 (Feature_8_t3, Feature_19_t3)
        if len(close_prices) >= 10:
            # Use adaptive lookback based on available data
            lookback = min(14, len(close_prices)-1)
            deltas = np.diff(close_prices[-lookback-1:])
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            
            # Use exponential weighting for gains and losses
            weights = np.exp(np.linspace(0, 1, len(gains)))
            weights = weights / np.sum(weights)
            avg_gain = np.sum(weights * gains)
            avg_loss = np.sum(weights * losses)
            
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            
            # Transform RSI to emphasize extremes (more predictive for reversals)
            # Improved transformation from iteration 7 to better capture extremes
            rsi_transformed = 0.0
            if rsi < 30:
                # Oversold: stronger signal as RSI approaches 0
                rsi_transformed = -1.0 * np.power((30 - rsi) / 30, 1.5)
            elif rsi > 70:
                # Overbought: stronger signal as RSI approaches 100
                rsi_transformed = np.power((rsi - 70) / 30, 1.5)
            else:
                # Middle range gets less weight (closer to 0)
                rsi_transformed = (rsi - 50) / 40  # Middle: -0.5 to 0.5
            
            eng.append(rsi_transformed)
        else:
            eng.append(0.0)
        
        # 8. Short interest trend reversal signal
        # High importance in iterations 4-7 (Feature_11_t3, Feature_17_t3)
        if t >= 2:
            si_history = [data[max(0, t-2+i), 0] for i in range(3)]
            
            # Calculate first and second derivatives
            si_change = (si_history[1] - si_history[0]) / max(si_history[0], 1e-8)
            si_accel = ((si_history[2] - si_history[1]) / max(si_history[1], 1e-8)) - si_change
            
            # Strong signal when acceleration is opposite to direction (trend reversal)
            if np.sign(si_accel) != np.sign(si_change):
                # Increased reversal strength multiplier from 2 to 3 (new in iteration 8)
                reversal_strength = abs(si_accel) * 3
                # Direction: positive when SI decreasing, negative when SI increasing
                si_reversal = -np.sign(si_history[2] - si_history[1]) * reversal_strength
            else:
                # Continuation signal (weaker)
                si_reversal = -np.sign(si_history[2] - si_history[1]) * abs(si_accel) * 0.5
            
            # Apply sigmoid-like transformation to handle outliers
            si_reversal_transformed = np.sign(si_reversal) * np.log1p(abs(si_reversal) * 5)
            eng.append(si_reversal_transformed)
        else:
            eng.append(0.0)
        
        # 9. Volume trend with exponential weighting
        # High importance in iterations 5-6 (Feature_21_t3)
        if t >= 3:
            volumes = [data[max(0, t-i), 1] for i in range(3, -1, -1)]
            weights = np.exp(np.linspace(0, 1, len(volumes)))
            weights = weights / np.sum(weights)
            weighted_volume = np.sum(weights * np.array(volumes))
            
            # Normalize to recent average to detect relative changes
            volume_trend = avg_volume / max(weighted_volume, 1e-8)
            
            # Apply log transformation to handle skewness
            volume_trend_log = np.log1p(volume_trend) if volume_trend > 0 else 0
            eng.append(volume_trend_log)
        else:
            eng.append(0.0)
        
        # 10. Short interest to market cap proxy ratio
        # High importance in iterations 5-7 (Feature_17_t3)
        if len(close_prices) >= 1:
            # Calculate market cap proxy
            market_cap_proxy = close_prices[-1] * avg_volume
            
            # Calculate short interest relative to market cap
            si_to_mcap = short_interest / max(market_cap_proxy, 1e-8)
            
            # Log transform to handle skewness
            # Increased scaling factor from 1000 to 2000 to better capture significant changes
            si_to_mcap_log = np.log1p(si_to_mcap * 2000) if si_to_mcap > 0 else 0
            
            eng.append(si_to_mcap_log)
        else:
            eng.append(0.0)
        
        # 11. Short interest momentum divergence
        # High importance in iterations 5-6 (Feature_20_t1)
        if t >= 3 and len(close_prices) >= 5:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(3, -1, -1)]
            
            # Calculate short interest momentum
            si_momentum = (si_history[-1] - si_history[0]) / max(si_history[0], 1e-8)
            
            # Calculate price momentum
            price_momentum = (close_prices[-1] / max(close_prices[-5], 1e-8)) - 1.0
            
            # Divergence occurs when price and short interest move in opposite directions
            # Stronger signal when both movements are significant
            divergence = -1 * si_momentum * price_momentum
            
            # Amplify signal when divergence is positive (true divergence)
            # Increased amplification factor from 2 to 3 (new in iteration 8)
            if divergence > 0:
                divergence *= 3
            
            # Apply sigmoid-like transformation to handle outliers
            divergence_transformed = np.sign(divergence) * np.log1p(abs(divergence) * 10)
            eng.append(divergence_transformed)
        else:
            eng.append(0.0)
        
        # 12. Short interest efficiency ratio
        # High importance in iterations 6-7 (Feature_18_t2)
        if t >= 5:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(5, -1, -1)]
            
            # Calculate directional movement
            directional_movement = abs(si_history[-1] - si_history[0])
            
            # Calculate total movement (sum of absolute changes)
            total_movement = 0
            for i in range(1, len(si_history)):
                total_movement += abs(si_history[i] - si_history[i-1])
            
            # Efficiency ratio (0 to 1)
            # Higher values indicate more directional movement (trend)
            # Lower values indicate more choppy movement (range-bound)
            efficiency = directional_movement / max(total_movement, 1e-8)
            
            # Transform to highlight extremes
            # Values close to 0 or 1 are more significant
            # Improved transformation from iteration 7 to better capture extremes
            if efficiency < 0.5:
                efficiency_transformed = -1 * np.power((0.5 - efficiency) * 2, 0.8)  # -1 to 0
            else:
                efficiency_transformed = np.power((efficiency - 0.5) * 2, 0.8)  # 0 to 1
            
            eng.append(efficiency_transformed)
        else:
            eng.append(0.0)
        
        # 13. Price-volume correlation (improved divergence detection)
        # High importance in iterations 6-7 (Feature_6_t3)
        if len(close_prices) >= 5 and t >= 4:
            # Get price and volume histories
            price_history = close_prices[-5:]
            volume_history = [data[max(0, t-4+i), 1] for i in range(5)]
            
            # Calculate price and volume changes
            price_changes = np.diff(price_history)
            volume_changes = np.diff(volume_history)
            
            # Calculate correlation if we have enough data
            if len(price_changes) >= 3 and len(volume_changes) >= 3:
                # Use sign correlation for robustness
                price_signs = np.sign(price_changes)
                volume_signs = np.sign(volume_changes)
                
                # Count matching and opposite signs
                matches = np.sum(price_signs * volume_signs)
                correlation = matches / len(price_signs)
                
                # Transform to highlight divergence (-1 to 1 scale)
                # Apply non-linear transformation to emphasize extremes (new in iteration 8)
                correlation_transformed = np.sign(correlation) * np.power(abs(correlation), 0.7)
                eng.append(correlation_transformed)
            else:
                eng.append(0.0)
        else:
            eng.append(0.0)
        
        # 14. NEW: Relative short interest change vs. price change
        # Combining insights from high-importance features in iterations 4-7
        if t >= 5 and len(close_prices) >= 5:
            # Get short interest and price histories
            si_history = [data[max(0, t-i), 0] for i in range(5, -1, -1)]
            price_history = close_prices[-6:]
            
            # Calculate percentage changes
            si_pct_change = (si_history[-1] / max(si_history[0], 1e-8)) - 1.0
            price_pct_change = (price_history[-1] / max(price_history[0], 1e-8)) - 1.0
            
            # Calculate relative change ratio
            # Positive: SI increasing faster than price (or SI increasing while price decreasing)
            # Negative: SI decreasing faster than price (or SI decreasing while price increasing)
            relative_change = si_pct_change - price_pct_change
            
            # Apply sigmoid-like transformation to handle outliers
            relative_change_transformed = np.sign(relative_change) * np.log1p(abs(relative_change) * 5)
            
            eng.append(relative_change_transformed)
        else:
            eng.append(0.0)
        
        # 15. NEW: Short interest volatility ratio
        # Combining insights from high-importance features in iterations 4-7
        if t >= 10:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(10, -1, -1)]
            
            # Calculate short-term and long-term volatility
            short_term_si = si_history[-5:]
            long_term_si = si_history
            
            # Calculate volatility (standard deviation / mean)
            short_term_vol = np.std(short_term_si) / max(np.mean(short_term_si), 1e-8)
            long_term_vol = np.std(long_term_si) / max(np.mean(long_term_si), 1e-8)
            
            # Calculate volatility ratio
            # > 1: short-term volatility is higher than long-term (increasing uncertainty)
            # < 1: short-term volatility is lower than long-term (decreasing uncertainty)
            vol_ratio = short_term_vol / max(long_term_vol, 1e-8)
            
            # Apply log transformation to handle skewness
            vol_ratio_log = np.log(vol_ratio) if vol_ratio > 0 else 0
            
            eng.append(vol_ratio_log)
        else:
            eng.append(0.0)
        
        # 16. NEW: Short interest regime change detection with volume confirmation
        # Combining insights from high-importance features in iterations 4-7
        if t >= 10:
            # Get short interest and volume history
            si_history = [data[max(0, t-i), 0] for i in range(10, -1, -1)]
            volume_history = [data[max(0, t-i), 1] for i in range(10, -1, -1)]
            
            # Calculate moving averages for short interest
            short_ma = np.mean(si_history[-3:])
            long_ma = np.mean(si_history)
            
            # Calculate regime indicator
            # Positive: short-term average > long-term average (increasing trend)
            # Negative: short-term average < long-term average (decreasing trend)
            regime = (short_ma / max(long_ma, 1e-8)) - 1.0
            
            # Calculate volume trend
            recent_volume = np.mean(volume_history[-3:])
            avg_volume = np.mean(volume_history)
            volume_trend = (recent_volume / max(avg_volume, 1e-8)) - 1.0
            
            # Combine regime with volume confirmation
            # Stronger signal when volume trend confirms short interest regime
            if np.sign(regime) == np.sign(volume_trend):
                regime_signal = regime * (1 + abs(volume_trend))
            else:
                regime_signal = regime * 0.5  # Weaker signal when volume doesn't confirm
            
            # Apply sigmoid-like transformation to handle outliers
            regime_signal_transformed = np.sign(regime_signal) * np.log1p(abs(regime_signal) * 10)
            
            eng.append(regime_signal_transformed)
        else:
            eng.append(0.0)
        
        # 17. NEW: Short interest mean reversion potential
        # Combining insights from high-importance features in iterations 4-7
        if t >= 10:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(10, -1, -1)]
            
            # Calculate z-score of current short interest relative to history
            si_mean = np.mean(si_history[:-1])  # Exclude current
            si_std = np.std(si_history[:-1])
            
            # Z-score: how many standard deviations from the mean
            z_score = (short_interest - si_mean) / max(si_std, 1e-8)
            
            # Calculate mean reversion potential
            # Higher absolute z-score indicates higher mean reversion potential
            # Sign indicates direction: negative z-score means potential increase, positive means potential decrease
            reversion_potential = -1 * z_score  # Invert sign to indicate direction of expected movement
            
            # Apply sigmoid function to handle extreme values
            reversion_potential_transformed = np.tanh(reversion_potential)
            
            eng.append(reversion_potential_transformed)
        else:
            eng.append(0.0)
        
        # 18. NEW: Short interest trend strength with price confirmation
        # Combining insights from high-importance features in iterations 4-7
        if t >= 5 and len(close_prices) >= 5:
            # Get short interest and price histories
            si_history = [data[max(0, t-i), 0] for i in range(5, -1, -1)]
            price_history = close_prices[-6:]
            
            # Calculate linear regression slope for short interest
            x = np.arange(len(si_history))
            si_slope = np.polyfit(x, si_history, 1)[0]
            si_slope_norm = si_slope / max(np.mean(si_history), 1e-8)
            
            # Calculate linear regression slope for price
            price_slope = np.polyfit(x, price_history, 1)[0]
            price_slope_norm = price_slope / max(np.mean(price_history), 1e-8)
            
            # Calculate trend strength with price confirmation
            # Stronger signal when price trend confirms short interest trend
            if np.sign(si_slope_norm) != np.sign(price_slope_norm):
                # Divergence: short interest and price moving in opposite directions
                # This is often a stronger signal
                trend_strength = si_slope_norm * 2
            else:
                # Convergence: short interest and price moving in same direction
                # This is often a weaker signal
                trend_strength = si_slope_norm * 0.5
            
            # Apply sigmoid-like transformation to handle outliers
            trend_strength_transformed = np.sign(trend_strength) * np.log1p(abs(trend_strength) * 20)
            
            eng.append(trend_strength_transformed)
        else:
            eng.append(0.0)
        
        # 19. NEW: Short interest volatility adjusted for price volatility
        # Combining insights from high-importance features in iterations 4-7
        if t >= 5 and len(close_prices) >= 5:
            # Get short interest and price histories
            si_history = [data[max(0, t-i), 0] for i in range(5, -1, -1)]
            price_history = close_prices[-6:]
            
            # Calculate volatility (coefficient of variation)
            si_vol = np.std(si_history) / max(np.mean(si_history), 1e-8)
            price_vol = np.std(price_history) / max(np.mean(price_history), 1e-8)
            
            # Calculate relative volatility
            # > 1: short interest more volatile than price
            # < 1: price more volatile than short interest
            rel_vol = si_vol / max(price_vol, 1e-8)
            
            # Apply log transformation to handle skewness
            rel_vol_log = np.log(rel_vol) if rel_vol > 0 else 0
            
            eng.append(rel_vol_log)
        else:
            eng.append(0.0)
        
        # 20. NEW: Short interest momentum with volume confirmation
        # Combining insights from high-importance features in iterations 4-7
        if t >= 3:
            # Get short interest and volume histories
            si_history = [data[max(0, t-i), 0] for i in range(3, -1, -1)]
            volume_history = [data[max(0, t-i), 1] for i in range(3, -1, -1)]
            
            # Calculate short interest momentum
            si_momentum = (si_history[-1] - si_history[0]) / max(si_history[0], 1e-8)
            
            # Calculate volume momentum
            volume_momentum = (volume_history[-1] - volume_history[0]) / max(volume_history[0], 1e-8)
            
            # Combine short interest momentum with volume confirmation
            # Stronger signal when volume confirms short interest movement
            if np.sign(si_momentum) == np.sign(volume_momentum):
                # Volume confirms short interest movement
                momentum_signal = si_momentum * (1 + abs(volume_momentum))
            else:
                # Volume contradicts short interest movement
                momentum_signal = si_momentum * 0.5
            
            # Apply sigmoid-like transformation to handle outliers
            momentum_signal_transformed = np.sign(momentum_signal) * np.log1p(abs(momentum_signal) * 10)
            
            eng.append(momentum_signal_transformed)
        else:
            eng.append(0.0)
        
        # 21. NEW: Short interest trend stability
        # Combining insights from high-importance features in iterations 4-7
        if t >= 10:
            # Get short interest history
            si_history = [data[max(0, t-i), 0] for i in range(10, -1, -1)]
            
            # Calculate consecutive movements in same direction
            consecutive_same_dir = 0
            max_consecutive = 0
            
            for i in range(1, len(si_history)):
                current_dir = np.sign(si_history[i] - si_history[i-1])
                
                if i > 1:
                    prev_dir = np.sign(si_history[i-1] - si_history[i-2])
                    if current_dir == prev_dir and current_dir != 0:
                        consecutive_same_dir += 1
                    else:
                        consecutive_same_dir = 0
                
                max_consecutive = max(max_consecutive, consecutive_same_dir)
            
            # Calculate trend stability (0 to 1)
            # Higher values indicate more stable trend
            trend_stability = max_consecutive / (len(si_history) - 1)
            
            # Transform to highlight extremes (-1 to 1 scale)
            # Negative: unstable trend, Positive: stable trend
            trend_stability_transformed = 2 * trend_stability - 1
            
            eng.append(trend_stability_transformed)
        else:
            eng.append(0.0)
        
        # 22. NEW: Short interest to price range ratio
        # Combining insights from high-importance features in iterations 4-7
        if len(close_prices) >= 10:
            # Calculate price range
            price_range = (np.max(high_prices[-10:]) - np.min(low_prices[-10:])) / max(np.mean(close_prices[-10:]), 1e-8)
            
            # Calculate short interest relative to price range
            si_to_range = short_interest * price_range / max(avg_volume, 1e-8)
            
            # Log transform to handle skewness
            si_to_range_log = np.log1p(si_to_range * 10) if si_to_range > 0 else 0
            
            eng.append(si_to_range_log)
        else:
            eng.append(0.0)
        
        # Ensure we don't exceed MAX_NEW
        eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent size
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
            
        features[t, :] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

Performance of this code: MAPE = 7.28%
Change from previous: -0.68%
Statistical Analysis: 42/100 features were significant (p < 0.05), 34 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 9):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 6.60%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 62`
  - `MAX_TOTAL = 25`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 62+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve the raw features**.
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 25**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
