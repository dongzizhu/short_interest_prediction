
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 97)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2]` → **days to cover** The number of days it would take to cover all short positions based on average daily trading volume.
  - `data[t, 3:63]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 3:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.
  - `data[t, 63]` → **options_put_call_volume_ratio** The ratio of the volume of put options to call options traded on that day.
  - `data[t, 64]` → **options_synthetic_short_cost** The cost associated with creating a synthetic short position using options.
  - `data[t, 65]` → **options_avg_implied_volatility** The average implied volatility of options, reflecting market expectations of future stock price volatility.
  - `data[t, 66]` → **shares_outstanding** The total number of shares of a company that are currently owned by all its shareholders, including restricted shares owned by company insiders and institutional investors.
  - `data[t, 67:82]` → Daily short interest volume for the stock over the past 15 days, flattened as **15 days × 1 columns**. Represents the total number of shares that were sold short on each day.
  - `data[t, 82:97]` → Daily total trading volume for the stock over the past 15 days, flattened as **15 days × 1 columns**.  Represents the total number of shares traded (buy + sell) on that trading day.

Total: 3 + 60 + 4 + 30 = 97 features(dimensions) per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 25 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy after the second iteration.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 10.95%
  Features: All 97 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_72_t0 (importance=0.0010), Feature_71_t0 (importance=0.0005), Feature_65_t2 (importance=0.0004), Feature_76_t1 (importance=0.0004), Feature_69_t3 (importance=0.0004)

Iteration 7: Iteration 7 - MAPE: 10.43% (Improvement Over Previous Best: -0.6%) (Improvement Over Last Iteration: +0.5%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_14_t2 (importance=0.0007), Feature_22_t1 (importance=0.0007), Feature_10_t1 (importance=0.0007), Feature_14_t0 (importance=0.0006), Feature_8_t2 (importance=0.0005)







PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 7):

```python
def construct_features(data):
    """
    Constructs features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 97)
        
    Returns:
        numpy array of shape (lookback_window, MAX_TOTAL)
    """
    RAW_DIM = 97
    MAX_TOTAL = 25
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Initialize lists for raw and engineered features
        raw_keep = []
        eng = []
        
        # Keep critical baseline features - these showed high importance
        raw_keep.append(data[t, 0])  # short interest (most important)
        raw_keep.append(data[t, 1])  # average daily volume
        raw_keep.append(data[t, 2])  # days to cover
        
        # Keep key options data which showed high importance
        raw_keep.append(data[t, 63])  # options_put_call_volume_ratio
        raw_keep.append(data[t, 64])  # options_synthetic_short_cost
        raw_keep.append(data[t, 65])  # options_avg_implied_volatility
        
        # Extract OHLC data for the past 15 days
        ohlc = data[t, 3:63].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Extract short volume and total volume data
        short_volume = data[t, 67:82]
        total_volume = data[t, 82:97]
        shares_outstanding = data[t, 66]
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        
        # 1. Short Volume Ratio (daily short volume / total volume)
        # Consistently high importance in previous iterations
        short_volume_ratio = np.zeros_like(short_volume)
        for i in range(len(short_volume)):
            denom = max(abs(total_volume[i]), 1e-8)
            short_volume_ratio[i] = short_volume[i] / denom
        
        # Average short volume ratio over past 15 days
        eng.append(np.mean(short_volume_ratio))
        
        # 2. Short Interest to Float Ratio
        # Key metric showing what percentage of available shares are being shorted
        shares_out = max(shares_outstanding, 1e-8)
        short_interest_to_float = data[t, 0] / shares_out
        eng.append(short_interest_to_float)
        
        # 3. Short Interest Change Rate
        # Rate of change in short interest, a key predictor
        if t > 0:
            prev_si = max(data[t-1, 0], 1e-8)
            current_si = data[t, 0]
            si_change_rate = (current_si / prev_si) - 1
            eng.append(si_change_rate)
        else:
            eng.append(0.0)
        
        # 4. Exponentially Weighted Short Volume Ratio
        # Gives more weight to recent short volume activity
        if len(short_volume_ratio) >= 10:
            weights = np.exp(np.linspace(0, 1, 10))
            weights = weights / np.sum(weights)  # Normalize weights
            exp_weighted_svr = np.sum(short_volume_ratio[-10:] * weights)
            eng.append(exp_weighted_svr)
        else:
            eng.append(np.mean(short_volume_ratio))
        
        # 5. Price Volatility (Normalized)
        # Measures recent price volatility which can trigger short covering
        if len(close_prices) >= 10:
            price_std = np.std(close_prices[-10:])
            price_mean = max(np.mean(close_prices[-10:]), 1e-8)
            normalized_volatility = price_std / price_mean
            eng.append(normalized_volatility)
        else:
            eng.append(0.0)
        
        # 6. RSI (Relative Strength Index)
        # Technical indicator that can signal potential short covering
        if len(close_prices) >= 14:
            delta = np.diff(close_prices[-15:])
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            avg_gain = np.mean(gain)
            avg_loss = max(np.mean(loss), 1e-8)
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
        else:
            eng.append(50.0)  # Neutral RSI value
        
        # 7. Volume Trend
        # Increasing volume can signal stronger covering potential
        if len(total_volume) >= 10:
            recent_vol_avg = np.mean(total_volume[-5:])
            older_vol_avg = max(np.mean(total_volume[-10:-5]), 1e-8)
            volume_trend = (recent_vol_avg / older_vol_avg) - 1
            eng.append(volume_trend)
        else:
            eng.append(0.0)
        
        # 8. Short Interest to Days to Cover Ratio
        # Relates total short interest to the time needed to cover all shorts
        days_to_cover = max(data[t, 2], 1e-8)
        short_interest = data[t, 0]
        si_to_dtc = short_interest / days_to_cover
        eng.append(si_to_dtc)
        
        # 9. Options Implied Volatility to Historical Volatility Ratio
        # Higher ratio may indicate increased likelihood of short covering
        implied_vol = data[t, 65]
        if len(close_prices) >= 10:
            # Calculate historical volatility (annualized)
            returns = np.diff(close_prices[-11:]) / np.maximum(close_prices[-11:-1], 1e-8)
            hist_vol = max(np.std(returns) * np.sqrt(252), 1e-8)
            iv_to_hv = implied_vol / hist_vol
            eng.append(iv_to_hv)
        else:
            eng.append(1.0)
        
        # 10. Bollinger Band Position
        # Position of current price within Bollinger Bands can signal potential reversals
        if len(close_prices) >= 20:
            sma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            upper_band = sma20 + (2 * std20)
            lower_band = sma20 - (2 * std20)
            band_width = max(upper_band - lower_band, 1e-8)
            bb_position = (close_prices[-1] - lower_band) / band_width
            eng.append(bb_position)
        else:
            eng.append(0.5)  # Middle of the band
        
        # 11. Short Volume Concentration
        # Measures recent concentration of short volume compared to older periods
        if len(short_volume) >= 10:
            recent_short_vol = np.mean(short_volume[-5:])
            older_short_vol = max(np.mean(short_volume[-10:-5]), 1e-8)
            short_vol_concentration = recent_short_vol / older_short_vol
            eng.append(short_vol_concentration)
        else:
            eng.append(1.0)
        
        # 12. Short Interest to Options Put/Call Ratio
        # Relationship between short interest and options sentiment
        put_call_ratio = max(data[t, 63], 1e-8)
        si_to_options = data[t, 0] / put_call_ratio
        eng.append(si_to_options)
        
        # 13. VWAP Distance
        # Distance of current price from Volume Weighted Average Price
        if len(close_prices) >= 5 and len(total_volume) >= 5:
            vwap = np.sum(close_prices[-5:] * total_volume[-5:]) / max(np.sum(total_volume[-5:]), 1e-8)
            vwap_distance = (close_prices[-1] / max(vwap, 1e-8)) - 1
            eng.append(vwap_distance)
        else:
            eng.append(0.0)
        
        # 14. Money Flow Index (MFI)
        # Volume-weighted RSI that can indicate potential reversals
        if len(close_prices) >= 14 and len(total_volume) >= 14:
            typical_price = (high_prices[-14:] + low_prices[-14:] + close_prices[-14:]) / 3
            money_flow = typical_price * total_volume[-14:]
            
            delta_tp = np.diff(typical_price)
            positive_flow = np.sum(money_flow[1:][delta_tp > 0])
            negative_flow = np.sum(money_flow[1:][delta_tp < 0])
            
            if negative_flow == 0:
                mfi = 100
            else:
                money_ratio = positive_flow / max(negative_flow, 1e-8)
                mfi = 100 - (100 / (1 + money_ratio))
            
            eng.append(mfi)
        else:
            eng.append(50.0)  # Neutral MFI value
        
        # 15. Short Interest Utilization
        # Percentage of shares outstanding that are being shorted
        shares_out = max(shares_outstanding, 1e-8)
        utilization = data[t, 0] / shares_out
        eng.append(utilization)
        
        # 16. Price Momentum
        # Recent price momentum can trigger short covering
        if len(close_prices) >= 10:
            price_momentum = (close_prices[-1] / max(close_prices[-10], 1e-8)) - 1
            eng.append(price_momentum)
        else:
            eng.append(0.0)
        
        # 17. Average True Range (ATR) - Volatility Indicator
        # Measures market volatility
        if len(high_prices) >= 14 and len(low_prices) >= 14 and len(close_prices) >= 14:
            tr_values = []
            for i in range(1, 14):
                high_low = high_prices[-i] - low_prices[-i]
                high_close = abs(high_prices[-i] - close_prices[-(i+1)])
                low_close = abs(low_prices[-i] - close_prices[-(i+1)])
                tr = max(high_low, high_close, low_close)
                tr_values.append(tr)
            atr = np.mean(tr_values)
            # Normalize by price level
            norm_atr = atr / max(close_prices[-1], 1e-8)
            eng.append(norm_atr)
        else:
            eng.append(0.0)
        
        # 18. Short Volume Trend
        # Trend in short volume can indicate changing sentiment
        if len(short_volume) >= 10:
            recent_short = np.mean(short_volume[-5:])
            older_short = max(np.mean(short_volume[-10:-5]), 1e-8)
            short_trend = (recent_short / older_short) - 1
            eng.append(short_trend)
        else:
            eng.append(0.0)
        
        # 19. Synthetic Short Cost to Implied Volatility Ratio
        # Relates cost of shorting to market expectations of volatility
        synthetic_cost = data[t, 64]
        implied_vol = max(data[t, 65], 1e-8)
        cost_to_iv = synthetic_cost / implied_vol
        eng.append(cost_to_iv)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width
        if row.size < MAX_TOTAL:
            # Pad with zeros if needed
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            # Truncate if needed
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

Performance of this code: MAPE = 10.43%
Change from previous: -0.57%
Statistical Analysis: 93/100 features were significant (p < 0.05), 79 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 8):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 9.87%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 97`
  - `MAX_TOTAL = 25`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 97+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 97) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Preserve the useful raw features**.
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 25**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
