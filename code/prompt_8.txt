
You are a financial data scientist specializing in **feature engineering for short-interest prediction** on equity time series.

## Data schema
- Input to your function: a **numpy array** `data` with shape **(lookback_window, 62)** for a *single* sample.
- Feature layout at each timestep `t`:
  - `data[t, 0]` → **short interest** at time *T* (reported every 15 days)
  - `data[t, 1]` → **average daily volume (past 15 days)**
  - `data[t, 2:62]` → **OHLC** over the past 15 days, flattened as **15 days × 4 columns** in order **[O, H, L, C]**  
    Use: `ohlc = data[t, 2:].reshape(15, 4)` → `open, high, low, close = ohlc[:,0], ohlc[:,1], ohlc[:,2], ohlc[:,3]`.

Total: 1 + 1 + 60 = 62 features per timestamp.

## Dataset constraints
- Only ~180 total samples available (very small).
- To reduce overfitting: **keep only the useful raw channels** and add **new features** so that **(kept raw + new) ≤ 85 total columns**.
- You **may drop** raw channels with consistently low importance or redundancy.
- Avoid redundant or near-duplicate engineered features. Prefer a small, diverse set.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 7.74%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0473), Feature_1 (importance=0.0098), Feature_40 (importance=0.0020), Feature_55 (importance=0.0018), Feature_27 (importance=0.0013)

Iteration 1: Iteration 1 - MAPE: 8.03% (Improvement Over Baseline: -0.3%) (Improvement Over Last: -0.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_21 (importance=0.0067), Feature_22 (importance=0.0024), Feature_9 (importance=0.0014), Feature_18 (importance=0.0014), Feature_25 (importance=0.0011)

Iteration 2: Iteration 2 - MAPE: 9.49% (Improvement Over Baseline: -1.7%) (Improvement Over Last: -1.5%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_5 (importance=0.0034), Feature_6 (importance=0.0023), Feature_7 (importance=0.0022), Feature_1 (importance=0.0020), Feature_0 (importance=0.0019)

Iteration 3: Iteration 3 - MAPE: 7.29% (Improvement Over Baseline: +0.5%) (Improvement Over Last: +2.2%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_8 (importance=0.0026), Feature_24 (importance=0.0009), Feature_37 (importance=0.0009), Feature_32 (importance=0.0007), Feature_11 (importance=0.0007)

Iteration 4: Iteration 4 - MAPE: 9.99% (Improvement Over Baseline: -2.7%) (Improvement Over Last: -2.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_7 (importance=0.0009), Feature_31 (importance=0.0007), Feature_50 (importance=0.0006), Feature_0 (importance=0.0006), Feature_15 (importance=0.0005)

Iteration 5: Iteration 5 - MAPE: 9.63% (Improvement Over Baseline: -2.3%) (Improvement Over Last: +0.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_27 (importance=0.0014), Feature_32 (importance=0.0009), Feature_50 (importance=0.0009), Feature_0 (importance=0.0008), Feature_44 (importance=0.0006)

Iteration 6: Iteration 6 - MAPE: 7.70% (Improvement Over Baseline: -0.4%) (Improvement Over Last: +1.9%)
  Features: fallback feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0084), Feature_9 (importance=0.0057), Feature_6 (importance=0.0047), Feature_7 (importance=0.0028), Feature_14 (importance=0.0019)

Iteration 7: Iteration 7 - MAPE: 9.39% (Improvement Over Baseline: -2.1%) (Improvement Over Last: -1.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_23 (importance=0.0043), Feature_13 (importance=0.0029), Feature_12 (importance=0.0015), Feature_0 (importance=0.0014), Feature_11 (importance=0.0013)









PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 7):

```python
def construct_features(data):
    # Constants
    RAW_DIM = 62
    MAX_TOTAL = 80
    
    lookback_window = data.shape[0]
    features = np.zeros((lookback_window, MAX_TOTAL), dtype=np.float32)
    
    for t in range(lookback_window):
        # Extract raw data for this timestep
        short_interest = data[t, 0]
        avg_volume = data[t, 1]
        ohlc = data[t, 2:].reshape(15, 4)
        open_prices, high_prices, low_prices, close_prices = ohlc[:, 0], ohlc[:, 1], ohlc[:, 2], ohlc[:, 3]
        
        # Keep essential raw features
        raw_keep = [
            short_interest,  # Always keep short interest (most important in baseline)
            avg_volume,      # Always keep average volume (2nd most important in baseline)
            close_prices[-1],  # Latest close price
            high_prices[-1],   # Latest high price
            low_prices[-1],    # Latest low price
            open_prices[-1],   # Latest open price
        ]
        
        # Calculate MAX_NEW based on raw features kept
        MAX_NEW = MAX_TOTAL - len(raw_keep)
        eng = []
        
        # Price-based features
        
        # 1. Recent price changes (short-term momentum)
        if len(close_prices) > 1:
            daily_returns = np.diff(close_prices) / np.maximum(close_prices[:-1], 1e-8)
            last_return = daily_returns[-1]
            eng.append(last_return)  # Last day return
            
            # 3-day return
            if len(close_prices) >= 4:
                return_3d = (close_prices[-1] / max(close_prices[-4], 1e-8)) - 1
                eng.append(return_3d)
            else:
                eng.append(0.0)
                
            # 5-day return
            if len(close_prices) >= 6:
                return_5d = (close_prices[-1] / max(close_prices[-6], 1e-8)) - 1
                eng.append(return_5d)
            else:
                eng.append(0.0)
                
            # 10-day return
            if len(close_prices) >= 11:
                return_10d = (close_prices[-1] / max(close_prices[-11], 1e-8)) - 1
                eng.append(return_10d)
            else:
                eng.append(0.0)
        else:
            eng.extend([0.0, 0.0, 0.0, 0.0])  # Placeholders if not enough data
        
        # 2. Volatility measures
        if len(close_prices) > 1:
            # Daily volatility (std of returns)
            volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0
            eng.append(volatility)
            
            # High-Low range relative to close (normalized)
            avg_hl_range = np.mean((high_prices - low_prices) / np.maximum(close_prices, 1e-8))
            eng.append(avg_hl_range)
            
            # Recent volatility (last 5 days)
            recent_vol = np.std(daily_returns[-5:]) if len(daily_returns) >= 5 else volatility
            eng.append(recent_vol)
        else:
            eng.extend([0.0, 0.0, 0.0])
        
        # 3. Volume-based features
        # Volume relative to short interest
        vol_to_si_ratio = avg_volume / max(short_interest, 1e-8)
        eng.append(vol_to_si_ratio)
        
        # 4. Technical indicators
        # RSI (Relative Strength Index)
        if len(daily_returns) >= 14:
            gains = np.maximum(daily_returns, 0)
            losses = np.maximum(-daily_returns, 0)
            avg_gain = np.mean(gains[-14:])
            avg_loss = np.mean(losses[-14:])
            rs = avg_gain / max(avg_loss, 1e-8)
            rsi = 100 - (100 / (1 + rs))
            eng.append(rsi)
            
            # RSI momentum (change in RSI)
            if len(daily_returns) >= 15:
                prev_gains = np.maximum(daily_returns[:-1], 0)
                prev_losses = np.maximum(-daily_returns[:-1], 0)
                prev_avg_gain = np.mean(prev_gains[-14:])
                prev_avg_loss = np.mean(prev_losses[-14:])
                prev_rs = prev_avg_gain / max(prev_avg_loss, 1e-8)
                prev_rsi = 100 - (100 / (1 + prev_rs))
                rsi_momentum = rsi - prev_rsi
                eng.append(rsi_momentum)
            else:
                eng.append(0.0)
        else:
            eng.extend([50.0, 0.0])  # Default RSI is 50, momentum is 0
        
        # 5. Moving averages and crossovers
        if len(close_prices) >= 5:
            sma5 = np.mean(close_prices[-5:])
            eng.append(sma5)
            # Price to SMA5 ratio
            price_to_sma5 = close_prices[-1] / max(sma5, 1e-8)
            eng.append(price_to_sma5)
        else:
            eng.extend([close_prices[-1] if len(close_prices) > 0 else 0.0, 1.0])
            
        if len(close_prices) >= 10:
            sma10 = np.mean(close_prices[-10:])
            eng.append(sma10)
            # SMA5 to SMA10 ratio (moving average crossover signal)
            sma5_to_sma10 = sma5 / max(sma10, 1e-8)
            eng.append(sma5_to_sma10)
        else:
            eng.extend([close_prices[-1] if len(close_prices) > 0 else 0.0, 1.0])
        
        # 6. Price patterns
        if len(close_prices) >= 3:
            # Candlestick pattern: Doji (open close are very close)
            doji = abs(open_prices[-1] - close_prices[-1]) / max(high_prices[-1] - low_prices[-1], 1e-8)
            eng.append(doji)
            
            # Hammer pattern (long lower shadow)
            lower_shadow = (close_prices[-1] - low_prices[-1]) / max(high_prices[-1] - low_prices[-1], 1e-8)
            eng.append(lower_shadow)
            
            # Upper shadow (potential reversal signal)
            upper_shadow = (high_prices[-1] - max(open_prices[-1], close_prices[-1])) / max(high_prices[-1] - low_prices[-1], 1e-8)
            eng.append(upper_shadow)
        else:
            eng.extend([0.0, 0.0, 0.0])
        
        # 7. Short interest specific features
        # Short interest momentum (change over time)
        if t > 0:
            si_change = (short_interest / max(data[t-1, 0], 1e-8)) - 1
            eng.append(si_change)
        else:
            eng.append(0.0)
            
        # Short interest to price ratio
        si_to_price = short_interest / max(close_prices[-1], 1e-8)
        eng.append(si_to_price)
        
        # 8. Advanced technical indicators
        if len(close_prices) >= 14:
            # Bollinger Band width (volatility measure)
            sma20 = np.mean(close_prices[-14:])
            std20 = np.std(close_prices[-14:])
            bb_width = (2 * std20) / max(sma20, 1e-8)
            eng.append(bb_width)
            
            # Price position within Bollinger Bands
            upper_band = sma20 + 2 * std20
            lower_band = sma20 - 2 * std20
            bb_position = (close_prices[-1] - lower_band) / max(upper_band - lower_band, 1e-8)
            eng.append(bb_position)
        else:
            eng.extend([0.0, 0.5])
        
        # 9. Mean reversion indicators
        if len(close_prices) >= 10:
            # Z-score of price (how many std devs from mean)
            price_mean = np.mean(close_prices[-10:])
            price_std = np.std(close_prices[-10:])
            z_score = (close_prices[-1] - price_mean) / max(price_std, 1e-8)
            eng.append(z_score)
        else:
            eng.append(0.0)
            
        # 10. Trend strength
        if len(close_prices) >= 14:
            # ADX-like measure (trend strength)
            dx_sum = 0
            for i in range(len(close_prices)-13, len(close_prices)):
                if i > 0:
                    dx = abs(close_prices[i] - close_prices[i-1]) / max(high_prices[i] - low_prices[i], 1e-8)
                    dx_sum += dx
            adx_like = dx_sum / 13
            eng.append(adx_like)
        else:
            eng.append(0.0)
        
        # 11. Gap analysis
        if len(close_prices) >= 2 and len(open_prices) >= 2:
            # Overnight gap
            overnight_gap = (open_prices[-1] - close_prices[-2]) / max(close_prices[-2], 1e-8)
            eng.append(overnight_gap)
        else:
            eng.append(0.0)
        
        # 12. Price acceleration
        if len(daily_returns) >= 3:
            # Second derivative of price (acceleration)
            returns_diff = np.diff(daily_returns)
            price_accel = returns_diff[-1]
            eng.append(price_accel)
        else:
            eng.append(0.0)
        
        # 13. Volume-weighted metrics
        if len(close_prices) >= 5:
            # Simple VWAP approximation using average volume
            vwap = np.mean((high_prices[-5:] + low_prices[-5:] + close_prices[-5:]) / 3)
            price_to_vwap = close_prices[-1] / max(vwap, 1e-8)
            eng.append(price_to_vwap)
        else:
            eng.append(1.0)
        
        # Ensure we don't exceed MAX_NEW
        if len(eng) > MAX_NEW:
            eng = eng[:MAX_NEW]
        
        # Combine raw and engineered features
        row = np.array(raw_keep + eng, dtype=np.float32)
        
        # Ensure consistent width with padding or truncation
        if row.size < MAX_TOTAL:
            row = np.pad(row, (0, MAX_TOTAL - row.size), 'constant')
        elif row.size > MAX_TOTAL:
            row = row[:MAX_TOTAL]
        
        features[t] = row
    
    # Handle NaN, inf values
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features
```

Performance of this code: MAPE = 9.39%
Change from previous: -2.10%
Statistical Analysis: 25/80 features were significant (p < 0.05), 16 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 8):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 7.29%.

### Strategy
- Learn from previous iterations: refine or extend **high-importance** areas, drop or transform **low-importance** ones.
- Learn from previous iterations: keep the features with high feature importance, drop the features with low feature importance.
- Use **financial domain knowledge**.
- Maintain **LSTM-compatible** time series structure.
- Keep the feature set **compact and non-redundant** due to the small sample size.

### HARD IMPLEMENTATION RULES (must follow to avoid index errors and ensure a stable shape)
- Define constants at the top of the function:
  - `RAW_DIM = 62`
  - `MAX_TOTAL = 80`
  - `MAX_NEW = MAX_TOTAL - 1`  # upper bound; actual new count is determined after raw selection, see below
- **Do NOT preallocate** a fixed-width array and write with a moving `idx`.  
  Instead, for each timestep `t`:
  1) Build two Python lists:
     - `raw_keep = []`  (subset of raw features you choose to keep at t, but your selection logic must be **the same for all timesteps** so the final width is constant)
     - `eng = []`       (engineered features you append one by one)
  2) Always include in `raw_keep`: short interest (index 0) and average volume (index 1).
     Prefer **compact OHLC summaries** over copying all 60 OHLC channels (e.g., last-bar O,H,L,C; mean/median close over last 5; normalized range).
  3) After `raw_keep` is formed, compute `MAX_NEW = MAX_TOTAL - len(raw_keep)`.  
     **Never exceed this cap** when appending to `eng`.
  4) For every engineered candidate, **append to `eng`**.  
     If you hit the cap (`len(eng) == MAX_NEW`), **stop adding** more features (no exceptions).
  5) **Never reference** engineered columns by hard-coded indices (e.g., `features[t, 62+7]` is forbidden).  
     If you need a previously computed engineered value, **reuse the local variable** (e.g., `rsi_val`), not a column number.
  6) Ensure the column count is **identical for all timesteps** (no branch-induced width changes).  
     If a feature cannot be computed (e.g., insufficient points), **append a 0 placeholder** for that slot so widths remain equal.
  7) Construct the row with concatenation:
     - `row = np.array(raw_keep + eng, dtype=np.float32)`
     - If `row.size < MAX_TOTAL`, **pad with zeros** to length `MAX_TOTAL`.
     - If `row.size > MAX_TOTAL`, **truncate the tail** to `MAX_TOTAL`.
- After looping over timesteps, stack rows into a 2D array with shape `(lookback_window, MAX_TOTAL)` and return it.
- The function must **never attempt to write past** column index `MAX_TOTAL - 1`.

### Requirements
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features).
2. The function must:
   - **Select and preserve only the useful raw features** (you may drop low-importance raw channels).
   - Add **new, diverse features** while enforcing **(kept raw + new) ≤ 80**.
   - Avoid near-duplicates: do not include multiple horizons of the same measure unless clearly distinct.
   - Use **eps clamping** for all divisions: `den = max(abs(den), 1e-8)`.
   - Apply `np.nan_to_num(..., nan=0.0, posinf=0.0, neginf=0.0)` before return.
3. Process each timestep independently but maintain the temporal axis (lookback_window).
4. Focus on the **most predictive and stable** features using DL-based importance + domain knowledge.
5. Include **inline comments** explaining how you improved on previous attempts and why each new feature matters.
6. Code must be **production-ready**: numerically safe, vectorized where reasonable, no randomness or printing.
7. DO NOT include imports — these are already available: `np, pd, math, statistics, stats, StandardScaler, MinMaxScaler, mean_squared_error, mean_absolute_error, datetime, time`.
8. Return a **2D numpy array** `(lookback_window, constructed_features)` with dtype `float32`.

### Strong redundancy rules
- **One per family** unless clearly distinct (e.g., choose either SMA ratio or z-score, not both).
- Drop overlapping or affine equivalents (e.g., SMA ratio vs z-score with same window).
- Avoid fragile ops (`np.corrcoef`, polynomial fits, EMA on <3 points); prefer simple, stable ratios.

### Deliverable
Return **ONLY** the Python function code (no text outside the code).
