
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 5.90%
  Features: All 62 original features
  Statistical Analysis:
    • Total features: 62
    • Significant features (p < 0.05): 27
    • Highly significant features (p < 0.01): 21
    • Top significant features: Feature_0, Feature_1, Feature_2
    • Most significant feature: Feature_60 (p=0.0000)

Iteration 1: Iteration 1 - MAPE: 7.64% (Improvement: -1.7%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 20
    • Significant features (p < 0.05): 2
    • Highly significant features (p < 0.01): 2
    • Top significant features: Feature_5, Feature_6
    • Most significant feature: Feature_5 (p=0.0027)

Iteration 2: Iteration 2 - MAPE: 5.87% (Improvement: +0.0%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 40
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 3: Iteration 3 - MAPE: 5.47% (Improvement: +0.4%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 50
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 4: Iteration 4 - MAPE: 5.60% (Improvement: -0.1%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 45
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 5: Iteration 5 - MAPE: 5.87% (Improvement: -0.4%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 35
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 6: Iteration 6 - MAPE: 5.19% (Improvement: +0.3%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 55
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 7: Iteration 7 - MAPE: 6.42% (Improvement: -1.2%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 40
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)

Iteration 8: Iteration 8 - MAPE: 6.22% (Improvement: -1.0%)
  Features: claude feature engineering
  Statistical Analysis:
    • Total features: 55
    • Significant features (p < 0.05): 0
    • Highly significant features (p < 0.01): 0
    • Most significant feature: Feature_0 (p=0.5000)




STATISTICAL INSIGHTS FROM BEST MODEL (MAPE: 5.19%):
- Most predictive features (lowest p-values): Feature_0 (p=0.5000), Feature_1 (p=0.5000), Feature_2 (p=0.5000), Feature_3 (p=0.5000), Feature_4 (p=0.5000)
- Least predictive features (highest p-values): Feature_0 (p=0.5000), Feature_1 (p=0.5000), Feature_2 (p=0.5000)
- Feature significance ratio: 0/55 features are statistically significant






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 8):

```python
def construct_features(data):
    """
    Constructs features for short interest prediction based on comprehensive analysis of previous iterations.
    
    Key insights from performance history:
    - Best performance was Iteration 6 (MAPE: 5.19%) with 55 features
    - Baseline model with all 62 original features had 27 significant features (MAPE: 5.90%)
    - Statistical significance appears inconsistent across iterations
    - Iteration 7 increased complexity but worsened performance (MAPE: 6.42%)
    
    This implementation:
    - Returns to a feature count closer to the best-performing iteration (55)
    - Preserves more of the original features that were significant in the baseline
    - Simplifies some of the complex transformations from Iteration 7
    - Focuses on direct relationships between short interest, volume, and price action
    - Balances between technical indicators and raw data features
    
    Args:
        data: numpy array of shape (lookback_window, 62) containing financial data
            - Feature 0: Short interest
            - Feature 1: Average daily volume
            - Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, 55) with engineered features
    """
    # Handle edge cases
    if data is None or data.size == 0:
        return np.zeros((0, 55))
    
    lookback_window = data.shape[0]
    
    # Initialize output array with 55 features (matching best performing iteration)
    output_features = np.zeros((lookback_window, 55))
    
    # Process each timestamp independently
    for t in range(lookback_window):
        # Extract current timestamp data
        current_data = np.nan_to_num(data[t], nan=0.0)
        
        # Extract key components
        short_interest = current_data[0]  # Feature_0
        avg_volume = current_data[1]      # Feature_1
        
        # Reshape OHLC data for easier processing
        ohlc_data = current_data[2:62].reshape(15, 4)  # (15 days, 4 OHLC values)
        open_prices = ohlc_data[:, 0]
        high_prices = ohlc_data[:, 1]
        low_prices = ohlc_data[:, 2]
        close_prices = ohlc_data[:, 3]
        
        # Calculate daily returns
        daily_returns = np.zeros(len(close_prices)-1)
        if len(close_prices) > 1:
            daily_returns = np.diff(close_prices) / (close_prices[:-1] + 1e-8)
        
        # 1. PRESERVE ORIGINAL FEATURES
        # The baseline model had 27 significant features out of 62
        # We'll keep the original short interest and volume as they're directly relevant
        output_features[t, 0] = short_interest  # Raw short interest
        output_features[t, 1] = avg_volume      # Raw average volume
        
        # Include a subset of the original OHLC data (focusing on close prices)
        # This preserves some of the raw signal that was likely significant in the baseline
        for i in range(min(10, len(close_prices))):
            output_features[t, 2+i] = close_prices[-(i+1)]  # Most recent 10 close prices
        
        # 2. SHORT INTEREST RELATIVE METRICS
        # Days to cover - key metric for short squeeze potential
        output_features[t, 12] = short_interest / (avg_volume + 1e-8)
        
        # Short interest momentum (rate of change)
        if t > 0 and data[t-1, 0] > 0:
            output_features[t, 13] = short_interest / data[t-1, 0] - 1
        
        # Short interest acceleration (second derivative)
        if t >= 2 and data[t-1, 0] > 0 and data[t-2, 0] > 0:
            prev_momentum = data[t-1, 0] / data[t-2, 0] - 1
            current_momentum = short_interest / data[t-1, 0] - 1
            output_features[t, 14] = current_momentum - prev_momentum
        
        # 3. PRICE ACTION FEATURES
        # These capture the relationship between price movements and short interest
        if len(close_prices) > 0:
            # Recent price levels relative to short interest
            output_features[t, 15] = close_prices[-1] * short_interest
            
            # Price change during the period
            if len(close_prices) > 1:
                price_change = close_prices[-1] / close_prices[0] - 1
                output_features[t, 16] = price_change
                output_features[t, 17] = price_change * short_interest  # Interaction term
        
        # 4. VOLATILITY METRICS
        if len(daily_returns) >= 5:
            volatility_5d = np.std(daily_returns[-5:])
            output_features[t, 18] = volatility_5d
            output_features[t, 19] = volatility_5d * short_interest  # Interaction with short interest
            
            # Longer-term volatility
            if len(daily_returns) >= 10:
                volatility_10d = np.std(daily_returns[-10:])
                output_features[t, 20] = volatility_10d
                
                # Volatility trend (increasing/decreasing)
                output_features[t, 21] = volatility_5d / (volatility_10d + 1e-8) - 1
        
        # 5. VOLUME ANALYSIS
        # Volume is critical for short interest dynamics
        if t > 0 and data[t-1, 1] > 0:
            vol_change = avg_volume / data[t-1, 1] - 1
            output_features[t, 22] = vol_change
            
            # Volume * short interest interaction
            output_features[t, 23] = vol_change * short_interest
            
            # Volume trend over multiple periods
            if t >= 2 and data[t-2, 1] > 0:
                vol_trend = (avg_volume - data[t-2, 1]) / data[t-2, 1]
                output_features[t, 24] = vol_trend
        
        # 6. TECHNICAL INDICATORS
        # Moving Averages
        if len(close_prices) >= 5:
            ma5 = np.mean(close_prices[-5:])
            output_features[t, 25] = ma5
            output_features[t, 26] = close_prices[-1] / ma5 - 1  # Distance from MA
            
            if len(close_prices) >= 10:
                ma10 = np.mean(close_prices[-10:])
                output_features[t, 27] = ma10
                output_features[t, 28] = ma5 / ma10 - 1  # MA crossover indicator
        
        # RSI (Relative Strength Index)
        if len(daily_returns) >= 14:
            gains = np.maximum(daily_returns[-14:], 0)
            losses = np.abs(np.minimum(daily_returns[-14:], 0))
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            if avg_loss > 0:
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
            else:
                rsi = 100
            
            output_features[t, 29] = rsi
            output_features[t, 30] = rsi * short_interest / 100  # RSI-weighted short interest
        
        # Bollinger Bands
        if len(close_prices) >= 20:
            ma20 = np.mean(close_prices[-20:])
            std20 = np.std(close_prices[-20:])
            
            upper_band = ma20 + 2 * std20
            lower_band = ma20 - 2 * std20
            
            # Bollinger Band width (volatility indicator)
            output_features[t, 31] = (upper_band - lower_band) / ma20
            
            # Price position within Bollinger Bands
            band_position = (close_prices[-1] - lower_band) / (upper_band - lower_band + 1e-8)
            output_features[t, 32] = band_position
            
            # Bollinger Band squeeze indicator
            output_features[t, 33] = std20 / ma20  # Normalized standard deviation
        
        # 7. PRICE PATTERNS AND MOMENTUM
        if len(close_prices) >= 5:
            # Recent momentum (5-day)
            momentum_5d = close_prices[-1] / close_prices[-5] - 1
            output_features[t, 34] = momentum_5d
            
            # Momentum * short interest interaction
            output_features[t, 35] = momentum_5d * short_interest
            
            # Directional movement
            if len(close_prices) >= 10:
                momentum_10d = close_prices[-1] / close_prices[-10] - 1
                output_features[t, 36] = momentum_10d
                
                # Momentum acceleration/deceleration
                output_features[t, 37] = momentum_5d - momentum_10d
        
        # 8. SHORT SQUEEZE POTENTIAL INDICATORS
        # Composite metrics that combine short interest, price action, and volume
        if len(close_prices) >= 5:
            # Basic short squeeze potential formula
            recent_return = close_prices[-1] / close_prices[-5] - 1 if len(close_prices) >= 5 else 0
            squeeze_potential = short_interest * max(0, recent_return) * (avg_volume + 1e-8)
            output_features[t, 38] = squeeze_potential
            
            # Days to cover * recent return interaction
            days_to_cover = short_interest / (avg_volume + 1e-8)
            output_features[t, 39] = days_to_cover * max(0, recent_return)
        
        # 9. TREND ANALYSIS
        if len(close_prices) >= 10:
            x = np.arange(len(close_prices[-10:]))
            slope, intercept = np.polyfit(x, close_prices[-10:], 1)
            
            # Normalized slope (trend direction and strength)
            normalized_slope = slope / (np.mean(close_prices[-10:]) + 1e-8)
            output_features[t, 40] = normalized_slope
            
            # Trend * short interest interaction
            output_features[t, 41] = normalized_slope * short_interest
            
            # Trend consistency (R-squared)
            y_pred = slope * x + intercept
            ss_tot = np.sum((close_prices[-10:] - np.mean(close_prices[-10:]))**2)
            ss_res = np.sum((close_prices[-10:] - y_pred)**2)
            r_squared = 1 - (ss_res / (ss_tot + 1e-8))
            output_features[t, 42] = r_squared
        
        # 10. MARKET SENTIMENT INDICATORS
        if len(daily_returns) >= 10:
            # Positive vs negative days ratio
            pos_days = np.sum(daily_returns[-10:] > 0)
            neg_days = np.sum(daily_returns[-10:] < 0)
            sentiment_ratio = pos_days / (neg_days + 1e-8)
            output_features[t, 43] = sentiment_ratio
            
            # Sentiment * short interest interaction
            output_features[t, 44] = sentiment_ratio * short_interest
        
        # 11. HISTORICAL SHORT INTEREST PATTERNS
        if t >= 3:
            # Historical short interest values
            si_values = [data[max(0, t-i), 0] for i in range(3)]
            
            # Short interest trend
            if len(si_values) >= 2:
                si_trend = (si_values[0] - si_values[1]) / (si_values[1] + 1e-8)
                output_features[t, 45] = si_trend
            
            # Short interest volatility
            if len(si_values) >= 3:
                si_volatility = np.std(si_values) / (np.mean(si_values) + 1e-8)
                output_features[t, 46] = si_volatility
        
        # 12. PRICE RANGE AND GAPS
        if len(high_prices) > 0 and len(low_prices) > 0:
            # High-Low range (normalized)
            avg_price = (np.mean(high_prices) + np.mean(low_prices)) / 2
            price_range = (np.max(high_prices) - np.min(low_prices)) / (avg_price + 1e-8)
            output_features[t, 47] = price_range
            
            # Range * short interest interaction
            output_features[t, 48] = price_range * short_interest
            
            # Recent gap analysis
            if len(open_prices) >= 2 and len(close_prices) >= 2:
                gap = open_prices[-1] / close_prices[-2] - 1
                output_features[t, 49] = gap
                
                # Gap * short interest interaction
                output_features[t, 50] = gap * short_interest
        
        # 13. SIMPLIFIED TRANSFORMATIONS
        # Log transformation of short interest (better statistical properties)
        output_features[t, 51] = np.log1p(short_interest)
        
        # Short interest to price ratio
        if len(close_prices) > 0 and close_prices[-1] > 0:
            output_features[t, 52] = short_interest / close_prices[-1]
        
        # Short interest to volume ratio (alternative days to cover calculation)
        output_features[t, 53] = short_interest / (avg_volume + 1e-8)
        
        # Squared short interest (captures non-linear effects)
        output_features[t, 54] = short_interest ** 2
    
    # Handle any NaN values that might have been created
    output_features = np.nan_to_num(output_features, nan=0.0)
    
    return output_features
```

Performance of this code: MAPE = 6.22%
Change from previous: -1.03%
Statistical Analysis: 0/55 features were significant (p < 0.05), 0 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 9):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 5.19%.

Based on the performance history, statistical analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses p-value insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using statistical significance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and statistical insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
