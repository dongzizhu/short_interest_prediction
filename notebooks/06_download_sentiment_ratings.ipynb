{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1010b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gathering more features at 2025-10-01 13:16:01.752002\n"
     ]
    }
   ],
   "source": [
    "# Project Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Starting gathering more features at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83208bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  - Project root: /Users/joshuacroppenstedt/Desktop/Work/JP Morgan/short_interest_prediction\n",
      "  - Windows: [10, 21, 42, 63, 126, 252]\n",
      "  - Use cache: True\n",
      "  - Timestamp: 20251001_1319\n"
     ]
    }
   ],
   "source": [
    "# Constants and Configuration\n",
    "PROJECT_ROOT = Path('/Users/joshuacroppenstedt/Desktop/Work/JP Morgan/short_interest_prediction')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "CACHE_DIR = DATA_DIR / 'cache'\n",
    "PRICES_DIR = DATA_DIR / 'prices'\n",
    "FINRA_DIR = DATA_DIR / 'finra_clean'\n",
    "FEATURES_DIR = DATA_DIR / 'features'\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [CACHE_DIR, CACHE_DIR / 'alphavantage', CACHE_DIR / 'finnhub', CACHE_DIR / 'eodhd']:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Trading day windows (approximations)\n",
    "WINDOWS = [10, 21, 42, 63, 126, 252]  # 2w, 1m, 2m, 3m, 6m, 12m\n",
    "WINDOW_NAMES = ['2w', '1m', '2m', '3m', '6m', '12m']\n",
    "WINDOW_MAPPING = dict(zip(WINDOWS, WINDOW_NAMES))\n",
    "\n",
    "# Global settings\n",
    "USE_CACHE = True\n",
    "MAX_WORKERS = 4  # For parallel API calls\n",
    "RATE_LIMIT_DELAY = 0.1  # Base delay between API calls\n",
    "\n",
    "# Generate timestamp for output files\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  - Windows: {WINDOWS}\")\n",
    "print(f\"  - Use cache: {USE_CACHE}\")\n",
    "print(f\"  - Timestamp: {TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a638d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 13:19:54,501 - INFO - All API keys loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys status: ['alphavantage✓', 'finnhub✓', 'eodhd✓']\n"
     ]
    }
   ],
   "source": [
    "# Load API Keys\n",
    "load_dotenv(PROJECT_ROOT / 'notebooks' / '.env')\n",
    "\n",
    "API_KEYS = {\n",
    "    'alphavantage': os.getenv('ALPHAVANTAGE_API_KEY'),\n",
    "    'finnhub': os.getenv('FINNHUB_API_KEY'),\n",
    "    'eodhd': os.getenv('EODHD_API_KEY')\n",
    "}\n",
    "\n",
    "# Verify keys are loaded (don't print actual keys)\n",
    "missing_keys = [k for k, v in API_KEYS.items() if not v]\n",
    "if missing_keys:\n",
    "    logger.warning(f\"Missing API keys: {missing_keys}\")\n",
    "else:\n",
    "    logger.info(\"All API keys loaded successfully\")\n",
    "\n",
    "print(f\"API keys status: {[k + ('✓' if v else '✗') for k, v in API_KEYS.items()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236542c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shares outstanding data...\n",
      "Loading: shares_outstanding_multiindex_corrected_20250929_204539.parquet\n",
      "Unified price data shape: (1803, 2273)\n",
      "Date range: 2008-03-28 00:00:00 to 2025-08-24 00:00:00\n",
      "Total tickers: 2273\n",
      "Sample tickers: ['A', 'AA', 'AAL', 'AAME', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAT', 'ABBV']\n",
      "Feature date range: 2008-03-28 00:00:00 to 2025-08-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load shares outstanding data to get ticker universe\n",
    "print(\"Loading shares outstanding data...\")\n",
    "\n",
    "# Find the most recent unified price data file\n",
    "shares_outstanding_files = list(FEATURES_DIR.glob(\"shares_outstanding_multiindex_corrected_*.parquet\"))\n",
    "if not shares_outstanding_files:\n",
    "    raise FileNotFoundError(\"No shares outstanding data files found. Please run the price cleaning notebook first.\")\n",
    "\n",
    "# Use the most recent file\n",
    "shares_outstanding_file = max(shares_outstanding_files, key=lambda x: x.stat().st_mtime)\n",
    "print(f\"Loading: {shares_outstanding_file.name}\")\n",
    "\n",
    "# Load the unified price data\n",
    "shares_outstanding_data = pd.read_parquet(shares_outstanding_file)\n",
    "print(f\"Unified price data shape: {shares_outstanding_data.shape}\")\n",
    "print(f\"Date range: {shares_outstanding_data.index.min()} to {shares_outstanding_data.index.max()}\")\n",
    "\n",
    "# Extract ticker list\n",
    "if isinstance(shares_outstanding_data.columns, pd.MultiIndex):\n",
    "    tickers = sorted(shares_outstanding_data.columns.get_level_values(0).unique().tolist())\n",
    "else:\n",
    "    raise ValueError(\"Expected MultiIndex columns in unified price data\")\n",
    "\n",
    "print(f\"Total tickers: {len(tickers)}\")\n",
    "print(f\"Sample tickers: {tickers[:10]}\")\n",
    "\n",
    "# Store date range for feature alignment\n",
    "feature_start_date = shares_outstanding_data.index.min()\n",
    "feature_end_date = shares_outstanding_data.index.max()\n",
    "print(f\"Feature date range: {feature_start_date} to {feature_end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7dc8c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eodhd import APIClient\n",
    "\n",
    "api_token = API_KEYS['eodhd']\n",
    "\n",
    "def get_sentiment_data(ticker, from_date, to_date):\n",
    "    url = f'https://eodhd.com/api/sentiments'\n",
    "    query = {'api_token': api_token, 's': ticker, 'from': from_date, 'to': to_date, 'fmt': 'json'}\n",
    "    response = requests.get(url, params=query)\n",
    "    if response.status_code != 200:\n",
    "            print(f\"Error retrieving sentiment data: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return None\n",
    "    sentiment_data = response.json()\n",
    "    # Access the sentiment data using the ticker symbol as a key\n",
    "    #sentiment_df = pd.DataFrame(sentiment_data[ticker])\n",
    "    # Convert date string to datetime\n",
    "    #sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "    # Set date as index\n",
    "    #sentiment_df.set_index('date', inplace=True)\n",
    "    # Sort by date (ascending)\n",
    "    #sentiment_df.sort_index(inplace=True)\n",
    "    # Rename column normalized to sentiment\n",
    "    #sentiment_df.rename(columns={'normalized': 'sentiment'}, inplace=True)\n",
    "    #return sentiment_df\n",
    "    return sentiment_data\n",
    "#sentiment_df = get_sentiment_data(TICKER, from_date, to_date)\n",
    "\n",
    "sentiment_df = get_sentiment_data('TSLA.US', '2017-12-01', '2018-01-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "efa8b7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TSLA.US': [{'date': '2018-01-03', 'count': 1, 'normalized': 0.994}]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28b7d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting finnhub-python\n",
      "  Downloading finnhub_python-2.4.25-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/joshuacroppenstedt/opt/anaconda3/lib/python3.12/site-packages (from finnhub-python) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joshuacroppenstedt/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshuacroppenstedt/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joshuacroppenstedt/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuacroppenstedt/opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (2025.1.31)\n",
      "Downloading finnhub_python-2.4.25-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: finnhub-python\n",
      "Successfully installed finnhub-python-2.4.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install finnhub-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba4474a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [], 'symbol': 'MSFT'}\n"
     ]
    }
   ],
   "source": [
    "import finnhub\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=API_KEYS['finnhub'])\n",
    "\n",
    "print(finnhub_client.stock_social_sentiment('MSFT', _from='2021-06-01', to='2021-04-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07541fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
