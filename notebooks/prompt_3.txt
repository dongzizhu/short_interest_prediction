
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 6.87%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_35 (importance=0.0001), Feature_0 (importance=0.0001), Feature_12 (importance=0.0001), Feature_26 (importance=0.0000), Feature_1 (importance=0.0000)

Iteration 1: Iteration 1 - MAPE: 8.25% (Improvement: -1.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_97 (importance=0.0010), Feature_98 (importance=0.0006), Feature_66 (importance=0.0002), Feature_12 (importance=0.0002), Feature_145 (importance=0.0002)

Iteration 2: Iteration 2 - MAPE: 8.67% (Improvement: -1.8%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_22 (importance=0.0017), Feature_23 (importance=0.0013), Feature_113 (importance=0.0005), Feature_43 (importance=0.0005), Feature_8 (importance=0.0005)




DL-BASED FEATURE IMPORTANCE INSIGHTS FROM BEST MODEL (MAPE: 6.87%):
- Most important features: Feature_35 (importance=0.0001), Feature_0 (importance=0.0001), Feature_12 (importance=0.0001), Feature_26 (importance=0.0000), Feature_1 (importance=0.0000)
- Least important features: Feature_52 (importance=-0.0000), Feature_47 (importance=-0.0000), Feature_14 (importance=-0.0000)






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 2):

```python
def construct_features(data):
    """
    Constructs engineered features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
              where data[:, 0] = short interest
                    data[:, 1] = average daily volume
                    data[:, 2:62] = OHLC prices for past 15 days (4 features × 15 days)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    
    # Handle NaN values
    data = np.nan_to_num(data, nan=0.0)
    
    # Extract key components
    short_interest = data[:, 0:1]  # Feature_0 was identified as important
    avg_volume = data[:, 1:2]
    
    # Reshape OHLC data for easier manipulation
    # Each day has 4 values (OHLC), and we have 15 days of data
    ohlc_data = data[:, 2:62].reshape(lookback_window, 15, 4)
    
    # Extract OHLC components
    open_prices = ohlc_data[:, :, 0]
    high_prices = ohlc_data[:, :, 1]
    low_prices = ohlc_data[:, :, 2]
    close_prices = ohlc_data[:, :, 3]
    
    # Initialize list to store all features
    all_features = []
    
    # 1. Include original important features
    # Based on DL importance analysis, Feature_0 (short interest) and Feature_35 were most important
    all_features.append(short_interest)  # Feature_0 (short interest)
    
    # Keep the original OHLC data for the most recent days (which includes Feature_35)
    # This preserves the raw data that was identified as important
    all_features.append(ohlc_data[:, -5:, :].reshape(lookback_window, -1))
    
    # 2. Short interest specific features
    # Short interest momentum (change over time)
    si_momentum = np.zeros((lookback_window, 1))
    for i in range(1, lookback_window):
        si_momentum[i, 0] = (short_interest[i, 0] / (short_interest[i-1, 0] + 1e-8)) - 1
    
    # Short interest to volume ratio
    si_to_volume = short_interest / (avg_volume + 1e-8)
    
    # Short interest acceleration (rate of change of momentum)
    si_acceleration = np.zeros((lookback_window, 1))
    for i in range(2, lookback_window):
        si_acceleration[i, 0] = si_momentum[i, 0] - si_momentum[i-1, 0]
    
    all_features.append(si_momentum)
    all_features.append(si_to_volume)
    all_features.append(si_acceleration)
    
    # 3. Price momentum features with focus on recent days
    # Calculate returns over different periods with emphasis on recent data
    returns_1d = np.zeros((lookback_window, 14))
    returns_3d = np.zeros((lookback_window, 12))
    returns_5d = np.zeros((lookback_window, 10))
    
    for i in range(lookback_window):
        for j in range(14):
            returns_1d[i, j] = (close_prices[i, j+1] / close_prices[i, j]) - 1
        for j in range(12):
            returns_3d[i, j] = (close_prices[i, j+3] / close_prices[i, j]) - 1
        for j in range(10):
            returns_5d[i, j] = (close_prices[i, j+5] / close_prices[i, j]) - 1
    
    # Focus on recent returns (last 5 days) which are more relevant
    all_features.append(returns_1d[:, -5:])
    all_features.append(returns_3d[:, -5:])
    all_features.append(returns_5d[:, -5:])
    
    # 4. Volatility features
    # Calculate daily true range and volatility
    true_range = np.zeros((lookback_window, 14))
    for i in range(lookback_window):
        for j in range(1, 15):
            high_low = high_prices[i, j] - low_prices[i, j]
            high_close_prev = np.abs(high_prices[i, j] - close_prices[i, j-1])
            low_close_prev = np.abs(low_prices[i, j] - close_prices[i, j-1])
            true_range[i, j-1] = np.max([high_low, high_close_prev, low_close_prev])
    
    # Average True Range (ATR) - 5-day and 10-day
    atr_5d = np.zeros((lookback_window, 10))
    atr_10d = np.zeros((lookback_window, 5))
    
    for i in range(lookback_window):
        for j in range(10):
            atr_5d[i, j] = np.mean(true_range[i, j:j+5])
        for j in range(5):
            atr_10d[i, j] = np.mean(true_range[i, j:j+10])
    
    # 5-day rolling volatility
    volatility_5d = np.zeros((lookback_window, 10))
    for i in range(lookback_window):
        for j in range(10):
            volatility_5d[i, j] = np.std(returns_1d[i, j:j+5])
    
    # Focus on recent volatility
    all_features.append(true_range[:, -5:])
    all_features.append(atr_5d[:, -3:])
    all_features.append(atr_10d)
    all_features.append(volatility_5d[:, -5:])
    
    # 5. Volume-based features
    # Volume relative to moving average
    volume_ratio = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        if i > 0:  # Compare to previous timestamp's volume
            volume_ratio[i, 0] = avg_volume[i, 0] / (avg_volume[i-1, 0] + 1e-8)
        else:
            volume_ratio[i, 0] = 1.0
    
    # Volume momentum (rate of change)
    volume_momentum = np.zeros((lookback_window, 1))
    for i in range(1, lookback_window):
        if i > 1:
            prev_ratio = avg_volume[i-1, 0] / (avg_volume[i-2, 0] + 1e-8)
            volume_momentum[i, 0] = volume_ratio[i, 0] - prev_ratio
    
    all_features.append(volume_ratio)
    all_features.append(volume_momentum)
    all_features.append(avg_volume)  # Original volume feature
    
    # 6. Price patterns and technical indicators
    # Moving averages of close prices
    ma_5d = np.zeros((lookback_window, 11))
    ma_10d = np.zeros((lookback_window, 6))
    
    for i in range(lookback_window):
        for j in range(11):
            ma_5d[i, j] = np.mean(close_prices[i, j:j+5])
        for j in range(6):
            ma_10d[i, j] = np.mean(close_prices[i, j:j+10])
    
    # Exponential moving averages (EMA) with more weight on recent prices
    ema_5d = np.zeros((lookback_window, 11))
    for i in range(lookback_window):
        for j in range(11):
            # Simple EMA calculation
            weights = np.exp(np.linspace(0, 1, 5))
            weights = weights / np.sum(weights)
            ema_5d[i, j] = np.sum(close_prices[i, j:j+5] * weights)
    
    # MACD-like indicator (difference between short and long EMAs)
    macd = np.zeros((lookback_window, 6))
    for i in range(lookback_window):
        for j in range(6):
            macd[i, j] = ema_5d[i, j] - ma_10d[i, j]
    
    # Price relative to moving averages
    price_to_ma5 = np.zeros((lookback_window, 11))
    price_to_ma10 = np.zeros((lookback_window, 6))
    
    for i in range(lookback_window):
        for j in range(11):
            price_to_ma5[i, j] = close_prices[i, j+4] / (ma_5d[i, j] + 1e-8)
        for j in range(6):
            price_to_ma10[i, j] = close_prices[i, j+9] / (ma_10d[i, j] + 1e-8)
    
    # Focus on recent technical indicators
    all_features.append(ma_5d[:, -5:])
    all_features.append(ma_10d)
    all_features.append(ema_5d[:, -5:])
    all_features.append(macd)
    all_features.append(price_to_ma5[:, -5:])
    all_features.append(price_to_ma10)
    
    # 7. RSI (Relative Strength Index)
    rsi_14d = np.zeros((lookback_window, 2))
    for i in range(lookback_window):
        gains = np.zeros(14)
        losses = np.zeros(14)
        for j in range(1, 15):
            change = close_prices[i, j] - close_prices[i, j-1]
            if change > 0:
                gains[j-1] = change
            else:
                losses[j-1] = -change
        
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        
        if avg_loss == 0:
            rsi_14d[i, 0] = 100
        else:
            rs = avg_gain / (avg_loss + 1e-8)
            rsi_14d[i, 0] = 100 - (100 / (1 + rs))
        
        # RSI for the last 7 days
        gains = np.zeros(7)
        losses = np.zeros(7)
        for j in range(8, 15):
            change = close_prices[i, j] - close_prices[i, j-1]
            if change > 0:
                gains[j-8] = change
            else:
                losses[j-8] = -change
        
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        
        if avg_loss == 0:
            rsi_14d[i, 1] = 100
        else:
            rs = avg_gain / (avg_loss + 1e-8)
            rsi_14d[i, 1] = 100 - (100 / (1 + rs))
    
    all_features.append(rsi_14d)
    
    # 8. Bollinger Bands
    bb_width = np.zeros((lookback_window, 6))
    for i in range(lookback_window):
        for j in range(6):
            std = np.std(close_prices[i, j:j+10])
            bb_width[i, j] = 2 * std / (ma_10d[i, j] + 1e-8)  # Normalized by price
    
    # Price position within Bollinger Bands
    bb_position = np.zeros((lookback_window, 6))
    for i in range(lookback_window):
        for j in range(6):
            upper_band = ma_10d[i, j] + 2 * np.std(close_prices[i, j:j+10])
            lower_band = ma_10d[i, j] - 2 * np.std(close_prices[i, j:j+10])
            bb_position[i, j] = (close_prices[i, j+9] - lower_band) / (upper_band - lower_band + 1e-8)
    
    all_features.append(bb_width)
    all_features.append(bb_position)
    
    # 9. Feature interactions (combining important features)
    # Interaction between short interest and volatility
    si_vol_interaction = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        if volatility_5d.shape[1] > 0:
            si_vol_interaction[i, 0] = short_interest[i, 0] * np.mean(volatility_5d[i])
    
    # Interaction between short interest and volume
    si_volume_interaction = short_interest * avg_volume
    
    # Interaction between short interest and price momentum
    si_momentum_interaction = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        if returns_5d.shape[1] > 0:
            si_momentum_interaction[i, 0] = short_interest[i, 0] * np.mean(returns_5d[i])
    
    # Interaction between short interest and RSI
    si_rsi_interaction = short_interest * rsi_14d
    
    all_features.append(si_vol_interaction)
    all_features.append(si_volume_interaction)
    all_features.append(si_momentum_interaction)
    all_features.append(si_rsi_interaction)
    
    # 10. Non-linear transformations of important features
    # Log transformation of short interest
    log_si = np.log1p(short_interest)
    
    # Square root transformation of volume
    sqrt_volume = np.sqrt(avg_volume)
    
    # Squared short interest (to capture non-linear effects)
    squared_si = short_interest ** 2
    
    all_features.append(log_si)
    all_features.append(sqrt_volume)
    all_features.append(squared_si)
    
    # 11. Temporal features within the lookback window
    # Days since highest short interest in window
    days_since_max_si = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        if i > 0:
            max_idx = np.argmax(short_interest[:i+1, 0])
            days_since_max_si[i, 0] = i - max_idx
    
    # Days since lowest short interest in window
    days_since_min_si = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        if i > 0:
            min_idx = np.argmin(short_interest[:i+1, 0])
            days_since_min_si[i, 0] = i - min_idx
    
    all_features.append(days_since_max_si)
    all_features.append(days_since_min_si)
    
    # Concatenate all features
    result = np.concatenate([feat.reshape(lookback_window, -1) for feat in all_features], axis=1)
    
    # Final check for NaN values
    result = np.nan_to_num(result, nan=0.0)
    
    return result
```

Performance of this code: MAPE = 8.67%
Change from previous: -1.80%
Statistical Analysis: 43/117 features were significant (p < 0.05), 20 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 3):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 6.87%.

Based on the performance history, DL-based feature importance analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses DL-based feature importance insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using DL-based feature importance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and importance insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
