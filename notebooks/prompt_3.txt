
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 17.25%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_1 (importance=0.0003), Feature_48 (importance=0.0003), Feature_40 (importance=0.0003), Feature_27 (importance=0.0002), Feature_10 (importance=0.0002)

Iteration 1: Iteration 1 - MAPE: 18.14% (Improvement: -0.9%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_8 (importance=0.0238), Feature_16 (importance=0.0194), Feature_13 (importance=0.0135), Feature_11 (importance=0.0102), Feature_7 (importance=0.0082)

Iteration 2: Iteration 2 - MAPE: 13.23% (Improvement: +4.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_23 (importance=0.0243), Feature_14 (importance=0.0124), Feature_10 (importance=0.0106), Feature_17 (importance=0.0074), Feature_0 (importance=0.0071)




DL-BASED FEATURE IMPORTANCE INSIGHTS FROM BEST MODEL (MAPE: 13.23%):
- Most important features: Feature_23 (importance=0.0243), Feature_14 (importance=0.0124), Feature_10 (importance=0.0106), Feature_17 (importance=0.0074), Feature_0 (importance=0.0071)
- Least important features: Feature_5 (importance=0.0003), Feature_15 (importance=0.0009), Feature_7 (importance=0.0011)






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 2):

```python
def construct_features(data):
    """
    Constructs features for short interest prediction based on historical performance analysis.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - data[:, 0]: Short interest
            - data[:, 1]: Average daily volume
            - data[:, 2:62]: OHLC prices for past 15 days (4 features × 15 days)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    lookback_window = data.shape[0]
    
    # Handle NaN values
    data = np.nan_to_num(data, nan=0.0)
    
    # Extract key components
    short_interest = data[:, 0]
    avg_volume = data[:, 1]
    
    # Reshape OHLC data for easier processing
    # Original format: 60 columns (4 OHLC × 15 days flattened)
    # New format: (lookback_window, 15, 4) where 4 is OHLC
    ohlc_data = np.zeros((lookback_window, 15, 4))
    for i in range(15):
        ohlc_data[:, i, 0] = data[:, 2 + i*4]     # Open
        ohlc_data[:, i, 1] = data[:, 2 + i*4 + 1] # High
        ohlc_data[:, i, 2] = data[:, 2 + i*4 + 2] # Low
        ohlc_data[:, i, 3] = data[:, 2 + i*4 + 3] # Close
    
    # Initialize feature list
    feature_list = []
    
    # 1. Original features that were identified as important in DL-based feature importance
    # Feature_1 (Short Interest), Feature_48, Feature_40, Feature_27, Feature_10
    # Keep original short interest (Feature_1)
    feature_list.append(short_interest.reshape(lookback_window, 1))
    
    # Keep original Feature_48 (corresponds to a specific OHLC value)
    # Feature_48 is the 12th day's Low price (index 11, feature 2)
    feature_list.append(ohlc_data[:, 11, 2].reshape(lookback_window, 1))
    
    # Keep original Feature_40 (corresponds to a specific OHLC value)
    # Feature_40 is the 10th day's High price (index 9, feature 1)
    feature_list.append(ohlc_data[:, 9, 1].reshape(lookback_window, 1))
    
    # Keep original Feature_27 (corresponds to a specific OHLC value)
    # Feature_27 is the 7th day's Close price (index 6, feature 3)
    feature_list.append(ohlc_data[:, 6, 3].reshape(lookback_window, 1))
    
    # Keep original Feature_10 (corresponds to a specific OHLC value)
    # Feature_10 is the 3rd day's Low price (index 2, feature 2)
    feature_list.append(ohlc_data[:, 2, 2].reshape(lookback_window, 1))
    
    # 2. Short interest features - more nuanced than previous iteration
    # Short interest momentum (rate of change) - different timeframes
    si_momentum_1 = np.zeros((lookback_window, 1))
    si_momentum_1[1:, 0] = (short_interest[1:] - short_interest[:-1]) / (short_interest[:-1] + 1e-8)
    feature_list.append(si_momentum_1)
    
    # Short interest relative to its moving average
    si_ma = np.zeros((lookback_window, 1))
    for i in range(lookback_window):
        start_idx = max(0, i-2)
        si_ma[i, 0] = np.mean(short_interest[start_idx:i+1])
    si_relative_to_ma = short_interest.reshape(lookback_window, 1) / (si_ma + 1e-8)
    feature_list.append(si_relative_to_ma)
    
    # 3. Volume features - enhanced from previous iteration
    # Normalized volume (using min-max scaling for better normalization)
    min_vol = np.min(avg_volume) if lookback_window > 1 else avg_volume[0]
    max_vol = np.max(avg_volume) if lookback_window > 1 else avg_volume[0]
    range_vol = max_vol - min_vol + 1e-8
    norm_volume = (avg_volume - min_vol) / range_vol
    feature_list.append(norm_volume.reshape(lookback_window, 1))
    
    # Volume momentum
    vol_momentum = np.zeros((lookback_window, 1))
    vol_momentum[1:, 0] = (avg_volume[1:] - avg_volume[:-1]) / (avg_volume[:-1] + 1e-8)
    feature_list.append(vol_momentum)
    
    # Volume acceleration
    vol_accel = np.zeros((lookback_window, 1))
    if lookback_window > 2:
        prev_vol_momentum = np.zeros(lookback_window-1)
        prev_vol_momentum[1:] = (avg_volume[1:-1] - avg_volume[:-2]) / (avg_volume[:-2] + 1e-8)
        vol_accel[1:, 0] = vol_momentum[1:, 0] - prev_vol_momentum
    feature_list.append(vol_accel)
    
    # 4. Price features - focusing on the important days identified by feature importance
    # Extract close prices for each day
    close_prices = ohlc_data[:, :, 3]  # All close prices
    open_prices = ohlc_data[:, :, 0]   # All open prices
    high_prices = ohlc_data[:, :, 1]   # All high prices
    low_prices = ohlc_data[:, :, 2]    # All low prices
    
    # Calculate returns for each day
    returns = np.zeros((lookback_window, 15))
    for i in range(15):
        if i > 0:
            returns[:, i] = (close_prices[:, i] - close_prices[:, i-1]) / (close_prices[:, i-1] + 1e-8)
    
    # 5. Specific day features - focusing on days that were important in feature importance
    # Day 3, 7, 10, and 12 price patterns (based on important features)
    for day_idx in [2, 6, 9, 11]:  # 0-indexed
        # Daily range
        daily_range = (high_prices[:, day_idx] - low_prices[:, day_idx]) / (open_prices[:, day_idx] + 1e-8)
        feature_list.append(daily_range.reshape(lookback_window, 1))
        
        # Daily return
        if day_idx > 0:
            daily_return = (close_prices[:, day_idx] - close_prices[:, day_idx-1]) / (close_prices[:, day_idx-1] + 1e-8)
            feature_list.append(daily_return.reshape(lookback_window, 1))
        
        # Intraday movement
        intraday_move = (close_prices[:, day_idx] - open_prices[:, day_idx]) / (open_prices[:, day_idx] + 1e-8)
        feature_list.append(intraday_move.reshape(lookback_window, 1))
    
    # 6. Short interest to volume relationships - enhanced
    # Short interest to volume ratio (important relationship)
    si_volume_ratio = short_interest / (avg_volume + 1e-8)
    feature_list.append(si_volume_ratio.reshape(lookback_window, 1))
    
    # Log-transformed SI to volume ratio (to handle skewness)
    log_si_vol_ratio = np.log1p(short_interest) - np.log1p(avg_volume)
    feature_list.append(log_si_vol_ratio.reshape(lookback_window, 1))
    
    # Short interest to volume ratio momentum
    si_vol_ratio_momentum = np.zeros((lookback_window, 1))
    si_vol_ratio_momentum[1:, 0] = (si_volume_ratio[1:] - si_volume_ratio[:-1]) / (si_volume_ratio[:-1] + 1e-8)
    feature_list.append(si_vol_ratio_momentum)
    
    # 7. Technical indicators focused on important days
    # Exponential weighted returns for important days (3, 7, 10, 12)
    important_days = [2, 6, 9, 11]  # 0-indexed
    for day_idx in important_days:
        if day_idx >= 4:  # Need at least 5 days for EMA
            # Calculate 5-day EMA of returns ending on this day
            weights = np.exp(np.linspace(-1, 0, 5))
            weights = weights / np.sum(weights)
            ema_returns = np.zeros(lookback_window)
            for i in range(lookback_window):
                if day_idx >= 4:
                    day_returns = returns[i, day_idx-4:day_idx+1]
                    ema_returns[i] = np.sum(day_returns * weights)
            feature_list.append(ema_returns.reshape(lookback_window, 1))
    
    # 8. Volatility features focused on important days
    for day_idx in important_days:
        if day_idx >= 4:  # Need at least 5 days
            # Calculate volatility over 5 days ending on this day
            vol_window = np.zeros(lookback_window)
            for i in range(lookback_window):
                day_returns = returns[i, day_idx-4:day_idx+1]
                vol_window[i] = np.std(day_returns)
            feature_list.append(vol_window.reshape(lookback_window, 1))
    
    # 9. Price momentum relative to short interest changes
    # Calculate price momentum over the same period as short interest reporting (15 days)
    price_momentum_15d = (close_prices[:, -1] - close_prices[:, 0]) / (close_prices[:, 0] + 1e-8)
    
    # Ratio of short interest change to price momentum
    si_momentum_15d = np.zeros(lookback_window)
    if lookback_window > 1:
        si_momentum_15d[1:] = (short_interest[1:] - short_interest[:-1]) / (short_interest[:-1] + 1e-8)
    
    si_price_momentum_ratio = si_momentum_15d / (np.abs(price_momentum_15d) + 1e-8)
    feature_list.append(si_price_momentum_ratio.reshape(lookback_window, 1))
    
    # 10. Advanced technical indicators
    # MACD-like indicator (difference between fast and slow EMAs)
    if lookback_window > 1:
        # Fast EMA (5-day)
        alpha_fast = 2 / (5 + 1)
        ema_fast = np.zeros(lookback_window)
        ema_fast[0] = close_prices[0, -1]
        for i in range(1, lookback_window):
            ema_fast[i] = close_prices[i, -1] * alpha_fast + ema_fast[i-1] * (1 - alpha_fast)
        
        # Slow EMA (10-day)
        alpha_slow = 2 / (10 + 1)
        ema_slow = np.zeros(lookback_window)
        ema_slow[0] = close_prices[0, -1]
        for i in range(1, lookback_window):
            ema_slow[i] = close_prices[i, -1] * alpha_slow + ema_slow[i-1] * (1 - alpha_slow)
        
        # MACD
        macd = ema_fast - ema_slow
        feature_list.append(macd.reshape(lookback_window, 1))
    
    # 11. Bollinger Bands-like features for short interest
    if lookback_window > 1:
        # Calculate rolling mean and std of short interest
        si_mean = np.zeros(lookback_window)
        si_std = np.zeros(lookback_window)
        
        for i in range(lookback_window):
            start_idx = max(0, i-3)  # Use up to 4 previous points
            si_window = short_interest[start_idx:i+1]
            si_mean[i] = np.mean(si_window)
            si_std[i] = np.std(si_window) if len(si_window) > 1 else 0
        
        # Calculate Bollinger Band positions
        bb_position = (short_interest - si_mean) / (si_std + 1e-8)
        feature_list.append(bb_position.reshape(lookback_window, 1))
    
    # 12. Combine all features
    combined_features = np.hstack(feature_list)
    
    # Final check for NaN values
    combined_features = np.nan_to_num(combined_features, nan=0.0)
    
    return combined_features
```

Performance of this code: MAPE = 13.23%
Improvement over previous: +4.02%
Statistical Analysis: 30/34 features were significant (p < 0.05), 19 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 3):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 13.23%.

Based on the performance history, DL-based feature importance analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses DL-based feature importance insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using DL-based feature importance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and importance insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
