
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 10.64%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_1 (importance=0.0006), Feature_0 (importance=0.0006), Feature_5 (importance=0.0004), Feature_16 (importance=0.0002), Feature_60 (importance=0.0002)

Iteration 1: Iteration 1 - MAPE: 12.17% (Improvement: -1.5%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_12 (importance=0.0033), Feature_0 (importance=0.0033), Feature_7 (importance=0.0031), Feature_4 (importance=0.0030), Feature_11 (importance=0.0030)

Iteration 2: Iteration 2 - MAPE: 10.50% (Improvement: +0.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_2 (importance=0.0012), Feature_0 (importance=0.0008), Feature_9 (importance=0.0008), Feature_27 (importance=0.0008), Feature_5 (importance=0.0007)

Iteration 3: Iteration 3 - MAPE: 10.32% (Improvement: +0.2%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_30 (importance=0.0017), Feature_41 (importance=0.0016), Feature_32 (importance=0.0010), Feature_6 (importance=0.0009), Feature_7 (importance=0.0008)

Iteration 4: Iteration 4 - MAPE: 21.42% (Improvement: -11.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_51 (importance=0.0014), Feature_42 (importance=0.0012), Feature_41 (importance=0.0011), Feature_44 (importance=0.0011), Feature_63 (importance=0.0010)

Iteration 5: Iteration 5 - MAPE: 11.37% (Improvement: -1.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_35 (importance=0.0043), Feature_143 (importance=0.0037), Feature_144 (importance=0.0020), Feature_0 (importance=0.0018), Feature_142 (importance=0.0016)

Iteration 6: Iteration 6 - MAPE: 13.02% (Improvement: -2.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_121 (importance=0.0008), Feature_123 (importance=0.0006), Feature_120 (importance=0.0006), Feature_125 (importance=0.0005), Feature_90 (importance=0.0005)

Iteration 7: Iteration 7 - MAPE: 13.01% (Improvement: -2.7%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_82 (importance=0.0018), Feature_45 (importance=0.0006), Feature_81 (importance=0.0005), Feature_80 (importance=0.0005), Feature_60 (importance=0.0005)




DL-BASED FEATURE IMPORTANCE INSIGHTS FROM BEST MODEL (MAPE: 10.32%):
- Most important features: Feature_30 (importance=0.0017), Feature_41 (importance=0.0016), Feature_32 (importance=0.0010), Feature_6 (importance=0.0009), Feature_7 (importance=0.0008)
- Least important features: Feature_18 (importance=0.0000), Feature_27 (importance=0.0001), Feature_26 (importance=0.0001)






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 7):

```python
def construct_features(data):
    """
    Constructs features for short interest prediction based on previous iterations' insights.
    
    This iteration focuses on:
    1. Returning to the successful approaches from iterations 2 and 3 (MAPE 10.32%)
    2. Focusing on the most important features identified in those iterations
    3. Reducing feature complexity to avoid overfitting (iterations 4-6 had worse performance)
    4. Creating more targeted financial indicators specific to short interest dynamics
    5. Emphasizing the relationship between price action, volume, and short interest
    
    Args:
        data: numpy array of shape (lookback_window, 62)
        
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    # Ensure data is properly formatted
    lookback_window = data.shape[0]
    
    # Handle potential NaN values
    data = np.nan_to_num(data, nan=0.0)
    
    # Extract features based on their indices
    short_interest = data[:, 0]
    avg_daily_volume = data[:, 1]
    
    # Extract OHLC data (features 2-61)
    # Reshape to (lookback_window, 15, 4) for easier manipulation
    ohlc_data = data[:, 2:62].reshape(lookback_window, 15, 4)
    
    # Extract open, high, low, close prices
    open_prices = ohlc_data[:, :, 0]
    high_prices = ohlc_data[:, :, 1]
    low_prices = ohlc_data[:, :, 2]
    close_prices = ohlc_data[:, :, 3]
    
    # Initialize list to store all features
    all_features = []
    
    # 1. Original important features
    # Short interest and volume were consistently important across iterations
    all_features.append(short_interest.reshape(lookback_window, 1))
    all_features.append(avg_daily_volume.reshape(lookback_window, 1))
    
    # 2. Focus on the most important features from the best model (MAPE: 10.32%)
    # Feature_30, Feature_41, Feature_32, Feature_6, Feature_7
    # These were the top features in our best performing model
    important_indices = [30, 41, 32, 6, 7]
    important_prices = np.zeros((lookback_window, len(important_indices)))
    
    for i, idx in enumerate(important_indices):
        # Adjust for the offset (first 2 features aren't OHLC)
        adjusted_idx = idx - 2
        day_idx = adjusted_idx // 4
        price_type_idx = adjusted_idx % 4
        
        # Extract the specific price point
        if day_idx < 15 and price_type_idx < 4:  # Ensure indices are valid
            important_prices[:, i] = ohlc_data[:, day_idx, price_type_idx]
    
    all_features.append(important_prices)
    
    # 3. Price action features from the best days
    # Extract specific days that were most predictive based on feature importance
    best_days_indices = [7, 10, 2]  # Days that contained important features
    best_days_ohlc = ohlc_data[:, best_days_indices, :]
    best_days_ohlc_flat = best_days_ohlc.reshape(lookback_window, -1)
    all_features.append(best_days_ohlc_flat)
    
    # 4. Short interest dynamics - core predictors
    si_dynamics = np.zeros((lookback_window, 4))
    for i in range(lookback_window):
        # SI level (normalized by volume to get days-to-cover)
        si_dynamics[i, 0] = short_interest[i] / (avg_daily_volume[i] + 0.0001)
        
        # SI momentum (1-step)
        if i > 0:
            si_dynamics[i, 1] = (short_interest[i] - short_interest[i-1]) / (np.abs(short_interest[i-1]) + 0.0001)
        
        # SI acceleration (2-step)
        if i > 1:
            mom_current = (short_interest[i] - short_interest[i-1]) / (np.abs(short_interest[i-1]) + 0.0001)
            mom_prev = (short_interest[i-1] - short_interest[i-2]) / (np.abs(short_interest[i-2]) + 0.0001)
            si_dynamics[i, 2] = mom_current - mom_prev
        
        # SI to volume ratio change
        if i > 0:
            prev_ratio = short_interest[i-1] / (avg_daily_volume[i-1] + 0.0001)
            curr_ratio = short_interest[i] / (avg_daily_volume[i] + 0.0001)
            si_dynamics[i, 3] = (curr_ratio - prev_ratio) / (np.abs(prev_ratio) + 0.0001)
    
    all_features.append(si_dynamics)
    
    # 5. Price momentum for important prices - focused on different timeframes
    # This was effective in iterations 2-3 but got too complex in later iterations
    price_momentum = np.zeros((lookback_window, len(important_indices) * 2))
    for i in range(lookback_window):
        for j in range(len(important_indices)):
            # Short-term momentum (1-day)
            if i > 0:
                price_momentum[i, j*2] = (important_prices[i, j] - important_prices[i-1, j]) / (np.abs(important_prices[i-1, j]) + 0.0001)
            
            # Medium-term momentum (3-day)
            if i > 2:
                price_momentum[i, j*2+1] = (important_prices[i, j] - important_prices[i-3, j]) / (np.abs(important_prices[i-3, j]) + 0.0001)
    
    all_features.append(price_momentum)
    
    # 6. Volatility features - simplified from previous iterations
    # Focus on the volatility of the most important price points
    volatility_features = np.zeros((lookback_window, len(important_indices)))
    for i in range(lookback_window):
        for j in range(len(important_indices)):
            # 3-day volatility (optimal window from previous iterations)
            if i >= 2:
                returns = np.zeros(3)
                for k in range(3):
                    if i-k > 0:
                        returns[k] = (important_prices[i-k, j] - important_prices[i-k-1, j]) / (np.abs(important_prices[i-k-1, j]) + 0.0001)
                volatility_features[i, j] = np.std(returns)
    
    all_features.append(volatility_features)
    
    # 7. Volume dynamics - volume was consistently important
    volume_dynamics = np.zeros((lookback_window, 3))
    for i in range(lookback_window):
        # Volume momentum (1-step)
        if i > 0:
            volume_dynamics[i, 0] = (avg_daily_volume[i] - avg_daily_volume[i-1]) / (np.abs(avg_daily_volume[i-1]) + 0.0001)
        
        # Volume acceleration (2-step)
        if i > 1:
            mom_current = (avg_daily_volume[i] - avg_daily_volume[i-1]) / (np.abs(avg_daily_volume[i-1]) + 0.0001)
            mom_prev = (avg_daily_volume[i-1] - avg_daily_volume[i-2]) / (np.abs(avg_daily_volume[i-2]) + 0.0001)
            volume_dynamics[i, 1] = mom_current - mom_prev
        
        # Volume relative to 3-day average
        if i >= 2:
            vol_3day_avg = np.mean(avg_daily_volume[max(0, i-2):i+1])
            volume_dynamics[i, 2] = avg_daily_volume[i] / (vol_3day_avg + 0.0001)
    
    all_features.append(volume_dynamics)
    
    # 8. Price-volume relationship - key for detecting institutional activity
    # Simplified from previous iterations to focus on the most predictive relationships
    price_volume_features = np.zeros((lookback_window, 3))
    for i in range(lookback_window):
        # Average price-volume correlation
        avg_price = np.mean(important_prices[i, :])
        price_volume_features[i, 0] = avg_price * avg_daily_volume[i]
        
        # Price-volume momentum
        if i > 0:
            avg_prev_price = np.mean(important_prices[i-1, :])
            prev_pv = avg_prev_price * avg_daily_volume[i-1]
            curr_pv = avg_price * avg_daily_volume[i]
            price_volume_features[i, 1] = (curr_pv - prev_pv) / (np.abs(prev_pv) + 0.0001)
        
        # Volume-weighted price change
        if i > 0:
            avg_price_change = (avg_price - np.mean(important_prices[i-1, :])) / (np.abs(np.mean(important_prices[i-1, :])) + 0.0001)
            price_volume_features[i, 2] = avg_price_change * avg_daily_volume[i]
    
    all_features.append(price_volume_features)
    
    # 9. Moving averages for important prices - simplified from previous iterations
    # Focus on the 3-day MA which was most predictive in previous iterations
    ma_features = np.zeros((lookback_window, len(important_indices) * 2))
    for i in range(lookback_window):
        for j in range(len(important_indices)):
            # 3-day MA
            if i >= 2:
                ma_features[i, j*2] = np.mean(important_prices[max(0, i-2):i+1, j])
                # Price relative to 3-day MA
                ma_features[i, j*2+1] = important_prices[i, j] / (ma_features[i, j*2] + 0.0001) - 1
    
    all_features.append(ma_features)
    
    # 10. Short interest to price relationship - key for short squeeze potential
    # Focus on the relationship between SI and the most important price points
    si_price_features = np.zeros((lookback_window, len(important_indices) * 2))
    for i in range(lookback_window):
        for j in range(len(important_indices)):
            # SI to price ratio
            si_price_features[i, j*2] = short_interest[i] / (important_prices[i, j] + 0.0001)
            
            # SI to price ratio change
            if i > 0:
                prev_ratio = short_interest[i-1] / (important_prices[i-1, j] + 0.0001)
                curr_ratio = short_interest[i] / (important_prices[i, j] + 0.0001)
                si_price_features[i, j*2+1] = (curr_ratio - prev_ratio) / (np.abs(prev_ratio) + 0.0001)
    
    all_features.append(si_price_features)
    
    # 11. Short squeeze potential indicators - specialized for short interest prediction
    # These were effective in iterations 2-3 but got too complex in later iterations
    squeeze_indicators = np.zeros((lookback_window, 3))
    for i in range(lookback_window):
        # Days to cover (how many days of average volume would be needed to cover all short positions)
        squeeze_indicators[i, 0] = short_interest[i] / (avg_daily_volume[i] + 0.0001)
        
        # Days to cover change
        if i > 0:
            prev_dtc = short_interest[i-1] / (avg_daily_volume[i-1] + 0.0001)
            curr_dtc = short_interest[i] / (avg_daily_volume[i] + 0.0001)
            squeeze_indicators[i, 1] = (curr_dtc - prev_dtc) / (np.abs(prev_dtc) + 0.0001)
        
        # Price momentum to short interest ratio
        # Higher values indicate price is moving up while shorts are still high (squeeze potential)
        if i > 0:
            avg_price_momentum = 0
            for j in range(len(important_indices)):
                avg_price_momentum += (important_prices[i, j] - important_prices[i-1, j]) / (np.abs(important_prices[i-1, j]) + 0.0001)
            avg_price_momentum /= len(important_indices)
            squeeze_indicators[i, 2] = avg_price_momentum * short_interest[i]
    
    all_features.append(squeeze_indicators)
    
    # 12. Interaction terms between short interest and volume
    # These were consistently important in previous iterations
    si_vol_interactions = np.zeros((lookback_window, 3))
    for i in range(lookback_window):
        # SI * Volume
        si_vol_interactions[i, 0] = short_interest[i] * avg_daily_volume[i]
        
        # SI / Volume (days to cover)
        si_vol_interactions[i, 1] = short_interest[i] / (avg_daily_volume[i] + 0.0001)
        
        # SI momentum * Volume momentum (interaction of changes)
        if i > 0:
            si_momentum = (short_interest[i] - short_interest[i-1]) / (np.abs(short_interest[i-1]) + 0.0001)
            vol_momentum = (avg_daily_volume[i] - avg_daily_volume[i-1]) / (np.abs(avg_daily_volume[i-1]) + 0.0001)
            si_vol_interactions[i, 2] = si_momentum * vol_momentum
    
    all_features.append(si_vol_interactions)
    
    # 13. Technical indicators focused on the most important price points
    # Simplified from previous iterations to focus on the most predictive indicators
    tech_indicators = np.zeros((lookback_window, len(important_indices) * 2))
    for i in range(lookback_window):
        for j in range(len(important_indices)):
            # Relative Strength Index (RSI) - simplified calculation
            if i >= 6:  # Need at least 7 data points for a simplified RSI
                # Calculate price changes
                price_changes = np.zeros(7)
                for k in range(1, 7):
                    if i-k >= 0 and i-k+1 >= 0:
                        price_changes[k] = important_prices[i-k+1, j] - important_prices[i-k, j]
                
                # Separate gains and losses
                gains = np.where(price_changes > 0, price_changes, 0)
                losses = np.where(price_changes < 0, -price_changes, 0)
                
                # Calculate average gain and loss
                avg_gain = np.mean(gains)
                avg_loss = np.mean(losses)
                
                # Calculate RSI
                if avg_loss == 0:
                    tech_indicators[i, j*2] = 100
                else:
                    rs = avg_gain / (avg_loss + 0.0001)
                    tech_indicators[i, j*2] = 100 - (100 / (1 + rs))
            
            # Price Rate of Change (ROC)
            if i >= 5:
                tech_indicators[i, j*2+1] = (important_prices[i, j] - important_prices[i-5, j]) / (important_prices[i-5, j] + 0.0001) * 100
    
    all_features.append(tech_indicators)
    
    # 14. Nonlinear transformations of key features
    # Focus on the most predictive nonlinear transformations from previous iterations
    nonlinear_features = np.zeros((lookback_window, 4))
    for i in range(lookback_window):
        # Log of short interest to volume ratio (days to cover)
        ratio = short_interest[i] / (avg_daily_volume[i] + 0.0001)
        nonlinear_features[i, 0] = np.log1p(ratio)
        
        # Square root of short interest
        nonlinear_features[i, 1] = np.sqrt(np.abs(short_interest[i]))
        
        # Squared SI to volume ratio
        nonlinear_features[i, 2] = ratio ** 2
        
        # Cube root of SI
        nonlinear_features[i, 3] = np.cbrt(np.abs(short_interest[i])) * np.sign(short_interest[i])
    
    all_features.append(nonlinear_features)
    
    # 15. Combine all features
    result = np.hstack(all_features)
    
    # Final check for NaN values
    result = np.nan_to_num(result, nan=0.0)
    
    return result
```

Performance of this code: MAPE = 13.01%
Change from previous: -2.68%
Statistical Analysis: 68/84 features were significant (p < 0.05), 57 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 8):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 10.32%.

Based on the performance history, DL-based feature importance analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses DL-based feature importance insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using DL-based feature importance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and importance insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
