
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 17.25%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_1 (importance=0.0003), Feature_48 (importance=0.0003), Feature_40 (importance=0.0003), Feature_27 (importance=0.0002), Feature_10 (importance=0.0002)

Iteration 1: Iteration 1 - MAPE: 18.14% (Improvement: -0.9%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_8 (importance=0.0238), Feature_16 (importance=0.0194), Feature_13 (importance=0.0135), Feature_11 (importance=0.0102), Feature_7 (importance=0.0082)

Iteration 2: Iteration 2 - MAPE: 13.23% (Improvement: +4.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_23 (importance=0.0243), Feature_14 (importance=0.0124), Feature_10 (importance=0.0106), Feature_17 (importance=0.0074), Feature_0 (importance=0.0071)

Iteration 3: Iteration 3 - MAPE: 16.37% (Improvement: -3.1%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_35 (importance=0.0061), Feature_7 (importance=0.0057), Feature_30 (importance=0.0053), Feature_12 (importance=0.0051), Feature_41 (importance=0.0039)

Iteration 4: Iteration 4 - MAPE: 22.22% (Improvement: -9.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_10 (importance=0.0042), Feature_25 (importance=0.0028), Feature_21 (importance=0.0027), Feature_91 (importance=0.0022), Feature_64 (importance=0.0021)

Iteration 5: Iteration 5 - MAPE: 17.27% (Improvement: -4.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_27 (importance=0.0071), Feature_23 (importance=0.0056), Feature_39 (importance=0.0051), Feature_21 (importance=0.0050), Feature_1 (importance=0.0049)

Iteration 6: Iteration 6 - MAPE: 22.50% (Improvement: -9.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_43 (importance=0.0038), Feature_35 (importance=0.0035), Feature_4 (importance=0.0033), Feature_13 (importance=0.0033), Feature_5 (importance=0.0028)




DL-BASED FEATURE IMPORTANCE INSIGHTS FROM BEST MODEL (MAPE: 13.23%):
- Most important features: Feature_23 (importance=0.0243), Feature_14 (importance=0.0124), Feature_10 (importance=0.0106), Feature_17 (importance=0.0074), Feature_0 (importance=0.0071)
- Least important features: Feature_5 (importance=0.0003), Feature_15 (importance=0.0009), Feature_7 (importance=0.0011)






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 6):

```python
def construct_features(data):
    """
    Constructs features for short interest prediction based on historical performance analysis.
    
    This iteration focuses on:
    1. Returning to the successful approach from Iteration 2 (MAPE 13.23%)
    2. Emphasizing the most important features identified in the best model
    3. Reducing dimensionality while preserving predictive power
    4. Adding targeted financial indicators with strong theoretical basis
    5. Improving numerical stability with better normalization techniques
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            - data[:, 0]: Short interest
            - data[:, 1]: Average daily volume
            - data[:, 2:62]: OHLC prices for past 15 days (4 features × 15 days)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    # Ensure we have the correct input shape
    if len(data.shape) != 2 or data.shape[1] != 62:
        # Handle error gracefully - return empty array with correct dimensions
        return np.zeros((data.shape[0], 1))
    
    lookback_window = data.shape[0]
    
    # Handle NaN values
    data = np.nan_to_num(data, nan=0.0)
    
    # Extract key components
    short_interest = data[:, 0]
    avg_volume = data[:, 1]
    
    # Reshape OHLC data for easier processing
    # Original format: 60 columns (4 OHLC × 15 days flattened)
    # New format: (lookback_window, 15, 4) where 4 is OHLC
    ohlc_data = np.zeros((lookback_window, 15, 4))
    for i in range(15):
        ohlc_data[:, i, 0] = data[:, 2 + i*4]     # Open
        ohlc_data[:, i, 1] = data[:, 2 + i*4 + 1] # High
        ohlc_data[:, i, 2] = data[:, 2 + i*4 + 2] # Low
        ohlc_data[:, i, 3] = data[:, 2 + i*4 + 3] # Close
    
    # Initialize feature list
    feature_list = []
    
    # Extract price data for easier access
    close_prices = ohlc_data[:, :, 3]  # All close prices
    open_prices = ohlc_data[:, :, 0]   # All open prices
    high_prices = ohlc_data[:, :, 1]   # All high prices
    low_prices = ohlc_data[:, :, 2]    # All low prices
    
    # -------------------------------------------------------------------------
    # 1. CORE FEATURES - Based on DL importance analysis from best model (13.23% MAPE)
    # -------------------------------------------------------------------------
    
    # Feature_0: Short interest itself (consistently important across iterations)
    feature_list.append(short_interest.reshape(lookback_window, 1))
    
    # Feature_23: 6th day's Close price (most important in best model)
    # Normalize by dividing by the mean close price to make it scale-invariant
    mean_close = np.mean(close_prices, axis=1, keepdims=True) + 1e-8
    norm_close_6 = ohlc_data[:, 5, 3] / mean_close.flatten()
    feature_list.append(norm_close_6.reshape(lookback_window, 1))
    
    # Feature_14: 4th day's Low price (2nd most important in best model)
    # Normalize by dividing by the mean low price
    mean_low = np.mean(low_prices, axis=1, keepdims=True) + 1e-8
    norm_low_4 = ohlc_data[:, 3, 2] / mean_low.flatten()
    feature_list.append(norm_low_4.reshape(lookback_window, 1))
    
    # Feature_10: 3rd day's Low price (3rd most important in best model)
    norm_low_3 = ohlc_data[:, 2, 2] / mean_low.flatten()
    feature_list.append(norm_low_3.reshape(lookback_window, 1))
    
    # Feature_17: 5th day's Low price (4th most important in best model)
    norm_low_5 = ohlc_data[:, 4, 2] / mean_low.flatten()
    feature_list.append(norm_low_5.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 2. IMPROVED SHORT INTEREST FEATURES
    # -------------------------------------------------------------------------
    
    # Short interest momentum (rate of change) with improved stability
    si_momentum_1 = np.zeros((lookback_window, 1))
    si_momentum_1[1:, 0] = np.log1p(short_interest[1:] + 1e-8) - np.log1p(short_interest[:-1] + 1e-8)
    # Using log differences provides better numerical stability than percentage changes
    feature_list.append(si_momentum_1)
    
    # Short interest acceleration (2nd derivative) with improved stability
    si_accel = np.zeros((lookback_window, 1))
    if lookback_window > 2:
        si_accel[2:, 0] = si_momentum_1[2:, 0] - si_momentum_1[1:-1, 0]
        # Clip extreme values
        si_accel = np.clip(si_accel, -0.5, 0.5)
    feature_list.append(si_accel)
    
    # Short interest relative to its moving average (3-day window)
    # This was effective in iteration 2 but with improved calculation
    if lookback_window >= 3:
        si_ma3 = np.zeros(lookback_window)
        for i in range(lookback_window):
            start_idx = max(0, i-2)  # 3-day window
            si_ma3[i] = np.mean(short_interest[start_idx:i+1])
        si_relative_to_ma3 = short_interest / (si_ma3 + 1e-8)
        # Clip extreme values
        si_relative_to_ma3 = np.clip(si_relative_to_ma3, 0.7, 1.3)
        feature_list.append(si_relative_to_ma3.reshape(lookback_window, 1))
    
    # Short interest z-score (how many standard deviations from mean)
    # This provides a mean-reversion signal
    si_zscore = np.zeros(lookback_window)
    if lookback_window > 2:
        for i in range(lookback_window):
            start_idx = max(0, i-3)  # Use up to 4 days of history
            window_si = short_interest[start_idx:i+1]
            if len(window_si) > 1:
                si_mean = np.mean(window_si)
                si_std = np.std(window_si)
                if si_std > 0:
                    si_zscore[i] = (short_interest[i] - si_mean) / si_std
        si_zscore = np.clip(si_zscore, -3, 3)  # Clip to typical z-score range
    feature_list.append(si_zscore.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 3. VOLUME FEATURES - Refined from previous iterations
    # -------------------------------------------------------------------------
    
    # Log-transformed volume (better for skewed distributions)
    log_volume = np.log1p(avg_volume)
    feature_list.append(log_volume.reshape(lookback_window, 1))
    
    # Days to cover ratio (short interest / average daily volume)
    # This is a key metric used by traders to assess short squeeze potential
    days_to_cover = short_interest / (avg_volume + 1e-8)
    # Clip to reasonable range based on financial domain knowledge
    days_to_cover = np.clip(days_to_cover, 0, 20)
    feature_list.append(days_to_cover.reshape(lookback_window, 1))
    
    # Volume trend using exponential weighting (more recent days matter more)
    vol_trend = np.zeros(lookback_window)
    if lookback_window > 2:
        for i in range(lookback_window):
            if i >= 2:  # Need at least 3 points for meaningful trend
                weights = np.exp(np.arange(i+1) - i)  # Exponential weights
                weights = weights / np.sum(weights)  # Normalize weights
                x = np.arange(i+1)
                x_centered = x - np.average(x, weights=weights)
                y_centered = log_volume[:i+1] - np.average(log_volume[:i+1], weights=weights)
                if np.sum(x_centered**2) > 0:
                    vol_trend[i] = np.sum(weights * x_centered * y_centered) / np.sum(weights * x_centered**2)
        vol_trend = np.clip(vol_trend, -0.5, 0.5)
    feature_list.append(vol_trend.reshape(lookback_window, 1))
    
    # Volume volatility (standard deviation of volume)
    vol_volatility = np.zeros(lookback_window)
    if lookback_window > 2:
        for i in range(lookback_window):
            if i >= 2:
                vol_volatility[i] = np.std(log_volume[max(0, i-3):i+1])
        vol_volatility = np.clip(vol_volatility, 0, 2)
    feature_list.append(vol_volatility.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 4. PRICE FEATURES - Focused on important days identified by feature importance
    # -------------------------------------------------------------------------
    
    # Important days based on feature importance analysis from best model
    # Days 3, 4, 5, 6 were most important (indices 2, 3, 4, 5)
    important_days = [2, 3, 4, 5]
    
    # Calculate returns for each day
    returns = np.zeros((lookback_window, 15))
    for i in range(15):
        if i > 0:
            returns[:, i] = (close_prices[:, i] - close_prices[:, i-1]) / (close_prices[:, i-1] + 1e-8)
            # Clip extreme returns for stability
            returns[:, i] = np.clip(returns[:, i], -0.2, 0.2)
    
    # For each important day, create specialized features
    for day_idx in important_days:
        # Daily range (normalized by opening price)
        daily_range = (high_prices[:, day_idx] - low_prices[:, day_idx]) / (open_prices[:, day_idx] + 1e-8)
        daily_range = np.clip(daily_range, 0, 0.15)  # Clip to reasonable range
        feature_list.append(daily_range.reshape(lookback_window, 1))
        
        # Daily return
        if day_idx > 0:
            daily_return = returns[:, day_idx]
            feature_list.append(daily_return.reshape(lookback_window, 1))
        
        # Intraday movement pattern
        intraday_move = (close_prices[:, day_idx] - open_prices[:, day_idx]) / (open_prices[:, day_idx] + 1e-8)
        intraday_move = np.clip(intraday_move, -0.1, 0.1)
        feature_list.append(intraday_move.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 5. TECHNICAL INDICATORS - Focused on important days with improved calculations
    # -------------------------------------------------------------------------
    
    # Calculate moving averages for important days
    for day_idx in important_days:
        if day_idx >= 4:  # Need at least 5 days for 5-day MA
            # 5-day moving average
            ma5 = np.zeros(lookback_window)
            for i in range(lookback_window):
                ma5[i] = np.mean(close_prices[i, day_idx-4:day_idx+1])
            
            # Price relative to MA
            price_to_ma = close_prices[:, day_idx] / (ma5 + 1e-8)
            price_to_ma = np.clip(price_to_ma, 0.85, 1.15)
            feature_list.append(price_to_ma.reshape(lookback_window, 1))
    
    # RSI for important days (improved calculation)
    for day_idx in important_days:
        if day_idx >= 6:  # Need at least 7 days for reliable RSI
            # Calculate gains and losses over 7-day window
            gains = np.zeros((lookback_window, 7))
            losses = np.zeros((lookback_window, 7))
            
            for w in range(7):
                day = day_idx - w
                if day > 0:
                    daily_change = returns[:, day]
                    gains[:, w] = np.maximum(0, daily_change)
                    losses[:, w] = np.maximum(0, -daily_change)
            
            # Use exponential weighting for more recent days
            weights = np.exp(np.arange(7) - 6)
            weights = weights / np.sum(weights)
            
            avg_gain = np.zeros(lookback_window)
            avg_loss = np.zeros(lookback_window)
            
            for i in range(lookback_window):
                avg_gain[i] = np.sum(gains[i] * weights)
                avg_loss[i] = np.sum(losses[i] * weights)
            
            # Calculate RSI
            rs = avg_gain / (avg_loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            rsi = np.clip(rsi, 0, 100)
            feature_list.append(rsi.reshape(lookback_window, 1))
    
    # Bollinger Bands for important days (improved calculation)
    for day_idx in important_days:
        if day_idx >= 4:  # Need at least 5 days
            # Calculate 5-day moving average and standard deviation
            ma5 = np.zeros(lookback_window)
            std5 = np.zeros(lookback_window)
            
            for i in range(lookback_window):
                price_window = close_prices[i, day_idx-4:day_idx+1]
                ma5[i] = np.mean(price_window)
                std5[i] = np.std(price_window) if len(price_window) > 1 else 0
            
            # Calculate Bollinger Band position
            bb_position = (close_prices[:, day_idx] - ma5) / (2 * std5 + 1e-8)
            bb_position = np.clip(bb_position, -2, 2)
            feature_list.append(bb_position.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 6. SHORT INTEREST TO PRICE RELATIONSHIPS - Improved calculations
    # -------------------------------------------------------------------------
    
    # Short interest to price ratio for important days
    for day_idx in important_days:
        si_price_ratio = short_interest / (close_prices[:, day_idx] + 1e-8)
        
        # Normalize using z-score instead of simple scaling
        if lookback_window > 2:
            si_price_mean = np.mean(si_price_ratio)
            si_price_std = np.std(si_price_ratio)
            if si_price_std > 0:
                si_price_ratio = (si_price_ratio - si_price_mean) / si_price_std
                si_price_ratio = np.clip(si_price_ratio, -3, 3)
        
        feature_list.append(si_price_ratio.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 7. VOLATILITY FEATURES - Improved with exponential weighting
    # -------------------------------------------------------------------------
    
    # Calculate volatility over 5-day window for important days
    for day_idx in important_days:
        if day_idx >= 4:  # Need at least 5 days
            # Calculate rolling volatility with exponential weighting
            window_returns = returns[:, day_idx-4:day_idx+1]
            rolling_vol = np.zeros(lookback_window)
            
            for i in range(lookback_window):
                if np.count_nonzero(~np.isnan(window_returns[i])) > 1:
                    # Exponential weights - more recent days matter more
                    weights = np.exp(np.arange(5) - 4)
                    weights = weights / np.sum(weights)
                    # Weighted standard deviation
                    weighted_mean = np.sum(window_returns[i] * weights)
                    weighted_var = np.sum(weights * ((window_returns[i] - weighted_mean) ** 2))
                    rolling_vol[i] = np.sqrt(weighted_var)
            
            rolling_vol = np.clip(rolling_vol, 0, 0.1)
            feature_list.append(rolling_vol.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 8. INTERACTION FEATURES - Improved with more targeted interactions
    # -------------------------------------------------------------------------
    
    # Interaction between short interest and volume (log-transformed)
    si_vol_interaction = np.log1p(short_interest) * log_volume
    # Normalize using z-score
    if lookback_window > 2:
        si_vol_mean = np.mean(si_vol_interaction)
        si_vol_std = np.std(si_vol_interaction)
        if si_vol_std > 0:
            si_vol_interaction = (si_vol_interaction - si_vol_mean) / si_vol_std
            si_vol_interaction = np.clip(si_vol_interaction, -3, 3)
    feature_list.append(si_vol_interaction.reshape(lookback_window, 1))
    
    # Interaction between short interest momentum and price momentum for important days
    for day_idx in important_days:
        if day_idx > 0:
            price_momentum = returns[:, day_idx]
            interaction = si_momentum_1[:, 0] * price_momentum
            interaction = np.clip(interaction, -0.1, 0.1)
            feature_list.append(interaction.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 9. NEW: ADVANCED TECHNICAL INDICATORS
    # -------------------------------------------------------------------------
    
    # MACD-like indicator for important days
    for day_idx in important_days:
        if day_idx >= 12:  # Need at least 13 days for calculation
            # Calculate 12-day and 26-day EMAs
            ema12 = np.zeros(lookback_window)
            ema26 = np.zeros(lookback_window)
            
            for i in range(lookback_window):
                # Simple approximation of EMA using weighted average
                weights12 = np.exp(np.arange(12) - 11)
                weights12 = weights12 / np.sum(weights12)
                
                weights26 = np.exp(np.arange(min(26, day_idx+1)) - min(25, day_idx))
                weights26 = weights26 / np.sum(weights26)
                
                ema12[i] = np.sum(close_prices[i, max(0, day_idx-11):day_idx+1] * weights12[-min(12, day_idx+1):])
                ema26[i] = np.sum(close_prices[i, max(0, day_idx-25):day_idx+1] * weights26[-min(26, day_idx+1):])
            
            # MACD line
            macd = ema12 - ema26
            
            # Normalize MACD by price level
            norm_macd = macd / (close_prices[:, day_idx] + 1e-8)
            norm_macd = np.clip(norm_macd, -0.05, 0.05)
            feature_list.append(norm_macd.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 10. NEW: PRICE PATTERN RECOGNITION
    # -------------------------------------------------------------------------
    
    # Detect potential reversal patterns in important days
    for day_idx in important_days:
        if day_idx >= 2:  # Need at least 3 days
            # Calculate body sizes (absolute difference between open and close)
            body_sizes = np.abs(close_prices[:, day_idx-2:day_idx+1] - open_prices[:, day_idx-2:day_idx+1])
            
            # Calculate shadows (wicks)
            upper_shadows = high_prices[:, day_idx-2:day_idx+1] - np.maximum(close_prices[:, day_idx-2:day_idx+1], 
                                                                           open_prices[:, day_idx-2:day_idx+1])
            lower_shadows = np.minimum(close_prices[:, day_idx-2:day_idx+1], open_prices[:, day_idx-2:day_idx+1]) - \
                           low_prices[:, day_idx-2:day_idx+1]
            
            # Normalize by average price
            avg_prices = (high_prices[:, day_idx-2:day_idx+1] + low_prices[:, day_idx-2:day_idx+1]) / 2
            norm_body = body_sizes / (avg_prices + 1e-8)
            norm_upper = upper_shadows / (avg_prices + 1e-8)
            norm_lower = lower_shadows / (avg_prices + 1e-8)
            
            # Pattern features - focus on most recent day
            pattern_features = np.zeros((lookback_window, 3))
            pattern_features[:, 0] = norm_body[:, -1]  # Body size of most recent day
            pattern_features[:, 1] = norm_upper[:, -1]  # Upper shadow of most recent day
            pattern_features[:, 2] = norm_lower[:, -1]  # Lower shadow of most recent day
            
            # Clip to reasonable ranges
            pattern_features = np.clip(pattern_features, 0, 0.1)
            
            # Add pattern features
            feature_list.append(pattern_features)
    
    # -------------------------------------------------------------------------
    # 11. NEW: SHORT INTEREST PREDICTION-SPECIFIC FEATURES
    # -------------------------------------------------------------------------
    
    # Short interest to volume ratio change
    si_vol_ratio = short_interest / (avg_volume + 1e-8)
    si_vol_ratio_change = np.zeros(lookback_window)
    if lookback_window > 1:
        si_vol_ratio_change[1:] = (si_vol_ratio[1:] - si_vol_ratio[:-1]) / (si_vol_ratio[:-1] + 1e-8)
        si_vol_ratio_change = np.clip(si_vol_ratio_change, -1, 1)
    feature_list.append(si_vol_ratio_change.reshape(lookback_window, 1))
    
    # Short interest momentum relative to price momentum
    si_price_momentum_ratio = np.zeros(lookback_window)
    if lookback_window > 1:
        # Calculate average price momentum across important days
        avg_price_momentum = np.zeros(lookback_window)
        for day_idx in important_days:
            if day_idx > 0:
                avg_price_momentum += returns[:, day_idx]
        avg_price_momentum /= len(important_days)
        
        # Calculate short interest momentum
        si_mom = np.zeros(lookback_window)
        si_mom[1:] = (short_interest[1:] - short_interest[:-1]) / (short_interest[:-1] + 1e-8)
        
        # Calculate ratio (with stability)
        for i in range(1, lookback_window):
            if abs(avg_price_momentum[i]) > 1e-8:
                si_price_momentum_ratio[i] = si_mom[i] / (avg_price_momentum[i] + 1e-8)
        
        si_price_momentum_ratio = np.clip(si_price_momentum_ratio, -10, 10)
    feature_list.append(si_price_momentum_ratio.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 12. NEW: ENSEMBLE OF PREDICTORS FROM BEST ITERATIONS
    # -------------------------------------------------------------------------
    
    # From Iteration 2 (best model) - Feature 23 (6th day's Close price)
    # Already included above
    
    # From Iteration 1 - Feature 8 (2nd day's High price)
    norm_high_2 = ohlc_data[:, 1, 1] / (np.mean(high_prices, axis=1) + 1e-8)
    feature_list.append(norm_high_2.reshape(lookback_window, 1))
    
    # From Iteration 1 - Feature 16 (4th day's High price)
    norm_high_4 = ohlc_data[:, 3, 1] / (np.mean(high_prices, axis=1) + 1e-8)
    feature_list.append(norm_high_4.reshape(lookback_window, 1))
    
    # From Iteration 3 - Feature 35 (9th day's Low price)
    if ohlc_data.shape[1] > 8:  # Ensure we have enough days
        norm_low_9 = ohlc_data[:, 8, 2] / (np.mean(low_prices, axis=1) + 1e-8)
        feature_list.append(norm_low_9.reshape(lookback_window, 1))
    
    # -------------------------------------------------------------------------
    # 13. COMBINE ALL FEATURES
    # -------------------------------------------------------------------------
    
    combined_features = np.hstack(feature_list)
    
    # Final check for NaN values
    combined_features = np.nan_to_num(combined_features, nan=0.0)
    
    # Final check for infinite values
    combined_features = np.clip(combined_features, -1e9, 1e9)
    
    return combined_features
```

Performance of this code: MAPE = 22.50%
Change from previous: -9.27%
Statistical Analysis: 48/57 features were significant (p < 0.05), 47 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 7):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 13.23%.

Based on the performance history, DL-based feature importance analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses DL-based feature importance insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using DL-based feature importance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and importance insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
