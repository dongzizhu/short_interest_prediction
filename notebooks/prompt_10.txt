
You are a financial data scientist expert in feature engineering for Short Interest prediction models. 

I have financial time series data with the following structure:
- Shape: (samples, lookback_window=4, features=62)
- Features at each timestamp T include:
  1. Short interest at time T reported every 15 days (1 dimension)
  2. Average daily volume quantity of past 15 days (1 dimension) 
  3. OHLC (Open, High, Low, Close) prices for past 15 days (4 × 15 = 60 dimensions)

Total: 1 + 1 + 60 = 62 features(dimensions) per timestamp.



PERFORMANCE HISTORY:
Iteration 0: Baseline - MAPE: 60.85%
  Features: All 62 original features
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0243), Feature_1 (importance=0.0059), Feature_48 (importance=0.0047), Feature_17 (importance=0.0031), Feature_46 (importance=0.0027)

Iteration 1: Iteration 1 - MAPE: 60.08% (Improvement: +0.8%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_13 (importance=0.0474), Feature_12 (importance=0.0309), Feature_14 (importance=0.0237), Feature_10 (importance=0.0140), Feature_3 (importance=0.0123)

Iteration 2: Iteration 2 - MAPE: 69.52% (Improvement: -9.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_19 (importance=0.0128), Feature_17 (importance=0.0120), Feature_14 (importance=0.0081), Feature_23 (importance=0.0081), Feature_3 (importance=0.0079)

Iteration 3: Iteration 3 - MAPE: 57.46% (Improvement: +2.6%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0238), Feature_19 (importance=0.0204), Feature_23 (importance=0.0139), Feature_21 (importance=0.0092), Feature_1 (importance=0.0069)

Iteration 4: Iteration 4 - MAPE: 53.03% (Improvement: +4.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_0 (importance=0.0129), Feature_4 (importance=0.0104), Feature_39 (importance=0.0078), Feature_38 (importance=0.0069), Feature_5 (importance=0.0066)

Iteration 5: Iteration 5 - MAPE: 42.00% (Improvement: +11.0%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_21 (importance=0.0232), Feature_33 (importance=0.0106), Feature_9 (importance=0.0077), Feature_13 (importance=0.0053), Feature_8 (importance=0.0053)

Iteration 6: Iteration 6 - MAPE: 48.27% (Improvement: -6.3%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_6 (importance=0.0106), Feature_49 (importance=0.0061), Feature_9 (importance=0.0059), Feature_41 (importance=0.0052), Feature_32 (importance=0.0042)

Iteration 7: Iteration 7 - MAPE: 42.54% (Improvement: -0.5%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_36 (importance=0.0094), Feature_38 (importance=0.0089), Feature_31 (importance=0.0085), Feature_39 (importance=0.0082), Feature_20 (importance=0.0079)

Iteration 8: Iteration 8 - MAPE: 54.38% (Improvement: -12.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_3 (importance=0.0173), Feature_33 (importance=0.0074), Feature_20 (importance=0.0049), Feature_5 (importance=0.0049), Feature_15 (importance=0.0044)

Iteration 9: Iteration 9 - MAPE: 42.35% (Improvement: -0.4%)
  Features: claude feature engineering
  DL-Based Feature Importance Analysis:
    • Top important features: Feature_17 (importance=0.0094), Feature_12 (importance=0.0051), Feature_29 (importance=0.0048), Feature_1 (importance=0.0044), Feature_31 (importance=0.0042)




DL-BASED FEATURE IMPORTANCE INSIGHTS FROM BEST MODEL (MAPE: 42.00%):
- Most important features: Feature_21 (importance=0.0232), Feature_33 (importance=0.0106), Feature_9 (importance=0.0077), Feature_13 (importance=0.0053), Feature_8 (importance=0.0053)
- Least important features: Feature_31 (importance=0.0000), Feature_32 (importance=0.0000), Feature_20 (importance=0.0006)






PREVIOUS ITERATION CODE:
The following code was used in the most recent iteration (Iteration 9):

```python
def construct_features(data):
    """
    Constructs engineered features for short interest prediction.
    
    Args:
        data: numpy array of shape (lookback_window, 62)
            Feature 0: Short interest at time T
            Feature 1: Average daily volume quantity of past 15 days
            Features 2-61: OHLC prices for past 15 days (4 × 15 = 60 dimensions)
    
    Returns:
        numpy array of shape (lookback_window, constructed_features)
    """
    # Handle edge case of empty data
    if data.shape[0] == 0:
        return np.zeros((0, 35))  # Return empty array with expected feature count
    
    # Initialize output array
    lookback_window = data.shape[0]
    engineered_features = []
    
    # Analysis of previous iterations:
    # - Iteration 5 (MAPE 42.00%) was the best model with fewer features
    # - Iteration 8 (MAPE 54.38%) had too many features (40) and likely overfit
    # - Key insights:
    #   1. Need to focus on the most important features from best model (Feature_21, Feature_33, Feature_9, Feature_13, Feature_8)
    #   2. Reduce feature count to avoid overfitting (target ~30-35 features)
    #   3. Improve normalization and scaling
    #   4. Focus on financial domain knowledge and SI-volume-price relationships
    #   5. Reduce multicollinearity between features
    
    for t in range(lookback_window):
        # Get data for current timestamp
        current_data = data[t]
        
        # Handle NaN values
        current_data = np.nan_to_num(current_data, nan=0.0)
        
        # Extract key components
        short_interest = current_data[0]
        avg_volume = current_data[1]
        
        # Extract OHLC data (reshape to 15 days x 4 OHLC values)
        ohlc_data = current_data[2:62].reshape(15, 4)
        
        # Extract specific components
        opens = ohlc_data[:, 0]
        highs = ohlc_data[:, 1]
        lows = ohlc_data[:, 2]
        closes = ohlc_data[:, 3]
        
        # Feature Group 1: Core SI and Volume Features
        # These were consistently important across iterations
        features = [
            short_interest,                                # Original short interest (consistently important)
            avg_volume,                                    # Original volume
            np.log1p(short_interest),                      # Log-transformed SI (for better scaling)
            np.log1p(avg_volume),                          # Log-transformed volume
            short_interest / (avg_volume + 1e-8),          # Days to cover (key metric for short squeeze potential)
        ]
        
        # Feature Group 2: SI History and Dynamics
        # Extract SI history for temporal analysis
        si_history = []
        for i in range(min(4, t+1)):
            prev_si = data[t-i, 0] if t-i >= 0 else 0
            prev_si = 0 if np.isnan(prev_si) else prev_si
            si_history.append(prev_si)
        
        # Fill history if needed
        while len(si_history) < 4:
            si_history.append(0)
        
        # SI change metrics (focus on robust calculations)
        si_change = si_history[0] - si_history[1] if len(si_history) > 1 else 0
        si_pct_change = si_change / (si_history[1] + 1e-8) if len(si_history) > 1 else 0
        
        # SI momentum (rate of change over different time periods)
        si_momentum_2d = (si_history[0] - si_history[2]) / 2 if len(si_history) > 2 else 0
        
        # Exponentially weighted SI (gives more weight to recent values)
        if len(si_history) >= 3:
            weights = np.array([0.7, 0.2, 0.1])
            ema_si = np.sum(weights * si_history[:3]) / np.sum(weights)
            # SI deviation from trend
            si_trend_deviation = si_history[0] / (ema_si + 1e-8) - 1
        else:
            ema_si = si_history[0]
            si_trend_deviation = 0
        
        features.extend([
            si_change,                # Absolute change in SI
            si_pct_change,            # Percentage change in SI
            si_momentum_2d,           # 2-day SI momentum
            si_trend_deviation        # SI deviation from trend
        ])
        
        # Feature Group 3: Price Dynamics
        # These features capture price movements that might influence short interest
        
        # Basic price metrics
        if len(closes) > 0:
            current_close = closes[0]
            current_open = opens[0]
            
            # Price returns over different timeframes
            daily_return = (current_close/current_open - 1) if current_open != 0 else 0
            
            returns_1d = (current_close/closes[1] - 1) if len(closes) > 1 else 0
            returns_3d = (current_close/closes[3] - 1) if len(closes) > 3 else 0
            returns_5d = (current_close/closes[5] - 1) if len(closes) > 5 else 0
            
            # Price volatility measures
            if len(closes) >= 5:
                price_std = np.std(closes[:5])
                price_volatility = price_std / (np.mean(closes[:5]) + 1e-8)
                
                # High-Low range volatility
                hl_ranges = [(highs[i] - lows[i])/(opens[i] + 1e-8) for i in range(5)]
                hl_volatility = np.mean(hl_ranges)  # Average daily range
            else:
                price_volatility = 0
                hl_volatility = 0
            
            # Moving averages
            ma5 = np.mean(closes[:5]) if len(closes) >= 5 else current_close
            ma10 = np.mean(closes[:10]) if len(closes) >= 10 else current_close
            
            # Price relative to moving averages
            price_to_ma5 = current_close/ma5 - 1
            price_to_ma10 = current_close/ma10 - 1
            
            features.extend([
                daily_return,         # Daily return
                returns_3d,           # 3-day return
                returns_5d,           # 5-day return
                price_volatility,     # Price volatility
                hl_volatility,        # High-Low range volatility
                price_to_ma5,         # Price relative to 5-day MA
                price_to_ma10,        # Price relative to 10-day MA
            ])
        else:
            features.extend([0, 0, 0, 0, 0, 0, 0])
        
        # Feature Group 4: Volume Dynamics
        # Volume patterns are critical for short interest prediction
        
        # Extract volume history
        vol_history = []
        for i in range(min(4, t+1)):
            prev_vol = data[t-i, 1] if t-i >= 0 else avg_volume
            prev_vol = avg_volume if np.isnan(prev_vol) else prev_vol
            vol_history.append(prev_vol)
        
        # Fill history if needed
        while len(vol_history) < 4:
            vol_history.append(avg_volume)
        
        # Volume change metrics
        vol_pct_change = vol_history[0] / (vol_history[1] + 1e-8) - 1
        
        # Volume trend
        if len(vol_history) >= 3:
            weights = np.array([0.6, 0.3, 0.1])
            vol_trend = np.sum(weights * vol_history[:3]) / np.sum(weights)
            vol_trend_ratio = vol_history[0] / (vol_trend + 1e-8)
        else:
            vol_trend_ratio = 1.0
        
        # Volume volatility
        vol_volatility = np.std(vol_history) / (np.mean(vol_history) + 1e-8) if len(vol_history) > 1 else 0
        
        features.extend([
            vol_pct_change,           # Percentage change in volume
            vol_trend_ratio,          # Volume relative to trend
            vol_volatility            # Volume volatility
        ])
        
        # Feature Group 5: SI-Price-Volume Relationships
        # These interaction features capture complex relationships between SI, price, and volume
        
        # SI relative to price
        si_to_price = short_interest / (closes[0] + 1e-8) if len(closes) > 0 else 0
        
        # SI change relative to price change
        si_price_change_ratio = 0
        if len(closes) > 1 and len(si_history) > 1:
            price_change = closes[0] - closes[1]
            si_price_change_ratio = si_change / (np.abs(price_change) + 1e-8)
        
        # SI relative to price volatility
        si_vol_interaction = short_interest * price_volatility if len(closes) >= 5 else 0
        
        # Short squeeze potential indicator (refined from previous iteration)
        short_squeeze_indicator = 0
        if len(closes) >= 5 and short_interest > 0:
            # Refined short squeeze indicator based on SI, price momentum, and volume
            days_to_cover = short_interest / (avg_volume + 1e-8)
            price_momentum = returns_5d
            vol_surge = vol_pct_change
            
            # Weighted combination of factors
            short_squeeze_indicator = (
                0.4 * np.tanh(days_to_cover / 5) +  # Normalized days to cover
                0.4 * np.tanh(price_momentum * 10) +  # Normalized price momentum
                0.2 * np.tanh(vol_surge * 5)  # Normalized volume surge
            )
        
        features.extend([
            si_to_price,              # SI relative to price
            si_price_change_ratio,    # SI change relative to price change
            si_vol_interaction,       # SI-volatility interaction
            short_squeeze_indicator   # Short squeeze potential
        ])
        
        # Feature Group 6: Technical Indicators
        # These indicators were important in previous iterations
        
        if len(closes) >= 14:
            # RSI calculation (14-day)
            diff = np.diff(np.concatenate(([closes[0]], closes[:13])))
            gains = np.maximum(diff, 0)
            losses = np.maximum(-diff, 0)
            avg_gain = np.mean(gains)
            avg_loss = np.mean(losses)
            rs = avg_gain / (avg_loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            
            # Bollinger Bands (20-day)
            ma20 = np.mean(closes[:min(20, len(closes))])
            std20 = np.std(closes[:min(20, len(closes))])
            upper_band = ma20 + (2 * std20)
            lower_band = ma20 - (2 * std20)
            bb_width = (upper_band - lower_band)/(ma20 + 1e-8)
            bb_position = (closes[0] - lower_band)/(upper_band - lower_band + 1e-8)
            
            # MACD (12-26-9)
            # Simplified calculation using different weights
            weights_12 = np.array([0.15, 0.12, 0.1, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.01])[:min(12, len(closes))]
            weights_26 = np.array([0.08, 0.07, 0.07, 0.06, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03, 0.03, 0.02, 0.02, 0.01, 0.01] * 2)[:min(26, len(closes))]
            
            ema12 = np.sum(weights_12 * closes[:len(weights_12)]) / np.sum(weights_12)
            ema26 = np.sum(weights_26 * closes[:len(weights_26)]) / np.sum(weights_26)
            macd = ema12 - ema26
            
            # Stochastic Oscillator (14-day)
            if len(closes) >= 14 and len(highs) >= 14 and len(lows) >= 14:
                highest_high = np.max(highs[:14])
                lowest_low = np.min(lows[:14])
                stoch_k = 100 * (closes[0] - lowest_low) / (highest_high - lowest_low + 1e-8)
                
                # Simplified %D calculation (3-day SMA of %K)
                stoch_d = stoch_k  # Simplified for single timepoint
            else:
                stoch_k = 50
                stoch_d = 50
            
            features.extend([
                rsi / 100,            # RSI (normalized to 0-1)
                bb_width,             # Bollinger Band width
                bb_position,          # Position within Bollinger Bands
                macd / (closes[0] + 1e-8),  # MACD normalized by price
                stoch_k / 100         # Stochastic %K (normalized to 0-1)
            ])
        else:
            features.extend([0.5, 0, 0.5, 0, 0.5])  # Default values for technical indicators
        
        # Feature Group 7: Advanced SI Indicators
        # These are specialized indicators for short interest prediction
        
        # SI Pressure Index: Combines SI level, price momentum, and volume
        si_pressure = 0
        if len(closes) >= 5 and short_interest > 0:
            days_to_cover = short_interest / (avg_volume + 1e-8)
            price_momentum = returns_5d
            
            # SI pressure increases with high days to cover and positive price momentum
            si_pressure = days_to_cover * (1 + price_momentum)
            
            # Normalize using tanh to keep within reasonable range
            si_pressure = np.tanh(si_pressure / 10)
        
        # SI Reversal Signal: Detects potential reversals in short interest
        si_reversal = 0
        if len(si_history) >= 3 and len(closes) >= 3:
            # SI trend
            si_trend = si_history[0] - si_history[2]
            
            # Price trend opposite to SI trend could signal reversal
            price_trend = closes[0] - closes[2]
            
            # If SI is increasing but price is also increasing, potential reversal
            # If SI is decreasing but price is also decreasing, potential reversal
            if (si_trend > 0 and price_trend > 0) or (si_trend < 0 and price_trend < 0):
                si_reversal = np.sign(si_trend) * np.abs(price_trend) / (closes[2] + 1e-8)
                
                # Normalize
                si_reversal = np.tanh(si_reversal * 5)
        
        # SI Concentration: Ratio of SI to market cap (approximated by price)
        si_concentration = short_interest / (closes[0] + 1e-8) if len(closes) > 0 else 0
        
        # SI Efficiency: How effectively SI predicts price movements
        si_efficiency = 0
        if len(si_history) >= 2 and len(closes) >= 2:
            si_direction = np.sign(si_history[0] - si_history[1])
            price_direction = np.sign(closes[1] - closes[0])  # Inverse relationship expected
            
            # +1 if SI correctly predicts price (inverse relationship), -1 if wrong
            si_efficiency = -1 * si_direction * price_direction
        
        features.extend([
            si_pressure,              # SI Pressure Index
            si_reversal,              # SI Reversal Signal
            si_concentration,         # SI Concentration
            si_efficiency             # SI Efficiency
        ])
        
        # Add features for current timestamp to output
        engineered_features.append(features)
    
    # Convert to numpy array
    result = np.array(engineered_features)
    
    # Final safety check for NaN and inf values
    result = np.nan_to_num(result, nan=0.0, posinf=1e9, neginf=-1e9)
    
    # Normalize features to improve model training
    # This helps prevent features with large magnitudes from dominating
    for i in range(result.shape[1]):
        col = result[:, i]
        col_range = np.max(col) - np.min(col)
        if col_range > 0:
            # Scale to approximately [-1, 1] range while preserving 0
            max_abs = max(abs(np.max(col)), abs(np.min(col)))
            if max_abs > 0:
                result[:, i] = col / max_abs
    
    return result
```

Performance of this code: MAPE = 42.35%
Change from previous: -0.35%
Statistical Analysis: 26/32 features were significant (p < 0.05), 19 were highly significant (p < 0.01)

INSTRUCTIONS FOR NEW CODE:
- Analyze the previous code and understand what features it tried to create
- Identify what worked well and what didn't work based on performance and statistical significance
- If the previous code worked but had poor performance, try different feature engineering approaches
- Consider the statistical significance of features - focus on creating features that are likely to be statistically significant
- Your new code should be an improvement over the previous attempt
- Think about what additional financial insights or technical indicators could be valuable


CURRENT TASK (Iteration 10):
Your goal is to create an improved feature engineering function that will achieve better performance than the current best MAPE of 42.00%.

Based on the performance history, DL-based feature importance analysis, and previous code above, analyze what worked and what didn't, then create a new feature engineering approach that:
1. Learns from previous iterations' successes and failures
2. Analyzes the previous code to understand what features were attempted and their effectiveness
3. Builds upon successful feature patterns while avoiding problematic approaches
4. Considers financial domain knowledge (momentum, volatility, volume patterns, etc.)
5. Maintains LSTM-compatible time series structure
6. Uses DL-based feature importance insights to prioritize feature construction
7. Improves upon the previous iteration's approach

Requirements:
1. Write a function called `construct_features` that takes a numpy array of shape (lookback_window, 62) and returns a numpy array of shape (lookback_window, constructed_features)
2. The function should process each timestamp independently but maintain the temporal structure
3. Focus on the most predictive features for each time step, using DL-based feature importance as guidance
4. Consider financial domain knowledge (e.g., price momentum, volatility, volume patterns, etc.)
5. The output should be a 2D numpy array with shape (lookback_window, constructed_features)
6. Include comments explaining your feature engineering choices and how they address previous performance issues and importance insights
7. Make sure the code is production-ready and handles edge cases
8. DO NOT include any import statements - only use numpy (available as 'np') and built-in Python functions
9. The function must return a 2D array where each row represents features for one time step
10. Use numpy nan_to_num to handle NaN values
11. Analyze the previous iteration's code and explain in comments how your approach differs and improves upon it

Please provide ONLY the Python function code, no explanations outside the code comments.

Feature description: Stock short interest prediction with past short interest, volume, and OHLC data for iterative improvement
